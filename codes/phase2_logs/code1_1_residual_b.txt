Script started on 2024-10-24 20:49:13-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/31" COLUMNS="203" LINES="53"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> conda activate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python code1_1_residual_b[Kcustom_b.py
GPU 7: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
Iteration: 2 | Episodes: 100 | Median Reward: 18.79 | Max Reward: 28.79
Iteration: 4 | Episodes: 200 | Median Reward: 10.07 | Max Reward: 30.24
Iteration: 7 | Episodes: 300 | Median Reward: 18.21 | Max Reward: 30.24
Iteration: 9 | Episodes: 400 | Median Reward: 21.41 | Max Reward: 30.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -82         |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 10          |
|    time_elapsed         | 42          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.042502254 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.3         |
|    entropy_loss         | -95.5       |
|    explained_variance   | 0.0254      |
|    learning_rate        | 0.0005      |
|    loss                 | 127         |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.0166      |
|    std                  | 1           |
|    value_loss           | 282         |
-----------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 13.73 | Max Reward: 30.24
Iteration: 14 | Episodes: 600 | Median Reward: 11.82 | Max Reward: 32.26
Iteration: 17 | Episodes: 700 | Median Reward: 13.26 | Max Reward: 32.54
Iteration: 19 | Episodes: 800 | Median Reward: 10.82 | Max Reward: 32.54
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -88        |
| time/                   |            |
|    fps                  | 957        |
|    iterations           | 20         |
|    time_elapsed         | 85         |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.02392143 |
|    clip_fraction        | 0.0819     |
|    clip_range           | 0.3        |
|    entropy_loss         | -94.7      |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.7        |
|    n_updates            | 190        |
|    policy_gradient_loss | 0.0204     |
|    std                  | 1.01       |
|    value_loss           | 30.9       |
----------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 10.78 | Max Reward: 32.54
Iteration: 24 | Episodes: 1000 | Median Reward: 10.79 | Max Reward: 32.54
Iteration: 27 | Episodes: 1100 | Median Reward: 10.90 | Max Reward: 32.54
Iteration: 29 | Episodes: 1200 | Median Reward: 10.27 | Max Reward: 32.54
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -85.2     |
| time/                   |           |
|    fps                  | 958       |
|    iterations           | 30        |
|    time_elapsed         | 128       |
|    total_timesteps      | 122880    |
| train/                  |           |
|    approx_kl            | 0.0460852 |
|    clip_fraction        | 0.163     |
|    clip_range           | 0.3       |
|    entropy_loss         | -95.8     |
|    explained_variance   | 0.967     |
|    learning_rate        | 0.0005    |
|    loss                 | -4.12     |
|    n_updates            | 290       |
|    policy_gradient_loss | 0.0379    |
|    std                  | 1.01      |
|    value_loss           | 13.3      |
---------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 18.06 | Max Reward: 39.66
Iteration: 34 | Episodes: 1400 | Median Reward: 3.94 | Max Reward: 39.66
Iteration: 36 | Episodes: 1500 | Median Reward: 11.34 | Max Reward: 39.66
Iteration: 39 | Episodes: 1600 | Median Reward: 17.27 | Max Reward: 39.66
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -86.9      |
| time/                   |            |
|    fps                  | 959        |
|    iterations           | 40         |
|    time_elapsed         | 170        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.05230557 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.3        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.968      |
|    learning_rate        | 0.0005     |
|    loss                 | -8.55      |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.00706   |
|    std                  | 1.01       |
|    value_loss           | 7.89       |
----------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 12.90 | Max Reward: 39.66
Iteration: 44 | Episodes: 1800 | Median Reward: 18.60 | Max Reward: 39.66
Iteration: 46 | Episodes: 1900 | Median Reward: 8.48 | Max Reward: 39.66
Iteration: 49 | Episodes: 2000 | Median Reward: 13.69 | Max Reward: 39.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -85.9       |
| time/                   |             |
|    fps                  | 959         |
|    iterations           | 50          |
|    time_elapsed         | 213         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.016582506 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -122        |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0005      |
|    loss                 | -8.86       |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00533    |
|    std                  | 1.01        |
|    value_loss           | 9.74        |
-----------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 21.15 | Max Reward: 39.66
Iteration: 54 | Episodes: 2200 | Median Reward: 12.60 | Max Reward: 39.66
Iteration: 56 | Episodes: 2300 | Median Reward: 19.05 | Max Reward: 39.66
Iteration: 59 | Episodes: 2400 | Median Reward: 24.63 | Max Reward: 39.66
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -84        |
| time/                   |            |
|    fps                  | 958        |
|    iterations           | 60         |
|    time_elapsed         | 256        |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.13903797 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.3        |
|    entropy_loss         | -131       |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.0005     |
|    loss                 | -9.03      |
|    n_updates            | 590        |
|    policy_gradient_loss | 0.0203     |
|    std                  | 1.01       |
|    value_loss           | 10.9       |
----------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 21.41 | Max Reward: 39.66
Iteration: 64 | Episodes: 2600 | Median Reward: 5.74 | Max Reward: 39.66
Iteration: 66 | Episodes: 2700 | Median Reward: 13.55 | Max Reward: 39.66
Iteration: 69 | Episodes: 2800 | Median Reward: 15.55 | Max Reward: 39.66
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -82.9       |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 70          |
|    time_elapsed         | 299         |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.052324407 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.3         |
|    entropy_loss         | -136        |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0005      |
|    loss                 | -12         |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.029      |
|    std                  | 1.01        |
|    value_loss           | 5.39        |
-----------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 19.32 | Max Reward: 39.66
Iteration: 73 | Episodes: 3000 | Median Reward: 19.72 | Max Reward: 39.66
Iteration: 76 | Episodes: 3100 | Median Reward: 14.98 | Max Reward: 39.66
Iteration: 78 | Episodes: 3200 | Median Reward: 21.54 | Max Reward: 40.82
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.7     |
| time/                   |           |
|    fps                  | 957       |
|    iterations           | 80        |
|    time_elapsed         | 342       |
|    total_timesteps      | 327680    |
| train/                  |           |
|    approx_kl            | 0.1847885 |
|    clip_fraction        | 0.3       |
|    clip_range           | 0.3       |
|    entropy_loss         | -143      |
|    explained_variance   | 0.989     |
|    learning_rate        | 0.0005    |
|    loss                 | -12.7     |
|    n_updates            | 790       |
|    policy_gradient_loss | -0.0599   |
|    std                  | 1.01      |
|    value_loss           | 5.33      |
---------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 23.80 | Max Reward: 40.82
Iteration: 83 | Episodes: 3400 | Median Reward: 20.54 | Max Reward: 40.82
Iteration: 86 | Episodes: 3500 | Median Reward: 23.91 | Max Reward: 40.82
Iteration: 88 | Episodes: 3600 | Median Reward: 25.22 | Max Reward: 40.82
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -72.8      |
| time/                   |            |
|    fps                  | 959        |
|    iterations           | 90         |
|    time_elapsed         | 384        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.03357085 |
|    clip_fraction        | 0.0752     |
|    clip_range           | 0.3        |
|    entropy_loss         | -150       |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0005     |
|    loss                 | -13.8      |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0286    |
|    std                  | 1.02       |
|    value_loss           | 3.07       |
----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 25.12 | Max Reward: 40.82
Iteration: 93 | Episodes: 3800 | Median Reward: 24.34 | Max Reward: 40.82
Iteration: 96 | Episodes: 3900 | Median Reward: 32.49 | Max Reward: 40.82
Iteration: 98 | Episodes: 4000 | Median Reward: 20.13 | Max Reward: 40.82
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.5        |
| time/                   |              |
|    fps                  | 959          |
|    iterations           | 100          |
|    time_elapsed         | 426          |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.0027722784 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -156         |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.7        |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00275     |
|    std                  | 1.02         |
|    value_loss           | 6.89         |
------------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 30.60 | Max Reward: 40.82
Iteration: 103 | Episodes: 4200 | Median Reward: 25.39 | Max Reward: 40.82
Iteration: 106 | Episodes: 4300 | Median Reward: 25.97 | Max Reward: 40.82
Iteration: 108 | Episodes: 4400 | Median Reward: 25.17 | Max Reward: 40.82
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.5       |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 110         |
|    time_elapsed         | 470         |
|    total_timesteps      | 450560      |
| train/                  |             |
|    approx_kl            | 0.015852118 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.3         |
|    entropy_loss         | -158        |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0005      |
|    loss                 | -13.8       |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0244     |
|    std                  | 1.02        |
|    value_loss           | 5.45        |
-----------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 24.52 | Max Reward: 40.82
Iteration: 113 | Episodes: 4600 | Median Reward: 27.63 | Max Reward: 40.82
Iteration: 115 | Episodes: 4700 | Median Reward: 28.56 | Max Reward: 41.96
Iteration: 118 | Episodes: 4800 | Median Reward: 27.59 | Max Reward: 41.96
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -76.2      |
| time/                   |            |
|    fps                  | 957        |
|    iterations           | 120        |
|    time_elapsed         | 513        |
|    total_timesteps      | 491520     |
| train/                  |            |
|    approx_kl            | 0.07820994 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.3        |
|    entropy_loss         | -163       |
|    explained_variance   | 0.986      |
|    learning_rate        | 0.0005     |
|    loss                 | -15.2      |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.000187  |
|    std                  | 1.02       |
|    value_loss           | 5.94       |
----------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 27.55 | Max Reward: 41.96
Iteration: 123 | Episodes: 5000 | Median Reward: 27.50 | Max Reward: 41.96
Iteration: 125 | Episodes: 5100 | Median Reward: 24.38 | Max Reward: 41.96
Iteration: 128 | Episodes: 5200 | Median Reward: 23.48 | Max Reward: 41.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -79.5       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 130         |
|    time_elapsed         | 556         |
|    total_timesteps      | 532480      |
| train/                  |             |
|    approx_kl            | 0.031953245 |
|    clip_fraction        | 0.0551      |
|    clip_range           | 0.3         |
|    entropy_loss         | -168        |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0005      |
|    loss                 | -1.71       |
|    n_updates            | 1290        |
|    policy_gradient_loss | 0.0053      |
|    std                  | 1.02        |
|    value_loss           | 23.6        |
-----------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 17.88 | Max Reward: 41.96
Iteration: 133 | Episodes: 5400 | Median Reward: 25.07 | Max Reward: 41.96
Iteration: 135 | Episodes: 5500 | Median Reward: 24.96 | Max Reward: 41.96
Iteration: 138 | Episodes: 5600 | Median Reward: 30.75 | Max Reward: 41.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.9       |
| time/                   |             |
|    fps                  | 956         |
|    iterations           | 140         |
|    time_elapsed         | 599         |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.009424033 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -171        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0005      |
|    loss                 | -16         |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0119     |
|    std                  | 1.03        |
|    value_loss           | 4.62        |
-----------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 25.00 | Max Reward: 41.96
Iteration: 143 | Episodes: 5800 | Median Reward: 27.24 | Max Reward: 41.96
Iteration: 145 | Episodes: 5900 | Median Reward: 24.33 | Max Reward: 41.96
Iteration: 147 | Episodes: 6000 | Median Reward: 32.29 | Max Reward: 41.96
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -68        |
| time/                   |            |
|    fps                  | 956        |
|    iterations           | 150        |
|    time_elapsed         | 642        |
|    total_timesteps      | 614400     |
| train/                  |            |
|    approx_kl            | 0.01368235 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -172       |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0005     |
|    loss                 | -14.5      |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0206    |
|    std                  | 1.03       |
|    value_loss           | 4.59       |
----------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 31.26 | Max Reward: 41.96
Iteration: 152 | Episodes: 6200 | Median Reward: 32.03 | Max Reward: 44.44
Iteration: 155 | Episodes: 6300 | Median Reward: 32.22 | Max Reward: 44.44
Iteration: 157 | Episodes: 6400 | Median Reward: 30.93 | Max Reward: 44.44
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72.2        |
| time/                   |              |
|    fps                  | 956          |
|    iterations           | 160          |
|    time_elapsed         | 685          |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 0.0011899422 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -175         |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0005       |
|    loss                 | -16.3        |
|    n_updates            | 1590         |
|    policy_gradient_loss | -4.61e-05    |
|    std                  | 1.03         |
|    value_loss           | 6.39         |
------------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 27.09 | Max Reward: 44.44
Iteration: 162 | Episodes: 6600 | Median Reward: 36.42 | Max Reward: 44.73
Iteration: 165 | Episodes: 6700 | Median Reward: 34.91 | Max Reward: 44.73
Iteration: 167 | Episodes: 6800 | Median Reward: 32.12 | Max Reward: 44.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.6       |
| time/                   |             |
|    fps                  | 955         |
|    iterations           | 170         |
|    time_elapsed         | 728         |
|    total_timesteps      | 696320      |
| train/                  |             |
|    approx_kl            | 0.011324159 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -178        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0005      |
|    loss                 | -15.7       |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.012      |
|    std                  | 1.03        |
|    value_loss           | 3.83        |
-----------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 32.39 | Max Reward: 44.73
Iteration: 172 | Episodes: 7000 | Median Reward: 31.03 | Max Reward: 44.73
Iteration: 175 | Episodes: 7100 | Median Reward: 35.08 | Max Reward: 44.73
Iteration: 177 | Episodes: 7200 | Median Reward: 36.08 | Max Reward: 44.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.9       |
| time/                   |             |
|    fps                  | 955         |
|    iterations           | 180         |
|    time_elapsed         | 771         |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.050913546 |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.3         |
|    entropy_loss         | -184        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0005      |
|    loss                 | -16.2       |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0182     |
|    std                  | 1.04        |
|    value_loss           | 4.02        |
-----------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 34.14 | Max Reward: 44.73
Iteration: 182 | Episodes: 7400 | Median Reward: 35.74 | Max Reward: 44.73
Iteration: 184 | Episodes: 7500 | Median Reward: 34.97 | Max Reward: 44.73
Iteration: 187 | Episodes: 7600 | Median Reward: 36.12 | Max Reward: 44.79
Iteration: 189 | Episodes: 7700 | Median Reward: 33.80 | Max Reward: 44.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.6       |
| time/                   |             |
|    fps                  | 956         |
|    iterations           | 190         |
|    time_elapsed         | 813         |
|    total_timesteps      | 778240      |
| train/                  |             |
|    approx_kl            | 0.010853327 |
|    clip_fraction        | 0.000488    |
|    clip_range           | 0.3         |
|    entropy_loss         | -189        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -18.2       |
|    n_updates            | 1890        |
|    policy_gradient_loss | 0.00408     |
|    std                  | 1.04        |
|    value_loss           | 2.75        |
-----------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 33.00 | Max Reward: 44.79
Iteration: 194 | Episodes: 7900 | Median Reward: 33.68 | Max Reward: 44.79
Iteration: 197 | Episodes: 8000 | Median Reward: 32.74 | Max Reward: 44.79
Iteration: 199 | Episodes: 8100 | Median Reward: 30.20 | Max Reward: 44.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.7       |
| time/                   |             |
|    fps                  | 956         |
|    iterations           | 200         |
|    time_elapsed         | 856         |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.052236255 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.3         |
|    entropy_loss         | -192        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -18.4       |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.0727     |
|    std                  | 1.04        |
|    value_loss           | 3.08        |
-----------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 33.66 | Max Reward: 44.79
Iteration: 204 | Episodes: 8300 | Median Reward: 35.94 | Max Reward: 44.79
Iteration: 207 | Episodes: 8400 | Median Reward: 33.03 | Max Reward: 44.79
Iteration: 209 | Episodes: 8500 | Median Reward: 35.89 | Max Reward: 44.79
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -66        |
| time/                   |            |
|    fps                  | 956        |
|    iterations           | 210        |
|    time_elapsed         | 899        |
|    total_timesteps      | 860160     |
| train/                  |            |
|    approx_kl            | 0.01584984 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -195       |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0005     |
|    loss                 | -16.6      |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.0256    |
|    std                  | 1.04       |
|    value_loss           | 7.44       |
----------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 36.40 | Max Reward: 44.79
Iteration: 214 | Episodes: 8700 | Median Reward: 33.94 | Max Reward: 44.79
Iteration: 216 | Episodes: 8800 | Median Reward: 38.33 | Max Reward: 44.79
Iteration: 219 | Episodes: 8900 | Median Reward: 35.62 | Max Reward: 44.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.8        |
| time/                   |              |
|    fps                  | 956          |
|    iterations           | 220          |
|    time_elapsed         | 941          |
|    total_timesteps      | 901120       |
| train/                  |              |
|    approx_kl            | 0.0033765153 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -197         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.08        |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00246     |
|    std                  | 1.05         |
|    value_loss           | 19.9         |
------------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 36.68 | Max Reward: 44.79
Iteration: 224 | Episodes: 9100 | Median Reward: 37.39 | Max Reward: 44.79
Iteration: 226 | Episodes: 9200 | Median Reward: 35.76 | Max Reward: 44.79
Iteration: 229 | Episodes: 9300 | Median Reward: 35.85 | Max Reward: 44.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.6       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 230         |
|    time_elapsed         | 983         |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.008348092 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -199        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -16.8       |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00736    |
|    std                  | 1.05        |
|    value_loss           | 3.45        |
-----------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 39.25 | Max Reward: 44.79
Iteration: 234 | Episodes: 9500 | Median Reward: 38.61 | Max Reward: 44.79
Iteration: 236 | Episodes: 9600 | Median Reward: 39.02 | Max Reward: 44.79
Iteration: 239 | Episodes: 9700 | Median Reward: 39.02 | Max Reward: 44.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.9        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 240          |
|    time_elapsed         | 1026         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0026142034 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -200         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -18.7        |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 1.05         |
|    value_loss           | 7.53         |
------------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 40.69 | Max Reward: 44.79
Iteration: 244 | Episodes: 9900 | Median Reward: 37.64 | Max Reward: 44.79
Iteration: 246 | Episodes: 10000 | Median Reward: 39.34 | Max Reward: 44.81
Iteration: 249 | Episodes: 10100 | Median Reward: 39.76 | Max Reward: 44.81
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.9        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 250          |
|    time_elapsed         | 1068         |
|    total_timesteps      | 1024000      |
| train/                  |              |
|    approx_kl            | 0.0048846463 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -203         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -18.8        |
|    n_updates            | 2490         |
|    policy_gradient_loss | -0.00514     |
|    std                  | 1.05         |
|    value_loss           | 10.9         |
------------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 36.31 | Max Reward: 44.81
Iteration: 253 | Episodes: 10300 | Median Reward: 37.17 | Max Reward: 44.81
Iteration: 256 | Episodes: 10400 | Median Reward: 36.58 | Max Reward: 44.81
Iteration: 258 | Episodes: 10500 | Median Reward: 34.46 | Max Reward: 44.81
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.1       |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 260         |
|    time_elapsed         | 1111        |
|    total_timesteps      | 1064960     |
| train/                  |             |
|    approx_kl            | 0.002965865 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -204        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -19.8       |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.00475    |
|    std                  | 1.05        |
|    value_loss           | 2.05        |
-----------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 33.23 | Max Reward: 44.81
Iteration: 263 | Episodes: 10700 | Median Reward: 37.62 | Max Reward: 44.81
Iteration: 266 | Episodes: 10800 | Median Reward: 35.80 | Max Reward: 44.81
Iteration: 268 | Episodes: 10900 | Median Reward: 36.12 | Max Reward: 44.81
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -63.3      |
| time/                   |            |
|    fps                  | 958        |
|    iterations           | 270        |
|    time_elapsed         | 1153       |
|    total_timesteps      | 1105920    |
| train/                  |            |
|    approx_kl            | 0.02889839 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.3        |
|    entropy_loss         | -206       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0005     |
|    loss                 | -16.1      |
|    n_updates            | 2690       |
|    policy_gradient_loss | -0.0254    |
|    std                  | 1.06       |
|    value_loss           | 4.83       |
----------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 36.54 | Max Reward: 44.81
Iteration: 273 | Episodes: 11100 | Median Reward: 37.87 | Max Reward: 44.81
Iteration: 276 | Episodes: 11200 | Median Reward: 39.33 | Max Reward: 44.81
Iteration: 278 | Episodes: 11300 | Median Reward: 38.35 | Max Reward: 44.81
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.5        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 280          |
|    time_elapsed         | 1196         |
|    total_timesteps      | 1146880      |
| train/                  |              |
|    approx_kl            | 0.0012309386 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -207         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.1        |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.00435     |
|    std                  | 1.06         |
|    value_loss           | 4.72         |
------------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 38.95 | Max Reward: 44.81
Iteration: 283 | Episodes: 11500 | Median Reward: 36.63 | Max Reward: 44.81
Iteration: 286 | Episodes: 11600 | Median Reward: 40.63 | Max Reward: 44.81
Iteration: 288 | Episodes: 11700 | Median Reward: 39.78 | Max Reward: 45.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.9       |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 290         |
|    time_elapsed         | 1239        |
|    total_timesteps      | 1187840     |
| train/                  |             |
|    approx_kl            | 0.004198966 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -208        |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0005      |
|    loss                 | -19.2       |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00595    |
|    std                  | 1.06        |
|    value_loss           | 4.09        |
-----------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 40.55 | Max Reward: 45.25
Iteration: 293 | Episodes: 11900 | Median Reward: 37.13 | Max Reward: 45.25
Iteration: 295 | Episodes: 12000 | Median Reward: 37.79 | Max Reward: 45.25
Iteration: 298 | Episodes: 12100 | Median Reward: 39.70 | Max Reward: 45.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.9       |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 300         |
|    time_elapsed         | 1281        |
|    total_timesteps      | 1228800     |
| train/                  |             |
|    approx_kl            | 0.030045595 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -210        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -20.8       |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.0454     |
|    std                  | 1.06        |
|    value_loss           | 1.27        |
-----------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 36.35 | Max Reward: 45.25
Iteration: 303 | Episodes: 12300 | Median Reward: 40.10 | Max Reward: 45.25
Iteration: 305 | Episodes: 12400 | Median Reward: 40.10 | Max Reward: 45.25
Iteration: 308 | Episodes: 12500 | Median Reward: 41.25 | Max Reward: 45.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.8       |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 310         |
|    time_elapsed         | 1324        |
|    total_timesteps      | 1269760     |
| train/                  |             |
|    approx_kl            | 0.027231619 |
|    clip_fraction        | 0.0502      |
|    clip_range           | 0.3         |
|    entropy_loss         | -212        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -20.5       |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.0301     |
|    std                  | 1.07        |
|    value_loss           | 0.938       |
-----------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: 42.32 | Max Reward: 45.25
Iteration: 313 | Episodes: 12700 | Median Reward: 40.48 | Max Reward: 45.25
Iteration: 315 | Episodes: 12800 | Median Reward: 38.57 | Max Reward: 45.25
Iteration: 318 | Episodes: 12900 | Median Reward: 42.64 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.4        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 320          |
|    time_elapsed         | 1367         |
|    total_timesteps      | 1310720      |
| train/                  |              |
|    approx_kl            | 0.0006677492 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -215         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.8        |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.000329    |
|    std                  | 1.07         |
|    value_loss           | 1.31         |
------------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: 39.81 | Max Reward: 45.95
Iteration: 323 | Episodes: 13100 | Median Reward: 42.69 | Max Reward: 45.95
Iteration: 325 | Episodes: 13200 | Median Reward: 37.12 | Max Reward: 45.95
Iteration: 327 | Episodes: 13300 | Median Reward: 39.65 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.3        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 330          |
|    time_elapsed         | 1410         |
|    total_timesteps      | 1351680      |
| train/                  |              |
|    approx_kl            | 0.0009042557 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -216         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0005       |
|    loss                 | -19.4        |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.00139     |
|    std                  | 1.07         |
|    value_loss           | 4.45         |
------------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: 41.37 | Max Reward: 45.95
Iteration: 332 | Episodes: 13500 | Median Reward: 40.91 | Max Reward: 45.95
Iteration: 335 | Episodes: 13600 | Median Reward: 41.07 | Max Reward: 45.95
Iteration: 337 | Episodes: 13700 | Median Reward: 40.65 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60         |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 340         |
|    time_elapsed         | 1452        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.003987624 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -218        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.4       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00512    |
|    std                  | 1.08        |
|    value_loss           | 0.796       |
-----------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: 40.88 | Max Reward: 45.95
Iteration: 342 | Episodes: 13900 | Median Reward: 43.19 | Max Reward: 45.95
Iteration: 345 | Episodes: 14000 | Median Reward: 40.69 | Max Reward: 45.95
Iteration: 347 | Episodes: 14100 | Median Reward: 40.63 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.3        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 350          |
|    time_elapsed         | 1494         |
|    total_timesteps      | 1433600      |
| train/                  |              |
|    approx_kl            | 0.0035949177 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -220         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -20          |
|    n_updates            | 3490         |
|    policy_gradient_loss | -0.00333     |
|    std                  | 1.08         |
|    value_loss           | 4.47         |
------------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: 41.11 | Max Reward: 45.95
Iteration: 352 | Episodes: 14300 | Median Reward: 40.14 | Max Reward: 45.95
Iteration: 355 | Episodes: 14400 | Median Reward: 40.14 | Max Reward: 45.95
Iteration: 357 | Episodes: 14500 | Median Reward: 41.07 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.5       |
| time/                   |             |
|    fps                  | 959         |
|    iterations           | 360         |
|    time_elapsed         | 1536        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.021120857 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.3         |
|    entropy_loss         | -221        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -14.9       |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.0243     |
|    std                  | 1.08        |
|    value_loss           | 3.7         |
-----------------------------------------
Iteration: 360 | Episodes: 14600 | Median Reward: 37.14 | Max Reward: 45.95
Iteration: 362 | Episodes: 14700 | Median Reward: 40.77 | Max Reward: 45.95
Iteration: 364 | Episodes: 14800 | Median Reward: 39.91 | Max Reward: 45.95
Iteration: 367 | Episodes: 14900 | Median Reward: 40.80 | Max Reward: 45.95
Iteration: 369 | Episodes: 15000 | Median Reward: 42.67 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.7         |
| time/                   |               |
|    fps                  | 959           |
|    iterations           | 370           |
|    time_elapsed         | 1579          |
|    total_timesteps      | 1515520       |
| train/                  |               |
|    approx_kl            | 0.00045948796 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -222          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -16.7         |
|    n_updates            | 3690          |
|    policy_gradient_loss | -0.00037      |
|    std                  | 1.09          |
|    value_loss           | 3.88          |
-------------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: 39.78 | Max Reward: 45.95
Iteration: 374 | Episodes: 15200 | Median Reward: 41.28 | Max Reward: 45.95
Iteration: 377 | Episodes: 15300 | Median Reward: 40.14 | Max Reward: 45.95
Iteration: 379 | Episodes: 15400 | Median Reward: 41.25 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.6       |
| time/                   |             |
|    fps                  | 959         |
|    iterations           | 380         |
|    time_elapsed         | 1622        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.052171577 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.3         |
|    entropy_loss         | -223        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -22         |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.0437     |
|    std                  | 1.09        |
|    value_loss           | 1.97        |
-----------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: 42.69 | Max Reward: 45.95
Iteration: 384 | Episodes: 15600 | Median Reward: 43.19 | Max Reward: 45.95
Iteration: 387 | Episodes: 15700 | Median Reward: 41.17 | Max Reward: 45.95
Iteration: 389 | Episodes: 15800 | Median Reward: 42.87 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.9       |
| time/                   |             |
|    fps                  | 959         |
|    iterations           | 390         |
|    time_elapsed         | 1664        |
|    total_timesteps      | 1597440     |
| train/                  |             |
|    approx_kl            | 0.037519164 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.3         |
|    entropy_loss         | -224        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.7       |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.0615     |
|    std                  | 1.09        |
|    value_loss           | 2.9         |
-----------------------------------------
Iteration: 392 | Episodes: 15900 | Median Reward: 42.62 | Max Reward: 45.95
Iteration: 394 | Episodes: 16000 | Median Reward: 40.15 | Max Reward: 45.95
Iteration: 396 | Episodes: 16100 | Median Reward: 40.86 | Max Reward: 45.95
Iteration: 399 | Episodes: 16200 | Median Reward: 41.71 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.4       |
| time/                   |             |
|    fps                  | 959         |
|    iterations           | 400         |
|    time_elapsed         | 1707        |
|    total_timesteps      | 1638400     |
| train/                  |             |
|    approx_kl            | 0.008138256 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -226        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.6       |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.0108     |
|    std                  | 1.1         |
|    value_loss           | 15.6        |
-----------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: 40.84 | Max Reward: 45.95
Iteration: 404 | Episodes: 16400 | Median Reward: 41.21 | Max Reward: 45.95
Iteration: 406 | Episodes: 16500 | Median Reward: 43.87 | Max Reward: 45.95
Iteration: 409 | Episodes: 16600 | Median Reward: 40.63 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.1       |
| time/                   |             |
|    fps                  | 959         |
|    iterations           | 410         |
|    time_elapsed         | 1750        |
|    total_timesteps      | 1679360     |
| train/                  |             |
|    approx_kl            | 0.004520223 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -227        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.8       |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.0114     |
|    std                  | 1.1         |
|    value_loss           | 1.78        |
-----------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: 43.43 | Max Reward: 45.95
Iteration: 414 | Episodes: 16800 | Median Reward: 40.85 | Max Reward: 45.95
Iteration: 416 | Episodes: 16900 | Median Reward: 43.82 | Max Reward: 45.95
Iteration: 419 | Episodes: 17000 | Median Reward: 42.99 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.6       |
| time/                   |             |
|    fps                  | 959         |
|    iterations           | 420         |
|    time_elapsed         | 1793        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.009798152 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -228        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.4       |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.0246     |
|    std                  | 1.1         |
|    value_loss           | 3.38        |
-----------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: 43.58 | Max Reward: 45.95
Iteration: 424 | Episodes: 17200 | Median Reward: 41.13 | Max Reward: 45.95
Iteration: 426 | Episodes: 17300 | Median Reward: 41.13 | Max Reward: 45.95
Iteration: 429 | Episodes: 17400 | Median Reward: 41.68 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.6       |
| time/                   |             |
|    fps                  | 959         |
|    iterations           | 430         |
|    time_elapsed         | 1836        |
|    total_timesteps      | 1761280     |
| train/                  |             |
|    approx_kl            | 0.000331007 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -229        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -21         |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.000313   |
|    std                  | 1.11        |
|    value_loss           | 1.97        |
-----------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: 40.54 | Max Reward: 45.95
Iteration: 433 | Episodes: 17600 | Median Reward: 41.85 | Max Reward: 45.95
Iteration: 436 | Episodes: 17700 | Median Reward: 43.26 | Max Reward: 45.95
Iteration: 438 | Episodes: 17800 | Median Reward: 41.18 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.8       |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 440         |
|    time_elapsed         | 1879        |
|    total_timesteps      | 1802240     |
| train/                  |             |
|    approx_kl            | 0.038687143 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.3         |
|    entropy_loss         | -230        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.9       |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0244     |
|    std                  | 1.11        |
|    value_loss           | 0.613       |
-----------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 42.38 | Max Reward: 45.95
Iteration: 443 | Episodes: 18000 | Median Reward: 43.74 | Max Reward: 45.95
Iteration: 446 | Episodes: 18100 | Median Reward: 41.29 | Max Reward: 45.95
Iteration: 448 | Episodes: 18200 | Median Reward: 41.13 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57         |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 450         |
|    time_elapsed         | 1922        |
|    total_timesteps      | 1843200     |
| train/                  |             |
|    approx_kl            | 0.055314843 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.3         |
|    entropy_loss         | -231        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -22         |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.0218     |
|    std                  | 1.11        |
|    value_loss           | 3.42        |
-----------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 44.14 | Max Reward: 45.95
Iteration: 453 | Episodes: 18400 | Median Reward: 40.70 | Max Reward: 45.95
Iteration: 456 | Episodes: 18500 | Median Reward: 40.81 | Max Reward: 45.95
Iteration: 458 | Episodes: 18600 | Median Reward: 40.98 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 460          |
|    time_elapsed         | 1966         |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 0.0023401314 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -232         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -22.1        |
|    n_updates            | 4590         |
|    policy_gradient_loss | 0.000783     |
|    std                  | 1.12         |
|    value_loss           | 6.28         |
------------------------------------------
Iteration: 461 | Episodes: 18700 | Median Reward: 43.27 | Max Reward: 45.95
Iteration: 463 | Episodes: 18800 | Median Reward: 43.21 | Max Reward: 45.95
Iteration: 466 | Episodes: 18900 | Median Reward: 41.64 | Max Reward: 45.95
Iteration: 468 | Episodes: 19000 | Median Reward: 40.69 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.3       |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 470         |
|    time_elapsed         | 2008        |
|    total_timesteps      | 1925120     |
| train/                  |             |
|    approx_kl            | 0.057152133 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.3         |
|    entropy_loss         | -233        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.6       |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.0354     |
|    std                  | 1.12        |
|    value_loss           | 1.13        |
-----------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 43.19 | Max Reward: 45.95
Iteration: 473 | Episodes: 19200 | Median Reward: 42.65 | Max Reward: 45.95
Iteration: 475 | Episodes: 19300 | Median Reward: 43.85 | Max Reward: 45.95
Iteration: 478 | Episodes: 19400 | Median Reward: 43.89 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 480          |
|    time_elapsed         | 2050         |
|    total_timesteps      | 1966080      |
| train/                  |              |
|    approx_kl            | 0.0007387978 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -234         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.2        |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.000166    |
|    std                  | 1.13         |
|    value_loss           | 3.12         |
------------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 43.89 | Max Reward: 45.95
Iteration: 483 | Episodes: 19600 | Median Reward: 41.35 | Max Reward: 45.95
Iteration: 485 | Episodes: 19700 | Median Reward: 40.85 | Max Reward: 45.95
Iteration: 488 | Episodes: 19800 | Median Reward: 43.85 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.2         |
| time/                   |               |
|    fps                  | 958           |
|    iterations           | 490           |
|    time_elapsed         | 2093          |
|    total_timesteps      | 2007040       |
| train/                  |               |
|    approx_kl            | 0.00012938578 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -235          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -22.1         |
|    n_updates            | 4890          |
|    policy_gradient_loss | 7.49e-05      |
|    std                  | 1.13          |
|    value_loss           | 3.83          |
-------------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 43.94 | Max Reward: 45.95
Iteration: 493 | Episodes: 20000 | Median Reward: 43.21 | Max Reward: 45.95
Iteration: 495 | Episodes: 20100 | Median Reward: 43.17 | Max Reward: 45.95
Iteration: 498 | Episodes: 20200 | Median Reward: 41.13 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 500          |
|    time_elapsed         | 2136         |
|    total_timesteps      | 2048000      |
| train/                  |              |
|    approx_kl            | 0.0036087206 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -235         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -21.4        |
|    n_updates            | 4990         |
|    policy_gradient_loss | -0.00281     |
|    std                  | 1.13         |
|    value_loss           | 1.34         |
------------------------------------------
Iteration: 500 | Episodes: 20300 | Median Reward: 43.84 | Max Reward: 45.95
Iteration: 503 | Episodes: 20400 | Median Reward: 43.81 | Max Reward: 45.95
Iteration: 505 | Episodes: 20500 | Median Reward: 43.79 | Max Reward: 45.95
Iteration: 507 | Episodes: 20600 | Median Reward: 43.36 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.3         |
| time/                   |               |
|    fps                  | 958           |
|    iterations           | 510           |
|    time_elapsed         | 2178          |
|    total_timesteps      | 2088960       |
| train/                  |               |
|    approx_kl            | 0.00040984125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -237          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -23.4         |
|    n_updates            | 5090          |
|    policy_gradient_loss | -0.00137      |
|    std                  | 1.14          |
|    value_loss           | 1.53          |
-------------------------------------------
Iteration: 510 | Episodes: 20700 | Median Reward: 43.53 | Max Reward: 45.95
Iteration: 512 | Episodes: 20800 | Median Reward: 42.97 | Max Reward: 45.95
Iteration: 515 | Episodes: 20900 | Median Reward: 41.37 | Max Reward: 45.95
Iteration: 517 | Episodes: 21000 | Median Reward: 43.43 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.3       |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 520         |
|    time_elapsed         | 2222        |
|    total_timesteps      | 2129920     |
| train/                  |             |
|    approx_kl            | 0.001372946 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -238        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.3       |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.00184    |
|    std                  | 1.14        |
|    value_loss           | 1.75        |
-----------------------------------------
Iteration: 520 | Episodes: 21100 | Median Reward: 41.28 | Max Reward: 45.95
Iteration: 522 | Episodes: 21200 | Median Reward: 43.84 | Max Reward: 45.95
Iteration: 525 | Episodes: 21300 | Median Reward: 43.86 | Max Reward: 45.95
Iteration: 527 | Episodes: 21400 | Median Reward: 44.15 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.4         |
| time/                   |               |
|    fps                  | 958           |
|    iterations           | 530           |
|    time_elapsed         | 2265          |
|    total_timesteps      | 2170880       |
| train/                  |               |
|    approx_kl            | 0.00016588847 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -238          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -22.6         |
|    n_updates            | 5290          |
|    policy_gradient_loss | -0.00054      |
|    std                  | 1.15          |
|    value_loss           | 5.22          |
-------------------------------------------
Iteration: 530 | Episodes: 21500 | Median Reward: 43.87 | Max Reward: 45.95
Iteration: 532 | Episodes: 21600 | Median Reward: 43.91 | Max Reward: 45.95
Iteration: 535 | Episodes: 21700 | Median Reward: 43.82 | Max Reward: 45.95
Iteration: 537 | Episodes: 21800 | Median Reward: 44.22 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58         |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 540         |
|    time_elapsed         | 2308        |
|    total_timesteps      | 2211840     |
| train/                  |             |
|    approx_kl            | 0.024685934 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.3         |
|    entropy_loss         | -239        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.3       |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0265     |
|    std                  | 1.15        |
|    value_loss           | 1.3         |
-----------------------------------------
Iteration: 540 | Episodes: 21900 | Median Reward: 43.75 | Max Reward: 45.95
Iteration: 542 | Episodes: 22000 | Median Reward: 44.15 | Max Reward: 45.95
Iteration: 544 | Episodes: 22100 | Median Reward: 43.14 | Max Reward: 45.95
Iteration: 547 | Episodes: 22200 | Median Reward: 40.77 | Max Reward: 45.95
Iteration: 549 | Episodes: 22300 | Median Reward: 44.07 | Max Reward: 45.95
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -57.6      |
| time/                   |            |
|    fps                  | 958        |
|    iterations           | 550        |
|    time_elapsed         | 2351       |
|    total_timesteps      | 2252800    |
| train/                  |            |
|    approx_kl            | 0.03721856 |
|    clip_fraction        | 0.075      |
|    clip_range           | 0.3        |
|    entropy_loss         | -240       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -23.7      |
|    n_updates            | 5490       |
|    policy_gradient_loss | -0.0236    |
|    std                  | 1.16       |
|    value_loss           | 0.599      |
----------------------------------------
Iteration: 552 | Episodes: 22400 | Median Reward: 44.04 | Max Reward: 45.95
Iteration: 554 | Episodes: 22500 | Median Reward: 44.07 | Max Reward: 45.95
Iteration: 557 | Episodes: 22600 | Median Reward: 43.88 | Max Reward: 45.95
Iteration: 559 | Episodes: 22700 | Median Reward: 43.27 | Max Reward: 45.95
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -58.3      |
| time/                   |            |
|    fps                  | 958        |
|    iterations           | 560        |
|    time_elapsed         | 2393       |
|    total_timesteps      | 2293760    |
| train/                  |            |
|    approx_kl            | 0.04929372 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.3        |
|    entropy_loss         | -241       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -22.6      |
|    n_updates            | 5590       |
|    policy_gradient_loss | -0.037     |
|    std                  | 1.16       |
|    value_loss           | 1.52       |
----------------------------------------
Iteration: 562 | Episodes: 22800 | Median Reward: 43.88 | Max Reward: 45.95
Iteration: 564 | Episodes: 22900 | Median Reward: 44.27 | Max Reward: 45.95
Iteration: 567 | Episodes: 23000 | Median Reward: 43.91 | Max Reward: 45.95
Iteration: 569 | Episodes: 23100 | Median Reward: 43.78 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.6       |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 570         |
|    time_elapsed         | 2436        |
|    total_timesteps      | 2334720     |
| train/                  |             |
|    approx_kl            | 0.042336725 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -241        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.8       |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.0218     |
|    std                  | 1.17        |
|    value_loss           | 1.02        |
-----------------------------------------
Iteration: 572 | Episodes: 23200 | Median Reward: 43.00 | Max Reward: 45.95
Iteration: 574 | Episodes: 23300 | Median Reward: 44.41 | Max Reward: 45.95
Iteration: 577 | Episodes: 23400 | Median Reward: 43.11 | Max Reward: 45.95
Iteration: 579 | Episodes: 23500 | Median Reward: 43.82 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.7       |
| time/                   |             |
|    fps                  | 958         |
|    iterations           | 580         |
|    time_elapsed         | 2479        |
|    total_timesteps      | 2375680     |
| train/                  |             |
|    approx_kl            | 0.004126683 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -242        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.2       |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.00651    |
|    std                  | 1.17        |
|    value_loss           | 1.65        |
-----------------------------------------
Iteration: 581 | Episodes: 23600 | Median Reward: 43.80 | Max Reward: 45.95
Iteration: 584 | Episodes: 23700 | Median Reward: 43.88 | Max Reward: 45.95
Iteration: 586 | Episodes: 23800 | Median Reward: 43.83 | Max Reward: 45.95
Iteration: 589 | Episodes: 23900 | Median Reward: 42.54 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.8       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 590         |
|    time_elapsed         | 2522        |
|    total_timesteps      | 2416640     |
| train/                  |             |
|    approx_kl            | 0.009738488 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.3         |
|    entropy_loss         | -243        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.6       |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.00771    |
|    std                  | 1.18        |
|    value_loss           | 1.9         |
-----------------------------------------
Iteration: 591 | Episodes: 24000 | Median Reward: 44.13 | Max Reward: 45.95
Iteration: 594 | Episodes: 24100 | Median Reward: 44.20 | Max Reward: 45.95
Iteration: 596 | Episodes: 24200 | Median Reward: 43.86 | Max Reward: 45.95
Iteration: 599 | Episodes: 24300 | Median Reward: 44.45 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.8         |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 600           |
|    time_elapsed         | 2565          |
|    total_timesteps      | 2457600       |
| train/                  |               |
|    approx_kl            | 0.00013721095 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -244          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -23.7         |
|    n_updates            | 5990          |
|    policy_gradient_loss | -4.12e-05     |
|    std                  | 1.18          |
|    value_loss           | 2.44          |
-------------------------------------------
Iteration: 601 | Episodes: 24400 | Median Reward: 43.78 | Max Reward: 45.95
Iteration: 604 | Episodes: 24500 | Median Reward: 43.81 | Max Reward: 45.95
Iteration: 606 | Episodes: 24600 | Median Reward: 43.90 | Max Reward: 45.95
Iteration: 609 | Episodes: 24700 | Median Reward: 44.46 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.5        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 610          |
|    time_elapsed         | 2608         |
|    total_timesteps      | 2498560      |
| train/                  |              |
|    approx_kl            | 0.0073690913 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -245         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.3        |
|    n_updates            | 6090         |
|    policy_gradient_loss | -0.00557     |
|    std                  | 1.19         |
|    value_loss           | 1.08         |
------------------------------------------
Iteration: 611 | Episodes: 24800 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 613 | Episodes: 24900 | Median Reward: 43.87 | Max Reward: 45.95
Iteration: 616 | Episodes: 25000 | Median Reward: 43.87 | Max Reward: 45.95
Iteration: 618 | Episodes: 25100 | Median Reward: 44.20 | Max Reward: 45.95
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -56.4      |
| time/                   |            |
|    fps                  | 957        |
|    iterations           | 620        |
|    time_elapsed         | 2651       |
|    total_timesteps      | 2539520    |
| train/                  |            |
|    approx_kl            | 0.03379988 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.3        |
|    entropy_loss         | -246       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -23.3      |
|    n_updates            | 6190       |
|    policy_gradient_loss | -0.0126    |
|    std                  | 1.19       |
|    value_loss           | 1.34       |
----------------------------------------
Iteration: 621 | Episodes: 25200 | Median Reward: 44.01 | Max Reward: 45.95
Iteration: 623 | Episodes: 25300 | Median Reward: 44.15 | Max Reward: 45.95
Iteration: 626 | Episodes: 25400 | Median Reward: 44.22 | Max Reward: 45.95
Iteration: 628 | Episodes: 25500 | Median Reward: 44.01 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 630          |
|    time_elapsed         | 2694         |
|    total_timesteps      | 2580480      |
| train/                  |              |
|    approx_kl            | 0.0016805425 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -247         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -24.6        |
|    n_updates            | 6290         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 1.2          |
|    value_loss           | 0.39         |
------------------------------------------
Iteration: 631 | Episodes: 25600 | Median Reward: 44.12 | Max Reward: 45.95
Iteration: 633 | Episodes: 25700 | Median Reward: 44.37 | Max Reward: 45.95
Iteration: 636 | Episodes: 25800 | Median Reward: 43.95 | Max Reward: 45.95
Iteration: 638 | Episodes: 25900 | Median Reward: 44.52 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.3         |
| time/                   |               |
|    fps                  | 958           |
|    iterations           | 640           |
|    time_elapsed         | 2736          |
|    total_timesteps      | 2621440       |
| train/                  |               |
|    approx_kl            | 2.9830262e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -248          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -24           |
|    n_updates            | 6390          |
|    policy_gradient_loss | -1.81e-05     |
|    std                  | 1.2           |
|    value_loss           | 1.29          |
-------------------------------------------
Iteration: 641 | Episodes: 26000 | Median Reward: 44.21 | Max Reward: 45.95
Iteration: 643 | Episodes: 26100 | Median Reward: 44.47 | Max Reward: 45.95
Iteration: 646 | Episodes: 26200 | Median Reward: 44.10 | Max Reward: 45.95
Iteration: 648 | Episodes: 26300 | Median Reward: 44.41 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.1        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 650          |
|    time_elapsed         | 2779         |
|    total_timesteps      | 2662400      |
| train/                  |              |
|    approx_kl            | 0.0045120977 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.6        |
|    n_updates            | 6490         |
|    policy_gradient_loss | -0.0122      |
|    std                  | 1.21         |
|    value_loss           | 3.08         |
------------------------------------------
Iteration: 650 | Episodes: 26400 | Median Reward: 44.62 | Max Reward: 45.95
Iteration: 653 | Episodes: 26500 | Median Reward: 44.44 | Max Reward: 45.95
Iteration: 655 | Episodes: 26600 | Median Reward: 43.88 | Max Reward: 45.95
Iteration: 658 | Episodes: 26700 | Median Reward: 44.10 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 660          |
|    time_elapsed         | 2822         |
|    total_timesteps      | 2703360      |
| train/                  |              |
|    approx_kl            | 0.0061732833 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.9        |
|    n_updates            | 6590         |
|    policy_gradient_loss | -0.01        |
|    std                  | 1.21         |
|    value_loss           | 1.52         |
------------------------------------------
Iteration: 660 | Episodes: 26800 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 663 | Episodes: 26900 | Median Reward: 44.07 | Max Reward: 45.95
Iteration: 665 | Episodes: 27000 | Median Reward: 44.20 | Max Reward: 45.95
Iteration: 668 | Episodes: 27100 | Median Reward: 44.17 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 670          |
|    time_elapsed         | 2865         |
|    total_timesteps      | 2744320      |
| train/                  |              |
|    approx_kl            | 0.0006997381 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -251         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.8        |
|    n_updates            | 6690         |
|    policy_gradient_loss | 0.000404     |
|    std                  | 1.22         |
|    value_loss           | 1.2          |
------------------------------------------
Iteration: 670 | Episodes: 27200 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 673 | Episodes: 27300 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 675 | Episodes: 27400 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 678 | Episodes: 27500 | Median Reward: 43.91 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.7       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 680         |
|    time_elapsed         | 2908        |
|    total_timesteps      | 2785280     |
| train/                  |             |
|    approx_kl            | 0.021097688 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -252        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -24.7       |
|    n_updates            | 6790        |
|    policy_gradient_loss | -0.0146     |
|    std                  | 1.22        |
|    value_loss           | 0.739       |
-----------------------------------------
Iteration: 680 | Episodes: 27600 | Median Reward: 44.62 | Max Reward: 45.95
Iteration: 683 | Episodes: 27700 | Median Reward: 44.29 | Max Reward: 45.95
Iteration: 685 | Episodes: 27800 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 687 | Episodes: 27900 | Median Reward: 44.22 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 690           |
|    time_elapsed         | 2950          |
|    total_timesteps      | 2826240       |
| train/                  |               |
|    approx_kl            | 0.00016910916 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -253          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -23.8         |
|    n_updates            | 6890          |
|    policy_gradient_loss | -0.000184     |
|    std                  | 1.23          |
|    value_loss           | 1.82          |
-------------------------------------------
Iteration: 690 | Episodes: 28000 | Median Reward: 44.53 | Max Reward: 45.95
Iteration: 692 | Episodes: 28100 | Median Reward: 44.29 | Max Reward: 45.95
Iteration: 695 | Episodes: 28200 | Median Reward: 43.55 | Max Reward: 45.95
Iteration: 697 | Episodes: 28300 | Median Reward: 43.21 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.6       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 700         |
|    time_elapsed         | 2993        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.021658614 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -253        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -24.8       |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.0146     |
|    std                  | 1.23        |
|    value_loss           | 0.827       |
-----------------------------------------
Iteration: 700 | Episodes: 28400 | Median Reward: 44.41 | Max Reward: 45.95
Iteration: 702 | Episodes: 28500 | Median Reward: 44.64 | Max Reward: 45.95
Iteration: 705 | Episodes: 28600 | Median Reward: 44.19 | Max Reward: 45.95
Iteration: 707 | Episodes: 28700 | Median Reward: 44.02 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.2       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 710         |
|    time_elapsed         | 3036        |
|    total_timesteps      | 2908160     |
| train/                  |             |
|    approx_kl            | 0.013412925 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -254        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -25.2       |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.00225    |
|    std                  | 1.24        |
|    value_loss           | 0.519       |
-----------------------------------------
Iteration: 710 | Episodes: 28800 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 712 | Episodes: 28900 | Median Reward: 43.78 | Max Reward: 45.95
Iteration: 715 | Episodes: 29000 | Median Reward: 44.07 | Max Reward: 45.95
Iteration: 717 | Episodes: 29100 | Median Reward: 44.10 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.4        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 720          |
|    time_elapsed         | 3078         |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0015559843 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -255         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.6        |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.00358     |
|    std                  | 1.24         |
|    value_loss           | 1.64         |
------------------------------------------
Iteration: 720 | Episodes: 29200 | Median Reward: 44.21 | Max Reward: 45.95
Iteration: 722 | Episodes: 29300 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 724 | Episodes: 29400 | Median Reward: 44.31 | Max Reward: 45.95
Iteration: 727 | Episodes: 29500 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 729 | Episodes: 29600 | Median Reward: 44.10 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 958           |
|    iterations           | 730           |
|    time_elapsed         | 3120          |
|    total_timesteps      | 2990080       |
| train/                  |               |
|    approx_kl            | 0.00018701468 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -255          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -23.7         |
|    n_updates            | 7290          |
|    policy_gradient_loss | 0.000209      |
|    std                  | 1.25          |
|    value_loss           | 2.22          |
-------------------------------------------
Iteration: 732 | Episodes: 29700 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 734 | Episodes: 29800 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 737 | Episodes: 29900 | Median Reward: 43.99 | Max Reward: 45.95
Iteration: 739 | Episodes: 30000 | Median Reward: 44.36 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.5        |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 740          |
|    time_elapsed         | 3163         |
|    total_timesteps      | 3031040      |
| train/                  |              |
|    approx_kl            | 0.0048393095 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -255         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.2        |
|    n_updates            | 7390         |
|    policy_gradient_loss | -0.00448     |
|    std                  | 1.25         |
|    value_loss           | 0.524        |
------------------------------------------
Iteration: 742 | Episodes: 30100 | Median Reward: 44.22 | Max Reward: 45.95
Iteration: 744 | Episodes: 30200 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 747 | Episodes: 30300 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 749 | Episodes: 30400 | Median Reward: 44.42 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 750          |
|    time_elapsed         | 3207         |
|    total_timesteps      | 3072000      |
| train/                  |              |
|    approx_kl            | 0.0020121702 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -255         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.3        |
|    n_updates            | 7490         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 1.26         |
|    value_loss           | 0.491        |
------------------------------------------
Iteration: 752 | Episodes: 30500 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 754 | Episodes: 30600 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 757 | Episodes: 30700 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 759 | Episodes: 30800 | Median Reward: 44.25 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 760          |
|    time_elapsed         | 3250         |
|    total_timesteps      | 3112960      |
| train/                  |              |
|    approx_kl            | 0.0010974093 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -255         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.4        |
|    n_updates            | 7590         |
|    policy_gradient_loss | -0.00189     |
|    std                  | 1.26         |
|    value_loss           | 0.763        |
------------------------------------------
Iteration: 761 | Episodes: 30900 | Median Reward: 44.13 | Max Reward: 45.95
Iteration: 764 | Episodes: 31000 | Median Reward: 44.35 | Max Reward: 45.95
Iteration: 766 | Episodes: 31100 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 769 | Episodes: 31200 | Median Reward: 44.07 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 770          |
|    time_elapsed         | 3293         |
|    total_timesteps      | 3153920      |
| train/                  |              |
|    approx_kl            | 0.0003604307 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -256         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.3        |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.00042     |
|    std                  | 1.27         |
|    value_loss           | 2.37         |
------------------------------------------
Iteration: 771 | Episodes: 31300 | Median Reward: 44.51 | Max Reward: 45.95
Iteration: 774 | Episodes: 31400 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 776 | Episodes: 31500 | Median Reward: 44.35 | Max Reward: 45.95
Iteration: 779 | Episodes: 31600 | Median Reward: 44.38 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.6       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 780         |
|    time_elapsed         | 3336        |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.015004437 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -256        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -25.6       |
|    n_updates            | 7790        |
|    policy_gradient_loss | -0.0125     |
|    std                  | 1.27        |
|    value_loss           | 0.71        |
-----------------------------------------
Iteration: 781 | Episodes: 31700 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 784 | Episodes: 31800 | Median Reward: 43.82 | Max Reward: 45.95
Iteration: 786 | Episodes: 31900 | Median Reward: 44.21 | Max Reward: 45.95
Iteration: 789 | Episodes: 32000 | Median Reward: 44.54 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 790          |
|    time_elapsed         | 3379         |
|    total_timesteps      | 3235840      |
| train/                  |              |
|    approx_kl            | 0.0007731202 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -258         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.6        |
|    n_updates            | 7890         |
|    policy_gradient_loss | -0.00226     |
|    std                  | 1.28         |
|    value_loss           | 0.842        |
------------------------------------------
Iteration: 791 | Episodes: 32100 | Median Reward: 44.55 | Max Reward: 45.95
Iteration: 793 | Episodes: 32200 | Median Reward: 44.36 | Max Reward: 45.95
Iteration: 796 | Episodes: 32300 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 798 | Episodes: 32400 | Median Reward: 44.22 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 800          |
|    time_elapsed         | 3422         |
|    total_timesteps      | 3276800      |
| train/                  |              |
|    approx_kl            | 0.0001859293 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -258         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.4        |
|    n_updates            | 7990         |
|    policy_gradient_loss | -0.000132    |
|    std                  | 1.29         |
|    value_loss           | 0.501        |
------------------------------------------
Iteration: 801 | Episodes: 32500 | Median Reward: 44.36 | Max Reward: 45.95
Iteration: 803 | Episodes: 32600 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 806 | Episodes: 32700 | Median Reward: 44.37 | Max Reward: 45.95
Iteration: 808 | Episodes: 32800 | Median Reward: 43.18 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.2        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 810          |
|    time_elapsed         | 3464         |
|    total_timesteps      | 3317760      |
| train/                  |              |
|    approx_kl            | 0.0043866737 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -258         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.4        |
|    n_updates            | 8090         |
|    policy_gradient_loss | -0.00488     |
|    std                  | 1.29         |
|    value_loss           | 0.92         |
------------------------------------------
Iteration: 811 | Episodes: 32900 | Median Reward: 43.16 | Max Reward: 45.95
Iteration: 813 | Episodes: 33000 | Median Reward: 44.12 | Max Reward: 45.95
Iteration: 816 | Episodes: 33100 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 818 | Episodes: 33200 | Median Reward: 44.49 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.5       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 820         |
|    time_elapsed         | 3507        |
|    total_timesteps      | 3358720     |
| train/                  |             |
|    approx_kl            | 0.027126763 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.3         |
|    entropy_loss         | -259        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -25.9       |
|    n_updates            | 8190        |
|    policy_gradient_loss | -0.0291     |
|    std                  | 1.3         |
|    value_loss           | 0.296       |
-----------------------------------------
Iteration: 821 | Episodes: 33300 | Median Reward: 44.11 | Max Reward: 45.95
Iteration: 823 | Episodes: 33400 | Median Reward: 44.46 | Max Reward: 45.95
Iteration: 826 | Episodes: 33500 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 828 | Episodes: 33600 | Median Reward: 44.64 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 830          |
|    time_elapsed         | 3550         |
|    total_timesteps      | 3399680      |
| train/                  |              |
|    approx_kl            | 0.0026076585 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -260         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.7        |
|    n_updates            | 8290         |
|    policy_gradient_loss | -0.00632     |
|    std                  | 1.31         |
|    value_loss           | 1.42         |
------------------------------------------
Iteration: 830 | Episodes: 33700 | Median Reward: 44.62 | Max Reward: 45.95
Iteration: 833 | Episodes: 33800 | Median Reward: 43.84 | Max Reward: 45.95
Iteration: 835 | Episodes: 33900 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 838 | Episodes: 34000 | Median Reward: 43.36 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 840          |
|    time_elapsed         | 3593         |
|    total_timesteps      | 3440640      |
| train/                  |              |
|    approx_kl            | 0.0002246254 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -261         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -19.8        |
|    n_updates            | 8390         |
|    policy_gradient_loss | -0.000449    |
|    std                  | 1.31         |
|    value_loss           | 6.82         |
------------------------------------------
Iteration: 840 | Episodes: 34100 | Median Reward: 44.21 | Max Reward: 45.95
Iteration: 843 | Episodes: 34200 | Median Reward: 44.25 | Max Reward: 45.95
Iteration: 845 | Episodes: 34300 | Median Reward: 43.89 | Max Reward: 45.95
Iteration: 848 | Episodes: 34400 | Median Reward: 44.64 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.6         |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 850           |
|    time_elapsed         | 3636          |
|    total_timesteps      | 3481600       |
| train/                  |               |
|    approx_kl            | 4.1914114e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -261          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -24           |
|    n_updates            | 8490          |
|    policy_gradient_loss | -7.43e-05     |
|    std                  | 1.32          |
|    value_loss           | 1.4           |
-------------------------------------------
Iteration: 850 | Episodes: 34500 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 853 | Episodes: 34600 | Median Reward: 44.65 | Max Reward: 45.95
Iteration: 855 | Episodes: 34700 | Median Reward: 44.39 | Max Reward: 45.95
Iteration: 858 | Episodes: 34800 | Median Reward: 44.30 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 860          |
|    time_elapsed         | 3679         |
|    total_timesteps      | 3522560      |
| train/                  |              |
|    approx_kl            | 0.0023658439 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -262         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.4        |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.00561     |
|    std                  | 1.32         |
|    value_loss           | 2.68         |
------------------------------------------
Iteration: 860 | Episodes: 34900 | Median Reward: 43.89 | Max Reward: 45.95
Iteration: 863 | Episodes: 35000 | Median Reward: 44.22 | Max Reward: 45.95
Iteration: 865 | Episodes: 35100 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 867 | Episodes: 35200 | Median Reward: 44.54 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.2         |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 870           |
|    time_elapsed         | 3722          |
|    total_timesteps      | 3563520       |
| train/                  |               |
|    approx_kl            | 9.8323755e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -263          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -25.8         |
|    n_updates            | 8690          |
|    policy_gradient_loss | -0.000177     |
|    std                  | 1.33          |
|    value_loss           | 0.794         |
-------------------------------------------
Iteration: 870 | Episodes: 35300 | Median Reward: 44.04 | Max Reward: 45.95
Iteration: 872 | Episodes: 35400 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 875 | Episodes: 35500 | Median Reward: 44.41 | Max Reward: 45.95
Iteration: 877 | Episodes: 35600 | Median Reward: 44.22 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.5        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 880          |
|    time_elapsed         | 3765         |
|    total_timesteps      | 3604480      |
| train/                  |              |
|    approx_kl            | 0.0005699501 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -263         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.7        |
|    n_updates            | 8790         |
|    policy_gradient_loss | -0.000173    |
|    std                  | 1.34         |
|    value_loss           | 0.934        |
------------------------------------------
Iteration: 880 | Episodes: 35700 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 882 | Episodes: 35800 | Median Reward: 44.64 | Max Reward: 45.95
Iteration: 885 | Episodes: 35900 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 887 | Episodes: 36000 | Median Reward: 44.64 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.7         |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 890           |
|    time_elapsed         | 3807          |
|    total_timesteps      | 3645440       |
| train/                  |               |
|    approx_kl            | 0.00046693796 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -263          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -26           |
|    n_updates            | 8890          |
|    policy_gradient_loss | -0.00062      |
|    std                  | 1.34          |
|    value_loss           | 0.959         |
-------------------------------------------
Iteration: 890 | Episodes: 36100 | Median Reward: 44.46 | Max Reward: 45.95
Iteration: 892 | Episodes: 36200 | Median Reward: 44.13 | Max Reward: 45.95
Iteration: 895 | Episodes: 36300 | Median Reward: 44.04 | Max Reward: 45.95
Iteration: 897 | Episodes: 36400 | Median Reward: 44.63 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.4         |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 900           |
|    time_elapsed         | 3850          |
|    total_timesteps      | 3686400       |
| train/                  |               |
|    approx_kl            | 0.00010974359 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -264          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -26.3         |
|    n_updates            | 8990          |
|    policy_gradient_loss | -0.000255     |
|    std                  | 1.35          |
|    value_loss           | 0.29          |
-------------------------------------------
Iteration: 900 | Episodes: 36500 | Median Reward: 44.54 | Max Reward: 45.95
Iteration: 902 | Episodes: 36600 | Median Reward: 44.20 | Max Reward: 45.95
Iteration: 904 | Episodes: 36700 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 907 | Episodes: 36800 | Median Reward: 44.66 | Max Reward: 45.95
Iteration: 909 | Episodes: 36900 | Median Reward: 44.45 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 910         |
|    time_elapsed         | 3893        |
|    total_timesteps      | 3727360     |
| train/                  |             |
|    approx_kl            | 0.007667442 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -264        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -25.6       |
|    n_updates            | 9090        |
|    policy_gradient_loss | -0.00769    |
|    std                  | 1.36        |
|    value_loss           | 0.821       |
-----------------------------------------
Iteration: 912 | Episodes: 37000 | Median Reward: 44.46 | Max Reward: 45.95
Iteration: 914 | Episodes: 37100 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 917 | Episodes: 37200 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 919 | Episodes: 37300 | Median Reward: 44.42 | Max Reward: 45.95
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -56.6      |
| time/                   |            |
|    fps                  | 957        |
|    iterations           | 920        |
|    time_elapsed         | 3936       |
|    total_timesteps      | 3768320    |
| train/                  |            |
|    approx_kl            | 0.02119001 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -265       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -26.4      |
|    n_updates            | 9190       |
|    policy_gradient_loss | -0.0198    |
|    std                  | 1.36       |
|    value_loss           | 0.633      |
----------------------------------------
Iteration: 922 | Episodes: 37400 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 924 | Episodes: 37500 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 927 | Episodes: 37600 | Median Reward: 40.36 | Max Reward: 45.95
Iteration: 929 | Episodes: 37700 | Median Reward: 44.26 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.8        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 930          |
|    time_elapsed         | 3979         |
|    total_timesteps      | 3809280      |
| train/                  |              |
|    approx_kl            | 0.0033443954 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -265         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26.2        |
|    n_updates            | 9290         |
|    policy_gradient_loss | -0.00472     |
|    std                  | 1.37         |
|    value_loss           | 0.416        |
------------------------------------------
Iteration: 932 | Episodes: 37800 | Median Reward: 44.55 | Max Reward: 45.95
Iteration: 934 | Episodes: 37900 | Median Reward: 44.46 | Max Reward: 45.95
Iteration: 937 | Episodes: 38000 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 939 | Episodes: 38100 | Median Reward: 44.51 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.9         |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 940           |
|    time_elapsed         | 4022          |
|    total_timesteps      | 3850240       |
| train/                  |               |
|    approx_kl            | 0.00033289276 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -266          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -24.5         |
|    n_updates            | 9390          |
|    policy_gradient_loss | -0.00071      |
|    std                  | 1.38          |
|    value_loss           | 1.88          |
-------------------------------------------
Iteration: 941 | Episodes: 38200 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 944 | Episodes: 38300 | Median Reward: 44.27 | Max Reward: 45.95
Iteration: 946 | Episodes: 38400 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 949 | Episodes: 38500 | Median Reward: 44.19 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 950           |
|    time_elapsed         | 4064          |
|    total_timesteps      | 3891200       |
| train/                  |               |
|    approx_kl            | 0.00019679227 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -266          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -25.6         |
|    n_updates            | 9490          |
|    policy_gradient_loss | -0.000133     |
|    std                  | 1.39          |
|    value_loss           | 0.888         |
-------------------------------------------
Iteration: 951 | Episodes: 38600 | Median Reward: 44.22 | Max Reward: 45.95
Iteration: 954 | Episodes: 38700 | Median Reward: 44.44 | Max Reward: 45.95
Iteration: 956 | Episodes: 38800 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 959 | Episodes: 38900 | Median Reward: 44.67 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 960          |
|    time_elapsed         | 4107         |
|    total_timesteps      | 3932160      |
| train/                  |              |
|    approx_kl            | 0.0016266172 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -266         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.3        |
|    n_updates            | 9590         |
|    policy_gradient_loss | -0.00177     |
|    std                  | 1.39         |
|    value_loss           | 0.836        |
------------------------------------------
Iteration: 961 | Episodes: 39000 | Median Reward: 44.14 | Max Reward: 45.95
Iteration: 964 | Episodes: 39100 | Median Reward: 44.64 | Max Reward: 45.95
Iteration: 966 | Episodes: 39200 | Median Reward: 44.15 | Max Reward: 45.95
Iteration: 969 | Episodes: 39300 | Median Reward: 44.45 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.5         |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 970           |
|    time_elapsed         | 4150          |
|    total_timesteps      | 3973120       |
| train/                  |               |
|    approx_kl            | 4.4457513e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -267          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -26.2         |
|    n_updates            | 9690          |
|    policy_gradient_loss | -5.81e-05     |
|    std                  | 1.4           |
|    value_loss           | 1.42          |
-------------------------------------------
Iteration: 971 | Episodes: 39400 | Median Reward: 44.51 | Max Reward: 45.95
Iteration: 973 | Episodes: 39500 | Median Reward: 44.36 | Max Reward: 45.95
Iteration: 976 | Episodes: 39600 | Median Reward: 44.73 | Max Reward: 45.95
Iteration: 978 | Episodes: 39700 | Median Reward: 44.40 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.9         |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 980           |
|    time_elapsed         | 4192          |
|    total_timesteps      | 4014080       |
| train/                  |               |
|    approx_kl            | 0.00011487176 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -268          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -26.7         |
|    n_updates            | 9790          |
|    policy_gradient_loss | -0.000158     |
|    std                  | 1.41          |
|    value_loss           | 0.371         |
-------------------------------------------
Iteration: 981 | Episodes: 39800 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 983 | Episodes: 39900 | Median Reward: 44.27 | Max Reward: 45.95
Iteration: 986 | Episodes: 40000 | Median Reward: 44.38 | Max Reward: 45.95
Iteration: 988 | Episodes: 40100 | Median Reward: 44.45 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.2       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 990         |
|    time_elapsed         | 4234        |
|    total_timesteps      | 4055040     |
| train/                  |             |
|    approx_kl            | 0.038974687 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.3         |
|    entropy_loss         | -268        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -26.7       |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.0323     |
|    std                  | 1.42        |
|    value_loss           | 0.406       |
-----------------------------------------
Iteration: 991 | Episodes: 40200 | Median Reward: 44.61 | Max Reward: 45.95
Iteration: 993 | Episodes: 40300 | Median Reward: 44.64 | Max Reward: 45.95
Iteration: 996 | Episodes: 40400 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 998 | Episodes: 40500 | Median Reward: 44.52 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.5        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1000         |
|    time_elapsed         | 4277         |
|    total_timesteps      | 4096000      |
| train/                  |              |
|    approx_kl            | 0.0007467384 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -269         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26          |
|    n_updates            | 9990         |
|    policy_gradient_loss | 0.00591      |
|    std                  | 1.42         |
|    value_loss           | 1.52         |
------------------------------------------
Iteration: 1001 | Episodes: 40600 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1003 | Episodes: 40700 | Median Reward: 44.47 | Max Reward: 45.95
Iteration: 1006 | Episodes: 40800 | Median Reward: 44.41 | Max Reward: 45.95
Iteration: 1008 | Episodes: 40900 | Median Reward: 44.51 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.1        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1010         |
|    time_elapsed         | 4320         |
|    total_timesteps      | 4136960      |
| train/                  |              |
|    approx_kl            | 0.0045023477 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -269         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26.8        |
|    n_updates            | 10090        |
|    policy_gradient_loss | -0.00105     |
|    std                  | 1.43         |
|    value_loss           | 0.23         |
------------------------------------------
Iteration: 1010 | Episodes: 41000 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1013 | Episodes: 41100 | Median Reward: 43.87 | Max Reward: 45.95
Iteration: 1015 | Episodes: 41200 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 1018 | Episodes: 41300 | Median Reward: 44.64 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 1020          |
|    time_elapsed         | 4362          |
|    total_timesteps      | 4177920       |
| train/                  |               |
|    approx_kl            | 3.9420032e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -270          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -26.4         |
|    n_updates            | 10190         |
|    policy_gradient_loss | -7.2e-05      |
|    std                  | 1.44          |
|    value_loss           | 1.16          |
-------------------------------------------
Iteration: 1020 | Episodes: 41400 | Median Reward: 44.66 | Max Reward: 45.95
Iteration: 1023 | Episodes: 41500 | Median Reward: 44.58 | Max Reward: 45.95
Iteration: 1025 | Episodes: 41600 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1028 | Episodes: 41700 | Median Reward: 44.38 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.5        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1030         |
|    time_elapsed         | 4405         |
|    total_timesteps      | 4218880      |
| train/                  |              |
|    approx_kl            | 0.0003363737 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -270         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.8        |
|    n_updates            | 10290        |
|    policy_gradient_loss | 0.000211     |
|    std                  | 1.44         |
|    value_loss           | 0.464        |
------------------------------------------
Iteration: 1030 | Episodes: 41800 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1033 | Episodes: 41900 | Median Reward: 44.32 | Max Reward: 45.95
Iteration: 1035 | Episodes: 42000 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1038 | Episodes: 42100 | Median Reward: 44.49 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.8         |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 1040          |
|    time_elapsed         | 4447          |
|    total_timesteps      | 4259840       |
| train/                  |               |
|    approx_kl            | 0.00029633724 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -271          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -26.8         |
|    n_updates            | 10390         |
|    policy_gradient_loss | -0.000302     |
|    std                  | 1.45          |
|    value_loss           | 0.925         |
-------------------------------------------
Iteration: 1040 | Episodes: 42200 | Median Reward: 44.58 | Max Reward: 45.95
Iteration: 1043 | Episodes: 42300 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 1045 | Episodes: 42400 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 1047 | Episodes: 42500 | Median Reward: 44.24 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.6        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1050         |
|    time_elapsed         | 4491         |
|    total_timesteps      | 4300800      |
| train/                  |              |
|    approx_kl            | 0.0002531265 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -271         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.6        |
|    n_updates            | 10490        |
|    policy_gradient_loss | 0.000625     |
|    std                  | 1.46         |
|    value_loss           | 1.3          |
------------------------------------------
Iteration: 1050 | Episodes: 42600 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 1052 | Episodes: 42700 | Median Reward: 44.59 | Max Reward: 45.95
Iteration: 1055 | Episodes: 42800 | Median Reward: 44.55 | Max Reward: 45.95
Iteration: 1057 | Episodes: 42900 | Median Reward: 43.80 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1060         |
|    time_elapsed         | 4533         |
|    total_timesteps      | 4341760      |
| train/                  |              |
|    approx_kl            | 0.0003746613 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -272         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.3        |
|    n_updates            | 10590        |
|    policy_gradient_loss | -5.03e-05    |
|    std                  | 1.46         |
|    value_loss           | 1.66         |
------------------------------------------
Iteration: 1060 | Episodes: 43000 | Median Reward: 44.50 | Max Reward: 45.95
Iteration: 1062 | Episodes: 43100 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1065 | Episodes: 43200 | Median Reward: 44.52 | Max Reward: 45.95
Iteration: 1067 | Episodes: 43300 | Median Reward: 44.40 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.9       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 1070        |
|    time_elapsed         | 4576        |
|    total_timesteps      | 4382720     |
| train/                  |             |
|    approx_kl            | 0.025098387 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -272        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -27.1       |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.0242     |
|    std                  | 1.47        |
|    value_loss           | 0.764       |
-----------------------------------------
Iteration: 1070 | Episodes: 43400 | Median Reward: 43.84 | Max Reward: 45.95
Iteration: 1072 | Episodes: 43500 | Median Reward: 44.50 | Max Reward: 45.95
Iteration: 1075 | Episodes: 43600 | Median Reward: 44.64 | Max Reward: 45.95
Iteration: 1077 | Episodes: 43700 | Median Reward: 44.52 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.9         |
| time/                   |               |
|    fps                  | 957           |
|    iterations           | 1080          |
|    time_elapsed         | 4619          |
|    total_timesteps      | 4423680       |
| train/                  |               |
|    approx_kl            | 0.00022107427 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -273          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -25.8         |
|    n_updates            | 10790         |
|    policy_gradient_loss | -0.000221     |
|    std                  | 1.48          |
|    value_loss           | 3.59          |
-------------------------------------------
Iteration: 1080 | Episodes: 43800 | Median Reward: 44.62 | Max Reward: 45.95
Iteration: 1082 | Episodes: 43900 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 1084 | Episodes: 44000 | Median Reward: 44.35 | Max Reward: 45.95
Iteration: 1087 | Episodes: 44100 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1089 | Episodes: 44200 | Median Reward: 44.64 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.5       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 1090        |
|    time_elapsed         | 4661        |
|    total_timesteps      | 4464640     |
| train/                  |             |
|    approx_kl            | 0.004116955 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -273        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -26.5       |
|    n_updates            | 10890       |
|    policy_gradient_loss | -0.00256    |
|    std                  | 1.48        |
|    value_loss           | 0.51        |
-----------------------------------------
Iteration: 1092 | Episodes: 44300 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1094 | Episodes: 44400 | Median Reward: 44.64 | Max Reward: 45.95
Iteration: 1097 | Episodes: 44500 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1099 | Episodes: 44600 | Median Reward: 44.56 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1100         |
|    time_elapsed         | 4703         |
|    total_timesteps      | 4505600      |
| train/                  |              |
|    approx_kl            | 0.0004184473 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -274         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -27          |
|    n_updates            | 10990        |
|    policy_gradient_loss | -0.00157     |
|    std                  | 1.49         |
|    value_loss           | 1.23         |
------------------------------------------
Iteration: 1102 | Episodes: 44700 | Median Reward: 44.55 | Max Reward: 45.95
Iteration: 1104 | Episodes: 44800 | Median Reward: 44.46 | Max Reward: 45.95
Iteration: 1107 | Episodes: 44900 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 1109 | Episodes: 45000 | Median Reward: 44.47 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.3        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1110         |
|    time_elapsed         | 4747         |
|    total_timesteps      | 4546560      |
| train/                  |              |
|    approx_kl            | 0.0017885929 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -274         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.2        |
|    n_updates            | 11090        |
|    policy_gradient_loss | 0.000322     |
|    std                  | 1.5          |
|    value_loss           | 0.397        |
------------------------------------------
Iteration: 1112 | Episodes: 45100 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1114 | Episodes: 45200 | Median Reward: 44.75 | Max Reward: 45.95
Iteration: 1117 | Episodes: 45300 | Median Reward: 44.61 | Max Reward: 45.95
Iteration: 1119 | Episodes: 45400 | Median Reward: 44.49 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.8        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1120         |
|    time_elapsed         | 4790         |
|    total_timesteps      | 4587520      |
| train/                  |              |
|    approx_kl            | 6.393857e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -275         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.8        |
|    n_updates            | 11190        |
|    policy_gradient_loss | -3.26e-05    |
|    std                  | 1.51         |
|    value_loss           | 0.947        |
------------------------------------------
Iteration: 1121 | Episodes: 45500 | Median Reward: 43.98 | Max Reward: 45.95
Iteration: 1124 | Episodes: 45600 | Median Reward: 44.48 | Max Reward: 45.95
Iteration: 1126 | Episodes: 45700 | Median Reward: 44.50 | Max Reward: 45.95
Iteration: 1129 | Episodes: 45800 | Median Reward: 44.45 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56         |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 1130        |
|    time_elapsed         | 4833        |
|    total_timesteps      | 4628480     |
| train/                  |             |
|    approx_kl            | 0.058738865 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.3         |
|    entropy_loss         | -275        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -27.5       |
|    n_updates            | 11290       |
|    policy_gradient_loss | -0.0519     |
|    std                  | 1.51        |
|    value_loss           | 0.17        |
-----------------------------------------
Iteration: 1131 | Episodes: 45900 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1134 | Episodes: 46000 | Median Reward: 44.38 | Max Reward: 45.95
Iteration: 1136 | Episodes: 46100 | Median Reward: 44.58 | Max Reward: 45.95
Iteration: 1139 | Episodes: 46200 | Median Reward: 44.43 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1140         |
|    time_elapsed         | 4876         |
|    total_timesteps      | 4669440      |
| train/                  |              |
|    approx_kl            | 0.0005112284 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -276         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.4        |
|    n_updates            | 11390        |
|    policy_gradient_loss | -0.000998    |
|    std                  | 1.52         |
|    value_loss           | 1.2          |
------------------------------------------
Iteration: 1141 | Episodes: 46300 | Median Reward: 44.44 | Max Reward: 45.95
Iteration: 1144 | Episodes: 46400 | Median Reward: 44.46 | Max Reward: 45.95
Iteration: 1146 | Episodes: 46500 | Median Reward: 44.64 | Max Reward: 45.95
Iteration: 1149 | Episodes: 46600 | Median Reward: 43.79 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.7       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 1150        |
|    time_elapsed         | 4918        |
|    total_timesteps      | 4710400     |
| train/                  |             |
|    approx_kl            | 0.010656824 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -277        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -27.6       |
|    n_updates            | 11490       |
|    policy_gradient_loss | -0.00042    |
|    std                  | 1.53        |
|    value_loss           | 0.855       |
-----------------------------------------
Iteration: 1151 | Episodes: 46700 | Median Reward: 44.22 | Max Reward: 45.95
Iteration: 1154 | Episodes: 46800 | Median Reward: 44.47 | Max Reward: 45.95
Iteration: 1156 | Episodes: 46900 | Median Reward: 44.52 | Max Reward: 45.95
Iteration: 1158 | Episodes: 47000 | Median Reward: 44.67 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.5       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 1160        |
|    time_elapsed         | 4961        |
|    total_timesteps      | 4751360     |
| train/                  |             |
|    approx_kl            | 0.002222126 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -277        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -26.7       |
|    n_updates            | 11590       |
|    policy_gradient_loss | -0.00515    |
|    std                  | 1.54        |
|    value_loss           | 1.02        |
-----------------------------------------
Iteration: 1161 | Episodes: 47100 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 1163 | Episodes: 47200 | Median Reward: 44.10 | Max Reward: 45.95
Iteration: 1166 | Episodes: 47300 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1168 | Episodes: 47400 | Median Reward: 44.51 | Max Reward: 45.95
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -56            |
| time/                   |                |
|    fps                  | 957            |
|    iterations           | 1170           |
|    time_elapsed         | 5003           |
|    total_timesteps      | 4792320        |
| train/                  |                |
|    approx_kl            | 0.000110241206 |
|    clip_fraction        | 0              |
|    clip_range           | 0.3            |
|    entropy_loss         | -278           |
|    explained_variance   | 0.998          |
|    learning_rate        | 0.0005         |
|    loss                 | -27.5          |
|    n_updates            | 11690          |
|    policy_gradient_loss | -5.42e-05      |
|    std                  | 1.55           |
|    value_loss           | 0.733          |
--------------------------------------------
Iteration: 1171 | Episodes: 47500 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 1173 | Episodes: 47600 | Median Reward: 44.12 | Max Reward: 45.95
Iteration: 1176 | Episodes: 47700 | Median Reward: 44.47 | Max Reward: 45.95
Iteration: 1178 | Episodes: 47800 | Median Reward: 44.69 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.5        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1180         |
|    time_elapsed         | 5046         |
|    total_timesteps      | 4833280      |
| train/                  |              |
|    approx_kl            | 0.0016211979 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -279         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.5        |
|    n_updates            | 11790        |
|    policy_gradient_loss | -0.00346     |
|    std                  | 1.56         |
|    value_loss           | 0.737        |
------------------------------------------
Iteration: 1181 | Episodes: 47900 | Median Reward: 44.57 | Max Reward: 45.95
Iteration: 1183 | Episodes: 48000 | Median Reward: 44.60 | Max Reward: 45.95
Iteration: 1186 | Episodes: 48100 | Median Reward: 44.62 | Max Reward: 45.95
Iteration: 1188 | Episodes: 48200 | Median Reward: 44.73 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.1        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1190         |
|    time_elapsed         | 5090         |
|    total_timesteps      | 4874240      |
| train/                  |              |
|    approx_kl            | 0.0007989041 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -279         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.4        |
|    n_updates            | 11890        |
|    policy_gradient_loss | -0.00225     |
|    std                  | 1.56         |
|    value_loss           | 0.908        |
------------------------------------------
Iteration: 1190 | Episodes: 48300 | Median Reward: 44.50 | Max Reward: 45.95
Iteration: 1193 | Episodes: 48400 | Median Reward: 44.74 | Max Reward: 45.95
Iteration: 1195 | Episodes: 48500 | Median Reward: 44.25 | Max Reward: 45.95
Iteration: 1198 | Episodes: 48600 | Median Reward: 44.48 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1200         |
|    time_elapsed         | 5134         |
|    total_timesteps      | 4915200      |
| train/                  |              |
|    approx_kl            | 0.0072893836 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -279         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.3        |
|    n_updates            | 11990        |
|    policy_gradient_loss | -0.00905     |
|    std                  | 1.57         |
|    value_loss           | 1.05         |
------------------------------------------
Iteration: 1200 | Episodes: 48700 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 1203 | Episodes: 48800 | Median Reward: 44.19 | Max Reward: 45.95
Iteration: 1205 | Episodes: 48900 | Median Reward: 44.06 | Max Reward: 45.95
Iteration: 1208 | Episodes: 49000 | Median Reward: 44.56 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.3        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1210         |
|    time_elapsed         | 5177         |
|    total_timesteps      | 4956160      |
| train/                  |              |
|    approx_kl            | 0.0007118705 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -280         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27.6        |
|    n_updates            | 12090        |
|    policy_gradient_loss | -0.00328     |
|    std                  | 1.58         |
|    value_loss           | 1.58         |
------------------------------------------
Iteration: 1210 | Episodes: 49100 | Median Reward: 44.08 | Max Reward: 45.95
Iteration: 1213 | Episodes: 49200 | Median Reward: 44.41 | Max Reward: 45.95
Iteration: 1215 | Episodes: 49300 | Median Reward: 44.46 | Max Reward: 45.95
Iteration: 1218 | Episodes: 49400 | Median Reward: 44.42 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.1        |
| time/                   |              |
|    fps                  | 957          |
|    iterations           | 1220         |
|    time_elapsed         | 5220         |
|    total_timesteps      | 4997120      |
| train/                  |              |
|    approx_kl            | 0.0003002105 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -280         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.6        |
|    n_updates            | 12190        |
|    policy_gradient_loss | -0.000711    |
|    std                  | 1.59         |
|    value_loss           | 7.75         |
------------------------------------------
Iteration: 1220 | Episodes: 49500 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1223 | Episodes: 49600 | Median Reward: 41.32 | Max Reward: 45.95
Iteration: 1225 | Episodes: 49700 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1227 | Episodes: 49800 | Median Reward: 44.63 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.8       |
| time/                   |             |
|    fps                  | 957         |
|    iterations           | 1230        |
|    time_elapsed         | 5263        |
|    total_timesteps      | 5038080     |
| train/                  |             |
|    approx_kl            | 0.004757857 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -280        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -27.6       |
|    n_updates            | 12290       |
|    policy_gradient_loss | -0.00308    |
|    std                  | 1.6         |
|    value_loss           | 0.849       |
-----------------------------------------
Iteration: 1230 | Episodes: 49900 | Median Reward: 44.46 | Max Reward: 45.95
Iteration: 1232 | Episodes: 50000 | Median Reward: 44.44 | Max Reward: 45.95
Iteration: 1235 | Episodes: 50100 | Median Reward: 44.58 | Max Reward: 45.95
Iteration: 1237 | Episodes: 50200 | Median Reward: 42.53 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.4        |
| time/                   |              |
|    fps                  | 953          |
|    iterations           | 1240         |
|    time_elapsed         | 5327         |
|    total_timesteps      | 5079040      |
| train/                  |              |
|    approx_kl            | 0.0031996712 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -281         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.8        |
|    n_updates            | 12390        |
|    policy_gradient_loss | -0.0037      |
|    std                  | 1.61         |
|    value_loss           | 0.93         |
------------------------------------------
Iteration: 1240 | Episodes: 50300 | Median Reward: 44.54 | Max Reward: 45.95
Iteration: 1242 | Episodes: 50400 | Median Reward: 43.91 | Max Reward: 45.95
Iteration: 1245 | Episodes: 50500 | Median Reward: 44.30 | Max Reward: 45.95
Iteration: 1247 | Episodes: 50600 | Median Reward: 44.31 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.4        |
| time/                   |              |
|    fps                  | 949          |
|    iterations           | 1250         |
|    time_elapsed         | 5390         |
|    total_timesteps      | 5120000      |
| train/                  |              |
|    approx_kl            | 0.0006364734 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -281         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28          |
|    n_updates            | 12490        |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.62         |
|    value_loss           | 0.487        |
------------------------------------------
Iteration: 1250 | Episodes: 50700 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1252 | Episodes: 50800 | Median Reward: 44.07 | Max Reward: 45.95
Iteration: 1255 | Episodes: 50900 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1257 | Episodes: 51000 | Median Reward: 44.47 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56         |
| time/                   |             |
|    fps                  | 945         |
|    iterations           | 1260        |
|    time_elapsed         | 5458        |
|    total_timesteps      | 5160960     |
| train/                  |             |
|    approx_kl            | 0.016653389 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -282        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -28.1       |
|    n_updates            | 12590       |
|    policy_gradient_loss | -0.0138     |
|    std                  | 1.63        |
|    value_loss           | 0.246       |
-----------------------------------------
Iteration: 1260 | Episodes: 51100 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1262 | Episodes: 51200 | Median Reward: 44.37 | Max Reward: 45.95
Iteration: 1264 | Episodes: 51300 | Median Reward: 43.21 | Max Reward: 45.95
Iteration: 1267 | Episodes: 51400 | Median Reward: 43.89 | Max Reward: 45.95
Iteration: 1269 | Episodes: 51500 | Median Reward: 44.49 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 942         |
|    iterations           | 1270        |
|    time_elapsed         | 5519        |
|    total_timesteps      | 5201920     |
| train/                  |             |
|    approx_kl            | 0.018144026 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.3         |
|    entropy_loss         | -283        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -28.1       |
|    n_updates            | 12690       |
|    policy_gradient_loss | -0.0288     |
|    std                  | 1.64        |
|    value_loss           | 0.677       |
-----------------------------------------
Iteration: 1272 | Episodes: 51600 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1274 | Episodes: 51700 | Median Reward: 44.51 | Max Reward: 45.95
Iteration: 1277 | Episodes: 51800 | Median Reward: 44.35 | Max Reward: 45.95
Iteration: 1279 | Episodes: 51900 | Median Reward: 44.58 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1280         |
|    time_elapsed         | 5568         |
|    total_timesteps      | 5242880      |
| train/                  |              |
|    approx_kl            | 5.071523e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -283         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27.6        |
|    n_updates            | 12790        |
|    policy_gradient_loss | -0.000181    |
|    std                  | 1.65         |
|    value_loss           | 0.988        |
------------------------------------------
Iteration: 1282 | Episodes: 52000 | Median Reward: 44.12 | Max Reward: 45.95
Iteration: 1284 | Episodes: 52100 | Median Reward: 44.66 | Max Reward: 45.95
Iteration: 1287 | Episodes: 52200 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1289 | Episodes: 52300 | Median Reward: 44.47 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.3         |
| time/                   |               |
|    fps                  | 941           |
|    iterations           | 1290          |
|    time_elapsed         | 5612          |
|    total_timesteps      | 5283840       |
| train/                  |               |
|    approx_kl            | 0.00023455266 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -283          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -28.3         |
|    n_updates            | 12890         |
|    policy_gradient_loss | 6.87e-05      |
|    std                  | 1.66          |
|    value_loss           | 0.469         |
-------------------------------------------
Iteration: 1292 | Episodes: 52400 | Median Reward: 44.13 | Max Reward: 45.95
Iteration: 1294 | Episodes: 52500 | Median Reward: 44.01 | Max Reward: 45.95
Iteration: 1297 | Episodes: 52600 | Median Reward: 44.02 | Max Reward: 45.95
Iteration: 1299 | Episodes: 52700 | Median Reward: 44.54 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1300         |
|    time_elapsed         | 5656         |
|    total_timesteps      | 5324800      |
| train/                  |              |
|    approx_kl            | 0.0014250283 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -284         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27.2        |
|    n_updates            | 12990        |
|    policy_gradient_loss | -0.00173     |
|    std                  | 1.67         |
|    value_loss           | 0.799        |
------------------------------------------
Iteration: 1301 | Episodes: 52800 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1304 | Episodes: 52900 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1306 | Episodes: 53000 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 1309 | Episodes: 53100 | Median Reward: 44.13 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1310         |
|    time_elapsed         | 5700         |
|    total_timesteps      | 5365760      |
| train/                  |              |
|    approx_kl            | 0.0022969139 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -284         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27.7        |
|    n_updates            | 13090        |
|    policy_gradient_loss | -0.00479     |
|    std                  | 1.68         |
|    value_loss           | 0.695        |
------------------------------------------
Iteration: 1311 | Episodes: 53200 | Median Reward: 44.20 | Max Reward: 45.95
Iteration: 1314 | Episodes: 53300 | Median Reward: 44.41 | Max Reward: 45.95
Iteration: 1316 | Episodes: 53400 | Median Reward: 44.01 | Max Reward: 45.95
Iteration: 1319 | Episodes: 53500 | Median Reward: 44.45 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.6       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 1320        |
|    time_elapsed         | 5744        |
|    total_timesteps      | 5406720     |
| train/                  |             |
|    approx_kl            | 0.002334984 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -284        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -27.5       |
|    n_updates            | 13190       |
|    policy_gradient_loss | 0.000261    |
|    std                  | 1.69        |
|    value_loss           | 1.28        |
-----------------------------------------
Iteration: 1321 | Episodes: 53600 | Median Reward: 44.35 | Max Reward: 45.95
Iteration: 1324 | Episodes: 53700 | Median Reward: 44.15 | Max Reward: 45.95
Iteration: 1326 | Episodes: 53800 | Median Reward: 43.88 | Max Reward: 45.95
Iteration: 1329 | Episodes: 53900 | Median Reward: 44.47 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.3       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 1330        |
|    time_elapsed         | 5788        |
|    total_timesteps      | 5447680     |
| train/                  |             |
|    approx_kl            | 6.53631e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -285        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -28         |
|    n_updates            | 13290       |
|    policy_gradient_loss | -6.5e-05    |
|    std                  | 1.69        |
|    value_loss           | 0.826       |
-----------------------------------------
Iteration: 1331 | Episodes: 54000 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1334 | Episodes: 54100 | Median Reward: 44.21 | Max Reward: 45.95
Iteration: 1336 | Episodes: 54200 | Median Reward: 44.47 | Max Reward: 45.95
Iteration: 1338 | Episodes: 54300 | Median Reward: 43.97 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.8        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1340         |
|    time_elapsed         | 5832         |
|    total_timesteps      | 5488640      |
| train/                  |              |
|    approx_kl            | 0.0060133664 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -285         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.1        |
|    n_updates            | 13390        |
|    policy_gradient_loss | -0.0266      |
|    std                  | 1.7          |
|    value_loss           | 1.43         |
------------------------------------------
Iteration: 1341 | Episodes: 54400 | Median Reward: 43.73 | Max Reward: 45.95
Iteration: 1343 | Episodes: 54500 | Median Reward: 44.58 | Max Reward: 45.95
Iteration: 1346 | Episodes: 54600 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1348 | Episodes: 54700 | Median Reward: 44.59 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.9         |
| time/                   |               |
|    fps                  | 941           |
|    iterations           | 1350          |
|    time_elapsed         | 5876          |
|    total_timesteps      | 5529600       |
| train/                  |               |
|    approx_kl            | 3.4201425e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -286          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -27.1         |
|    n_updates            | 13490         |
|    policy_gradient_loss | -0.00019      |
|    std                  | 1.71          |
|    value_loss           | 3.27          |
-------------------------------------------
Iteration: 1351 | Episodes: 54800 | Median Reward: 44.20 | Max Reward: 45.95
Iteration: 1353 | Episodes: 54900 | Median Reward: 43.70 | Max Reward: 45.95
Iteration: 1356 | Episodes: 55000 | Median Reward: 43.22 | Max Reward: 45.95
Iteration: 1358 | Episodes: 55100 | Median Reward: 44.38 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 940           |
|    iterations           | 1360          |
|    time_elapsed         | 5919          |
|    total_timesteps      | 5570560       |
| train/                  |               |
|    approx_kl            | 5.2024436e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -286          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -27.2         |
|    n_updates            | 13590         |
|    policy_gradient_loss | 7.81e-05      |
|    std                  | 1.72          |
|    value_loss           | 1.51          |
-------------------------------------------
Iteration: 1361 | Episodes: 55200 | Median Reward: 44.58 | Max Reward: 45.95
Iteration: 1363 | Episodes: 55300 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1366 | Episodes: 55400 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1368 | Episodes: 55500 | Median Reward: 44.35 | Max Reward: 45.95
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -58       |
| time/                   |           |
|    fps                  | 941       |
|    iterations           | 1370      |
|    time_elapsed         | 5962      |
|    total_timesteps      | 5611520   |
| train/                  |           |
|    approx_kl            | 0.1123454 |
|    clip_fraction        | 0.25      |
|    clip_range           | 0.3       |
|    entropy_loss         | -286      |
|    explained_variance   | 1         |
|    learning_rate        | 0.0005    |
|    loss                 | -28.2     |
|    n_updates            | 13690     |
|    policy_gradient_loss | -0.0402   |
|    std                  | 1.73      |
|    value_loss           | 0.542     |
---------------------------------------
Iteration: 1370 | Episodes: 55600 | Median Reward: 42.11 | Max Reward: 45.95
Iteration: 1373 | Episodes: 55700 | Median Reward: 44.20 | Max Reward: 45.95
Iteration: 1375 | Episodes: 55800 | Median Reward: 44.25 | Max Reward: 45.95
Iteration: 1378 | Episodes: 55900 | Median Reward: 43.21 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 940          |
|    iterations           | 1380         |
|    time_elapsed         | 6007         |
|    total_timesteps      | 5652480      |
| train/                  |              |
|    approx_kl            | 0.0010487785 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -287         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.5        |
|    n_updates            | 13790        |
|    policy_gradient_loss | -7.58e-05    |
|    std                  | 1.74         |
|    value_loss           | 0.448        |
------------------------------------------
Iteration: 1380 | Episodes: 56000 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 1383 | Episodes: 56100 | Median Reward: 44.46 | Max Reward: 45.95
Iteration: 1385 | Episodes: 56200 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1388 | Episodes: 56300 | Median Reward: 44.57 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.3        |
| time/                   |              |
|    fps                  | 940          |
|    iterations           | 1390         |
|    time_elapsed         | 6050         |
|    total_timesteps      | 5693440      |
| train/                  |              |
|    approx_kl            | 0.0010539066 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -288         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.5        |
|    n_updates            | 13890        |
|    policy_gradient_loss | -0.00256     |
|    std                  | 1.75         |
|    value_loss           | 0.493        |
------------------------------------------
Iteration: 1390 | Episodes: 56400 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 1393 | Episodes: 56500 | Median Reward: 44.39 | Max Reward: 45.95
Iteration: 1395 | Episodes: 56600 | Median Reward: 43.97 | Max Reward: 45.95
Iteration: 1398 | Episodes: 56700 | Median Reward: 44.58 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 1400        |
|    time_elapsed         | 6093        |
|    total_timesteps      | 5734400     |
| train/                  |             |
|    approx_kl            | 0.001435973 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -288        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -28.7       |
|    n_updates            | 13990       |
|    policy_gradient_loss | -0.00497    |
|    std                  | 1.76        |
|    value_loss           | 0.6         |
-----------------------------------------
Iteration: 1400 | Episodes: 56800 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1403 | Episodes: 56900 | Median Reward: 44.64 | Max Reward: 45.95
Iteration: 1405 | Episodes: 57000 | Median Reward: 44.65 | Max Reward: 45.95
Iteration: 1407 | Episodes: 57100 | Median Reward: 44.54 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1410         |
|    time_elapsed         | 6137         |
|    total_timesteps      | 5775360      |
| train/                  |              |
|    approx_kl            | 0.0068825325 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -290         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -28.6        |
|    n_updates            | 14090        |
|    policy_gradient_loss | -0.00106     |
|    std                  | 1.77         |
|    value_loss           | 0.372        |
------------------------------------------
Iteration: 1410 | Episodes: 57200 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1412 | Episodes: 57300 | Median Reward: 44.59 | Max Reward: 45.95
Iteration: 1415 | Episodes: 57400 | Median Reward: 44.64 | Max Reward: 45.95
Iteration: 1417 | Episodes: 57500 | Median Reward: 43.21 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.3       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 1420        |
|    time_elapsed         | 6180        |
|    total_timesteps      | 5816320     |
| train/                  |             |
|    approx_kl            | 0.010469206 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -291        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -28.6       |
|    n_updates            | 14190       |
|    policy_gradient_loss | -0.0134     |
|    std                  | 1.77        |
|    value_loss           | 0.463       |
-----------------------------------------
Iteration: 1420 | Episodes: 57600 | Median Reward: 44.22 | Max Reward: 45.95
Iteration: 1422 | Episodes: 57700 | Median Reward: 44.22 | Max Reward: 45.95
Iteration: 1425 | Episodes: 57800 | Median Reward: 44.56 | Max Reward: 45.95
Iteration: 1427 | Episodes: 57900 | Median Reward: 44.04 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.6         |
| time/                   |               |
|    fps                  | 940           |
|    iterations           | 1430          |
|    time_elapsed         | 6224          |
|    total_timesteps      | 5857280       |
| train/                  |               |
|    approx_kl            | 4.2749307e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -291          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -28.3         |
|    n_updates            | 14290         |
|    policy_gradient_loss | -1.84e-05     |
|    std                  | 1.78          |
|    value_loss           | 1.53          |
-------------------------------------------
Iteration: 1430 | Episodes: 58000 | Median Reward: 44.56 | Max Reward: 45.95
Iteration: 1432 | Episodes: 58100 | Median Reward: 44.52 | Max Reward: 45.95
Iteration: 1435 | Episodes: 58200 | Median Reward: 44.34 | Max Reward: 45.95
Iteration: 1437 | Episodes: 58300 | Median Reward: 44.21 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.2         |
| time/                   |               |
|    fps                  | 941           |
|    iterations           | 1440          |
|    time_elapsed         | 6267          |
|    total_timesteps      | 5898240       |
| train/                  |               |
|    approx_kl            | 1.9218234e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -291          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -29           |
|    n_updates            | 14390         |
|    policy_gradient_loss | -4.43e-06     |
|    std                  | 1.78          |
|    value_loss           | 0.487         |
-------------------------------------------
Iteration: 1440 | Episodes: 58400 | Median Reward: 44.57 | Max Reward: 45.95
Iteration: 1442 | Episodes: 58500 | Median Reward: 44.16 | Max Reward: 45.95
Iteration: 1444 | Episodes: 58600 | Median Reward: 44.58 | Max Reward: 45.95
Iteration: 1447 | Episodes: 58700 | Median Reward: 44.21 | Max Reward: 45.95
Iteration: 1449 | Episodes: 58800 | Median Reward: 44.22 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.5         |
| time/                   |               |
|    fps                  | 941           |
|    iterations           | 1450          |
|    time_elapsed         | 6311          |
|    total_timesteps      | 5939200       |
| train/                  |               |
|    approx_kl            | 0.00037043262 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -292          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -28.8         |
|    n_updates            | 14490         |
|    policy_gradient_loss | -0.000669     |
|    std                  | 1.79          |
|    value_loss           | 0.652         |
-------------------------------------------
Iteration: 1452 | Episodes: 58900 | Median Reward: 44.25 | Max Reward: 45.95
Iteration: 1454 | Episodes: 59000 | Median Reward: 44.30 | Max Reward: 45.95
Iteration: 1457 | Episodes: 59100 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1459 | Episodes: 59200 | Median Reward: 44.45 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.9         |
| time/                   |               |
|    fps                  | 941           |
|    iterations           | 1460          |
|    time_elapsed         | 6354          |
|    total_timesteps      | 5980160       |
| train/                  |               |
|    approx_kl            | 6.8194044e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -292          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -29.1         |
|    n_updates            | 14590         |
|    policy_gradient_loss | -0.000222     |
|    std                  | 1.8           |
|    value_loss           | 0.172         |
-------------------------------------------
Iteration: 1462 | Episodes: 59300 | Median Reward: 44.17 | Max Reward: 45.95
Iteration: 1464 | Episodes: 59400 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 1467 | Episodes: 59500 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1469 | Episodes: 59600 | Median Reward: 44.43 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1470         |
|    time_elapsed         | 6397         |
|    total_timesteps      | 6021120      |
| train/                  |              |
|    approx_kl            | 0.0002636304 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -292         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29          |
|    n_updates            | 14690        |
|    policy_gradient_loss | -0.001       |
|    std                  | 1.8          |
|    value_loss           | 0.54         |
------------------------------------------
Iteration: 1472 | Episodes: 59700 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 1474 | Episodes: 59800 | Median Reward: 44.65 | Max Reward: 45.95
Iteration: 1477 | Episodes: 59900 | Median Reward: 44.50 | Max Reward: 45.95
Iteration: 1479 | Episodes: 60000 | Median Reward: 44.67 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 941           |
|    iterations           | 1480          |
|    time_elapsed         | 6441          |
|    total_timesteps      | 6062080       |
| train/                  |               |
|    approx_kl            | 0.00077193376 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -292          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -29.1         |
|    n_updates            | 14790         |
|    policy_gradient_loss | 7.33e-05      |
|    std                  | 1.81          |
|    value_loss           | 0.537         |
-------------------------------------------
Iteration: 1481 | Episodes: 60100 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 1484 | Episodes: 60200 | Median Reward: 43.52 | Max Reward: 45.95
Iteration: 1486 | Episodes: 60300 | Median Reward: 44.52 | Max Reward: 45.95
Iteration: 1489 | Episodes: 60400 | Median Reward: 44.31 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1490         |
|    time_elapsed         | 6484         |
|    total_timesteps      | 6103040      |
| train/                  |              |
|    approx_kl            | 0.0059008347 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -293         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.3        |
|    n_updates            | 14890        |
|    policy_gradient_loss | -0.0224      |
|    std                  | 1.82         |
|    value_loss           | 1.5          |
------------------------------------------
Iteration: 1491 | Episodes: 60500 | Median Reward: 44.32 | Max Reward: 45.95
Iteration: 1494 | Episodes: 60600 | Median Reward: 44.64 | Max Reward: 45.95
Iteration: 1496 | Episodes: 60700 | Median Reward: 44.50 | Max Reward: 45.95
Iteration: 1499 | Episodes: 60800 | Median Reward: 44.64 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.7        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1500         |
|    time_elapsed         | 6527         |
|    total_timesteps      | 6144000      |
| train/                  |              |
|    approx_kl            | 0.0044612205 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -293         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.5        |
|    n_updates            | 14990        |
|    policy_gradient_loss | -0.0121      |
|    std                  | 1.82         |
|    value_loss           | 1.32         |
------------------------------------------
Iteration: 1501 | Episodes: 60900 | Median Reward: 44.22 | Max Reward: 45.95
Iteration: 1504 | Episodes: 61000 | Median Reward: 44.33 | Max Reward: 45.95
Iteration: 1506 | Episodes: 61100 | Median Reward: 44.66 | Max Reward: 45.95
Iteration: 1509 | Episodes: 61200 | Median Reward: 44.49 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.8        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1510         |
|    time_elapsed         | 6570         |
|    total_timesteps      | 6184960      |
| train/                  |              |
|    approx_kl            | 0.0002099591 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -293         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29.2        |
|    n_updates            | 15090        |
|    policy_gradient_loss | -0.000267    |
|    std                  | 1.83         |
|    value_loss           | 0.456        |
------------------------------------------
Iteration: 1511 | Episodes: 61300 | Median Reward: 44.25 | Max Reward: 45.95
Iteration: 1514 | Episodes: 61400 | Median Reward: 43.91 | Max Reward: 45.95
Iteration: 1516 | Episodes: 61500 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1518 | Episodes: 61600 | Median Reward: 44.25 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.5        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1520         |
|    time_elapsed         | 6613         |
|    total_timesteps      | 6225920      |
| train/                  |              |
|    approx_kl            | 0.0014771647 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -294         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29.2        |
|    n_updates            | 15190        |
|    policy_gradient_loss | -0.00824     |
|    std                  | 1.84         |
|    value_loss           | 2.1          |
------------------------------------------
Iteration: 1521 | Episodes: 61700 | Median Reward: 44.62 | Max Reward: 45.95
Iteration: 1523 | Episodes: 61800 | Median Reward: 44.50 | Max Reward: 45.95
Iteration: 1526 | Episodes: 61900 | Median Reward: 43.91 | Max Reward: 45.95
Iteration: 1528 | Episodes: 62000 | Median Reward: 44.42 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1530         |
|    time_elapsed         | 6656         |
|    total_timesteps      | 6266880      |
| train/                  |              |
|    approx_kl            | 0.0020716977 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -295         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -29.1        |
|    n_updates            | 15290        |
|    policy_gradient_loss | -0.00632     |
|    std                  | 1.85         |
|    value_loss           | 0.818        |
------------------------------------------
Iteration: 1531 | Episodes: 62100 | Median Reward: 44.35 | Max Reward: 45.95
Iteration: 1533 | Episodes: 62200 | Median Reward: 44.19 | Max Reward: 45.95
Iteration: 1536 | Episodes: 62300 | Median Reward: 44.41 | Max Reward: 45.95
Iteration: 1538 | Episodes: 62400 | Median Reward: 44.64 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.8       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 1540        |
|    time_elapsed         | 6700        |
|    total_timesteps      | 6307840     |
| train/                  |             |
|    approx_kl            | 0.004148078 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -295        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -29.4       |
|    n_updates            | 15390       |
|    policy_gradient_loss | -0.0107     |
|    std                  | 1.86        |
|    value_loss           | 0.932       |
-----------------------------------------
Iteration: 1541 | Episodes: 62500 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 1543 | Episodes: 62600 | Median Reward: 44.54 | Max Reward: 45.95
Iteration: 1546 | Episodes: 62700 | Median Reward: 44.56 | Max Reward: 45.95
Iteration: 1548 | Episodes: 62800 | Median Reward: 44.53 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1550         |
|    time_elapsed         | 6743         |
|    total_timesteps      | 6348800      |
| train/                  |              |
|    approx_kl            | 0.0015292035 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -295         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.9        |
|    n_updates            | 15490        |
|    policy_gradient_loss | -0.00242     |
|    std                  | 1.87         |
|    value_loss           | 0.413        |
------------------------------------------
Iteration: 1551 | Episodes: 62900 | Median Reward: 44.65 | Max Reward: 45.95
Iteration: 1553 | Episodes: 63000 | Median Reward: 44.21 | Max Reward: 45.95
Iteration: 1555 | Episodes: 63100 | Median Reward: 44.64 | Max Reward: 45.95
Iteration: 1558 | Episodes: 63200 | Median Reward: 44.41 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1560         |
|    time_elapsed         | 6786         |
|    total_timesteps      | 6389760      |
| train/                  |              |
|    approx_kl            | 0.0014061332 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -296         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -28.5        |
|    n_updates            | 15590        |
|    policy_gradient_loss | -0.00142     |
|    std                  | 1.88         |
|    value_loss           | 1.01         |
------------------------------------------
Iteration: 1560 | Episodes: 63300 | Median Reward: 44.58 | Max Reward: 45.95
Iteration: 1563 | Episodes: 63400 | Median Reward: 44.02 | Max Reward: 45.95
Iteration: 1565 | Episodes: 63500 | Median Reward: 44.12 | Max Reward: 45.95
Iteration: 1568 | Episodes: 63600 | Median Reward: 44.49 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.5       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 1570        |
|    time_elapsed         | 6829        |
|    total_timesteps      | 6430720     |
| train/                  |             |
|    approx_kl            | 0.017885482 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -296        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -28.4       |
|    n_updates            | 15690       |
|    policy_gradient_loss | -0.0415     |
|    std                  | 1.89        |
|    value_loss           | 0.905       |
-----------------------------------------
Iteration: 1570 | Episodes: 63700 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1573 | Episodes: 63800 | Median Reward: 44.15 | Max Reward: 45.95
Iteration: 1575 | Episodes: 63900 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1578 | Episodes: 64000 | Median Reward: 44.46 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.1        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1580         |
|    time_elapsed         | 6872         |
|    total_timesteps      | 6471680      |
| train/                  |              |
|    approx_kl            | 0.0008506958 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -296         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29.6        |
|    n_updates            | 15790        |
|    policy_gradient_loss | -0.00176     |
|    std                  | 1.9          |
|    value_loss           | 0.223        |
------------------------------------------
Iteration: 1580 | Episodes: 64100 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1583 | Episodes: 64200 | Median Reward: 44.47 | Max Reward: 45.95
Iteration: 1585 | Episodes: 64300 | Median Reward: 44.18 | Max Reward: 45.95
Iteration: 1587 | Episodes: 64400 | Median Reward: 44.47 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 1590        |
|    time_elapsed         | 6915        |
|    total_timesteps      | 6512640     |
| train/                  |             |
|    approx_kl            | 0.010894141 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -297        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -29.6       |
|    n_updates            | 15890       |
|    policy_gradient_loss | -0.0371     |
|    std                  | 1.91        |
|    value_loss           | 0.813       |
-----------------------------------------
Iteration: 1590 | Episodes: 64500 | Median Reward: 43.88 | Max Reward: 45.95
Iteration: 1592 | Episodes: 64600 | Median Reward: 43.87 | Max Reward: 45.95
Iteration: 1595 | Episodes: 64700 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1597 | Episodes: 64800 | Median Reward: 44.12 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1600         |
|    time_elapsed         | 6958         |
|    total_timesteps      | 6553600      |
| train/                  |              |
|    approx_kl            | 0.0037222956 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -297         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29.6        |
|    n_updates            | 15990        |
|    policy_gradient_loss | -0.0111      |
|    std                  | 1.91         |
|    value_loss           | 0.445        |
------------------------------------------
Iteration: 1600 | Episodes: 64900 | Median Reward: 44.25 | Max Reward: 45.95
Iteration: 1602 | Episodes: 65000 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1605 | Episodes: 65100 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 1607 | Episodes: 65200 | Median Reward: 44.48 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 1610         |
|    time_elapsed         | 7001         |
|    total_timesteps      | 6594560      |
| train/                  |              |
|    approx_kl            | 0.0009392328 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -298         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.5        |
|    n_updates            | 16090        |
|    policy_gradient_loss | -0.00313     |
|    std                  | 1.92         |
|    value_loss           | 1.28         |
------------------------------------------
Iteration: 1610 | Episodes: 65300 | Median Reward: 44.07 | Max Reward: 45.95
Iteration: 1612 | Episodes: 65400 | Median Reward: 43.87 | Max Reward: 45.95
Iteration: 1615 | Episodes: 65500 | Median Reward: 44.51 | Max Reward: 45.95
Iteration: 1617 | Episodes: 65600 | Median Reward: 44.21 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.3         |
| time/                   |               |
|    fps                  | 942           |
|    iterations           | 1620          |
|    time_elapsed         | 7043          |
|    total_timesteps      | 6635520       |
| train/                  |               |
|    approx_kl            | 4.4110027e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -298          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -29.2         |
|    n_updates            | 16190         |
|    policy_gradient_loss | -5.25e-05     |
|    std                  | 1.93          |
|    value_loss           | 1.38          |
-------------------------------------------
Iteration: 1620 | Episodes: 65700 | Median Reward: 43.62 | Max Reward: 45.95
Iteration: 1622 | Episodes: 65800 | Median Reward: 43.75 | Max Reward: 45.95
Iteration: 1624 | Episodes: 65900 | Median Reward: 43.88 | Max Reward: 45.95
Iteration: 1627 | Episodes: 66000 | Median Reward: 44.41 | Max Reward: 45.95
Iteration: 1629 | Episodes: 66100 | Median Reward: 44.56 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.8        |
| time/                   |              |
|    fps                  | 942          |
|    iterations           | 1630         |
|    time_elapsed         | 7086         |
|    total_timesteps      | 6676480      |
| train/                  |              |
|    approx_kl            | 7.082414e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -298         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29.5        |
|    n_updates            | 16290        |
|    policy_gradient_loss | -0.000416    |
|    std                  | 1.94         |
|    value_loss           | 0.395        |
------------------------------------------
Iteration: 1632 | Episodes: 66200 | Median Reward: 44.39 | Max Reward: 45.95
Iteration: 1634 | Episodes: 66300 | Median Reward: 44.58 | Max Reward: 45.95
Iteration: 1637 | Episodes: 66400 | Median Reward: 44.12 | Max Reward: 45.95
Iteration: 1639 | Episodes: 66500 | Median Reward: 44.18 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.3         |
| time/                   |               |
|    fps                  | 942           |
|    iterations           | 1640          |
|    time_elapsed         | 7128          |
|    total_timesteps      | 6717440       |
| train/                  |               |
|    approx_kl            | 0.00039949996 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -299          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -29           |
|    n_updates            | 16390         |
|    policy_gradient_loss | 0.000613      |
|    std                  | 1.95          |
|    value_loss           | 0.66          |
-------------------------------------------
Iteration: 1642 | Episodes: 66600 | Median Reward: 41.43 | Max Reward: 45.95
Iteration: 1644 | Episodes: 66700 | Median Reward: 44.00 | Max Reward: 45.95
Iteration: 1647 | Episodes: 66800 | Median Reward: 44.00 | Max Reward: 45.95
Iteration: 1649 | Episodes: 66900 | Median Reward: 44.44 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 942          |
|    iterations           | 1650         |
|    time_elapsed         | 7171         |
|    total_timesteps      | 6758400      |
| train/                  |              |
|    approx_kl            | 0.0052322447 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -299         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29.1        |
|    n_updates            | 16490        |
|    policy_gradient_loss | -0.00881     |
|    std                  | 1.97         |
|    value_loss           | 0.738        |
------------------------------------------
Iteration: 1652 | Episodes: 67000 | Median Reward: 43.97 | Max Reward: 45.95
Iteration: 1654 | Episodes: 67100 | Median Reward: 44.41 | Max Reward: 45.95
Iteration: 1657 | Episodes: 67200 | Median Reward: 44.37 | Max Reward: 45.95
Iteration: 1659 | Episodes: 67300 | Median Reward: 43.20 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.4         |
| time/                   |               |
|    fps                  | 940           |
|    iterations           | 1660          |
|    time_elapsed         | 7227          |
|    total_timesteps      | 6799360       |
| train/                  |               |
|    approx_kl            | 0.00083565485 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -300          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -29.9         |
|    n_updates            | 16590         |
|    policy_gradient_loss | -0.0012       |
|    std                  | 1.98          |
|    value_loss           | 0.581         |
-------------------------------------------
Iteration: 1661 | Episodes: 67400 | Median Reward: 43.68 | Max Reward: 45.95
Iteration: 1664 | Episodes: 67500 | Median Reward: 44.36 | Max Reward: 45.95
Iteration: 1666 | Episodes: 67600 | Median Reward: 44.58 | Max Reward: 45.95
Iteration: 1669 | Episodes: 67700 | Median Reward: 44.64 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.8         |
| time/                   |               |
|    fps                  | 937           |
|    iterations           | 1670          |
|    time_elapsed         | 7296          |
|    total_timesteps      | 6840320       |
| train/                  |               |
|    approx_kl            | 1.6370206e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -301          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -29.9         |
|    n_updates            | 16690         |
|    policy_gradient_loss | 7.17e-06      |
|    std                  | 1.99          |
|    value_loss           | 0.438         |
-------------------------------------------
Iteration: 1671 | Episodes: 67800 | Median Reward: 44.10 | Max Reward: 45.95
Iteration: 1674 | Episodes: 67900 | Median Reward: 44.02 | Max Reward: 45.95
Iteration: 1676 | Episodes: 68000 | Median Reward: 43.81 | Max Reward: 45.95
Iteration: 1679 | Episodes: 68100 | Median Reward: 44.53 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 935          |
|    iterations           | 1680         |
|    time_elapsed         | 7357         |
|    total_timesteps      | 6881280      |
| train/                  |              |
|    approx_kl            | 0.0001494068 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -301         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -30          |
|    n_updates            | 16790        |
|    policy_gradient_loss | -0.000326    |
|    std                  | 2.01         |
|    value_loss           | 0.582        |
------------------------------------------
Iteration: 1681 | Episodes: 68200 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1684 | Episodes: 68300 | Median Reward: 44.11 | Max Reward: 45.95
Iteration: 1686 | Episodes: 68400 | Median Reward: 44.66 | Max Reward: 45.95
Iteration: 1689 | Episodes: 68500 | Median Reward: 44.39 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.2         |
| time/                   |               |
|    fps                  | 932           |
|    iterations           | 1690          |
|    time_elapsed         | 7421          |
|    total_timesteps      | 6922240       |
| train/                  |               |
|    approx_kl            | 0.00010019766 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -301          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -30           |
|    n_updates            | 16890         |
|    policy_gradient_loss | -0.000619     |
|    std                  | 2.01          |
|    value_loss           | 0.604         |
-------------------------------------------
Iteration: 1691 | Episodes: 68600 | Median Reward: 44.50 | Max Reward: 45.95
Iteration: 1694 | Episodes: 68700 | Median Reward: 44.46 | Max Reward: 45.95
Iteration: 1696 | Episodes: 68800 | Median Reward: 44.65 | Max Reward: 45.95
Iteration: 1698 | Episodes: 68900 | Median Reward: 44.27 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.5       |
| time/                   |             |
|    fps                  | 930         |
|    iterations           | 1700        |
|    time_elapsed         | 7484        |
|    total_timesteps      | 6963200     |
| train/                  |             |
|    approx_kl            | 0.004172935 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -302        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -30.1       |
|    n_updates            | 16990       |
|    policy_gradient_loss | -0.00559    |
|    std                  | 2.03        |
|    value_loss           | 0.438       |
-----------------------------------------
Iteration: 1701 | Episodes: 69000 | Median Reward: 40.91 | Max Reward: 45.95
Iteration: 1703 | Episodes: 69100 | Median Reward: 44.59 | Max Reward: 45.95
Iteration: 1706 | Episodes: 69200 | Median Reward: 44.53 | Max Reward: 45.95
Iteration: 1708 | Episodes: 69300 | Median Reward: 44.47 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.5        |
| time/                   |              |
|    fps                  | 927          |
|    iterations           | 1710         |
|    time_elapsed         | 7548         |
|    total_timesteps      | 7004160      |
| train/                  |              |
|    approx_kl            | 0.0003440837 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -303         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29.2        |
|    n_updates            | 17090        |
|    policy_gradient_loss | -0.00157     |
|    std                  | 2.04         |
|    value_loss           | 0.624        |
------------------------------------------
Iteration: 1711 | Episodes: 69400 | Median Reward: 41.36 | Max Reward: 45.95
Iteration: 1713 | Episodes: 69500 | Median Reward: 43.84 | Max Reward: 45.95
Iteration: 1716 | Episodes: 69600 | Median Reward: 44.54 | Max Reward: 45.95
Iteration: 1718 | Episodes: 69700 | Median Reward: 44.25 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.5         |
| time/                   |               |
|    fps                  | 925           |
|    iterations           | 1720          |
|    time_elapsed         | 7613          |
|    total_timesteps      | 7045120       |
| train/                  |               |
|    approx_kl            | 3.1176314e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -304          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -30.3         |
|    n_updates            | 17190         |
|    policy_gradient_loss | -8.51e-06     |
|    std                  | 2.06          |
|    value_loss           | 0.423         |
-------------------------------------------
Iteration: 1721 | Episodes: 69800 | Median Reward: 43.32 | Max Reward: 45.95
Iteration: 1723 | Episodes: 69900 | Median Reward: 43.44 | Max Reward: 45.95
Iteration: 1726 | Episodes: 70000 | Median Reward: 44.47 | Max Reward: 45.95
Iteration: 1728 | Episodes: 70100 | Median Reward: 43.90 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.2        |
| time/                   |              |
|    fps                  | 923          |
|    iterations           | 1730         |
|    time_elapsed         | 7669         |
|    total_timesteps      | 7086080      |
| train/                  |              |
|    approx_kl            | 0.0015618899 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -304         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -30.1        |
|    n_updates            | 17290        |
|    policy_gradient_loss | -0.00356     |
|    std                  | 2.07         |
|    value_loss           | 1.07         |
------------------------------------------
Iteration: 1731 | Episodes: 70200 | Median Reward: 43.26 | Max Reward: 45.95
Iteration: 1733 | Episodes: 70300 | Median Reward: 43.22 | Max Reward: 45.95
Iteration: 1735 | Episodes: 70400 | Median Reward: 44.21 | Max Reward: 45.95
Iteration: 1738 | Episodes: 70500 | Median Reward: 44.33 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.1         |
| time/                   |               |
|    fps                  | 922           |
|    iterations           | 1740          |
|    time_elapsed         | 7723          |
|    total_timesteps      | 7127040       |
| train/                  |               |
|    approx_kl            | 0.00022091735 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -304          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -30.3         |
|    n_updates            | 17390         |
|    policy_gradient_loss | 0.000264      |
|    std                  | 2.08          |
|    value_loss           | 0.597         |
-------------------------------------------
Iteration: 1740 | Episodes: 70600 | Median Reward: 44.48 | Max Reward: 45.95
Iteration: 1743 | Episodes: 70700 | Median Reward: 44.25 | Max Reward: 45.95
Iteration: 1745 | Episodes: 70800 | Median Reward: 43.13 | Max Reward: 45.95
Iteration: 1748 | Episodes: 70900 | Median Reward: 43.12 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.2         |
| time/                   |               |
|    fps                  | 922           |
|    iterations           | 1750          |
|    time_elapsed         | 7769          |
|    total_timesteps      | 7168000       |
| train/                  |               |
|    approx_kl            | 0.00010923807 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -305          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -29.5         |
|    n_updates            | 17490         |
|    policy_gradient_loss | -0.000165     |
|    std                  | 2.09          |
|    value_loss           | 1.23          |
-------------------------------------------
Iteration: 1750 | Episodes: 71000 | Median Reward: 44.44 | Max Reward: 45.95
Iteration: 1753 | Episodes: 71100 | Median Reward: 44.50 | Max Reward: 45.95
Iteration: 1755 | Episodes: 71200 | Median Reward: 44.14 | Max Reward: 45.95
Iteration: 1758 | Episodes: 71300 | Median Reward: 44.25 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.4         |
| time/                   |               |
|    fps                  | 922           |
|    iterations           | 1760          |
|    time_elapsed         | 7813          |
|    total_timesteps      | 7208960       |
| train/                  |               |
|    approx_kl            | 1.1606171e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -305          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -30.4         |
|    n_updates            | 17590         |
|    policy_gradient_loss | -4.05e-05     |
|    std                  | 2.11          |
|    value_loss           | 2.02          |
-------------------------------------------
Iteration: 1760 | Episodes: 71400 | Median Reward: 44.39 | Max Reward: 45.95
Iteration: 1763 | Episodes: 71500 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 1765 | Episodes: 71600 | Median Reward: 44.47 | Max Reward: 45.95
Iteration: 1767 | Episodes: 71700 | Median Reward: 44.53 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.1        |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 1770         |
|    time_elapsed         | 7857         |
|    total_timesteps      | 7249920      |
| train/                  |              |
|    approx_kl            | 8.630232e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -305         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.2        |
|    n_updates            | 17690        |
|    policy_gradient_loss | -2.23e-05    |
|    std                  | 2.12         |
|    value_loss           | 1.6          |
------------------------------------------
Iteration: 1770 | Episodes: 71800 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1772 | Episodes: 71900 | Median Reward: 44.54 | Max Reward: 45.95
Iteration: 1775 | Episodes: 72000 | Median Reward: 43.86 | Max Reward: 45.95
Iteration: 1777 | Episodes: 72100 | Median Reward: 44.18 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.1         |
| time/                   |               |
|    fps                  | 922           |
|    iterations           | 1780          |
|    time_elapsed         | 7902          |
|    total_timesteps      | 7290880       |
| train/                  |               |
|    approx_kl            | 8.1616745e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -306          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -29.5         |
|    n_updates            | 17790         |
|    policy_gradient_loss | -5.22e-07     |
|    std                  | 2.13          |
|    value_loss           | 1.23          |
-------------------------------------------
Iteration: 1780 | Episodes: 72200 | Median Reward: 44.33 | Max Reward: 45.95
Iteration: 1782 | Episodes: 72300 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1785 | Episodes: 72400 | Median Reward: 44.22 | Max Reward: 45.95
Iteration: 1787 | Episodes: 72500 | Median Reward: 44.21 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.7       |
| time/                   |             |
|    fps                  | 922         |
|    iterations           | 1790        |
|    time_elapsed         | 7946        |
|    total_timesteps      | 7331840     |
| train/                  |             |
|    approx_kl            | 0.003872996 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -306        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -30.4       |
|    n_updates            | 17890       |
|    policy_gradient_loss | -0.00413    |
|    std                  | 2.14        |
|    value_loss           | 0.4         |
-----------------------------------------
Iteration: 1790 | Episodes: 72600 | Median Reward: 44.20 | Max Reward: 45.95
Iteration: 1792 | Episodes: 72700 | Median Reward: 44.25 | Max Reward: 45.95
Iteration: 1795 | Episodes: 72800 | Median Reward: 43.21 | Max Reward: 45.95
Iteration: 1797 | Episodes: 72900 | Median Reward: 44.58 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.1        |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 1800         |
|    time_elapsed         | 7988         |
|    total_timesteps      | 7372800      |
| train/                  |              |
|    approx_kl            | 8.894419e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -307         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -30.3        |
|    n_updates            | 17990        |
|    policy_gradient_loss | -0.000446    |
|    std                  | 2.16         |
|    value_loss           | 0.821        |
------------------------------------------
Iteration: 1800 | Episodes: 73000 | Median Reward: 44.67 | Max Reward: 45.95
Iteration: 1802 | Episodes: 73100 | Median Reward: 44.14 | Max Reward: 45.95
Iteration: 1804 | Episodes: 73200 | Median Reward: 44.73 | Max Reward: 45.95
Iteration: 1807 | Episodes: 73300 | Median Reward: 44.63 | Max Reward: 45.95
Iteration: 1809 | Episodes: 73400 | Median Reward: 44.33 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 923          |
|    iterations           | 1810         |
|    time_elapsed         | 8031         |
|    total_timesteps      | 7413760      |
| train/                  |              |
|    approx_kl            | 0.0009789354 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -307         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -30.6        |
|    n_updates            | 18090        |
|    policy_gradient_loss | -0.00183     |
|    std                  | 2.17         |
|    value_loss           | 0.148        |
------------------------------------------
Iteration: 1812 | Episodes: 73500 | Median Reward: 44.31 | Max Reward: 45.95
Iteration: 1814 | Episodes: 73600 | Median Reward: 44.36 | Max Reward: 45.95
Iteration: 1817 | Episodes: 73700 | Median Reward: 44.07 | Max Reward: 45.95
Iteration: 1819 | Episodes: 73800 | Median Reward: 44.15 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.3       |
| time/                   |             |
|    fps                  | 923         |
|    iterations           | 1820        |
|    time_elapsed         | 8074        |
|    total_timesteps      | 7454720     |
| train/                  |             |
|    approx_kl            | 0.000878707 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -307        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -30.6       |
|    n_updates            | 18190       |
|    policy_gradient_loss | -0.00214    |
|    std                  | 2.19        |
|    value_loss           | 0.661       |
-----------------------------------------
Iteration: 1822 | Episodes: 73900 | Median Reward: 44.52 | Max Reward: 45.95
Iteration: 1824 | Episodes: 74000 | Median Reward: 44.61 | Max Reward: 45.95
Iteration: 1827 | Episodes: 74100 | Median Reward: 44.10 | Max Reward: 45.95
Iteration: 1829 | Episodes: 74200 | Median Reward: 43.85 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.9       |
| time/                   |             |
|    fps                  | 923         |
|    iterations           | 1830        |
|    time_elapsed         | 8117        |
|    total_timesteps      | 7495680     |
| train/                  |             |
|    approx_kl            | 0.028956696 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.3         |
|    entropy_loss         | -307        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -30.5       |
|    n_updates            | 18290       |
|    policy_gradient_loss | -0.0424     |
|    std                  | 2.2         |
|    value_loss           | 2.34        |
-----------------------------------------
Iteration: 1832 | Episodes: 74300 | Median Reward: 43.32 | Max Reward: 45.95
Iteration: 1834 | Episodes: 74400 | Median Reward: 44.66 | Max Reward: 45.95
Iteration: 1837 | Episodes: 74500 | Median Reward: 44.45 | Max Reward: 45.95
Iteration: 1839 | Episodes: 74600 | Median Reward: 44.49 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 923           |
|    iterations           | 1840          |
|    time_elapsed         | 8159          |
|    total_timesteps      | 7536640       |
| train/                  |               |
|    approx_kl            | 4.3812775e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -308          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -30.2         |
|    n_updates            | 18390         |
|    policy_gradient_loss | -0.000171     |
|    std                  | 2.21          |
|    value_loss           | 0.574         |
-------------------------------------------
Iteration: 1841 | Episodes: 74700 | Median Reward: 44.14 | Max Reward: 45.95
Iteration: 1844 | Episodes: 74800 | Median Reward: 44.32 | Max Reward: 45.95
Iteration: 1846 | Episodes: 74900 | Median Reward: 44.14 | Max Reward: 45.95
Iteration: 1849 | Episodes: 75000 | Median Reward: 44.17 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 923          |
|    iterations           | 1850         |
|    time_elapsed         | 8202         |
|    total_timesteps      | 7577600      |
| train/                  |              |
|    approx_kl            | 0.0011246726 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -308         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -30.8        |
|    n_updates            | 18490        |
|    policy_gradient_loss | -0.00102     |
|    std                  | 2.22         |
|    value_loss           | 0.483        |
------------------------------------------
Iteration: 1851 | Episodes: 75100 | Median Reward: 44.59 | Max Reward: 45.95
Iteration: 1854 | Episodes: 75200 | Median Reward: 43.65 | Max Reward: 45.95
Iteration: 1856 | Episodes: 75300 | Median Reward: 44.53 | Max Reward: 45.95
Iteration: 1859 | Episodes: 75400 | Median Reward: 44.01 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.7       |
| time/                   |             |
|    fps                  | 924         |
|    iterations           | 1860        |
|    time_elapsed         | 8245        |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.010200325 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -308        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -30.6       |
|    n_updates            | 18590       |
|    policy_gradient_loss | -0.0233     |
|    std                  | 2.24        |
|    value_loss           | 0.757       |
-----------------------------------------
Iteration: 1861 | Episodes: 75500 | Median Reward: 44.25 | Max Reward: 45.95
Iteration: 1864 | Episodes: 75600 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 1866 | Episodes: 75700 | Median Reward: 44.21 | Max Reward: 45.95
Iteration: 1869 | Episodes: 75800 | Median Reward: 43.19 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.8        |
| time/                   |              |
|    fps                  | 924          |
|    iterations           | 1870         |
|    time_elapsed         | 8287         |
|    total_timesteps      | 7659520      |
| train/                  |              |
|    approx_kl            | 0.0017717509 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -309         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -30.8        |
|    n_updates            | 18690        |
|    policy_gradient_loss | -0.00507     |
|    std                  | 2.25         |
|    value_loss           | 0.716        |
------------------------------------------
Iteration: 1871 | Episodes: 75900 | Median Reward: 44.20 | Max Reward: 45.95
Iteration: 1874 | Episodes: 76000 | Median Reward: 44.42 | Max Reward: 45.95
Iteration: 1876 | Episodes: 76100 | Median Reward: 44.30 | Max Reward: 45.95
Iteration: 1878 | Episodes: 76200 | Median Reward: 43.53 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.6       |
| time/                   |             |
|    fps                  | 924         |
|    iterations           | 1880        |
|    time_elapsed         | 8329        |
|    total_timesteps      | 7700480     |
| train/                  |             |
|    approx_kl            | 0.005287771 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -309        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -30.8       |
|    n_updates            | 18790       |
|    policy_gradient_loss | -0.0149     |
|    std                  | 2.26        |
|    value_loss           | 0.676       |
-----------------------------------------
Iteration: 1881 | Episodes: 76300 | Median Reward: 44.44 | Max Reward: 45.95
Iteration: 1883 | Episodes: 76400 | Median Reward: 44.43 | Max Reward: 45.95
Iteration: 1886 | Episodes: 76500 | Median Reward: 44.30 | Max Reward: 45.95
Iteration: 1888 | Episodes: 76600 | Median Reward: 44.45 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.9         |
| time/                   |               |
|    fps                  | 924           |
|    iterations           | 1890          |
|    time_elapsed         | 8371          |
|    total_timesteps      | 7741440       |
| train/                  |               |
|    approx_kl            | 0.00038838165 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -309          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -30.7         |
|    n_updates            | 18890         |
|    policy_gradient_loss | -0.00127      |
|    std                  | 2.27          |
|    value_loss           | 0.289         |
-------------------------------------------
Iteration: 1891 | Episodes: 76700 | Median Reward: 41.29 | Max Reward: 45.95
Iteration: 1893 | Episodes: 76800 | Median Reward: 43.91 | Max Reward: 45.95
Iteration: 1896 | Episodes: 76900 | Median Reward: 44.38 | Max Reward: 45.95
Iteration: 1898 | Episodes: 77000 | Median Reward: 44.65 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 924          |
|    iterations           | 1900         |
|    time_elapsed         | 8414         |
|    total_timesteps      | 7782400      |
| train/                  |              |
|    approx_kl            | 0.0006450727 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -310         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -30.6        |
|    n_updates            | 18990        |
|    policy_gradient_loss | -0.00222     |
|    std                  | 2.29         |
|    value_loss           | 0.934        |
------------------------------------------
Iteration: 1901 | Episodes: 77100 | Median Reward: 44.36 | Max Reward: 45.95
Iteration: 1903 | Episodes: 77200 | Median Reward: 44.27 | Max Reward: 45.95
Iteration: 1906 | Episodes: 77300 | Median Reward: 43.94 | Max Reward: 45.95
Iteration: 1908 | Episodes: 77400 | Median Reward: 43.90 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.1        |
| time/                   |              |
|    fps                  | 925          |
|    iterations           | 1910         |
|    time_elapsed         | 8456         |
|    total_timesteps      | 7823360      |
| train/                  |              |
|    approx_kl            | 0.0007613164 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -310         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -31          |
|    n_updates            | 19090        |
|    policy_gradient_loss | -0.000361    |
|    std                  | 2.3          |
|    value_loss           | 0.336        |
------------------------------------------
Iteration: 1911 | Episodes: 77500 | Median Reward: 44.46 | Max Reward: 45.95
Iteration: 1913 | Episodes: 77600 | Median Reward: 44.06 | Max Reward: 45.95
Iteration: 1915 | Episodes: 77700 | Median Reward: 44.53 | Max Reward: 45.95
Iteration: 1918 | Episodes: 77800 | Median Reward: 43.80 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.5         |
| time/                   |               |
|    fps                  | 925           |
|    iterations           | 1920          |
|    time_elapsed         | 8499          |
|    total_timesteps      | 7864320       |
| train/                  |               |
|    approx_kl            | 4.8117465e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -311          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -30           |
|    n_updates            | 19190         |
|    policy_gradient_loss | -0.000291     |
|    std                  | 2.32          |
|    value_loss           | 2.41          |
-------------------------------------------
Iteration: 1920 | Episodes: 77900 | Median Reward: 41.09 | Max Reward: 45.95
Iteration: 1923 | Episodes: 78000 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 1925 | Episodes: 78100 | Median Reward: 44.47 | Max Reward: 45.95
Iteration: 1928 | Episodes: 78200 | Median Reward: 43.07 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 925          |
|    iterations           | 1930         |
|    time_elapsed         | 8543         |
|    total_timesteps      | 7905280      |
| train/                  |              |
|    approx_kl            | 0.0017494014 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -311         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -31          |
|    n_updates            | 19290        |
|    policy_gradient_loss | -0.00348     |
|    std                  | 2.34         |
|    value_loss           | 0.5          |
------------------------------------------
Iteration: 1930 | Episodes: 78300 | Median Reward: 44.09 | Max Reward: 45.95
Iteration: 1933 | Episodes: 78400 | Median Reward: 42.77 | Max Reward: 45.95
Iteration: 1935 | Episodes: 78500 | Median Reward: 43.14 | Max Reward: 45.95
Iteration: 1938 | Episodes: 78600 | Median Reward: 44.65 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57           |
| time/                   |               |
|    fps                  | 925           |
|    iterations           | 1940          |
|    time_elapsed         | 8586          |
|    total_timesteps      | 7946240       |
| train/                  |               |
|    approx_kl            | 0.00014604465 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -311          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -30           |
|    n_updates            | 19390         |
|    policy_gradient_loss | -0.000535     |
|    std                  | 2.35          |
|    value_loss           | 1.47          |
-------------------------------------------
Iteration: 1940 | Episodes: 78700 | Median Reward: 41.26 | Max Reward: 45.95
Iteration: 1943 | Episodes: 78800 | Median Reward: 44.47 | Max Reward: 45.95
Iteration: 1945 | Episodes: 78900 | Median Reward: 44.58 | Max Reward: 45.95
Iteration: 1947 | Episodes: 79000 | Median Reward: 44.36 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 925          |
|    iterations           | 1950         |
|    time_elapsed         | 8628         |
|    total_timesteps      | 7987200      |
| train/                  |              |
|    approx_kl            | 0.0008633776 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -312         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -31          |
|    n_updates            | 19490        |
|    policy_gradient_loss | -0.00235     |
|    std                  | 2.37         |
|    value_loss           | 0.231        |
------------------------------------------
Iteration: 1950 | Episodes: 79100 | Median Reward: 42.22 | Max Reward: 45.95
Iteration: 1952 | Episodes: 79200 | Median Reward: 42.18 | Max Reward: 45.95
Iteration: 1955 | Episodes: 79300 | Median Reward: 44.41 | Max Reward: 45.95
Iteration: 1957 | Episodes: 79400 | Median Reward: 44.14 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.3         |
| time/                   |               |
|    fps                  | 925           |
|    iterations           | 1960          |
|    time_elapsed         | 8671          |
|    total_timesteps      | 8028160       |
| train/                  |               |
|    approx_kl            | 0.00014616844 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -312          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -29.4         |
|    n_updates            | 19590         |
|    policy_gradient_loss | -0.000648     |
|    std                  | 2.38          |
|    value_loss           | 1.12          |
-------------------------------------------
Iteration: 1960 | Episodes: 79500 | Median Reward: 44.49 | Max Reward: 45.95
Iteration: 1962 | Episodes: 79600 | Median Reward: 43.86 | Max Reward: 45.95
Iteration: 1965 | Episodes: 79700 | Median Reward: 43.58 | Max Reward: 45.95
Iteration: 1967 | Episodes: 79800 | Median Reward: 41.25 | Max Reward: 45.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.9         |
| time/                   |               |
|    fps                  | 925           |
|    iterations           | 1970          |
|    time_elapsed         | 8714          |
|    total_timesteps      | 8069120       |
| train/                  |               |
|    approx_kl            | 0.00012695261 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -312          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -30.8         |
|    n_updates            | 19690         |
|    policy_gradient_loss | -0.00028      |
|    std                  | 2.4           |
|    value_loss           | 0.473         |
-------------------------------------------
Iteration: 1970 | Episodes: 79900 | Median Reward: 44.40 | Max Reward: 45.95
Iteration: 1972 | Episodes: 80000 | Median Reward: 44.12 | Max Reward: 45.95
Iteration: 1975 | Episodes: 80100 | Median Reward: 43.24 | Max Reward: 45.95
Iteration: 1977 | Episodes: 80200 | Median Reward: 44.47 | Max Reward: 45.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 926          |
|    iterations           | 1980         |
|    time_elapsed         | 8757         |
|    total_timesteps      | 8110080      |
| train/                  |              |
|    approx_kl            | 0.0018362053 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -313         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -31.2        |
|    n_updates            | 19790        |
|    policy_gradient_loss | -0.00401     |
|    std                  | 2.41         |
|    value_loss           | 0.578        |
------------------------------------------
Iteration: 1980 | Episodes: 80300 | Median Reward: 44.28 | Max Reward: 45.95
Iterat