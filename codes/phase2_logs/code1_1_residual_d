Script started on 2024-10-24 20:59:24-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/21" COLUMNS="203" LINES="53"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> py[K[Kconda activate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python code1_1_residual_d.py
GPU 6: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
Iteration: 2 | Episodes: 100 | Median Reward: 20.93 | Avg Reward: 18.19 | Max Reward: 32.69
Iteration: 4 | Episodes: 200 | Median Reward: 21.67 | Avg Reward: 20.46 | Max Reward: 32.69
Iteration: 7 | Episodes: 300 | Median Reward: 15.99 | Avg Reward: 16.82 | Max Reward: 32.69
Iteration: 9 | Episodes: 400 | Median Reward: 18.62 | Avg Reward: 17.34 | Max Reward: 32.69
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -82.9      |
| time/                   |            |
|    fps                  | 884        |
|    iterations           | 10         |
|    time_elapsed         | 46         |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.21404228 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.3        |
|    entropy_loss         | -78.6      |
|    explained_variance   | 0.0343     |
|    learning_rate        | 0.0005     |
|    loss                 | 87.7       |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.0198     |
|    std                  | 1.01       |
|    value_loss           | 207        |
----------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 14.64 | Avg Reward: 17.72 | Max Reward: 36.54
Iteration: 14 | Episodes: 600 | Median Reward: 23.30 | Avg Reward: 25.06 | Max Reward: 40.96
Iteration: 17 | Episodes: 700 | Median Reward: 18.75 | Avg Reward: 17.53 | Max Reward: 40.96
Iteration: 19 | Episodes: 800 | Median Reward: 14.20 | Avg Reward: 14.99 | Max Reward: 40.96
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -84.9      |
| time/                   |            |
|    fps                  | 900        |
|    iterations           | 20         |
|    time_elapsed         | 90         |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.14473018 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.3        |
|    entropy_loss         | -77.2      |
|    explained_variance   | 0.219      |
|    learning_rate        | 0.0005     |
|    loss                 | 72.9       |
|    n_updates            | 190        |
|    policy_gradient_loss | 0.0314     |
|    std                  | 1.01       |
|    value_loss           | 185        |
----------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 20.14 | Avg Reward: 19.66 | Max Reward: 40.96
Iteration: 24 | Episodes: 1000 | Median Reward: 23.72 | Avg Reward: 21.53 | Max Reward: 40.96
Iteration: 27 | Episodes: 1100 | Median Reward: 17.73 | Avg Reward: 18.19 | Max Reward: 40.96
Iteration: 29 | Episodes: 1200 | Median Reward: 17.06 | Avg Reward: 19.10 | Max Reward: 40.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80         |
| time/                   |             |
|    fps                  | 912         |
|    iterations           | 30          |
|    time_elapsed         | 134         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.013036694 |
|    clip_fraction        | 0.00769     |
|    clip_range           | 0.3         |
|    entropy_loss         | -81.9       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0005      |
|    loss                 | 1.11        |
|    n_updates            | 290         |
|    policy_gradient_loss | 0.00318     |
|    std                  | 1.01        |
|    value_loss           | 33.1        |
-----------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 20.60 | Avg Reward: 22.25 | Max Reward: 40.96
Iteration: 34 | Episodes: 1400 | Median Reward: 21.89 | Avg Reward: 20.16 | Max Reward: 40.96
Iteration: 36 | Episodes: 1500 | Median Reward: 21.84 | Avg Reward: 19.11 | Max Reward: 40.96
Iteration: 39 | Episodes: 1600 | Median Reward: 21.03 | Avg Reward: 19.37 | Max Reward: 40.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.1       |
| time/                   |             |
|    fps                  | 915         |
|    iterations           | 40          |
|    time_elapsed         | 178         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.034357287 |
|    clip_fraction        | 0.0814      |
|    clip_range           | 0.3         |
|    entropy_loss         | -92.8       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0005      |
|    loss                 | 3.58        |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.00568     |
|    std                  | 1.01        |
|    value_loss           | 39.6        |
-----------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 17.23 | Avg Reward: 18.51 | Max Reward: 40.96
Iteration: 44 | Episodes: 1800 | Median Reward: 18.63 | Avg Reward: 15.22 | Max Reward: 40.96
Iteration: 46 | Episodes: 1900 | Median Reward: 18.79 | Avg Reward: 19.93 | Max Reward: 40.96
Iteration: 49 | Episodes: 2000 | Median Reward: 21.08 | Avg Reward: 19.07 | Max Reward: 40.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.1       |
| time/                   |             |
|    fps                  | 919         |
|    iterations           | 50          |
|    time_elapsed         | 222         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.105183415 |
|    clip_fraction        | 0.457       |
|    clip_range           | 0.3         |
|    entropy_loss         | -101        |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.0685     |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0506     |
|    std                  | 1.02        |
|    value_loss           | 23.3        |
-----------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 25.86 | Avg Reward: 25.42 | Max Reward: 40.96
Iteration: 54 | Episodes: 2200 | Median Reward: 23.81 | Avg Reward: 20.13 | Max Reward: 40.96
Iteration: 56 | Episodes: 2300 | Median Reward: 20.90 | Avg Reward: 21.02 | Max Reward: 40.96
Iteration: 59 | Episodes: 2400 | Median Reward: 25.17 | Avg Reward: 23.47 | Max Reward: 40.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.1       |
| time/                   |             |
|    fps                  | 921         |
|    iterations           | 60          |
|    time_elapsed         | 266         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.015102522 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -109        |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0005      |
|    loss                 | -14.8       |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0221     |
|    std                  | 1.02        |
|    value_loss           | 6.53        |
-----------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 19.78 | Avg Reward: 20.62 | Max Reward: 40.96
Iteration: 64 | Episodes: 2600 | Median Reward: 25.54 | Avg Reward: 25.87 | Max Reward: 40.96
Iteration: 66 | Episodes: 2700 | Median Reward: 20.81 | Avg Reward: 20.33 | Max Reward: 40.96
Iteration: 69 | Episodes: 2800 | Median Reward: 22.75 | Avg Reward: 20.28 | Max Reward: 40.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76.6       |
| time/                   |             |
|    fps                  | 922         |
|    iterations           | 70          |
|    time_elapsed         | 310         |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.009694418 |
|    clip_fraction        | 0.00171     |
|    clip_range           | 0.3         |
|    entropy_loss         | -127        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.21        |
|    n_updates            | 690         |
|    policy_gradient_loss | 0.00938     |
|    std                  | 1.02        |
|    value_loss           | 32.9        |
-----------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 18.91 | Avg Reward: 21.28 | Max Reward: 40.96
Iteration: 73 | Episodes: 3000 | Median Reward: 27.89 | Avg Reward: 25.38 | Max Reward: 40.96
Iteration: 76 | Episodes: 3100 | Median Reward: 20.55 | Avg Reward: 20.38 | Max Reward: 40.96
Iteration: 78 | Episodes: 3200 | Median Reward: 25.73 | Avg Reward: 24.41 | Max Reward: 40.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.4        |
| time/                   |              |
|    fps                  | 921          |
|    iterations           | 80           |
|    time_elapsed         | 355          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0083359685 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.3          |
|    entropy_loss         | -133         |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0005       |
|    loss                 | -19          |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00784     |
|    std                  | 1.02         |
|    value_loss           | 4.15         |
------------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 24.68 | Avg Reward: 23.84 | Max Reward: 41.07
Iteration: 83 | Episodes: 3400 | Median Reward: 24.53 | Avg Reward: 25.18 | Max Reward: 41.07
Iteration: 86 | Episodes: 3500 | Median Reward: 30.16 | Avg Reward: 29.31 | Max Reward: 41.07
Iteration: 88 | Episodes: 3600 | Median Reward: 22.72 | Avg Reward: 24.05 | Max Reward: 41.07
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -74.1      |
| time/                   |            |
|    fps                  | 923        |
|    iterations           | 90         |
|    time_elapsed         | 399        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.01920563 |
|    clip_fraction        | 0.0262     |
|    clip_range           | 0.3        |
|    entropy_loss         | -143       |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0005     |
|    loss                 | -20.4      |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0164    |
|    std                  | 1.03       |
|    value_loss           | 5.31       |
----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 26.15 | Avg Reward: 24.89 | Max Reward: 41.07
Iteration: 93 | Episodes: 3800 | Median Reward: 26.26 | Avg Reward: 28.14 | Max Reward: 41.07
Iteration: 96 | Episodes: 3900 | Median Reward: 23.69 | Avg Reward: 23.70 | Max Reward: 41.07
Iteration: 98 | Episodes: 4000 | Median Reward: 26.22 | Avg Reward: 26.25 | Max Reward: 41.07
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.9         |
| time/                   |               |
|    fps                  | 919           |
|    iterations           | 100           |
|    time_elapsed         | 445           |
|    total_timesteps      | 409600        |
| train/                  |               |
|    approx_kl            | 0.00046255387 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -150          |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.0005        |
|    loss                 | -18.3         |
|    n_updates            | 990           |
|    policy_gradient_loss | -0.00193      |
|    std                  | 1.03          |
|    value_loss           | 5.72          |
-------------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 25.18 | Avg Reward: 25.53 | Max Reward: 41.07
Iteration: 103 | Episodes: 4200 | Median Reward: 29.16 | Avg Reward: 26.83 | Max Reward: 41.07
Iteration: 106 | Episodes: 4300 | Median Reward: 30.41 | Avg Reward: 27.66 | Max Reward: 41.07
Iteration: 108 | Episodes: 4400 | Median Reward: 33.00 | Avg Reward: 33.43 | Max Reward: 44.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.2        |
| time/                   |              |
|    fps                  | 920          |
|    iterations           | 110          |
|    time_elapsed         | 489          |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 0.0033015204 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -159         |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0005       |
|    loss                 | -17.8        |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00732     |
|    std                  | 1.03         |
|    value_loss           | 18.1         |
------------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 35.32 | Avg Reward: 34.33 | Max Reward: 44.77
Iteration: 113 | Episodes: 4600 | Median Reward: 29.12 | Avg Reward: 29.02 | Max Reward: 44.77
Iteration: 115 | Episodes: 4700 | Median Reward: 26.25 | Avg Reward: 26.26 | Max Reward: 44.77
Iteration: 118 | Episodes: 4800 | Median Reward: 31.31 | Avg Reward: 30.57 | Max Reward: 44.77
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.3     |
| time/                   |           |
|    fps                  | 919       |
|    iterations           | 120       |
|    time_elapsed         | 534       |
|    total_timesteps      | 491520    |
| train/                  |           |
|    approx_kl            | 0.1630976 |
|    clip_fraction        | 0.275     |
|    clip_range           | 0.3       |
|    entropy_loss         | -164      |
|    explained_variance   | 0.992     |
|    learning_rate        | 0.0005    |
|    loss                 | -20.4     |
|    n_updates            | 1190      |
|    policy_gradient_loss | -0.0428   |
|    std                  | 1.03      |
|    value_loss           | 5.5       |
---------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 31.09 | Avg Reward: 30.34 | Max Reward: 44.77
Iteration: 123 | Episodes: 5000 | Median Reward: 31.05 | Avg Reward: 30.67 | Max Reward: 44.77
Iteration: 125 | Episodes: 5100 | Median Reward: 33.74 | Avg Reward: 33.33 | Max Reward: 44.77
Iteration: 128 | Episodes: 5200 | Median Reward: 31.33 | Avg Reward: 28.98 | Max Reward: 44.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.1       |
| time/                   |             |
|    fps                  | 920         |
|    iterations           | 130         |
|    time_elapsed         | 578         |
|    total_timesteps      | 532480      |
| train/                  |             |
|    approx_kl            | 0.012006509 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -171        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.5       |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0166     |
|    std                  | 1.04        |
|    value_loss           | 7.14        |
-----------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 26.08 | Avg Reward: 26.93 | Max Reward: 44.77
Iteration: 133 | Episodes: 5400 | Median Reward: 33.28 | Avg Reward: 32.09 | Max Reward: 44.77
Iteration: 135 | Episodes: 5500 | Median Reward: 32.06 | Avg Reward: 32.95 | Max Reward: 44.77
Iteration: 138 | Episodes: 5600 | Median Reward: 33.59 | Avg Reward: 34.19 | Max Reward: 44.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.7       |
| time/                   |             |
|    fps                  | 919         |
|    iterations           | 140         |
|    time_elapsed         | 623         |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.021215998 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.3         |
|    entropy_loss         | -175        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -25.8       |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0183     |
|    std                  | 1.04        |
|    value_loss           | 1.47        |
-----------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 32.85 | Avg Reward: 32.70 | Max Reward: 44.77
Iteration: 143 | Episodes: 5800 | Median Reward: 35.08 | Avg Reward: 33.98 | Max Reward: 44.77
Iteration: 145 | Episodes: 5900 | Median Reward: 31.34 | Avg Reward: 31.10 | Max Reward: 44.77
Iteration: 147 | Episodes: 6000 | Median Reward: 35.48 | Avg Reward: 35.22 | Max Reward: 44.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.8        |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 150          |
|    time_elapsed         | 667          |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0033620573 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -179         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.1        |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00407     |
|    std                  | 1.04         |
|    value_loss           | 3.91         |
------------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 32.81 | Avg Reward: 34.18 | Max Reward: 44.77
Iteration: 152 | Episodes: 6200 | Median Reward: 33.64 | Avg Reward: 34.52 | Max Reward: 44.77
Iteration: 155 | Episodes: 6300 | Median Reward: 34.59 | Avg Reward: 34.12 | Max Reward: 44.77
Iteration: 157 | Episodes: 6400 | Median Reward: 33.08 | Avg Reward: 33.46 | Max Reward: 44.85
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.8        |
| time/                   |              |
|    fps                  | 921          |
|    iterations           | 160          |
|    time_elapsed         | 711          |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 0.0048754597 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -183         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -25          |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.00673     |
|    std                  | 1.05         |
|    value_loss           | 3.13         |
------------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 35.21 | Avg Reward: 35.03 | Max Reward: 44.85
Iteration: 162 | Episodes: 6600 | Median Reward: 34.10 | Avg Reward: 33.67 | Max Reward: 44.85
Iteration: 165 | Episodes: 6700 | Median Reward: 36.05 | Avg Reward: 35.24 | Max Reward: 44.85
Iteration: 167 | Episodes: 6800 | Median Reward: 36.78 | Avg Reward: 34.79 | Max Reward: 44.85
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.3        |
| time/                   |              |
|    fps                  | 921          |
|    iterations           | 170          |
|    time_elapsed         | 755          |
|    total_timesteps      | 696320       |
| train/                  |              |
|    approx_kl            | 0.0014494769 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -191         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0005       |
|    loss                 | -23          |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.00184     |
|    std                  | 1.05         |
|    value_loss           | 7.89         |
------------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 37.13 | Avg Reward: 34.94 | Max Reward: 44.85
Iteration: 172 | Episodes: 7000 | Median Reward: 33.95 | Avg Reward: 34.11 | Max Reward: 44.85
Iteration: 175 | Episodes: 7100 | Median Reward: 38.29 | Avg Reward: 38.68 | Max Reward: 45.31
Iteration: 177 | Episodes: 7200 | Median Reward: 36.87 | Avg Reward: 35.57 | Max Reward: 45.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.9       |
| time/                   |             |
|    fps                  | 921         |
|    iterations           | 180         |
|    time_elapsed         | 799         |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.020027671 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.3         |
|    entropy_loss         | -194        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -24.6       |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0353     |
|    std                  | 1.05        |
|    value_loss           | 6.17        |
-----------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 37.23 | Avg Reward: 36.18 | Max Reward: 45.31
Iteration: 182 | Episodes: 7400 | Median Reward: 37.73 | Avg Reward: 35.61 | Max Reward: 45.31
Iteration: 184 | Episodes: 7500 | Median Reward: 39.59 | Avg Reward: 38.88 | Max Reward: 45.31
Iteration: 187 | Episodes: 7600 | Median Reward: 39.34 | Avg Reward: 38.28 | Max Reward: 45.31
Iteration: 189 | Episodes: 7700 | Median Reward: 37.70 | Avg Reward: 38.20 | Max Reward: 45.31
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.6         |
| time/                   |               |
|    fps                  | 919           |
|    iterations           | 190           |
|    time_elapsed         | 846           |
|    total_timesteps      | 778240        |
| train/                  |               |
|    approx_kl            | 0.00017846498 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -199          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -26.8         |
|    n_updates            | 1890          |
|    policy_gradient_loss | -0.000189     |
|    std                  | 1.06          |
|    value_loss           | 6.15          |
-------------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 38.58 | Avg Reward: 36.04 | Max Reward: 45.31
Iteration: 194 | Episodes: 7900 | Median Reward: 37.22 | Avg Reward: 36.55 | Max Reward: 45.46
Iteration: 197 | Episodes: 8000 | Median Reward: 36.39 | Avg Reward: 36.80 | Max Reward: 45.46
Iteration: 199 | Episodes: 8100 | Median Reward: 37.57 | Avg Reward: 37.44 | Max Reward: 45.46
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.6       |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 200         |
|    time_elapsed         | 891         |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.009250468 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -200        |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0005      |
|    loss                 | -29         |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00588    |
|    std                  | 1.06        |
|    value_loss           | 3.07        |
-----------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 40.86 | Avg Reward: 39.67 | Max Reward: 45.46
Iteration: 204 | Episodes: 8300 | Median Reward: 35.34 | Avg Reward: 35.64 | Max Reward: 45.46
Iteration: 207 | Episodes: 8400 | Median Reward: 35.94 | Avg Reward: 35.65 | Max Reward: 45.46
Iteration: 209 | Episodes: 8500 | Median Reward: 38.84 | Avg Reward: 38.19 | Max Reward: 45.46
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61         |
| time/                   |             |
|    fps                  | 917         |
|    iterations           | 210         |
|    time_elapsed         | 937         |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.011457727 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -200        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.1       |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.0138     |
|    std                  | 1.06        |
|    value_loss           | 4.59        |
-----------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 40.47 | Avg Reward: 38.55 | Max Reward: 45.46
Iteration: 214 | Episodes: 8700 | Median Reward: 38.01 | Avg Reward: 37.17 | Max Reward: 45.46
Iteration: 216 | Episodes: 8800 | Median Reward: 39.37 | Avg Reward: 38.87 | Max Reward: 46.44
Iteration: 219 | Episodes: 8900 | Median Reward: 35.50 | Avg Reward: 34.27 | Max Reward: 46.44
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.3        |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 220          |
|    time_elapsed         | 983          |
|    total_timesteps      | 901120       |
| train/                  |              |
|    approx_kl            | 0.0037011674 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -203         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0005       |
|    loss                 | -29.5        |
|    n_updates            | 2190         |
|    policy_gradient_loss | 0.00119      |
|    std                  | 1.07         |
|    value_loss           | 2.37         |
------------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 38.87 | Avg Reward: 39.42 | Max Reward: 46.44
Iteration: 224 | Episodes: 9100 | Median Reward: 37.68 | Avg Reward: 37.20 | Max Reward: 46.44
Iteration: 226 | Episodes: 9200 | Median Reward: 39.82 | Avg Reward: 39.08 | Max Reward: 46.44
Iteration: 229 | Episodes: 9300 | Median Reward: 36.51 | Avg Reward: 36.01 | Max Reward: 46.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.4       |
| time/                   |             |
|    fps                  | 916         |
|    iterations           | 230         |
|    time_elapsed         | 1027        |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.030442512 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -204        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -27.8       |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0366     |
|    std                  | 1.07        |
|    value_loss           | 2.89        |
-----------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 40.02 | Avg Reward: 39.26 | Max Reward: 46.44
Iteration: 234 | Episodes: 9500 | Median Reward: 37.30 | Avg Reward: 37.79 | Max Reward: 46.44
Iteration: 236 | Episodes: 9600 | Median Reward: 37.34 | Avg Reward: 37.05 | Max Reward: 46.44
Iteration: 239 | Episodes: 9700 | Median Reward: 38.53 | Avg Reward: 38.37 | Max Reward: 46.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.2       |
| time/                   |             |
|    fps                  | 917         |
|    iterations           | 240         |
|    time_elapsed         | 1071        |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.020481378 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -206        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -30.8       |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00836    |
|    std                  | 1.07        |
|    value_loss           | 1.03        |
-----------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 38.52 | Avg Reward: 38.50 | Max Reward: 46.44
Iteration: 244 | Episodes: 9900 | Median Reward: 37.67 | Avg Reward: 38.42 | Max Reward: 46.44
Iteration: 246 | Episodes: 10000 | Median Reward: 38.19 | Avg Reward: 39.37 | Max Reward: 46.44
Iteration: 249 | Episodes: 10100 | Median Reward: 37.16 | Avg Reward: 37.71 | Max Reward: 46.44
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -63.3      |
| time/                   |            |
|    fps                  | 918        |
|    iterations           | 250        |
|    time_elapsed         | 1115       |
|    total_timesteps      | 1024000    |
| train/                  |            |
|    approx_kl            | 0.06830969 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.3        |
|    entropy_loss         | -209       |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0005     |
|    loss                 | -30.1      |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.062     |
|    std                  | 1.08       |
|    value_loss           | 2.98       |
----------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 35.92 | Avg Reward: 37.27 | Max Reward: 46.44
Iteration: 253 | Episodes: 10300 | Median Reward: 38.70 | Avg Reward: 39.14 | Max Reward: 46.44
Iteration: 256 | Episodes: 10400 | Median Reward: 38.68 | Avg Reward: 39.72 | Max Reward: 46.44
Iteration: 258 | Episodes: 10500 | Median Reward: 40.76 | Avg Reward: 39.26 | Max Reward: 46.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.9       |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 260         |
|    time_elapsed         | 1159        |
|    total_timesteps      | 1064960     |
| train/                  |             |
|    approx_kl            | 0.024148475 |
|    clip_fraction        | 0.0267      |
|    clip_range           | 0.3         |
|    entropy_loss         | -211        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -29.1       |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0114     |
|    std                  | 1.08        |
|    value_loss           | 2.87        |
-----------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 39.32 | Avg Reward: 38.75 | Max Reward: 46.44
Iteration: 263 | Episodes: 10700 | Median Reward: 41.38 | Avg Reward: 41.25 | Max Reward: 46.44
Iteration: 266 | Episodes: 10800 | Median Reward: 42.31 | Avg Reward: 41.47 | Max Reward: 46.44
Iteration: 268 | Episodes: 10900 | Median Reward: 41.76 | Avg Reward: 40.08 | Max Reward: 46.44
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 918          |
|    iterations           | 270          |
|    time_elapsed         | 1203         |
|    total_timesteps      | 1105920      |
| train/                  |              |
|    approx_kl            | 0.0070843636 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -212         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -31.2        |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.00841     |
|    std                  | 1.09         |
|    value_loss           | 1.27         |
------------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 41.57 | Avg Reward: 41.92 | Max Reward: 46.44
Iteration: 273 | Episodes: 11100 | Median Reward: 40.35 | Avg Reward: 39.43 | Max Reward: 46.44
Iteration: 276 | Episodes: 11200 | Median Reward: 37.96 | Avg Reward: 38.82 | Max Reward: 46.44
Iteration: 278 | Episodes: 11300 | Median Reward: 43.05 | Avg Reward: 42.35 | Max Reward: 46.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.7       |
| time/                   |             |
|    fps                  | 919         |
|    iterations           | 280         |
|    time_elapsed         | 1247        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.030180592 |
|    clip_fraction        | 0.0747      |
|    clip_range           | 0.3         |
|    entropy_loss         | -214        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -31.5       |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.0416     |
|    std                  | 1.09        |
|    value_loss           | 2.3         |
-----------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 42.44 | Avg Reward: 41.74 | Max Reward: 46.44
Iteration: 283 | Episodes: 11500 | Median Reward: 36.72 | Avg Reward: 38.48 | Max Reward: 46.44
Iteration: 286 | Episodes: 11600 | Median Reward: 36.97 | Avg Reward: 38.61 | Max Reward: 46.44
Iteration: 288 | Episodes: 11700 | Median Reward: 40.56 | Avg Reward: 40.19 | Max Reward: 46.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.1       |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 290         |
|    time_elapsed         | 1292        |
|    total_timesteps      | 1187840     |
| train/                  |             |
|    approx_kl            | 0.006795533 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -216        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -31.8       |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.013      |
|    std                  | 1.1         |
|    value_loss           | 1.82        |
-----------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 41.68 | Avg Reward: 40.68 | Max Reward: 46.44
Iteration: 293 | Episodes: 11900 | Median Reward: 39.01 | Avg Reward: 40.22 | Max Reward: 46.44
Iteration: 295 | Episodes: 12000 | Median Reward: 39.90 | Avg Reward: 38.94 | Max Reward: 46.44
Iteration: 298 | Episodes: 12100 | Median Reward: 42.19 | Avg Reward: 41.44 | Max Reward: 46.44
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.4        |
| time/                   |              |
|    fps                  | 918          |
|    iterations           | 300          |
|    time_elapsed         | 1337         |
|    total_timesteps      | 1228800      |
| train/                  |              |
|    approx_kl            | 0.0033954629 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -217         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -28.4        |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.00207     |
|    std                  | 1.1          |
|    value_loss           | 4.51         |
------------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 40.61 | Avg Reward: 39.32 | Max Reward: 46.44
Iteration: 303 | Episodes: 12300 | Median Reward: 38.41 | Avg Reward: 40.07 | Max Reward: 46.44
Iteration: 305 | Episodes: 12400 | Median Reward: 42.31 | Avg Reward: 42.34 | Max Reward: 46.44
Iteration: 308 | Episodes: 12500 | Median Reward: 44.50 | Avg Reward: 43.39 | Max Reward: 46.44
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.9         |
| time/                   |               |
|    fps                  | 919           |
|    iterations           | 310           |
|    time_elapsed         | 1381          |
|    total_timesteps      | 1269760       |
| train/                  |               |
|    approx_kl            | 0.00066469656 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -219          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -32.4         |
|    n_updates            | 3090          |
|    policy_gradient_loss | -0.00167      |
|    std                  | 1.11          |
|    value_loss           | 1.24          |
-------------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: 41.46 | Avg Reward: 40.46 | Max Reward: 46.44
Iteration: 313 | Episodes: 12700 | Median Reward: 42.00 | Avg Reward: 41.49 | Max Reward: 46.44
Iteration: 315 | Episodes: 12800 | Median Reward: 42.57 | Avg Reward: 42.29 | Max Reward: 46.44
Iteration: 318 | Episodes: 12900 | Median Reward: 40.21 | Avg Reward: 39.74 | Max Reward: 46.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.9       |
| time/                   |             |
|    fps                  | 919         |
|    iterations           | 320         |
|    time_elapsed         | 1425        |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.005632803 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -220        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -31.5       |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.0218     |
|    std                  | 1.11        |
|    value_loss           | 3.67        |
-----------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: 40.80 | Avg Reward: 39.57 | Max Reward: 46.44
Iteration: 323 | Episodes: 13100 | Median Reward: 39.53 | Avg Reward: 39.34 | Max Reward: 46.44
Iteration: 325 | Episodes: 13200 | Median Reward: 43.25 | Avg Reward: 42.74 | Max Reward: 46.44
Iteration: 327 | Episodes: 13300 | Median Reward: 44.61 | Avg Reward: 43.73 | Max Reward: 46.44
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 920         |
|    iterations           | 330         |
|    time_elapsed         | 1469        |
|    total_timesteps      | 1351680     |
| train/                  |             |
|    approx_kl            | 0.004346692 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -222        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -31         |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.00579    |
|    std                  | 1.12        |
|    value_loss           | 1.42        |
-----------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: 43.96 | Avg Reward: 42.91 | Max Reward: 46.44
Iteration: 332 | Episodes: 13500 | Median Reward: 44.28 | Avg Reward: 42.62 | Max Reward: 46.44
Iteration: 335 | Episodes: 13600 | Median Reward: 41.88 | Avg Reward: 41.21 | Max Reward: 46.44
Iteration: 337 | Episodes: 13700 | Median Reward: 42.66 | Avg Reward: 42.42 | Max Reward: 46.44
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 340          |
|    time_elapsed         | 1513         |
|    total_timesteps      | 1392640      |
| train/                  |              |
|    approx_kl            | 0.0014485607 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -224         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -32.6        |
|    n_updates            | 3390         |
|    policy_gradient_loss | 0.00133      |
|    std                  | 1.12         |
|    value_loss           | 2.26         |
------------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: 40.62 | Avg Reward: 40.89 | Max Reward: 46.44
Iteration: 342 | Episodes: 13900 | Median Reward: 41.21 | Avg Reward: 41.36 | Max Reward: 46.44
Iteration: 345 | Episodes: 14000 | Median Reward: 41.95 | Avg Reward: 42.26 | Max Reward: 46.44
Iteration: 347 | Episodes: 14100 | Median Reward: 42.67 | Avg Reward: 43.03 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.9       |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 350         |
|    time_elapsed         | 1560        |
|    total_timesteps      | 1433600     |
| train/                  |             |
|    approx_kl            | 0.009598309 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -225        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -32.4       |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.0166     |
|    std                  | 1.13        |
|    value_loss           | 3.59        |
-----------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: 44.43 | Avg Reward: 42.97 | Max Reward: 47.12
Iteration: 352 | Episodes: 14300 | Median Reward: 40.89 | Avg Reward: 41.38 | Max Reward: 47.12
Iteration: 355 | Episodes: 14400 | Median Reward: 41.19 | Avg Reward: 41.07 | Max Reward: 47.12
Iteration: 357 | Episodes: 14500 | Median Reward: 41.07 | Avg Reward: 40.57 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.2        |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 360          |
|    time_elapsed         | 1603         |
|    total_timesteps      | 1474560      |
| train/                  |              |
|    approx_kl            | 0.0017151249 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -226         |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0005       |
|    loss                 | -33.2        |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.00133     |
|    std                  | 1.13         |
|    value_loss           | 3.11         |
------------------------------------------
Iteration: 360 | Episodes: 14600 | Median Reward: 39.50 | Avg Reward: 39.82 | Max Reward: 47.12
Iteration: 362 | Episodes: 14700 | Median Reward: 40.15 | Avg Reward: 40.15 | Max Reward: 47.12
Iteration: 364 | Episodes: 14800 | Median Reward: 42.44 | Avg Reward: 41.92 | Max Reward: 47.12
Iteration: 367 | Episodes: 14900 | Median Reward: 41.72 | Avg Reward: 40.97 | Max Reward: 47.12
Iteration: 369 | Episodes: 15000 | Median Reward: 42.02 | Avg Reward: 40.36 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.8        |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 370          |
|    time_elapsed         | 1648         |
|    total_timesteps      | 1515520      |
| train/                  |              |
|    approx_kl            | 0.0096729845 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -227         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -33.7        |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00822     |
|    std                  | 1.14         |
|    value_loss           | 1.07         |
------------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: 40.09 | Avg Reward: 40.56 | Max Reward: 47.12
Iteration: 374 | Episodes: 15200 | Median Reward: 40.48 | Avg Reward: 41.11 | Max Reward: 47.12
Iteration: 377 | Episodes: 15300 | Median Reward: 41.50 | Avg Reward: 41.82 | Max Reward: 47.12
Iteration: 379 | Episodes: 15400 | Median Reward: 42.11 | Avg Reward: 41.15 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.9       |
| time/                   |             |
|    fps                  | 919         |
|    iterations           | 380         |
|    time_elapsed         | 1693        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.011728021 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -228        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -33.9       |
|    n_updates            | 3790        |
|    policy_gradient_loss | 0.00147     |
|    std                  | 1.15        |
|    value_loss           | 1.07        |
-----------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: 41.54 | Avg Reward: 41.67 | Max Reward: 47.12
Iteration: 384 | Episodes: 15600 | Median Reward: 42.29 | Avg Reward: 42.12 | Max Reward: 47.12
Iteration: 387 | Episodes: 15700 | Median Reward: 43.31 | Avg Reward: 42.19 | Max Reward: 47.12
Iteration: 389 | Episodes: 15800 | Median Reward: 41.88 | Avg Reward: 42.00 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.1       |
| time/                   |             |
|    fps                  | 919         |
|    iterations           | 390         |
|    time_elapsed         | 1736        |
|    total_timesteps      | 1597440     |
| train/                  |             |
|    approx_kl            | 0.001229223 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -229        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -31.5       |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00348    |
|    std                  | 1.15        |
|    value_loss           | 3.06        |
-----------------------------------------
Iteration: 392 | Episodes: 15900 | Median Reward: 39.49 | Avg Reward: 39.26 | Max Reward: 47.12
Iteration: 394 | Episodes: 16000 | Median Reward: 42.72 | Avg Reward: 41.53 | Max Reward: 47.12
Iteration: 396 | Episodes: 16100 | Median Reward: 39.81 | Avg Reward: 39.00 | Max Reward: 47.12
Iteration: 399 | Episodes: 16200 | Median Reward: 42.14 | Avg Reward: 41.90 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58         |
| time/                   |             |
|    fps                  | 919         |
|    iterations           | 400         |
|    time_elapsed         | 1782        |
|    total_timesteps      | 1638400     |
| train/                  |             |
|    approx_kl            | 0.012272162 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.3         |
|    entropy_loss         | -230        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -33.8       |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.0094     |
|    std                  | 1.15        |
|    value_loss           | 2.91        |
-----------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: 42.90 | Avg Reward: 42.48 | Max Reward: 47.12
Iteration: 404 | Episodes: 16400 | Median Reward: 42.65 | Avg Reward: 42.53 | Max Reward: 47.12
Iteration: 406 | Episodes: 16500 | Median Reward: 41.73 | Avg Reward: 41.74 | Max Reward: 47.12
Iteration: 409 | Episodes: 16600 | Median Reward: 43.28 | Avg Reward: 43.13 | Max Reward: 47.12
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.6         |
| time/                   |               |
|    fps                  | 919           |
|    iterations           | 410           |
|    time_elapsed         | 1827          |
|    total_timesteps      | 1679360       |
| train/                  |               |
|    approx_kl            | 0.00027496292 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -231          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -32           |
|    n_updates            | 4090          |
|    policy_gradient_loss | -0.000841     |
|    std                  | 1.16          |
|    value_loss           | 3.13          |
-------------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: 42.60 | Avg Reward: 42.47 | Max Reward: 47.12
Iteration: 414 | Episodes: 16800 | Median Reward: 42.52 | Avg Reward: 42.73 | Max Reward: 47.12
Iteration: 416 | Episodes: 16900 | Median Reward: 42.15 | Avg Reward: 42.23 | Max Reward: 47.12
Iteration: 419 | Episodes: 17000 | Median Reward: 42.76 | Avg Reward: 42.46 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 420          |
|    time_elapsed         | 1870         |
|    total_timesteps      | 1720320      |
| train/                  |              |
|    approx_kl            | 0.0007572403 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -232         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -34.3        |
|    n_updates            | 4190         |
|    policy_gradient_loss | 0.000273     |
|    std                  | 1.17         |
|    value_loss           | 2.05         |
------------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: 42.00 | Avg Reward: 41.98 | Max Reward: 47.12
Iteration: 424 | Episodes: 17200 | Median Reward: 42.53 | Avg Reward: 42.11 | Max Reward: 47.12
Iteration: 426 | Episodes: 17300 | Median Reward: 43.55 | Avg Reward: 42.88 | Max Reward: 47.12
Iteration: 429 | Episodes: 17400 | Median Reward: 41.60 | Avg Reward: 41.53 | Max Reward: 47.12
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.2         |
| time/                   |               |
|    fps                  | 919           |
|    iterations           | 430           |
|    time_elapsed         | 1916          |
|    total_timesteps      | 1761280       |
| train/                  |               |
|    approx_kl            | 0.00084216474 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -232          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -32           |
|    n_updates            | 4290          |
|    policy_gradient_loss | 0.00137       |
|    std                  | 1.17          |
|    value_loss           | 3.19          |
-------------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: 42.20 | Avg Reward: 42.17 | Max Reward: 47.12
Iteration: 433 | Episodes: 17600 | Median Reward: 43.26 | Avg Reward: 41.72 | Max Reward: 47.12
Iteration: 436 | Episodes: 17700 | Median Reward: 40.63 | Avg Reward: 40.32 | Max Reward: 47.12
Iteration: 438 | Episodes: 17800 | Median Reward: 42.05 | Avg Reward: 41.64 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.2       |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 440         |
|    time_elapsed         | 1961        |
|    total_timesteps      | 1802240     |
| train/                  |             |
|    approx_kl            | 0.004659878 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -233        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -33.3       |
|    n_updates            | 4390        |
|    policy_gradient_loss | 0.000491    |
|    std                  | 1.18        |
|    value_loss           | 2.82        |
-----------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 44.77 | Avg Reward: 44.24 | Max Reward: 47.12
Iteration: 443 | Episodes: 18000 | Median Reward: 42.03 | Avg Reward: 42.30 | Max Reward: 47.12
Iteration: 446 | Episodes: 18100 | Median Reward: 41.05 | Avg Reward: 39.51 | Max Reward: 47.12
Iteration: 448 | Episodes: 18200 | Median Reward: 38.70 | Avg Reward: 40.00 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.5       |
| time/                   |             |
|    fps                  | 919         |
|    iterations           | 450         |
|    time_elapsed         | 2005        |
|    total_timesteps      | 1843200     |
| train/                  |             |
|    approx_kl            | 0.032222625 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -234        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -33.6       |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.0229     |
|    std                  | 1.19        |
|    value_loss           | 1.26        |
-----------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 44.52 | Avg Reward: 43.54 | Max Reward: 47.12
Iteration: 453 | Episodes: 18400 | Median Reward: 43.81 | Avg Reward: 43.52 | Max Reward: 47.12
Iteration: 456 | Episodes: 18500 | Median Reward: 45.34 | Avg Reward: 42.74 | Max Reward: 47.12
Iteration: 458 | Episodes: 18600 | Median Reward: 43.30 | Avg Reward: 42.72 | Max Reward: 47.12
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -57.4      |
| time/                   |            |
|    fps                  | 918        |
|    iterations           | 460        |
|    time_elapsed         | 2051       |
|    total_timesteps      | 1884160    |
| train/                  |            |
|    approx_kl            | 0.00927265 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -235       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -32.4      |
|    n_updates            | 4590       |
|    policy_gradient_loss | -0.00657   |
|    std                  | 1.19       |
|    value_loss           | 2.28       |
----------------------------------------
Iteration: 461 | Episodes: 18700 | Median Reward: 44.95 | Avg Reward: 43.96 | Max Reward: 47.12
Iteration: 463 | Episodes: 18800 | Median Reward: 41.99 | Avg Reward: 42.12 | Max Reward: 47.12
Iteration: 466 | Episodes: 18900 | Median Reward: 38.96 | Avg Reward: 38.82 | Max Reward: 47.12
Iteration: 468 | Episodes: 19000 | Median Reward: 41.46 | Avg Reward: 39.86 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.2       |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 470         |
|    time_elapsed         | 2096        |
|    total_timesteps      | 1925120     |
| train/                  |             |
|    approx_kl            | 0.021576988 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -236        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -34.8       |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.0308     |
|    std                  | 1.2         |
|    value_loss           | 4.35        |
-----------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 42.46 | Avg Reward: 42.19 | Max Reward: 47.12
Iteration: 473 | Episodes: 19200 | Median Reward: 41.93 | Avg Reward: 41.63 | Max Reward: 47.12
Iteration: 475 | Episodes: 19300 | Median Reward: 43.21 | Avg Reward: 43.17 | Max Reward: 47.12
Iteration: 478 | Episodes: 19400 | Median Reward: 41.65 | Avg Reward: 41.61 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.2       |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 480         |
|    time_elapsed         | 2140        |
|    total_timesteps      | 1966080     |
| train/                  |             |
|    approx_kl            | 0.003873943 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -238        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -32.9       |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.00438    |
|    std                  | 1.2         |
|    value_loss           | 3.26        |
-----------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 39.38 | Avg Reward: 40.44 | Max Reward: 47.12
Iteration: 483 | Episodes: 19600 | Median Reward: 42.11 | Avg Reward: 41.52 | Max Reward: 47.12
Iteration: 485 | Episodes: 19700 | Median Reward: 42.89 | Avg Reward: 43.00 | Max Reward: 47.12
Iteration: 488 | Episodes: 19800 | Median Reward: 42.22 | Avg Reward: 42.42 | Max Reward: 47.12
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.9         |
| time/                   |               |
|    fps                  | 918           |
|    iterations           | 490           |
|    time_elapsed         | 2184          |
|    total_timesteps      | 2007040       |
| train/                  |               |
|    approx_kl            | 0.00042625918 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -238          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -34.6         |
|    n_updates            | 4890          |
|    policy_gradient_loss | -0.000139     |
|    std                  | 1.21          |
|    value_loss           | 3.06          |
-------------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 44.61 | Avg Reward: 44.30 | Max Reward: 47.12
Iteration: 493 | Episodes: 20000 | Median Reward: 44.24 | Avg Reward: 43.43 | Max Reward: 47.12
Iteration: 495 | Episodes: 20100 | Median Reward: 44.66 | Avg Reward: 43.05 | Max Reward: 47.12
Iteration: 498 | Episodes: 20200 | Median Reward: 44.16 | Avg Reward: 42.35 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 500          |
|    time_elapsed         | 2228         |
|    total_timesteps      | 2048000      |
| train/                  |              |
|    approx_kl            | 0.0013798017 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -240         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -35.8        |
|    n_updates            | 4990         |
|    policy_gradient_loss | -0.000906    |
|    std                  | 1.22         |
|    value_loss           | 0.563        |
------------------------------------------
Iteration: 500 | Episodes: 20300 | Median Reward: 43.85 | Avg Reward: 42.77 | Max Reward: 47.12
Iteration: 503 | Episodes: 20400 | Median Reward: 44.25 | Avg Reward: 43.35 | Max Reward: 47.12
Iteration: 505 | Episodes: 20500 | Median Reward: 42.58 | Avg Reward: 42.76 | Max Reward: 47.12
Iteration: 507 | Episodes: 20600 | Median Reward: 42.78 | Avg Reward: 41.76 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.8        |
| time/                   |              |
|    fps                  | 917          |
|    iterations           | 510          |
|    time_elapsed         | 2277         |
|    total_timesteps      | 2088960      |
| train/                  |              |
|    approx_kl            | 0.0012102816 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -241         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -35.7        |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.000585    |
|    std                  | 1.22         |
|    value_loss           | 2.65         |
------------------------------------------
Iteration: 510 | Episodes: 20700 | Median Reward: 42.65 | Avg Reward: 43.36 | Max Reward: 47.12
Iteration: 512 | Episodes: 20800 | Median Reward: 44.02 | Avg Reward: 42.29 | Max Reward: 47.12
Iteration: 515 | Episodes: 20900 | Median Reward: 43.33 | Avg Reward: 42.75 | Max Reward: 47.12
Iteration: 517 | Episodes: 21000 | Median Reward: 44.43 | Avg Reward: 43.35 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.3       |
| time/                   |             |
|    fps                  | 917         |
|    iterations           | 520         |
|    time_elapsed         | 2321        |
|    total_timesteps      | 2129920     |
| train/                  |             |
|    approx_kl            | 0.009258828 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -243        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -35.8       |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.00568    |
|    std                  | 1.23        |
|    value_loss           | 1.38        |
-----------------------------------------
Iteration: 520 | Episodes: 21100 | Median Reward: 44.79 | Avg Reward: 43.53 | Max Reward: 47.12
Iteration: 522 | Episodes: 21200 | Median Reward: 44.20 | Avg Reward: 43.04 | Max Reward: 47.12
Iteration: 525 | Episodes: 21300 | Median Reward: 40.65 | Avg Reward: 41.57 | Max Reward: 47.12
Iteration: 527 | Episodes: 21400 | Median Reward: 41.10 | Avg Reward: 41.25 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 917          |
|    iterations           | 530          |
|    time_elapsed         | 2366         |
|    total_timesteps      | 2170880      |
| train/                  |              |
|    approx_kl            | 0.0089993235 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -244         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -36.2        |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.00696     |
|    std                  | 1.24         |
|    value_loss           | 0.426        |
------------------------------------------
Iteration: 530 | Episodes: 21500 | Median Reward: 42.06 | Avg Reward: 42.18 | Max Reward: 47.12
Iteration: 532 | Episodes: 21600 | Median Reward: 42.09 | Avg Reward: 41.55 | Max Reward: 47.12
Iteration: 535 | Episodes: 21700 | Median Reward: 41.14 | Avg Reward: 40.93 | Max Reward: 47.12
Iteration: 537 | Episodes: 21800 | Median Reward: 42.01 | Avg Reward: 42.40 | Max Reward: 47.12
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.9         |
| time/                   |               |
|    fps                  | 917           |
|    iterations           | 540           |
|    time_elapsed         | 2410          |
|    total_timesteps      | 2211840       |
| train/                  |               |
|    approx_kl            | 0.00040078087 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -246          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -36.6         |
|    n_updates            | 5390          |
|    policy_gradient_loss | -3.59e-05     |
|    std                  | 1.25          |
|    value_loss           | 1.23          |
-------------------------------------------
Iteration: 540 | Episodes: 21900 | Median Reward: 43.28 | Avg Reward: 42.95 | Max Reward: 47.12
Iteration: 542 | Episodes: 22000 | Median Reward: 44.40 | Avg Reward: 43.62 | Max Reward: 47.12
Iteration: 544 | Episodes: 22100 | Median Reward: 44.72 | Avg Reward: 44.39 | Max Reward: 47.12
Iteration: 547 | Episodes: 22200 | Median Reward: 45.19 | Avg Reward: 44.44 | Max Reward: 47.12
Iteration: 549 | Episodes: 22300 | Median Reward: 44.41 | Avg Reward: 43.83 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.1        |
| time/                   |              |
|    fps                  | 917          |
|    iterations           | 550          |
|    time_elapsed         | 2456         |
|    total_timesteps      | 2252800      |
| train/                  |              |
|    approx_kl            | 0.0049236347 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -246         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -36.2        |
|    n_updates            | 5490         |
|    policy_gradient_loss | -0.00263     |
|    std                  | 1.26         |
|    value_loss           | 0.957        |
------------------------------------------
Iteration: 552 | Episodes: 22400 | Median Reward: 41.80 | Avg Reward: 41.38 | Max Reward: 47.12
Iteration: 554 | Episodes: 22500 | Median Reward: 43.14 | Avg Reward: 41.77 | Max Reward: 47.12
Iteration: 557 | Episodes: 22600 | Median Reward: 44.19 | Avg Reward: 43.62 | Max Reward: 47.12
Iteration: 559 | Episodes: 22700 | Median Reward: 44.11 | Avg Reward: 43.45 | Max Reward: 47.12
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -56.6      |
| time/                   |            |
|    fps                  | 916        |
|    iterations           | 560        |
|    time_elapsed         | 2503       |
|    total_timesteps      | 2293760    |
| train/                  |            |
|    approx_kl            | 0.00198119 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -247       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -36.9      |
|    n_updates            | 5590       |
|    policy_gradient_loss | -0.00427   |
|    std                  | 1.26       |
|    value_loss           | 1.48       |
----------------------------------------
Iteration: 562 | Episodes: 22800 | Median Reward: 44.42 | Avg Reward: 44.03 | Max Reward: 47.12
Iteration: 564 | Episodes: 22900 | Median Reward: 41.75 | Avg Reward: 41.52 | Max Reward: 47.12
Iteration: 567 | Episodes: 23000 | Median Reward: 44.01 | Avg Reward: 43.60 | Max Reward: 47.12
Iteration: 569 | Episodes: 23100 | Median Reward: 44.48 | Avg Reward: 43.13 | Max Reward: 47.12
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.1         |
| time/                   |               |
|    fps                  | 915           |
|    iterations           | 570           |
|    time_elapsed         | 2549          |
|    total_timesteps      | 2334720       |
| train/                  |               |
|    approx_kl            | 0.00017685069 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -248          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -36.7         |
|    n_updates            | 5690          |
|    policy_gradient_loss | 5.5e-05       |
|    std                  | 1.27          |
|    value_loss           | 0.873         |
-------------------------------------------
Iteration: 572 | Episodes: 23200 | Median Reward: 43.57 | Avg Reward: 42.93 | Max Reward: 47.12
Iteration: 574 | Episodes: 23300 | Median Reward: 44.54 | Avg Reward: 43.45 | Max Reward: 47.12
Iteration: 577 | Episodes: 23400 | Median Reward: 44.72 | Avg Reward: 43.05 | Max Reward: 47.12
Iteration: 579 | Episodes: 23500 | Median Reward: 44.79 | Avg Reward: 43.81 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.4        |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 580          |
|    time_elapsed         | 2596         |
|    total_timesteps      | 2375680      |
| train/                  |              |
|    approx_kl            | 0.0013022849 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -249         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -37.2        |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.00368     |
|    std                  | 1.28         |
|    value_loss           | 0.394        |
------------------------------------------
Iteration: 581 | Episodes: 23600 | Median Reward: 42.57 | Avg Reward: 41.52 | Max Reward: 47.12
Iteration: 584 | Episodes: 23700 | Median Reward: 43.25 | Avg Reward: 43.15 | Max Reward: 47.12
Iteration: 586 | Episodes: 23800 | Median Reward: 42.18 | Avg Reward: 42.54 | Max Reward: 47.12
Iteration: 589 | Episodes: 23900 | Median Reward: 44.87 | Avg Reward: 43.75 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 590          |
|    time_elapsed         | 2642         |
|    total_timesteps      | 2416640      |
| train/                  |              |
|    approx_kl            | 0.0011158152 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -250         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -37.2        |
|    n_updates            | 5890         |
|    policy_gradient_loss | -0.00224     |
|    std                  | 1.28         |
|    value_loss           | 1.06         |
------------------------------------------
Iteration: 591 | Episodes: 24000 | Median Reward: 44.46 | Avg Reward: 44.13 | Max Reward: 47.12
Iteration: 594 | Episodes: 24100 | Median Reward: 44.29 | Avg Reward: 42.79 | Max Reward: 47.12
Iteration: 596 | Episodes: 24200 | Median Reward: 44.18 | Avg Reward: 43.92 | Max Reward: 47.12
Iteration: 599 | Episodes: 24300 | Median Reward: 42.65 | Avg Reward: 43.07 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 600         |
|    time_elapsed         | 2687        |
|    total_timesteps      | 2457600     |
| train/                  |             |
|    approx_kl            | 0.008436262 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -250        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -37.3       |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.00976    |
|    std                  | 1.29        |
|    value_loss           | 0.465       |
-----------------------------------------
Iteration: 601 | Episodes: 24400 | Median Reward: 43.90 | Avg Reward: 43.42 | Max Reward: 47.12
Iteration: 604 | Episodes: 24500 | Median Reward: 44.34 | Avg Reward: 43.89 | Max Reward: 47.12
Iteration: 606 | Episodes: 24600 | Median Reward: 44.48 | Avg Reward: 43.53 | Max Reward: 47.12
Iteration: 609 | Episodes: 24700 | Median Reward: 44.30 | Avg Reward: 43.48 | Max Reward: 47.12
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -57        |
| time/                   |            |
|    fps                  | 914        |
|    iterations           | 610        |
|    time_elapsed         | 2732       |
|    total_timesteps      | 2498560    |
| train/                  |            |
|    approx_kl            | 0.00441803 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -251       |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0005     |
|    loss                 | -37.4      |
|    n_updates            | 6090       |
|    policy_gradient_loss | -0.0039    |
|    std                  | 1.3        |
|    value_loss           | 1.79       |
----------------------------------------
Iteration: 611 | Episodes: 24800 | Median Reward: 44.39 | Avg Reward: 42.71 | Max Reward: 47.12
Iteration: 613 | Episodes: 24900 | Median Reward: 44.15 | Avg Reward: 42.43 | Max Reward: 47.12
Iteration: 616 | Episodes: 25000 | Median Reward: 43.64 | Avg Reward: 42.72 | Max Reward: 47.12
Iteration: 618 | Episodes: 25100 | Median Reward: 42.61 | Avg Reward: 42.48 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.8       |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 620         |
|    time_elapsed         | 2777        |
|    total_timesteps      | 2539520     |
| train/                  |             |
|    approx_kl            | 0.003589262 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -252        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -28.2       |
|    n_updates            | 6190        |
|    policy_gradient_loss | -0.0115     |
|    std                  | 1.3         |
|    value_loss           | 12.8        |
-----------------------------------------
Iteration: 621 | Episodes: 25200 | Median Reward: 42.12 | Avg Reward: 42.68 | Max Reward: 47.12
Iteration: 623 | Episodes: 25300 | Median Reward: 43.64 | Avg Reward: 43.46 | Max Reward: 47.12
Iteration: 626 | Episodes: 25400 | Median Reward: 42.78 | Avg Reward: 43.16 | Max Reward: 47.12
Iteration: 628 | Episodes: 25500 | Median Reward: 44.86 | Avg Reward: 43.89 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 913          |
|    iterations           | 630          |
|    time_elapsed         | 2823         |
|    total_timesteps      | 2580480      |
| train/                  |              |
|    approx_kl            | 0.0001509185 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -252         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -36.5        |
|    n_updates            | 6290         |
|    policy_gradient_loss | -0.000966    |
|    std                  | 1.31         |
|    value_loss           | 5.3          |
------------------------------------------
Iteration: 631 | Episodes: 25600 | Median Reward: 45.08 | Avg Reward: 44.19 | Max Reward: 47.12
Iteration: 633 | Episodes: 25700 | Median Reward: 44.74 | Avg Reward: 43.20 | Max Reward: 47.12
Iteration: 636 | Episodes: 25800 | Median Reward: 44.73 | Avg Reward: 44.00 | Max Reward: 47.12
Iteration: 638 | Episodes: 25900 | Median Reward: 44.75 | Avg Reward: 43.85 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57         |
| time/                   |             |
|    fps                  | 913         |
|    iterations           | 640         |
|    time_elapsed         | 2868        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.001325119 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -253        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -37.7       |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.00228    |
|    std                  | 1.31        |
|    value_loss           | 4.82        |
-----------------------------------------
Iteration: 641 | Episodes: 26000 | Median Reward: 44.76 | Avg Reward: 43.37 | Max Reward: 47.12
Iteration: 643 | Episodes: 26100 | Median Reward: 44.67 | Avg Reward: 44.16 | Max Reward: 47.12
Iteration: 646 | Episodes: 26200 | Median Reward: 42.22 | Avg Reward: 42.71 | Max Reward: 47.12
Iteration: 648 | Episodes: 26300 | Median Reward: 44.58 | Avg Reward: 43.37 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 650          |
|    time_elapsed         | 2911         |
|    total_timesteps      | 2662400      |
| train/                  |              |
|    approx_kl            | 0.0008025308 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -253         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -36.6        |
|    n_updates            | 6490         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 1.32         |
|    value_loss           | 1.13         |
------------------------------------------
Iteration: 650 | Episodes: 26400 | Median Reward: 42.34 | Avg Reward: 42.49 | Max Reward: 47.12
Iteration: 653 | Episodes: 26500 | Median Reward: 42.21 | Avg Reward: 42.07 | Max Reward: 47.12
Iteration: 655 | Episodes: 26600 | Median Reward: 44.51 | Avg Reward: 41.82 | Max Reward: 47.12
Iteration: 658 | Episodes: 26700 | Median Reward: 44.52 | Avg Reward: 44.23 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 660          |
|    time_elapsed         | 2955         |
|    total_timesteps      | 2703360      |
| train/                  |              |
|    approx_kl            | 0.0003309634 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -254         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -37.6        |
|    n_updates            | 6590         |
|    policy_gradient_loss | -0.000355    |
|    std                  | 1.33         |
|    value_loss           | 1.25         |
------------------------------------------
Iteration: 660 | Episodes: 26800 | Median Reward: 44.48 | Avg Reward: 43.44 | Max Reward: 47.12
Iteration: 663 | Episodes: 26900 | Median Reward: 44.87 | Avg Reward: 43.65 | Max Reward: 47.12
Iteration: 665 | Episodes: 27000 | Median Reward: 42.51 | Avg Reward: 41.19 | Max Reward: 47.12
Iteration: 668 | Episodes: 27100 | Median Reward: 44.74 | Avg Reward: 43.84 | Max Reward: 47.12
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 670           |
|    time_elapsed         | 3000          |
|    total_timesteps      | 2744320       |
| train/                  |               |
|    approx_kl            | 0.00022759111 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -254          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -37.6         |
|    n_updates            | 6690          |
|    policy_gradient_loss | -0.00031      |
|    std                  | 1.34          |
|    value_loss           | 1.4           |
-------------------------------------------
Iteration: 670 | Episodes: 27200 | Median Reward: 43.66 | Avg Reward: 43.51 | Max Reward: 47.12
Iteration: 673 | Episodes: 27300 | Median Reward: 44.94 | Avg Reward: 44.51 | Max Reward: 47.12
Iteration: 675 | Episodes: 27400 | Median Reward: 44.66 | Avg Reward: 43.16 | Max Reward: 47.12
Iteration: 678 | Episodes: 27500 | Median Reward: 44.66 | Avg Reward: 43.25 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 915          |
|    iterations           | 680          |
|    time_elapsed         | 3043         |
|    total_timesteps      | 2785280      |
| train/                  |              |
|    approx_kl            | 0.0015428469 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -255         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -35          |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.0025      |
|    std                  | 1.35         |
|    value_loss           | 2.05         |
------------------------------------------
Iteration: 680 | Episodes: 27600 | Median Reward: 44.97 | Avg Reward: 44.09 | Max Reward: 47.12
Iteration: 683 | Episodes: 27700 | Median Reward: 43.25 | Avg Reward: 42.91 | Max Reward: 47.12
Iteration: 685 | Episodes: 27800 | Median Reward: 41.84 | Avg Reward: 42.35 | Max Reward: 47.12
Iteration: 687 | Episodes: 27900 | Median Reward: 43.52 | Avg Reward: 42.78 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.7       |
| time/                   |             |
|    fps                  | 915         |
|    iterations           | 690         |
|    time_elapsed         | 3088        |
|    total_timesteps      | 2826240     |
| train/                  |             |
|    approx_kl            | 0.016242389 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -255        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -38.3       |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.0221     |
|    std                  | 1.35        |
|    value_loss           | 0.559       |
-----------------------------------------
Iteration: 690 | Episodes: 28000 | Median Reward: 43.49 | Avg Reward: 43.04 | Max Reward: 47.12
Iteration: 692 | Episodes: 28100 | Median Reward: 44.46 | Avg Reward: 43.57 | Max Reward: 47.12
Iteration: 695 | Episodes: 28200 | Median Reward: 42.22 | Avg Reward: 42.06 | Max Reward: 47.12
Iteration: 697 | Episodes: 28300 | Median Reward: 42.64 | Avg Reward: 42.37 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57         |
| time/                   |             |
|    fps                  | 915         |
|    iterations           | 700         |
|    time_elapsed         | 3133        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.015077794 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -256        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -38.2       |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.00694    |
|    std                  | 1.37        |
|    value_loss           | 0.239       |
-----------------------------------------
Iteration: 700 | Episodes: 28400 | Median Reward: 44.35 | Avg Reward: 43.65 | Max Reward: 47.12
Iteration: 702 | Episodes: 28500 | Median Reward: 44.82 | Avg Reward: 44.06 | Max Reward: 47.12
Iteration: 705 | Episodes: 28600 | Median Reward: 42.69 | Avg Reward: 42.22 | Max Reward: 47.12
Iteration: 707 | Episodes: 28700 | Median Reward: 43.90 | Avg Reward: 43.02 | Max Reward: 47.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.7       |
| time/                   |             |
|    fps                  | 915         |
|    iterations           | 710         |
|    time_elapsed         | 3177        |
|    total_timesteps      | 2908160     |
| train/                  |             |
|    approx_kl            | 0.076347366 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.3         |
|    entropy_loss         | -257        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -38.2       |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.0366     |
|    std                  | 1.37        |
|    value_loss           | 1.19        |
-----------------------------------------
Iteration: 710 | Episodes: 28800 | Median Reward: 43.60 | Avg Reward: 42.33 | Max Reward: 47.12
Iteration: 712 | Episodes: 28900 | Median Reward: 44.46 | Avg Reward: 43.64 | Max Reward: 47.12
Iteration: 715 | Episodes: 29000 | Median Reward: 42.59 | Avg Reward: 42.59 | Max Reward: 47.12
Iteration: 717 | Episodes: 29100 | Median Reward: 41.80 | Avg Reward: 41.64 | Max Reward: 47.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.5        |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 720          |
|    time_elapsed         | 3223         |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0012189571 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -258         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -33.3        |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.00345     |
|    std                  | 1.38         |
|    value_loss           | 3.98         |
------------------------------------------
Iteration: 720 | Episodes: 29200 | Median Reward: 42.16 | Avg Reward: 42.54 | Max Reward: 47.12
Iteration: 722 | Episodes: 29300 | Median Reward: 44.40 | Avg Reward: 42.71 | Max Reward: 47.37
Iteration: 724 | Episodes: 29400 | Median Reward: 44.58 | Avg Reward: 43.91 | Max Reward: 47.37
Iteration: 727 | Episodes: 29500 | Median Reward: 43.54 | Avg Reward: 43.36 | Max Reward: 47.37
Iteration: 729 | Episodes: 29600 | Median Reward: 45.37 | Avg Reward: 44.87 | Max Reward: 47.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55          |
| time/                   |              |
|    fps                  | 915          |
|    iterations           | 730          |
|    time_elapsed         | 3267         |
|    total_timesteps      | 2990080      |
| train/                  |              |
|    approx_kl            | 0.0003160055 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -259         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -35.9        |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.000425    |
|    std                  | 1.39         |
|    value_loss           | 5.01         |
------------------------------------------
Iteration: 732 | Episodes: 29700 | Median Reward: 44.43 | Avg Reward: 43.67 | Max Reward: 47.37
Iteration: 734 | Episodes: 29800 | Median Reward: 41.65 | Avg Reward: 41.85 | Max Reward: 47.37
Iteration: 737 | Episodes: 29900 | Median Reward: 45.01 | Avg Reward: 43.49 | Max Reward: 47.37
Iteration: 739 | Episodes: 30000 | Median Reward: 44.81 | Avg Reward: 43.98 | Max Reward: 47.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 915          |
|    iterations           | 740          |
|    time_elapsed         | 3310         |
|    total_timesteps      | 3031040      |
| train/                  |              |
|    approx_kl            | 0.0058634556 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -260         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -36.1        |
|    n_updates            | 7390         |
|    policy_gradient_loss | -0.00765     |
|    std                  | 1.4          |
|    value_loss           | 2.49         |
------------------------------------------
Iteration: 742 | Episodes: 30100 | Median Reward: 42.78 | Avg Reward: 43.50 | Max Reward: 47.37
Iteration: 744 | Episodes: 30200 | Median Reward: 45.23 | Avg Reward: 44.66 | Max Reward: 47.37
Iteration: 747 | Episodes: 30300 | Median Reward: 43.33 | Avg Reward: 42.51 | Max Reward: 47.37
Iteration: 749 | Episodes: 30400 | Median Reward: 44.42 | Avg Reward: 44.07 | Max Reward: 47.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 750          |
|    time_elapsed         | 3358         |
|    total_timesteps      | 3072000      |
| train/                  |              |
|    approx_kl            | 0.0013741757 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -261         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -38.8        |
|    n_updates            | 7490         |
|    policy_gradient_loss | -0.00099     |
|    std                  | 1.4          |
|    value_loss           | 4.25         |
------------------------------------------
Iteration: 752 | Episodes: 30500 | Median Reward: 44.27 | Avg Reward: 43.99 | Max Reward: 47.37
Iteration: 754 | Episodes: 30600 | Median Reward: 45.09 | Avg Reward: 42.86 | Max Reward: 47.37
Iteration: 757 | Episodes: 30700 | Median Reward: 42.04 | Avg Reward: 40.99 | Max Reward: 47.37
Iteration: 759 | Episodes: 30800 | Median Reward: 44.29 | Avg Reward: 43.09 | Max Reward: 47.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.8       |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 760         |
|    time_elapsed         | 3404        |
|    total_timesteps      | 3112960     |
| train/                  |             |
|    approx_kl            | 0.060005303 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.3         |
|    entropy_loss         | -262        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -38.4       |
|    n_updates            | 7590        |
|    policy_gradient_loss | -0.0367     |
|    std                  | 1.41        |
|    value_loss           | 0.853       |
-----------------------------------------
Iteration: 761 | Episodes: 30900 | Median Reward: 44.22 | Avg Reward: 43.38 | Max Reward: 47.37
Iteration: 764 | Episodes: 31000 | Median Reward: 44.77 | Avg Reward: 43.39 | Max Reward: 47.37
Iteration: 766 | Episodes: 31100 | Median Reward: 42.38 | Avg Reward: 42.32 | Max Reward: 47.37
Iteration: 769 | Episodes: 31200 | Median Reward: 44.87 | Avg Reward: 44.09 | Max Reward: 47.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.4        |
| time/                   |              |
|    fps                  | 913          |
|    iterations           | 770          |
|    time_elapsed         | 3451         |
|    total_timesteps      | 3153920      |
| train/                  |              |
|    approx_kl            | 0.0040955567 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -263         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -39.2        |
|    n_updates            | 7690         |
|    policy_gradient_loss | 0.000423     |
|    std                  | 1.42         |
|    value_loss           | 0.653        |
------------------------------------------
Iteration: 771 | Episodes: 31300 | Median Reward: 44.93 | Avg Reward: 43.46 | Max Reward: 47.37
Iteration: 774 | Episodes: 31400 | Median Reward: 45.27 | Avg Reward: 44.07 | Max Reward: 47.37
Iteration: 776 | Episodes: 31500 | Median Reward: 44.23 | Avg Reward: 43.19 | Max Reward: 47.37
Iteration: 779 | Episodes: 31600 | Median Reward: 45.15 | Avg Reward: 44.85 | Max Reward: 47.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.3         |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 780           |
|    time_elapsed         | 3495          |
|    total_timesteps      | 3194880       |
| train/                  |               |
|    approx_kl            | 0.00022591495 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -263          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -39.2         |
|    n_updates            | 7790          |
|    policy_gradient_loss | -0.000598     |
|    std                  | 1.43          |
|    value_loss           | 0.691         |
-------------------------------------------
Iteration: 781 | Episodes: 31700 | Median Reward: 44.79 | Avg Reward: 43.42 | Max Reward: 47.37
Iteration: 784 | Episodes: 31800 | Median Reward: 44.72 | Avg Reward: 43.71 | Max Reward: 47.37
Iteration: 786 | Episodes: 31900 | Median Reward: 45.07 | Avg Reward: 44.40 | Max Reward: 47.37
Iteration: 789 | Episodes: 32000 | Median Reward: 45.07 | Avg Reward: 43.98 | Max Reward: 47.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.7        |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 790          |
|    time_elapsed         | 3540         |
|    total_timesteps      | 3235840      |
| train/                  |              |
|    approx_kl            | 0.0043993792 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -264         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -35.6        |
|    n_updates            | 7890         |
|    policy_gradient_loss | -0.006       |
|    std                  | 1.44         |
|    value_loss           | 3.53         |
------------------------------------------
Iteration: 791 | Episodes: 32100 | Median Reward: 44.54 | Avg Reward: 43.42 | Max Reward: 47.37
Iteration: 793 | Episodes: 32200 | Median Reward: 41.70 | Avg Reward: 42.51 | Max Reward: 47.37
Iteration: 796 | Episodes: 32300 | Median Reward: 45.35 | Avg Reward: 45.14 | Max Reward: 47.37
Iteration: 798 | Episodes: 32400 | Median Reward: 45.35 | Avg Reward: 43.92 | Max Reward: 47.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.6        |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 800          |
|    time_elapsed         | 3584         |
|    total_timesteps      | 3276800      |
| train/                  |              |
|    approx_kl            | 0.0011035616 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -265         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -38.3        |
|    n_updates            | 7990         |
|    policy_gradient_loss | -0.00665     |
|    std                  | 1.45         |
|    value_loss           | 4.79         |
------------------------------------------
Iteration: 801 | Episodes: 32500 | Median Reward: 41.71 | Avg Reward: 42.19 | Max Reward: 47.37
Iteration: 803 | Episodes: 32600 | Median Reward: 42.10 | Avg Reward: 41.84 | Max Reward: 47.37
Iteration: 806 | Episodes: 32700 | Median Reward: 44.01 | Avg Reward: 43.30 | Max Reward: 47.37
Iteration: 808 | Episodes: 32800 | Median Reward: 44.75 | Avg Reward: 43.65 | Max Reward: 47.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 810          |
|    time_elapsed         | 3628         |
|    total_timesteps      | 3317760      |
| train/                  |              |
|    approx_kl            | 0.0016024273 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -266         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -39.9        |
|    n_updates            | 8090         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 1.46         |
|    value_loss           | 0.685        |
------------------------------------------
Iteration: 811 | Episodes: 32900 | Median Reward: 44.53 | Avg Reward: 43.35 | Max Reward: 47.37
Iteration: 813 | Episodes: 33000 | Median Reward: 44.91 | Avg Reward: 43.12 | Max Reward: 47.37
Iteration: 816 | Episodes: 33100 | Median Reward: 44.46 | Avg Reward: 43.76 | Max Reward: 47.37
Iteration: 818 | Episodes: 33200 | Median Reward: 44.79 | Avg Reward: 44.42 | Max Reward: 47.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.9         |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 820           |
|    time_elapsed         | 3672          |
|    total_timesteps      | 3358720       |
| train/                  |               |
|    approx_kl            | 0.00010275094 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -267          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -38.9         |
|    n_updates            | 8190          |
|    policy_gradient_loss | -0.000131     |
|    std                  | 1.47          |
|    value_loss           | 1.77          |
-------------------------------------------
Iteration: 821 | Episodes: 33300 | Median Reward: 44.28 | Avg Reward: 43.52 | Max Reward: 47.37
Iteration: 823 | Episodes: 33400 | Median Reward: 44.77 | Avg Reward: 43.29 | Max Reward: 47.37
Iteration: 826 | Episodes: 33500 | Median Reward: 43.31 | Avg Reward: 42.69 | Max Reward: 47.37
Iteration: 828 | Episodes: 33600 | Median Reward: 44.76 | Avg Reward: 43.23 | Max Reward: 47.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.7       |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 830         |
|    time_elapsed         | 3717        |
|    total_timesteps      | 3399680     |
| train/                  |             |
|    approx_kl            | 0.004487736 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -268        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -40         |
|    n_updates            | 8290        |
|    policy_gradient_loss | -0.0103     |
|    std                  | 1.48        |
|    value_loss           | 0.464       |
-----------------------------------------
Iteration: 830 | Episodes: 33700 | Median Reward: 45.17 | Avg Reward: 44.11 | Max Reward: 47.37
Iteration: 833 | Episodes: 33800 | Median Reward: 45.32 | Avg Reward: 44.01 | Max Reward: 47.37
Iteration: 835 | Episodes: 33900 | Median Reward: 45.09 | Avg Reward: 44.49 | Max Reward: 47.37
Iteration: 838 | Episodes: 34000 | Median Reward: 42.98 | Avg Reward: 42.20 | Max Reward: 47.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -57.6      |
| time/                   |            |
|    fps                  | 914        |
|    iterations           | 840        |
|    time_elapsed         | 3762       |
|    total_timesteps      | 3440640    |
| train/                  |            |
|    approx_kl            | 0.06501664 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.3        |
|    entropy_loss         | -269       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -39.8      |
|    n_updates            | 8390       |
|    policy_gradient_loss | -0.0942    |
|    std                  | 1.49       |
|    value_loss           | 0.729      |
----------------------------------------
Iteration: 840 | Episodes: 34100 | Median Reward: 42.26 | Avg Reward: 42.26 | Max Reward: 47.37
Iteration: 843 | Episodes: 34200 | Median Reward: 45.11 | Avg Reward: 42.53 | Max Reward: 47.37
Iteration: 845 | Episodes: 34300 | Median Reward: 44.75 | Avg Reward: 43.71 | Max Reward: 47.37
Iteration: 848 | Episodes: 34400 | Median Reward: 44.45 | Avg Reward: 43.65 | Max Reward: 47.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 850          |
|    time_elapsed         | 3808         |
|    total_timesteps      | 3481600      |
| train/                  |              |
|    approx_kl            | 0.0063484497 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -269         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -40          |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.0151      |
|    std                  | 1.5          |
|    value_loss           | 1.38         |
------------------------------------------
Iteration: 850 | Episodes: 34500 | Median Reward: 43.43 | Avg Reward: 42.71 | Max Reward: 47.37
Iteration: 853 | Episodes: 34600 | Median Reward: 44.29 | Avg Reward: 42.98 | Max Reward: 47.37
Iteration: 855 | Episodes: 34700 | Median Reward: 45.18 | Avg Reward: 44.17 | Max Reward: 47.37
Iteration: 858 | Episodes: 34800 | Median Reward: 45.07 | Avg Reward: 43.57 | Max Reward: 47.37
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -57.8      |
| time/                   |            |
|    fps                  | 914        |
|    iterations           | 860        |
|    time_elapsed         | 3852       |
|    total_timesteps      | 3522560    |
| train/                  |            |
|    approx_kl            | 0.00955337 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -270       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -39.9      |
|    n_updates            | 8590       |
|    policy_gradient_loss | -0.0164    |
|    std                  | 1.51       |
|    value_loss           | 0.866      |
----------------------------------------
Iteration: 860 | Episodes: 34900 | Median Reward: 44.52 | Avg Reward: 42.36 | Max Reward: 47.37
Iteration: 863 | Episodes: 35000 | Median Reward: 44.73 | Avg Reward: 43.78 | Max Reward: 47.37
Iteration: 865 | Episodes: 35100 | Median Reward: 44.41 | Avg Reward: 43.46 | Max Reward: 47.37
Iteration: 867 | Episodes: 35200 | Median Reward: 44.03 | Avg Reward: 43.03 | Max Reward: 47.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 870           |
|    time_elapsed         | 3898          |
|    total_timesteps      | 3563520       |
| train/                  |               |
|    approx_kl            | 0.00043171353 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -271          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -40           |
|    n_updates            | 8690          |
|    policy_gradient_loss | 6.69e-05      |
|    std                  | 1.51          |
|    value_loss           | 2.77          |
-------------------------------------------
Iteration: 870 | Episodes: 35300 | Median Reward: 44.36 | Avg Reward: 43.47 | Max Reward: 47.37
Iteration: 872 | Episodes: 35400 | Median Reward: 44.41 | Avg Reward: 42.73 | Max Reward: 47.37
Iteration: 875 | Episodes: 35500 | Median Reward: 44.14 | Avg Reward: 42.71 | Max Reward: 47.37
Iteration: 877 | Episodes: 35600 | Median Reward: 42.52 | Avg Reward: 43.06 | Max Reward: 47.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.7       |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 880         |
|    time_elapsed         | 3943        |
|    total_timesteps      | 3604480     |
| train/                  |             |
|    approx_kl            | 0.013824912 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -272        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -40.6       |
|    n_updates            | 8790        |
|    policy_gradient_loss | -0.012      |
|    std                  | 1.52        |
|    value_loss           | 0.531       |
-----------------------------------------
Iteration: 880 | Episodes: 35700 | Median Reward: 44.48 | Avg Reward: 43.38 | Max Reward: 47.37
Iteration: 882 | Episodes: 35800 | Median Reward: 45.13 | Avg Reward: 43.72 | Max Reward: 47.37
Iteration: 885 | Episodes: 35900 | Median Reward: 44.84 | Avg Reward: 44.48 | Max Reward: 47.37
Iteration: 887 | Episodes: 36000 | Median Reward: 44.90 | Avg Reward: 44.21 | Max Reward: 47.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.4         |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 890           |
|    time_elapsed         | 3987          |
|    total_timesteps      | 3645440       |
| train/                  |               |
|    approx_kl            | 0.00026274275 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -273          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -40.2         |
|    n_updates            | 8890          |
|    policy_gradient_loss | -0.000706     |
|    std                  | 1.53          |
|    value_loss           | 1.12          |
-------------------------------------------
Iteration: 890 | Episodes: 36100 | Median Reward: 44.97 | Avg Reward: 44.49 | Max Reward: 47.37
Iteration: 892 | Episodes: 36200 | Median Reward: 45.40 | Avg Reward: 44.77 | Max Reward: 47.37
Iteration: 895 | Episodes: 36300 | Median Reward: 45.39 | Avg Reward: 44.75 | Max Reward: 47.37
Iteration: 897 | Episodes: 36400 | Median Reward: 44.75 | Avg Reward: 43.73 | Max Reward: 47.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.6         |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 900           |
|    time_elapsed         | 4031          |
|    total_timesteps      | 3686400       |
| train/                  |               |
|    approx_kl            | 0.00038982203 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -273          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -40.9         |
|    n_updates            | 8990          |
|    policy_gradient_loss | -9.41e-05     |
|    std                  | 1.55          |
|    value_loss           | 0.424         |
-------------------------------------------
Iteration: 900 | Episodes: 36500 | Median Reward: 45.27 | Avg Reward: 44.27 | Max Reward: 47.37
Iteration: 902 | Episodes: 36600 | Median Reward: 44.71 | Avg Reward: 43.42 | Max Reward: 47.37
Iteration: 904 | Episodes: 36700 | Median Reward: 43.37 | Avg Reward: 42.83 | Max Reward: 47.37
Iteration: 907 | Episodes: 36800 | Median Reward: 44.54 | Avg Reward: 43.46 | Max Reward: 47.37
Iteration: 909 | Episodes: 36900 | Median Reward: 44.20 | Avg Reward: 43.38 | Max Reward: 47.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.6         |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 910           |
|    time_elapsed         | 4076          |
|    total_timesteps      | 3727360       |
| train/                  |               |
|    approx_kl            | 0.00021351532 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -274          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -40.7         |
|    n_updates            | 9090          |
|    policy_gradient_loss | -0.000266     |
|    std                  | 1.55          |
|    value_loss           | 0.545         |
-------------------------------------------
Iteration: 912 | Episodes: 37000 | Median Reward: 44.28 | Avg Reward: 43.59 | Max Reward: 47.37
Iteration: 914 | Episodes: 37100 | Median Reward: 45.28 | Avg Reward: 43.60 | Max Reward: 47.37
Iteration: 917 | Episodes: 37200 | Median Reward: 44.92 | Avg Reward: 43.99 | Max Reward: 47.37
Iteration: 919 | Episodes: 37300 | Median Reward: 45.11 | Avg Reward: 44.50 | Max Reward: 47.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.5        |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 920          |
|    time_elapsed         | 4121         |
|    total_timesteps      | 3768320      |
| train/                  |              |
|    approx_kl            | 0.0001345318 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -274         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -40.8        |
|    n_updates            | 9190         |
|    policy_gradient_loss | -8.58e-05    |
|    std                  | 1.56         |
|    value_loss           | 0.471        |
------------------------------------------
Iteration: 922 | Episodes: 37400 | Median Reward: 45.30 | Avg Reward: 43.66 | Max Reward: 47.37
Iteration: 924 | Episodes: 37500 | Median Reward: 43.35 | Avg Reward: 43.23 | Max Reward: 47.37
Iteration: 927 | Episodes: 37600 | Median Reward: 44.43 | Avg Reward: 43.16 | Max Reward: 47.37
Iteration: 929 | Episodes: 37700 | Median Reward: 44.22 | Avg Reward: 43.15 | Max Reward: 47.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57           |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 930           |
|    time_elapsed         | 4166          |
|    total_timesteps      | 3809280       |
| train/                  |               |
|    approx_kl            | 6.4145715e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -275          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -41.1         |
|    n_updates            | 9290          |
|    policy_gradient_loss | -1.42e-06     |
|    std                  | 1.58          |
|    value_loss           | 0.986         |
-------------------------------------------
Iteration: 932 | Episodes: 37800 | Median Reward: 43.37 | Avg Reward: 42.80 | Max Reward: 47.37
Iteration: 934 | Episodes: 37900 | Median Reward: 42.61 | Avg Reward: 41.02 | Max Reward: 47.37
Iteration: 937 | Episodes: 38000 | Median Reward: 43.35 | Avg Reward: 42.28 | Max Reward: 47.37
Iteration: 939 | Episodes: 38100 | Median Reward: 44.90 | Avg Reward: 43.67 | Max Reward: 47.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.3         |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 940           |
|    time_elapsed         | 4211          |
|    total_timesteps      | 3850240       |
| train/                  |               |
|    approx_kl            | 0.00038206624 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -276          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -41           |
|    n_updates            | 9390          |
|    policy_gradient_loss | -0.00166      |
|    std                  | 1.58          |
|    value_loss           | 1.8           |
-------------------------------------------
Iteration: 941 | Episodes: 38200 | Median Reward: 45.30 | Avg Reward: 43.92 | Max Reward: 47.37
Iteration: 944 | Episodes: 38300 | Median Reward: 42.53 | Avg Reward: 42.45 | Max Reward: 47.37
Iteration: 946 | Episodes: 38400 | Median Reward: 44.66 | Avg Reward: 43.59 | Max Reward: 47.37
Iteration: 949 | Episodes: 38500 | Median Reward: 44.77 | Avg Reward: 43.43 | Max Reward: 47.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.6       |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 950         |
|    time_elapsed         | 4256        |
|    total_timesteps      | 3891200     |
| train/                  |             |
|    approx_kl            | 0.002465102 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -277        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -41.5       |
|    n_updates            | 9490        |
|    policy_gradient_loss | -0.00167    |
|    std                  | 1.6         |
|    value_loss           | 0.308       |
-----------------------------------------
Iteration: 951 | Episodes: 38600 | Median Reward: 44.78 | Avg Reward: 42.86 | Max Reward: 47.37
Iteration: 954 | Episodes: 38700 | Median Reward: 44.51 | Avg Reward: 43.06 | Max Reward: 47.37
Iteration: 956 | Episodes: 38800 | Median Reward: 44.77 | Avg Reward: 43.59 | Max Reward: 47.37
Iteration: 959 | Episodes: 38900 | Median Reward: 42.44 | Avg Reward: 42.99 | Max Reward: 47.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.2         |
| time/                   |               |
|    fps                  | 914           |
|    iterations           | 960           |
|    time_elapsed         | 4300          |
|    total_timesteps      | 3932160       |
| train/                  |               |
|    approx_kl            | 0.00036659447 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -277          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -41.3         |
|    n_updates            | 9590          |
|    policy_gradient_loss | -0.00101      |
|    std                  | 1.61          |
|    value_loss           | 0.629         |
-------------------------------------------
Iteration: 961 | Episodes: 39000 | Median Reward: 45.28 | Avg Reward: 43.16 | Max Reward: 47.37
Iteration: 964 | Episodes: 39100 | Median Reward: 44.55 | Avg Reward: 43.30 | Max Reward: 47.37
Iteration: 966 | Episodes: 39200 | Median Reward: 45.43 | Avg Reward: 44.37 | Max Reward: 47.37
Iteration: 969 | Episodes: 39300 | Median Reward: 43.31 | Avg Reward: 42.69 | Max Reward: 47.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.9       |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 970         |
|    time_elapsed         | 4345        |
|    total_timesteps      | 3973120     |
| train/                  |             |
|    approx_kl            | 9.45381e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -278        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -39.7       |
|    n_updates            | 9690        |
|    policy_gradient_loss | -0.000182   |
|    std                  | 1.62        |
|    value_loss           | 2.28        |
-----------------------------------------
Iteration: 971 | Episodes: 39400 | Median Reward: 45.34 | Avg Reward: 44.45 | Max Reward: 47.37
Iteration: 973 | Episodes: 39500 | Median Reward: 45.44 | Avg Reward: 45.13 | Max Reward: 47.37
Iteration: 976 | Episodes: 39600 | Median Reward: 45.49 | Avg Reward: 44.54 | Max Reward: 47.37
Iteration: 978 | Episodes: 39700 | Median Reward: 44.79 | Avg Reward: 44.19 | Max Reward: 47.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.6         |
| time/                   |               |
|    fps                  | 913           |
|    iterations           | 980           |
|    time_elapsed         | 4392          |
|    total_timesteps      | 4014080       |
| train/                  |               |
|    approx_kl            | 0.00041314616 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -278          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -41.6         |
|    n_updates            | 9790          |
|    policy_gradient_loss | -0.000212     |
|    std                  | 1.63          |
|    value_loss           | 0.483         |
-------------------------------------------
Iteration: 981 | Episodes: 39800 | Median Reward: 45.39 | Avg Reward: 44.34 | Max Reward: 47.37
Iteration: 983 | Episodes: 39900 | Median Reward: 43.62 | Avg Reward: 43.15 | Max Reward: 47.37
Iteration: 986 | Episodes: 40000 | Median Reward: 42.52 | Avg Reward: 42.66 | Max Reward: 47.37
Iteration: 988 | Episodes: 40100 | Median Reward: 43.39 | Avg Reward: 43.36 | Max Reward: 47.37
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.2         |
| time/                   |               |
|    fps                  | 913           |
|    iterations           | 990           |
|    time_elapsed         | 4438          |
|    total_timesteps      | 4055040       |
| train/                  |               |
|    approx_kl            | 0.00021844827 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -279          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -40.8         |
|    n_updates            | 9890          |
|    policy_gradient_loss | -0.000103     |
|    std                  | 1.64          |
|    value_loss           | 1.55          |
-------------------------------------------
Iteration: 991 | Episodes: 40200 | Median Reward: 43.92 | Avg Reward: 43.10 | Max Reward: 47.37
Iteration: 993 | Episodes: 40300 | Median Reward: 44.32 | Avg Reward: 44.01 | Max Reward: 47.37
Iteration: 996 | Episodes: 40400 | Median Reward: 42.44 | Avg Reward: 41.96 | Max Reward: 47.37
Iteration: 998 | Episodes: 40500 | Median Reward: 44.39 | Avg Reward: 43.91 | Max Reward: 47.37
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.5        |
| time/                   |              |
|    fps                  | 913          |
|    iterations           | 1000         |
|    time_elapsed         | 4484         |
|    total_timesteps      | 4096000      |
| train/                  |              |
|    approx_kl            | 0.0010791384 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -279         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -41.5        |
|    n_updates            | 9990         |
|    policy_gradient_loss | -0.00252     |
|    std                  | 1.65         |
|    value_loss           | 0.493        |
------------------------------------------
Training End | Episodes: 40552 | Median Reward: 45.11 | Avg Reward: 44.76 | Max Reward: 47.37
Plot saved as fig_code1_1_residual_d.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> nano code1_1_residual_d.py
[?2004h[?1049h[22;0;0t[1;53r(B[m[4l[?7h[39;49m[?1h=[?1h=[?25l[39;49m(B[m[H[2J[51;95H(B[0;7m[ Reading... ](B[m[51;93H(B[0;7m[ Read 426 lines ](B[m[H(B[0;7m  GNU nano 4.8                                                                               code1_1_residual_d.py                                                                                         [1;202H(B[m[52d(B[0;7m^G(B[m Get Help     (B[0;7m^O(B[m Write Out    (B[0;7m^W(B[m Where Is     (B[0;7m^K(B[m Cut Text     (B[0;7m^J(B[m Justify[81G(B[0;7m^C(B[m Cur Pos[97G(B[0;7mM-U(B[m Undo[52;113H(B[0;7mM-A(B[m Mark Text   (B[0;7mM-](B[m To Bracket  (B[0;7mM-Q(B[m Previous    (B[0;7m^B(B[m Back[52;177H(B[0;7m^◀(B[m Prev Word[53d(B[0;7m^X(B[m Exit[53;17H(B[0;7m^R(B[m Read File    (B[0;7m^\(B[m Replace[49G(B[0;7m^U(B[m Paste Text   (B[0;7m^T(B[m To Spell     (B[0;7m^_(B[m Go To Line   (B[0;7mM-E(B[m Redo[53;113H(B[0;7mM-6(B[m Copy Text   (B[0;7m^Q(B[m Where Was    (B[0;7mM-W(B[m Next[53;161H(B[0;7m^F(B[m Forward[177G(B[0;7m^▶(B[m Next Word[51d[2d(B[0;1m[36mimport[39m(B[m gymnasium (B[0;1m[36mas[39m(B[m gym[3d(B[0;1m[36mfrom[39m(B[m gymnasium (B[0;1m[36mimport[39m(B[m spaces[4d(B[0;1m[36mfrom[39m(B[m statistics (B[0;1m[36mimport[39m(B[m median[5d(B[0;1m[36mimport[39m(B[m numpy (B[0;1m[36mas[39m(B[m np[6d(B[0;1m[36mimport[39m(B[m mujoco_py[7d(B[0;1m[36mimport[39m(B[m os[8d(B[0;1m[36mimport[39m(B[m torch[9d(B[0;1m[36mfrom[39m(B[m torch (B[0;1m[36mimport[39m(B[m nn[10d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.vec_env (B[0;1m[36mimport[39m(B[m DummyVecEnv, VecMonitor[11d(B[0;1m[36mfrom[39m(B[m sb3_contrib (B[0;1m[36mimport[39m(B[m RecurrentPPO[12d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.callbacks (B[0;1m[36mimport[39m(B[m BaseCallback[13d(B[0;1m[36mfrom[39m(B[m sb3_contrib.ppo_recurrent.policies (B[0;1m[36mimport[39m(B[m MlpLstmPolicy[14d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.torch_layers (B[0;1m[36mimport[39m(B[m BaseFeaturesExtractor[15d(B[0;1m[36mimport[39m(B[m logging[16d(B[0;1m[36mimport[39m(B[m matplotlib.pyplot (B[0;1m[36mas[39m(B[m plt[42m  [17d(B[0;1m[36mimport[39m(B[m math[19d(B[0;1m[31m# Set up logging[20d[39m(B[mlogging.basicConfig(level=logging.INFO)[21d(B[0;1m[36mclass[39m(B[m ResidualBlock(nn.Module):[22;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self, size):[23;9Hsuper(ResidualBlock, self).__init__()[42m   [24;9H[49m(B[mself.fc1 = nn.Linear(size, size)[25;9Hself.relu = nn.ReLU()[26;9Hself.fc2 = nn.Linear(size, size)[27;9Hself.layer_norm = nn.LayerNorm(size)[29;5H(B[0;1m[36mdef[34m forward[39m(B[m(self, x):[30;9Hresidual = x[31;9Hout = self.fc1(x)[32;9Hout = self.relu(out)[33;9Hout = self.fc2(out)[34;9Hout += residual (B[0;1m[31m # Residual connection[35;9H[39m(B[mout = self.layer_norm(out)[36;9Hout = self.relu(out)[37;9H(B[0;1m[36mreturn[39m(B[m out[39d(B[0;1m[36mclass[39m(B[m CustomFE(BaseFeaturesExtractor):[40;5H(B[0;1m[32m"""[41d    Custom feature extractor for HandEnv.[42d    Increases network depth and incorporates layer normalization.[43d    """[45;5H[36mdef[34m __init__[39m(B[m(self, observation_space: gym.Space, features_dim: int = 256):[46;8H(B[0;1m[31m # Ensure the observation space is as expected[47;9H[36massert[39m(B[m isinstance(observation_space, spaces.Box), (B[0;1m[32m"Observation space must be of type Box"[49;9H[39m(B[msuper(CustomFE, self).__init__(observation_space, features_dim)[2d[?12l[?25h[?25l[8d[?12l[?25h[?25l[14d[?12l[?25h[?25l[20d[?12l[?25h[?25l[26d[?12l[?25h[?25l[51d[K[32d[?12l[?25h[?25l[38d[?12l[?25h[?25l[44d[?12l[?25h[?25l[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.initial_net = nn.Sequential([46;13Hnn.Linear(observation_space.shape[0], 128),[47;13Hnn.ReLU(),[48;13Hnn.LayerNorm(128)[49;9H)[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;8H(B[0;1m[31m # Add residual blocks[46;9H[39m(B[mself.residual_blocks = nn.Sequential([47;13HResidualBlock(128),[48;13HResidualBlock(128)[49;9H)[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.final_net = nn.Sequential([46;13Hnn.Linear(128, features_dim),[47;13Hnn.ReLU(),[48;13Hnn.LayerNorm(features_dim)[49;9H)[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5H(B[0;1m[36mdef[34m forward[39m(B[m(self, observations: torch.Tensor) -> torch.Tensor:[46;9Hx = self.initial_net(observations)[47;9Hx = self.residual_blocks(x)[48;9Hx = self.final_net(x)[49;9H(B[0;1m[36mreturn[39m(B[m x[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H(B[0;1m[36mclass[39m(B[m CustomACLstmPolicy(MlpLstmPolicy):[46;5H(B[0;1m[32m"""[47d    Custom Actor-Critic Policy with LSTM for RecurrentPPO.[48d    Integrates the CustomFeaturesExtractor.[49d    """[50d[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self, *args, **kwargs):[46;9Hsuper(CustomACLstmPolicy, self).__init__([47;13H*args,[48;13H**kwargs,[49;13Hfeatures_extractor_class=CustomFE,[50;13Hfeatures_extractor_kwargs=dict(features_dim=256),[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Hnet_arch=[dict(pi=[256, 256], vf=[256, 256])], (B[0;1m[31m # Corrected bracket[46;13H[39m(B[mactivation_fn=nn.ReLU,[47;13Hlstm_hidden_size=128 (B[0;1m[31m # Size of LSTM hidden state[48;9H[39m(B[m)[50d(B[0;1m[36mclass[39m(B[m HandEnv(gym.Env):[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self):[46;9Hsuper(HandEnv, self).__init__()[47;9Hxml_path = os.path.expanduser((B[0;1m[32m'/home/easgrad/dgusain/Bot_hand/bot_hand.xml'[39m(B[m)[48;9Hlogging.info(f(B[0;1m[32m"Attempting to load Mujoco model from: {xml_path}"[39m(B[m)[50;9H(B[0;1m[36mif[39m(B[m (B[0;1m[36mnot[39m(B[m os.path.isfile(xml_path):[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Hlogging.error(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[46;13H(B[0;1m[36mraise[39m(B[m FileNotFoundError(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[48;9H(B[0;1m[36mtry[39m(B[m:[49dself.model = mujoco_py.load_model_from_path(xml_path)[50;13Hself.sim = mujoco_py.MjSim(self.model)[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Hlogging.info((B[0;1m[32m"Mujoco model loaded successfully."[39m(B[m)[46;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[47;13Hlogging.error(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[48;13H(B[0;1m[36mraise[39m(B[m RuntimeError(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[50;9Hjoint_weights = [[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H0.5, (B[0;1m[31m # ID 0: WristJoint1[46;13H[39m(B[m0.5, (B[0;1m[31m # ID 1: WristJoint0[47;13H[39m(B[m1.5, (B[0;1m[31m # ID 2: ForeFingerJoint3 (MCP)[48;13H[39m(B[m2.5, (B[0;1m[31m # ID 3: ForeFingerJoint2 (MCP proximal)[49;13H[39m(B[m2.0, (B[0;1m[31m # ID 4: ForeFingerJoint1 (PIP)[50;13H[39m(B[m1.0, (B[0;1m[31m # ID 5: ForeFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H1.5, (B[0;1m[31m # ID 6: MiddleFingerJoint3 (MCP)[46;13H[39m(B[m2.5, (B[0;1m[31m # ID 7: MiddleFingerJoint2 (MCP proximal)[47;13H[39m(B[m2.0, (B[0;1m[31m # ID 8: MiddleFingerJoint1 (PIP)[48;13H[39m(B[m1.0, (B[0;1m[31m # ID 9: MiddleFingerJoint0 (DIP)[49;13H[39m(B[m1.5, (B[0;1m[31m # ID 10: RingFingerJoint3 (MCP)[50;13H[39m(B[m2.5, (B[0;1m[31m # ID 11: RingFingerJoint2 (MCP proximal)[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H2.0, (B[0;1m[31m # ID 12: RingFingerJoint1 (PIP)[46;13H[39m(B[m1.0, (B[0;1m[31m # ID 13: RingFingerJoint0 (DIP)[47;13H[39m(B[m1.5, (B[0;1m[31m # ID 14: LittleFingerJoint4 (CMC Joint rotation)[48;13H[39m(B[m1.5, (B[0;1m[31m # ID 15: LittleFingerJoint3 (MCP)[49;13H[39m(B[m2.5, (B[0;1m[31m # ID 16: LittleFingerJoint2 (MCP proximal)[50;13H[39m(B[m2.0, (B[0;1m[31m # ID 17: LittleFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H1.0, (B[0;1m[31m # ID 18: LittleFingerJoint0 (DIP)[46;13H[39m(B[m1.5, (B[0;1m[31m # ID 19: ThumbJoint4 (CMC Joint abduction/adduction)[47;13H[39m(B[m1.5, (B[0;1m[31m # ID 20: ThumbJoint3 (CMC Joint flexion/extension)[48;13H[39m(B[m1.5, (B[0;1m[31m # ID 21: ThumbJoint2 (CMC Joint rotation)[49;13H[39m(B[m3.0, (B[0;1m[31m # ID 22: ThumbJoint1 (MCP)[50;13H[39m(B[m0.5, (B[0;1m[31m # ID 23: ThumbJoint0 (IP)[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H0.0  (B[0;1m[31m # ID 24: None[46;9H[39m(B[m][47d(B[0;1m[31m # Converting joint weights to a tensor[48;9H[39m(B[mself.joint_weights = torch.tensor(joint_weights, dtype=torch.float32)[49;8H(B[0;1m[31m # Normalize the weights so that their sum equals 1[50;9H[39m(B[mself.joint_weights /= self.joint_weights.sum()[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;9Hself.initial_state = self.sim.get_state()[47;8H(B[0;1m[31m # Define observation and action spaces[48;9H[39m(B[mself.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(24,), dtype=np.float32)[49;9Hself.target_threshold = 99[50;9Hself.max_steps = 100[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.steps_taken = 0[46;9Hself.sim_coeff = 0.5[47;9Hself.ang_coeff = 1 - self.sim_coeff[49;8H(B[0;1m[31m # Initialize actuator ranges[50;9H[39m(B[mactuator_ranges = self.sim.model.actuator_ctrlrange[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.actuator_min = torch.tensor(actuator_ranges[:, 0], dtype=torch.float32)[46;9Hself.actuator_max = torch.tensor(actuator_ranges[:, 1], dtype=torch.float32)[48;9Hself.action_space = spaces.Box(low=-1, high=1, shape=(self.actuator_min.shape[0],), dtype=np.float32)[49;9Hself.ground_truth_quats = torch.tensor(self.get_ground_truth_quaternion(), dtype=torch.float32)[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5H(B[0;1m[36mdef[34m reset[39m(B[m(self, seed=(B[0;1m[35mNone[39m(B[m, options=(B[0;1m[35mNone[39m(B[m):[46;9Hsuper().reset(seed=seed)[47;8H(B[0;1m[31m #self.sim.reset()[48;9H[39m(B[mself.sim.set_state(self.initial_state)[49;9Hself.sim.forward()[50;9Hself.steps_taken = 0[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hobs = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[46;9H(B[0;1m[36mreturn[39m(B[m obs, {}[48;5H(B[0;1m[36mdef[34m step[39m(B[m(self, action: np.ndarray):[49;9Haction_tensor = torch.from_numpy(action).float()[50;9Hrescaled_action = self.actuator_min + (action_tensor + 1) * (self.actuator_max - self.actuator_min) / 2[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.sim.data.ctrl[:] = rescaled_action.numpy()[46;9Hself.sim.step()[48;8H(B[0;1m[31m # Fetch state as a tensor[49;9H[39m(B[mstate = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[50d[42m        [49m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;8H(B[0;1m[31m # Calculate done condition[46;9H[39m(B[mdone = self.calculate_done(state, flag=(B[0;1m[35mFalse[39m(B[m) (B[0;1m[36mor[39m(B[m self.steps_taken >= self.max_steps[47;9H(B[0;1m[36mif[39m(B[m done:[48;13Hreward = self.calculate_reward(state, flag=(B[0;1m[35mTrue[39m(B[m).item()[49;9H(B[0;1m[36melse[39m(B[m:[50dreward = -1.0 (B[0;1m[31m # Penalize extra steps[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;9Hself.steps_taken += 1[47;9Htruncated = (B[0;1m[35mFalse[48;9H[39m(B[minfo = {}[50;9H(B[0;1m[36mreturn[39m(B[m state.numpy(), reward, done, truncated, info[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;5H(B[0;1m[36mdef[34m calculate_done[39m(B[m(self, state: torch.Tensor, flag: bool) -> bool:[47;9H(B[0;1m[36mif[39m(B[m flag:[48;13H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mFalse[49;9H[39m(B[mconfidence = self.calculate_confidence(state)[50;9H(B[0;1m[36mreturn[39m(B[m confidence >= self.target_threshold[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;5H(B[0;1m[36mdef[34m calculate_reward[39m(B[m(self, state: torch.Tensor, flag: bool) -> torch.Tensor:[47;9Hconfidence = self.calculate_confidence(state)[48;9H(B[0;1m[36mif[39m(B[m flag:[49;13H(B[0;1m[36mreturn[39m(B[m confidence - 50 (B[0;1m[31m # Reward calculation[50;9H[36melse[39m(B[m:[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H(B[0;1m[36mif[39m(B[m confidence > 85:[46;17H(B[0;1m[36mreturn[39m(B[m (confidence - 50) / 2.0 (B[0;1m[31m # Encouraging the model in the right direction[47;13H[36melse[39m(B[m:[48d(B[0;1m[36mreturn[39m(B[m (confidence - 50) / 4.0 (B[0;1m[31m # Lesser reward[49d(B[0m[42m    [50d[49m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;5H(B[0;1m[36mdef[34m calculate_confidence[39m(B[m(self, state: torch.Tensor) -> torch.Tensor:[47;8H(B[0;1m[31m # Retrieve and normalize quaternions[48;9H[39m(B[mrendered_quat = self.get_rendered_pose_quaternion()[49;9Hrendered_quat = self.normalize_quaternion(rendered_quat)[50;9Hgt_quat = self.normalize_quaternion(self.ground_truth_quats)[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H[42m        [46d(B[0;1m[31m # Compute dot product between rendered and ground truth quaternions for each joint[47;9H[39m(B[mdot_product = torch.sum(rendered_quat * gt_quat, dim=1) (B[0;1m[31m # Shape: [24][48d(B[0m[42m        [49d(B[0;1m[31m # Similarity Calculation[50;9H[39m(B[msimilarity_per_joint = torch.abs(dot_product) (B[0;1m[31m # [24][39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hweighted_similarity = similarity_per_joint * self.joint_weights (B[0;1m[31m # [24][46;9H[39m(B[msimilarity_score = weighted_similarity.sum() * 100 (B[0;1m[31m # Scalar, 0-100[47d(B[0m[42m        [48d(B[0;1m[31m # Angular Displacement Calculation[49;9H[39m(B[mangular_displacement = 2 * torch.acos(torch.clamp(dot_product, -1.0, 1.0)) (B[0;1m[31m # [24], radians[50;9H[39m(B[mscaled_displacement = (angular_displacement / (2 * math.pi)) * 100 (B[0;1m[31m # [24], 0-100[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hscaled_displacement = torch.clamp(scaled_displacement, 0.0, 100.0) (B[0;1m[31m # Ensure values are within [0, 100][46;9H[39m(B[mweighted_angular_disp = scaled_displacement * self.joint_weights (B[0;1m[31m # [24][47;9H[39m(B[mangular_disp_score = weighted_angular_disp.sum() (B[0;1m[31m # Scalar, 0-100[48d(B[0m[42m        [49d(B[0;1m[31m # Invert Angular Displacement Score to Reflect Inverse Relationship[50;9H[39m(B[minverted_angular_disp_score = 100 - angular_disp_score (B[0;1m[31m # Scalar, 0-100[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H[42m        [46d(B[0;1m[31m # Combine Similarity and Inverted Angular Displacement[47;9H[39m(B[mavg_confidence = (similarity_score * self.sim_coeff) + (inverted_angular_disp_score * self.ang_coeff) (B[0;1m[31m # 0-100[48d(B[0m[42m        [49d(B[0;1m[36mreturn[39m(B[m avg_confidence[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[47;5H(B[0;1m[36mdef[34m get_rendered_pose_quaternion[39m(B[m(self) -> torch.Tensor:[48;9Hquaternions = [][49;9H(B[0;1m[36mfor[39m(B[m joint (B[0;1m[36min[39m(B[m range(self.model.njnt):[50;13Hq = self.sim.data.qpos[self.model.jnt_qposadr[joint]:self.model.jnt_qposadr[joint] + 4][?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Hquaternions.append(q)[46;8H(B[0;1m[31m # Convert list of arrays to a single NumPy array[47;9H[39m(B[mquaternions_np = np.array(quaternions, dtype=np.float32)[48;8H(B[0;1m[31m # Convert NumPy array to PyTorch tensor[49;9H[36mreturn[39m(B[m torch.from_numpy(quaternions_np)[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5H(B[0;1m[36mdef[34m normalize_quaternion[39m(B[m(self, q: torch.Tensor) -> torch.Tensor:[46;9Hnorm = torch.norm(q, dim=1, keepdim=(B[0;1m[35mTrue[39m(B[m)[47;9H(B[0;1m[36mreturn[39m(B[m q / norm.clamp(min=1e-8) (B[0;1m[31m # Prevent division by zero[49;5H[36mdef[34m get_ground_truth_quaternion[39m(B[m(self) -> np.ndarray:[50;9Hground_truth_quats = [[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H[6.32658406e-04,  1.25473697e-03, -9.99718591e-03,  1.56702220e+00],[46;13H[1.25473697e-03, -9.99718591e-03,  1.56702220e+00,  1.56730258e+00],[47;13H[-0.00999719,  1.5670222,   1.56730258,  1.5674143],[48;13H[1.5670222,   1.56730258,  1.5674143,  -0.00999801],[49;13H[1.56730258,  1.5674143,  -0.00999801,  1.56701926],[50;13H[1.5674143,  -0.00999801,  1.56701926,  1.56730194],[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H[-0.00999801,  1.56701926,  1.56730194,  1.5674143],[46;13H[1.56701926,  1.56730194,  1.5674143,  -0.00999756],[47;13H[1.56730194,  1.5674143,  -0.00999756,  1.56701717],[48;13H[1.5674143,  -0.00999756,  1.56701717,  1.56730177],[49;13H[-0.00999756,  1.56701717,  1.56730177,  1.56741429],[50;13H[1.56701717, 1.56730177, 1.56741429, 0.00868252],[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H[1.56730177,  1.56741429,  0.00868252, -0.01000805],[46;13H[1.56741429,  0.00868252, -0.01000805,  1.56708349],[47;13H[0.00868252, -0.01000805,  1.56708349,  1.56730911],[48;13H[-0.01000805,  1.56708349,  1.56730911,  1.56741508],[49;13H[1.56708349, 1.56730911, 1.56741508, 0.49916711],[50;13H[1.56730911, 1.56741508, 0.49916711, 0.50022545],[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H[1.56741508, 0.49916711, 0.50022545, 0.25330455],[46;13H[4.99167109e-01, 5.00225452e-01, 2.53304550e-01, 4.08440608e-04],[47;13H[5.00225452e-01,  2.53304550e-01,  4.08440608e-04, -9.00000689e-01],[48;13H[2.53304550e-01,  4.08440608e-04, -9.00000689e-01,  1.00000000e-01],[49;13H[4.08440608e-04, -9.00000689e-01,  1.00000000e-01, -1.00000000e-01],[50;13H[-0.90000069,  0.1,[50;40H-0.1,[50;54H0.01463168],[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H[0.1,[45;26H-0.1,[45;40H0.01463168,  1.0][46;9H][47d(B[0;1m[36mreturn[39m(B[m np.array(ground_truth_quats, dtype=np.float32)[49d(B[0;1m[36mclass[39m(B[m RewardCallback(BaseCallback):[50;5H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H(B[0;1m[32m    Custom callback for logging average rewards over every 100 episodes, episode numbers, and iteration numbers.[46d    Also stores average rewards for plotting.[47d    """[49;5H[36mdef[34m __init__[39m(B[m(self, avg_interval=100):[50;9Hsuper(RewardCallback, self).__init__()[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.episode_num = 0[46;9Hself.max_reward = -np.inf[47;9Hself.avg_interval = avg_interval (B[0;1m[31m # Number of episodes per average - not hyperparameter[48;9H[39m(B[mself.sum_rewards = 0.0[49;9Hself.count_rewards = 0[50;9Hself.iteration_num = 0[42m     [49m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.rewards_list = [][46;9Hself.median_rewards = {}[47;9Hself.avg_rewards = {} (B[0;1m[31m # New dictionary to store average rewards[49;5H[36mdef[34m _on_rollout_end[39m(B[m(self) -> bool:[50;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H(B[0;1m[32m        Called at the end of a rollout.[46d        Used to track the number of iterations.[47d        """[48;9H[39m(B[mself.iteration_num += 1[49;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[50d[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5H(B[0;1m[36mdef[34m _on_step[39m(B[m(self) -> bool:[46;9H(B[0;1m[32m"""[47d        Called at every step. Checks if any episode has finished and logs the reward.[48d        """[49;8H[31m # Retrieve 'dones' and 'rewards' from the current step[50;9H[39m(B[mdones = self.locals.get((B[0;1m[32m'dones'[39m(B[m, [])[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hrewards = self.locals.get((B[0;1m[32m'rewards'[39m(B[m, [])[47;8H(B[0;1m[31m # Check if any of the environments are done[48;9H[36mif[39m(B[m np.any(dones):[49;13H(B[0;1m[36mfor[39m(B[m done, reward (B[0;1m[36min[39m(B[m zip(dones, rewards):[50;17H(B[0;1m[36mif[39m(B[m done:[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;21Hself.episode_num += 1[46;21H(B[0;1m[36mif[39m(B[m reward > self.max_reward:[47;25Hself.max_reward = reward[48;21Hself.rewards_list.append(reward)[49;21H(B[0;1m[36mif[39m(B[m len(self.rewards_list) == self.avg_interval:[50;25Hmedian_reward = median(self.rewards_list)[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;25Havg_reward = np.mean(self.rewards_list)[42m [46;25H[49m(B[mblock_num = self.episode_num // self.avg_interval[47;25Hself.median_rewards[block_num] = median_reward[48;25Hself.avg_rewards[block_num] = avg_reward[42m [49;25H[49m(B[mprint(f(B[0;1m[32m"Iteration: {self.iteration_num} | Episodes: {block_num * self.avg_interval} | Median Reward: {median_reward:.2f} | Avg Reward: {avg_reward:.2f} | Max Reward: {self.max_re[39m(B[0;7m>[50;24H(B[0;1m[31m # Reset sum and count[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;25Hself.rewards_list = [][46;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[48;5H[36mdef[34m _on_training_end[39m(B[m(self) -> (B[0;1m[35mNone[39m(B[m:[49;9H(B[0;1m[32m"""[50d        Called at the end of training. If there are remaining episodes, compute and store the average.[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H(B[0;1m[32m        """[46;9H[36mif[39m(B[m len(self.rewards_list) > 0:[47;13Hmedian_reward = median(self.rewards_list)[48;13Havg_reward = np.mean(self.rewards_list)[49;13Hblock_num = self.episode_num // self.avg_interval + 1[50;13Hself.median_rewards[block_num] = median_reward[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Hself.avg_rewards[block_num] = avg_reward[46;13Hcurrent_max = max(self.rewards_list)[47;13H(B[0;1m[36mif[39m(B[m current_max > self.max_reward:[48;17Hself.max_reward = current_max[42m           [49;13H[49m(B[mprint(f(B[0;1m[32m"Training End | Episodes: {self.episode_num} | Median Reward: {median_reward:.2f} | Avg Reward: {avg_reward:.2f} | Max Reward: {self.max_reward:.2f}"[39m(B[m)[50;13Hself.rewards_list = [][?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;1H(B[0;1m[36mdef[34m make_env[39m(B[m():[47;5H(B[0;1m[32m"""Utility function to create a HandEnv without Monitor."""[48;5H[36mdef[34m _init[39m(B[m():[49;9H(B[0;1m[36mtry[39m(B[m:[50denv = HandEnv()[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H(B[0;1m[36mreturn[39m(B[m env[46;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[47;13Hlogging.error(f(B[0;1m[32m"Failed to initialize HandEnv: {e}"[39m(B[m)[48;13H(B[0;1m[36mraise[39m(B[m e[49;5H(B[0;1m[36mreturn[39m(B[m _init[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H(B[0;1m[36mdef[34m train_on_gpu[39m(B[m(gpu_id: int, num_envs: int, total_timesteps: int, num_steps: int, save_interval: int):[46;5Hos.environ[(B[0;1m[32m'CUDA_VISIBLE_DEVICES'[39m(B[m] = str(gpu_id)[47;5Hdevice = (B[0;1m[32m'cuda'[39m(B[m (B[0;1m[36mif[39m(B[m torch.cuda.is_available() (B[0;1m[36melse[39m(B[m (B[0;1m[32m'cpu'[48;5H[39m(B[mprint(f(B[0;1m[32m"GPU {gpu_id}: Using {device} device"[39m(B[m)[49;5Henv = DummyVecEnv([make_env() (B[0;1m[36mfor[39m(B[m _ (B[0;1m[36min[39m(B[m range(num_envs)])[42m  [50;5H[49m(B[menv = VecMonitor(env) (B[0;1m[31m # Use VecMonitor instead of individual Monitor wrappers[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;5Hmodel = RecurrentPPO([47;9HCustomACLstmPolicy, (B[0;1m[31m # Use the custom policy[48;9H[39m(B[menv,[49;9Hverbose=1,[50;9Hdevice=device,[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hent_coef=0.15,[45;36H(B[0;1m[31m # Adjusted entropy coefficient[46;9H[39m(B[mlearning_rate=0.0005,[47;9Hclip_range=0.3,[48;9Hn_steps=num_steps,[48;36H(B[0;1m[31m # Steps per environment per update[49;9H[39m(B[mbatch_size=4096,[49;36H(B[0;1m[31m # Adjusted batch size[50;9H[39m(B[mgamma=0.99,[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hgae_lambda=0.95,[46;9Hmax_grad_norm=0.5,[47;9Hvf_coef=0.5,[48;9Huse_sde=(B[0;1m[35mTrue[39m(B[m,[48;36H(B[0;1m[31m # Use State Dependent Exploration for better exploration[49;8H # Removed lstm_hidden_size=128[50;5H[39m(B[m)[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;5Hcallback = RewardCallback(avg_interval=100)[48;5H(B[0;1m[36mtry[39m(B[m:[49dmodel.learn(total_timesteps=total_timesteps, callback=callback, log_interval=10)[50;5H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hlogging.error(f(B[0;1m[32m"Error during training: {e}"[39m(B[m)[46;9H(B[0;1m[36mraise[39m(B[m e[48;5Hmodel.save(f(B[0;1m[32m"/home/easgrad/dgusain/Bot_hand/agents/agent_code1_1_residual_d_gpu{gpu_id}"[39m(B[m)[50;5H(B[0;1m[36mreturn[39m(B[m callback (B[0;1m[31m # Return the callback to access episode_rewards[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;1H(B[0;1m[36mdef[34m main[39m(B[m():[47;5Hgpu_id = 6[48;5Hnum_envs = 4[49;5Hn_iter = 1000[50;5Hn_steps = 1024     (B[0;1m[31m # Steps per environment per update[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5Htotal_timesteps = n_iter * n_steps * num_envs (B[0;1m[31m # Total training timesteps[46;5H[39m(B[msave_interval = 25[42m  [48;5H[49m(B[mcallback = train_on_gpu(gpu_id, num_envs, total_timesteps, n_steps, save_interval)[49;5Hblock_numbers = list(callback.median_rewards.keys())[50;5Hblock_median_rewards = list(callback.median_rewards.values())[?12l[?25h[?25l[45d[?12l[?25h[?25l[39d[?12l[?25h[?25l[33d[?12l[?25h[?25l[27d[?12l[?25h[?25l[21d[?12l[?25h[?25l[15d[?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l   [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25le[?12l[?25h[?25ln[?12l[?25h[?25lt[?12l[?25h[?25l_co[?12l[?25h[?25le[?12l[?25h[?25lf[?12l[?25h[?25l=[?12l[?25h[?25l0[?12l[?25h[?25l.[?12l[?25h[?25l1[?12l[?25h[?25l5[?12l[?25h[?25l[1;194H(B[0;7mModified(B[m[15;21H[1P[?12l[?25h[?25l[1P[?12l[?25h[?25l2,[15;34H (B[0;1m[31m # Adjusted entropy coefficient[15;21H[39m(B[m[?12l[?25h[?25l[20d[?12l[?25h[?25l[26;6H[?12l[?25h[?25l[32;21H[?12l[?25h[?25l[38d[?12l[?25h[?25l[44d[?12l[?25h[?25l[50d[?12l[?25h[?25l[45d[?12l[?25h[?25l[39d[?12l[?25h[?25l[40;12H[?12l[?25h[?25l[41;15H[?12l[?25h[?25l[42d 4[?12l[?25h[?25l[43d0[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l0[?12l[?25h[?25l0 [?12l[?25h[?25l500[?12l[?25h[?25l[51d(B[0;7mSave modified buffer?                                                                                                                                                                                      [52;1H Y(B[m Yes[K[53d(B[0;7m N(B[m No  [53;18H(B[0;7mC(B[m Cancel[K[51;23H[?12l[?25h[?25l[52d(B[0;7m^G(B[m Get Help[52;51H(B[0;7mM-D(B[m DOS Format[52;101H(B[0;7mM-A(B[m Append[52;151H(B[0;7mM-B(B[m Backup File[53d(B[0;7m^C(B[m Cancel[17G         [53;51H(B[0;7mM-M(B[m Mac Format[53;101H(B[0;7mM-P(B[m Prepend[53;151H(B[0;7m^T(B[m To Files[51d(B[0;7mFile Name to Write: code1_1_residual_d.py(B[m[51;42H[?12l[?25h[?25l[51;94H[1K (B[0;7m[ Writing... ](B[m[K[1;194H(B[0;7m        (B[m[51;93H(B[0;7m[ Wrote 426 lines ](B[m[J[53d[?12l[?25h[53;1H[?1049l[23;0;0t[?1l>[?2004l(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> clear
[H[2J(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python code1_1_residual_d.py
GPU 6: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
Iteration: 2 | Episodes: 100 | Median Reward: 24.52 | Avg Reward: 23.46 | Max Reward: 36.02
Iteration: 4 | Episodes: 200 | Median Reward: 12.77 | Avg Reward: 13.36 | Max Reward: 36.02
Iteration: 7 | Episodes: 300 | Median Reward: 24.95 | Avg Reward: 22.22 | Max Reward: 36.02
Iteration: 9 | Episodes: 400 | Median Reward: 17.47 | Avg Reward: 16.76 | Max Reward: 36.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -83.5       |
| time/                   |             |
|    fps                  | 645         |
|    iterations           | 10          |
|    time_elapsed         | 63          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.074577264 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.3         |
|    entropy_loss         | -76.8       |
|    explained_variance   | 0.0494      |
|    learning_rate        | 0.0005      |
|    loss                 | 80.2        |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.00977     |
|    std                  | 1.01        |
|    value_loss           | 202         |
-----------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 16.90 | Avg Reward: 16.91 | Max Reward: 36.02
Iteration: 14 | Episodes: 600 | Median Reward: 19.38 | Avg Reward: 18.30 | Max Reward: 36.51
Iteration: 17 | Episodes: 700 | Median Reward: 22.22 | Avg Reward: 22.39 | Max Reward: 36.51
Iteration: 19 | Episodes: 800 | Median Reward: 20.84 | Avg Reward: 22.14 | Max Reward: 39.73
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -77.3      |
| time/                   |            |
|    fps                  | 624        |
|    iterations           | 20         |
|    time_elapsed         | 131        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.09051873 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.3        |
|    entropy_loss         | -73.9      |
|    explained_variance   | 0.285      |
|    learning_rate        | 0.0005     |
|    loss                 | 74.8       |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0168    |
|    std                  | 1.01       |
|    value_loss           | 201        |
----------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 21.62 | Avg Reward: 19.66 | Max Reward: 39.73
Iteration: 24 | Episodes: 1000 | Median Reward: 17.99 | Avg Reward: 16.25 | Max Reward: 39.73
Iteration: 27 | Episodes: 1100 | Median Reward: 14.13 | Avg Reward: 14.27 | Max Reward: 39.73
Iteration: 29 | Episodes: 1200 | Median Reward: 20.58 | Avg Reward: 19.30 | Max Reward: 39.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.3       |
| time/                   |             |
|    fps                  | 621         |
|    iterations           | 30          |
|    time_elapsed         | 197         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.018975206 |
|    clip_fraction        | 0.029       |
|    clip_range           | 0.3         |
|    entropy_loss         | -84.8       |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.65        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0244     |
|    std                  | 1.01        |
|    value_loss           | 51.9        |
-----------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 17.85 | Avg Reward: 18.32 | Max Reward: 39.73
Iteration: 34 | Episodes: 1400 | Median Reward: 22.08 | Avg Reward: 21.68 | Max Reward: 39.73
Iteration: 36 | Episodes: 1500 | Median Reward: 18.76 | Avg Reward: 20.13 | Max Reward: 39.73
Iteration: 39 | Episodes: 1600 | Median Reward: 21.34 | Avg Reward: 19.15 | Max Reward: 39.73
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.9         |
| time/                   |               |
|    fps                  | 614           |
|    iterations           | 40            |
|    time_elapsed         | 266           |
|    total_timesteps      | 163840        |
| train/                  |               |
|    approx_kl            | 0.00083680276 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -92.5         |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.0005        |
|    loss                 | -14.2         |
|    n_updates            | 390           |
|    policy_gradient_loss | -0.00304      |
|    std                  | 1.01          |
|    value_loss           | 19            |
-------------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 21.04 | Avg Reward: 19.37 | Max Reward: 39.73
Iteration: 44 | Episodes: 1800 | Median Reward: 26.09 | Avg Reward: 23.56 | Max Reward: 39.73
Iteration: 46 | Episodes: 1900 | Median Reward: 26.74 | Avg Reward: 23.16 | Max Reward: 39.73
Iteration: 49 | Episodes: 2000 | Median Reward: 22.07 | Avg Reward: 22.18 | Max Reward: 39.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -77.3       |
| time/                   |             |
|    fps                  | 615         |
|    iterations           | 50          |
|    time_elapsed         | 332         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.058792654 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.3         |
|    entropy_loss         | -101        |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.0005      |
|    loss                 | -14         |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0222     |
|    std                  | 1.02        |
|    value_loss           | 13.2        |
-----------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 23.35 | Avg Reward: 22.75 | Max Reward: 39.73
Iteration: 54 | Episodes: 2200 | Median Reward: 23.02 | Avg Reward: 22.67 | Max Reward: 39.73
Iteration: 56 | Episodes: 2300 | Median Reward: 23.05 | Avg Reward: 22.49 | Max Reward: 39.73
Iteration: 59 | Episodes: 2400 | Median Reward: 24.11 | Avg Reward: 23.94 | Max Reward: 39.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76         |
| time/                   |             |
|    fps                  | 628         |
|    iterations           | 60          |
|    time_elapsed         | 391         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.020517241 |
|    clip_fraction        | 0.0473      |
|    clip_range           | 0.3         |
|    entropy_loss         | -114        |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0005      |
|    loss                 | -17.1       |
|    n_updates            | 590         |
|    policy_gradient_loss | 0.0127      |
|    std                  | 1.02        |
|    value_loss           | 21.9        |
-----------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 23.47 | Avg Reward: 22.64 | Max Reward: 39.73
Iteration: 64 | Episodes: 2600 | Median Reward: 20.44 | Avg Reward: 18.34 | Max Reward: 39.73
Iteration: 66 | Episodes: 2700 | Median Reward: 24.65 | Avg Reward: 23.03 | Max Reward: 39.73
Iteration: 69 | Episodes: 2800 | Median Reward: 23.00 | Avg Reward: 22.55 | Max Reward: 39.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76.1       |
| time/                   |             |
|    fps                  | 644         |
|    iterations           | 70          |
|    time_elapsed         | 445         |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.018195216 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.3         |
|    entropy_loss         | -130        |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0005      |
|    loss                 | -24.8       |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.015      |
|    std                  | 1.02        |
|    value_loss           | 5.27        |
-----------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 27.02 | Avg Reward: 24.12 | Max Reward: 39.73
Iteration: 73 | Episodes: 3000 | Median Reward: 23.03 | Avg Reward: 24.16 | Max Reward: 39.73
Iteration: 76 | Episodes: 3100 | Median Reward: 24.42 | Avg Reward: 21.89 | Max Reward: 39.73
Iteration: 78 | Episodes: 3200 | Median Reward: 23.60 | Avg Reward: 21.97 | Max Reward: 39.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -77.1       |
| time/                   |             |
|    fps                  | 664         |
|    iterations           | 80          |
|    time_elapsed         | 492         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.040674224 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.3         |
|    entropy_loss         | -139        |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0005      |
|    loss                 | -25         |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0264     |
|    std                  | 1.03        |
|    value_loss           | 6.67        |
-----------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 27.32 | Avg Reward: 26.06 | Max Reward: 39.73
Iteration: 83 | Episodes: 3400 | Median Reward: 29.26 | Avg Reward: 29.19 | Max Reward: 44.52
Iteration: 86 | Episodes: 3500 | Median Reward: 24.95 | Avg Reward: 25.34 | Max Reward: 44.52
Iteration: 88 | Episodes: 3600 | Median Reward: 22.38 | Avg Reward: 23.51 | Max Reward: 44.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.9       |
| time/                   |             |
|    fps                  | 684         |
|    iterations           | 90          |
|    time_elapsed         | 538         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.073235534 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.3         |
|    entropy_loss         | -151        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0005      |
|    loss                 | -25.9       |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0498     |
|    std                  | 1.03        |
|    value_loss           | 4.48        |
-----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 30.26 | Avg Reward: 29.20 | Max Reward: 44.52
Iteration: 93 | Episodes: 3800 | Median Reward: 28.06 | Avg Reward: 28.89 | Max Reward: 44.52
Iteration: 96 | Episodes: 3900 | Median Reward: 27.72 | Avg Reward: 27.84 | Max Reward: 44.52
Iteration: 98 | Episodes: 4000 | Median Reward: 25.73 | Avg Reward: 28.46 | Max Reward: 44.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.4       |
| time/                   |             |
|    fps                  | 700         |
|    iterations           | 100         |
|    time_elapsed         | 584         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.018155262 |
|    clip_fraction        | 0.0215      |
|    clip_range           | 0.3         |
|    entropy_loss         | -156        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0005      |
|    loss                 | -19.2       |
|    n_updates            | 990         |
|    policy_gradient_loss | 0.00485     |
|    std                  | 1.03        |
|    value_loss           | 11.9        |
-----------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 26.97 | Avg Reward: 27.80 | Max Reward: 44.52
Iteration: 103 | Episodes: 4200 | Median Reward: 28.64 | Avg Reward: 26.93 | Max Reward: 44.52
Iteration: 106 | Episodes: 4300 | Median Reward: 31.45 | Avg Reward: 30.83 | Max Reward: 44.52
Iteration: 108 | Episodes: 4400 | Median Reward: 30.40 | Avg Reward: 30.39 | Max Reward: 44.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.4       |
| time/                   |             |
|    fps                  | 714         |
|    iterations           | 110         |
|    time_elapsed         | 630         |
|    total_timesteps      | 450560      |
| train/                  |             |
|    approx_kl            | 0.044319678 |
|    clip_fraction        | 0.0573      |
|    clip_range           | 0.3         |
|    entropy_loss         | -161        |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0005      |
|    loss                 | -31.2       |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0225     |
|    std                  | 1.04        |
|    value_loss           | 6.61        |
-----------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 26.55 | Avg Reward: 27.57 | Max Reward: 44.52
Iteration: 113 | Episodes: 4600 | Median Reward: 32.49 | Avg Reward: 31.12 | Max Reward: 44.52
Iteration: 115 | Episodes: 4700 | Median Reward: 27.74 | Avg Reward: 27.99 | Max Reward: 44.52
Iteration: 118 | Episodes: 4800 | Median Reward: 30.07 | Avg Reward: 30.22 | Max Reward: 44.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -69.9      |
| time/                   |            |
|    fps                  | 726        |
|    iterations           | 120        |
|    time_elapsed         | 676        |
|    total_timesteps      | 491520     |
| train/                  |            |
|    approx_kl            | 0.08818641 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.3        |
|    entropy_loss         | -166       |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0005     |
|    loss                 | -31.5      |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0515    |
|    std                  | 1.05       |
|    value_loss           | 2.63       |
----------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 33.26 | Avg Reward: 31.88 | Max Reward: 44.52
Iteration: 123 | Episodes: 5000 | Median Reward: 30.06 | Avg Reward: 28.35 | Max Reward: 44.52
Iteration: 125 | Episodes: 5100 | Median Reward: 32.60 | Avg Reward: 28.64 | Max Reward: 44.52
Iteration: 128 | Episodes: 5200 | Median Reward: 30.09 | Avg Reward: 28.79 | Max Reward: 44.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -73        |
| time/                   |            |
|    fps                  | 739        |
|    iterations           | 130        |
|    time_elapsed         | 720        |
|    total_timesteps      | 532480     |
| train/                  |            |
|    approx_kl            | 0.06936483 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.3        |
|    entropy_loss         | -172       |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0005     |
|    loss                 | -33.7      |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.00675   |
|    std                  | 1.05       |
|    value_loss           | 1.73       |
----------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 28.35 | Avg Reward: 26.65 | Max Reward: 44.52
Iteration: 133 | Episodes: 5400 | Median Reward: 30.68 | Avg Reward: 29.57 | Max Reward: 44.52
Iteration: 135 | Episodes: 5500 | Median Reward: 32.86 | Avg Reward: 32.49 | Max Reward: 44.52
Iteration: 138 | Episodes: 5600 | Median Reward: 32.70 | Avg Reward: 31.41 | Max Reward: 44.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -67.5      |
| time/                   |            |
|    fps                  | 749        |
|    iterations           | 140        |
|    time_elapsed         | 765        |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.01351552 |
|    clip_fraction        | 0.00562    |
|    clip_range           | 0.3        |
|    entropy_loss         | -178       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0005     |
|    loss                 | -33.3      |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.016     |
|    std                  | 1.06       |
|    value_loss           | 3.1        |
----------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 33.03 | Avg Reward: 31.91 | Max Reward: 44.52
Iteration: 143 | Episodes: 5800 | Median Reward: 32.68 | Avg Reward: 32.06 | Max Reward: 44.52
Iteration: 145 | Episodes: 5900 | Median Reward: 33.90 | Avg Reward: 31.57 | Max Reward: 44.52
Iteration: 147 | Episodes: 6000 | Median Reward: 33.63 | Avg Reward: 32.10 | Max Reward: 44.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.1        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 150          |
|    time_elapsed         | 809          |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0028618947 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.3          |
|    entropy_loss         | -181         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -35.2        |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.0076      |
|    std                  | 1.06         |
|    value_loss           | 11.8         |
------------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 29.46 | Avg Reward: 28.56 | Max Reward: 44.52
Iteration: 152 | Episodes: 6200 | Median Reward: 31.30 | Avg Reward: 31.03 | Max Reward: 44.52
Iteration: 155 | Episodes: 6300 | Median Reward: 32.46 | Avg Reward: 31.63 | Max Reward: 44.52
Iteration: 157 | Episodes: 6400 | Median Reward: 29.83 | Avg Reward: 31.07 | Max Reward: 44.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -65.5      |
| time/                   |            |
|    fps                  | 767        |
|    iterations           | 160        |
|    time_elapsed         | 854        |
|    total_timesteps      | 655360     |
| train/                  |            |
|    approx_kl            | 0.03854034 |
|    clip_fraction        | 0.0524     |
|    clip_range           | 0.3        |
|    entropy_loss         | -183       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -35.1      |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0239    |
|    std                  | 1.07       |
|    value_loss           | 5.61       |
----------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 34.21 | Avg Reward: 34.15 | Max Reward: 44.52
Iteration: 162 | Episodes: 6600 | Median Reward: 34.14 | Avg Reward: 33.98 | Max Reward: 44.52
Iteration: 165 | Episodes: 6700 | Median Reward: 34.43 | Avg Reward: 33.34 | Max Reward: 44.52
Iteration: 167 | Episodes: 6800 | Median Reward: 34.46 | Avg Reward: 32.85 | Max Reward: 44.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.8        |
| time/                   |              |
|    fps                  | 774          |
|    iterations           | 170          |
|    time_elapsed         | 898          |
|    total_timesteps      | 696320       |
| train/                  |              |
|    approx_kl            | 0.0012462911 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -188         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -36.7        |
|    n_updates            | 1690         |
|    policy_gradient_loss | 9.42e-05     |
|    std                  | 1.07         |
|    value_loss           | 2.89         |
------------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 33.10 | Avg Reward: 33.17 | Max Reward: 44.52
Iteration: 172 | Episodes: 7000 | Median Reward: 35.29 | Avg Reward: 34.95 | Max Reward: 44.52
Iteration: 175 | Episodes: 7100 | Median Reward: 35.43 | Avg Reward: 33.77 | Max Reward: 44.52
Iteration: 177 | Episodes: 7200 | Median Reward: 33.38 | Avg Reward: 32.91 | Max Reward: 44.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.4       |
| time/                   |             |
|    fps                  | 781         |
|    iterations           | 180         |
|    time_elapsed         | 943         |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.004751114 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.3         |
|    entropy_loss         | -191        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -36.9       |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0016     |
|    std                  | 1.08        |
|    value_loss           | 6.11        |
-----------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 35.67 | Avg Reward: 35.90 | Max Reward: 44.52
Iteration: 182 | Episodes: 7400 | Median Reward: 38.63 | Avg Reward: 37.73 | Max Reward: 44.60
Iteration: 184 | Episodes: 7500 | Median Reward: 33.46 | Avg Reward: 34.17 | Max Reward: 44.60
Iteration: 187 | Episodes: 7600 | Median Reward: 33.15 | Avg Reward: 33.34 | Max Reward: 44.60
Iteration: 189 | Episodes: 7700 | Median Reward: 36.32 | Avg Reward: 37.03 | Max Reward: 44.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63         |
| time/                   |             |
|    fps                  | 787         |
|    iterations           | 190         |
|    time_elapsed         | 988         |
|    total_timesteps      | 778240      |
| train/                  |             |
|    approx_kl            | 0.007733631 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -194        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0005      |
|    loss                 | -37.2       |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00707    |
|    std                  | 1.08        |
|    value_loss           | 4.73        |
-----------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 36.04 | Avg Reward: 35.00 | Max Reward: 44.60
Iteration: 194 | Episodes: 7900 | Median Reward: 37.31 | Avg Reward: 36.44 | Max Reward: 44.60
Iteration: 197 | Episodes: 8000 | Median Reward: 34.56 | Avg Reward: 36.28 | Max Reward: 44.60
Iteration: 199 | Episodes: 8100 | Median Reward: 36.73 | Avg Reward: 35.56 | Max Reward: 44.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.3       |
| time/                   |             |
|    fps                  | 792         |
|    iterations           | 200         |
|    time_elapsed         | 1033        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.003524326 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -195        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -36         |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00576    |
|    std                  | 1.09        |
|    value_loss           | 4.09        |
-----------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 37.81 | Avg Reward: 36.81 | Max Reward: 45.91
Iteration: 204 | Episodes: 8300 | Median Reward: 37.20 | Avg Reward: 36.27 | Max Reward: 45.91
Iteration: 207 | Episodes: 8400 | Median Reward: 37.65 | Avg Reward: 36.32 | Max Reward: 45.91
Iteration: 209 | Episodes: 8500 | Median Reward: 38.67 | Avg Reward: 37.06 | Max Reward: 45.91
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.8         |
| time/                   |               |
|    fps                  | 798           |
|    iterations           | 210           |
|    time_elapsed         | 1077          |
|    total_timesteps      | 860160        |
| train/                  |               |
|    approx_kl            | 0.00013761329 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -198          |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0005        |
|    loss                 | -31           |
|    n_updates            | 2090          |
|    policy_gradient_loss | 0.000535      |
|    std                  | 1.09          |
|    value_loss           | 16.5          |
-------------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 36.55 | Avg Reward: 36.85 | Max Reward: 45.91
Iteration: 214 | Episodes: 8700 | Median Reward: 38.04 | Avg Reward: 39.19 | Max Reward: 45.91
Iteration: 216 | Episodes: 8800 | Median Reward: 39.11 | Avg Reward: 35.94 | Max Reward: 45.91
Iteration: 219 | Episodes: 8900 | Median Reward: 37.28 | Avg Reward: 38.04 | Max Reward: 45.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.8       |
| time/                   |             |
|    fps                  | 802         |
|    iterations           | 220         |
|    time_elapsed         | 1122        |
|    total_timesteps      | 901120      |
| train/                  |             |
|    approx_kl            | 0.028748587 |
|    clip_fraction        | 0.0502      |
|    clip_range           | 0.3         |
|    entropy_loss         | -200        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -38         |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.00617    |
|    std                  | 1.1         |
|    value_loss           | 2           |
-----------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 37.39 | Avg Reward: 37.39 | Max Reward: 45.91
Iteration: 224 | Episodes: 9100 | Median Reward: 38.27 | Avg Reward: 37.49 | Max Reward: 45.91
Iteration: 226 | Episodes: 9200 | Median Reward: 41.21 | Avg Reward: 39.39 | Max Reward: 45.91
Iteration: 229 | Episodes: 9300 | Median Reward: 35.79 | Avg Reward: 36.98 | Max Reward: 45.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61          |
| time/                   |              |
|    fps                  | 806          |
|    iterations           | 230          |
|    time_elapsed         | 1168         |
|    total_timesteps      | 942080       |
| train/                  |              |
|    approx_kl            | 0.0005046029 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -202         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -36.2        |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 1.1          |
|    value_loss           | 10.5         |
------------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 39.31 | Avg Reward: 37.61 | Max Reward: 45.91
Iteration: 234 | Episodes: 9500 | Median Reward: 34.25 | Avg Reward: 36.30 | Max Reward: 45.91
Iteration: 236 | Episodes: 9600 | Median Reward: 36.76 | Avg Reward: 37.65 | Max Reward: 45.91
Iteration: 239 | Episodes: 9700 | Median Reward: 36.49 | Avg Reward: 36.46 | Max Reward: 45.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.7        |
| time/                   |              |
|    fps                  | 810          |
|    iterations           | 240          |
|    time_elapsed         | 1212         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0006460056 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -204         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -39.6        |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 1.11         |
|    value_loss           | 5.17         |
------------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 38.49 | Avg Reward: 39.31 | Max Reward: 45.91
Iteration: 244 | Episodes: 9900 | Median Reward: 39.46 | Avg Reward: 38.64 | Max Reward: 45.91
Iteration: 246 | Episodes: 10000 | Median Reward: 34.35 | Avg Reward: 34.20 | Max Reward: 45.91
Iteration: 249 | Episodes: 10100 | Median Reward: 32.60 | Avg Reward: 31.25 | Max Reward: 45.91
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -65.3      |
| time/                   |            |
|    fps                  | 814        |
|    iterations           | 250        |
|    time_elapsed         | 1256       |
|    total_timesteps      | 1024000    |
| train/                  |            |
|    approx_kl            | 0.09303388 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.3        |
|    entropy_loss         | -204       |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0005     |
|    loss                 | -40.6      |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.0364    |
|    std                  | 1.11       |
|    value_loss           | 1.2        |
----------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 39.20 | Avg Reward: 38.24 | Max Reward: 45.91
Iteration: 253 | Episodes: 10300 | Median Reward: 37.53 | Avg Reward: 36.64 | Max Reward: 45.91
Iteration: 256 | Episodes: 10400 | Median Reward: 41.21 | Avg Reward: 37.94 | Max Reward: 45.91
Iteration: 258 | Episodes: 10500 | Median Reward: 39.16 | Avg Reward: 36.41 | Max Reward: 45.91
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -62.2      |
| time/                   |            |
|    fps                  | 818        |
|    iterations           | 260        |
|    time_elapsed         | 1301       |
|    total_timesteps      | 1064960    |
| train/                  |            |
|    approx_kl            | 0.06744976 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.3        |
|    entropy_loss         | -206       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -41        |
|    n_updates            | 2590       |
|    policy_gradient_loss | -0.0483    |
|    std                  | 1.12       |
|    value_loss           | 1.06       |
----------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 39.78 | Avg Reward: 39.95 | Max Reward: 45.91
Iteration: 263 | Episodes: 10700 | Median Reward: 38.46 | Avg Reward: 36.02 | Max Reward: 45.91
Iteration: 266 | Episodes: 10800 | Median Reward: 40.19 | Avg Reward: 39.23 | Max Reward: 45.91
Iteration: 268 | Episodes: 10900 | Median Reward: 38.13 | Avg Reward: 35.75 | Max Reward: 45.91
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -65        |
| time/                   |            |
|    fps                  | 821        |
|    iterations           | 270        |
|    time_elapsed         | 1346       |
|    total_timesteps      | 1105920    |
| train/                  |            |
|    approx_kl            | 0.06398848 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.3        |
|    entropy_loss         | -208       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0005     |
|    loss                 | -37.9      |
|    n_updates            | 2690       |
|    policy_gradient_loss | -0.0497    |
|    std                  | 1.13       |
|    value_loss           | 3.31       |
----------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 36.74 | Avg Reward: 36.48 | Max Reward: 45.91
Iteration: 273 | Episodes: 11100 | Median Reward: 39.94 | Avg Reward: 38.73 | Max Reward: 45.91
Iteration: 276 | Episodes: 11200 | Median Reward: 40.39 | Avg Reward: 39.97 | Max Reward: 45.91
Iteration: 278 | Episodes: 11300 | Median Reward: 39.25 | Avg Reward: 39.94 | Max Reward: 45.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.6       |
| time/                   |             |
|    fps                  | 824         |
|    iterations           | 280         |
|    time_elapsed         | 1391        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.016110249 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -209        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -40.7       |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.0215     |
|    std                  | 1.14        |
|    value_loss           | 1.31        |
-----------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 40.15 | Avg Reward: 37.97 | Max Reward: 45.91
Iteration: 283 | Episodes: 11500 | Median Reward: 41.19 | Avg Reward: 40.57 | Max Reward: 45.91
Iteration: 286 | Episodes: 11600 | Median Reward: 40.35 | Avg Reward: 39.77 | Max Reward: 45.91
Iteration: 288 | Episodes: 11700 | Median Reward: 37.82 | Avg Reward: 36.69 | Max Reward: 45.91
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -59.9     |
| time/                   |           |
|    fps                  | 827       |
|    iterations           | 290       |
|    time_elapsed         | 1434      |
|    total_timesteps      | 1187840   |
| train/                  |           |
|    approx_kl            | 0.1671913 |
|    clip_fraction        | 0.274     |
|    clip_range           | 0.3       |
|    entropy_loss         | -211      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.0005    |
|    loss                 | -40.4     |
|    n_updates            | 2890      |
|    policy_gradient_loss | -0.04     |
|    std                  | 1.14      |
|    value_loss           | 2.98      |
---------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 41.27 | Avg Reward: 41.00 | Max Reward: 45.91
Iteration: 293 | Episodes: 11900 | Median Reward: 38.04 | Avg Reward: 37.57 | Max Reward: 45.91
Iteration: 295 | Episodes: 12000 | Median Reward: 43.31 | Avg Reward: 42.25 | Max Reward: 45.91
Iteration: 298 | Episodes: 12100 | Median Reward: 40.74 | Avg Reward: 41.18 | Max Reward: 45.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.3        |
| time/                   |              |
|    fps                  | 830          |
|    iterations           | 300          |
|    time_elapsed         | 1480         |
|    total_timesteps      | 1228800      |
| train/                  |              |
|    approx_kl            | 0.0007731019 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -214         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0005       |
|    loss                 | -41          |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.000761    |
|    std                  | 1.15         |
|    value_loss           | 4.1          |
------------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 41.23 | Avg Reward: 40.79 | Max Reward: 45.91
Iteration: 303 | Episodes: 12300 | Median Reward: 41.30 | Avg Reward: 39.92 | Max Reward: 45.91
Iteration: 305 | Episodes: 12400 | Median Reward: 40.31 | Avg Reward: 39.57 | Max Reward: 45.91
Iteration: 308 | Episodes: 12500 | Median Reward: 40.95 | Avg Reward: 40.20 | Max Reward: 45.91
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.8         |
| time/                   |               |
|    fps                  | 832           |
|    iterations           | 310           |
|    time_elapsed         | 1524          |
|    total_timesteps      | 1269760       |
| train/                  |               |
|    approx_kl            | 0.00045717566 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -216          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -40.7         |
|    n_updates            | 3090          |
|    policy_gradient_loss | -0.00092      |
|    std                  | 1.15          |
|    value_loss           | 4.15          |
-------------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: 39.79 | Avg Reward: 39.13 | Max Reward: 45.91
Iteration: 313 | Episodes: 12700 | Median Reward: 43.26 | Avg Reward: 39.85 | Max Reward: 45.91
Iteration: 315 | Episodes: 12800 | Median Reward: 40.07 | Avg Reward: 39.54 | Max Reward: 45.91
Iteration: 318 | Episodes: 12900 | Median Reward: 39.74 | Avg Reward: 39.59 | Max Reward: 45.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.2       |
| time/                   |             |
|    fps                  | 834         |
|    iterations           | 320         |
|    time_elapsed         | 1570        |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.003484331 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -218        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -43.1       |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.00417    |
|    std                  | 1.16        |
|    value_loss           | 1.2         |
-----------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: 38.45 | Avg Reward: 38.33 | Max Reward: 45.91
Iteration: 323 | Episodes: 13100 | Median Reward: 40.57 | Avg Reward: 40.08 | Max Reward: 45.91
Iteration: 325 | Episodes: 13200 | Median Reward: 42.25 | Avg Reward: 41.30 | Max Reward: 45.91
Iteration: 327 | Episodes: 13300 | Median Reward: 42.63 | Avg Reward: 40.13 | Max Reward: 45.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.8        |
| time/                   |              |
|    fps                  | 836          |
|    iterations           | 330          |
|    time_elapsed         | 1615         |
|    total_timesteps      | 1351680      |
| train/                  |              |
|    approx_kl            | 0.0012870557 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -220         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -43          |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.0051      |
|    std                  | 1.17         |
|    value_loss           | 2.79         |
------------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: 39.24 | Avg Reward: 39.48 | Max Reward: 45.91
Iteration: 332 | Episodes: 13500 | Median Reward: 38.93 | Avg Reward: 37.85 | Max Reward: 45.91
Iteration: 335 | Episodes: 13600 | Median Reward: 40.64 | Avg Reward: 40.89 | Max Reward: 45.91
Iteration: 337 | Episodes: 13700 | Median Reward: 42.44 | Avg Reward: 41.82 | Max Reward: 45.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.6        |
| time/                   |              |
|    fps                  | 838          |
|    iterations           | 340          |
|    time_elapsed         | 1661         |
|    total_timesteps      | 1392640      |
| train/                  |              |
|    approx_kl            | 0.0009574648 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -222         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -44          |
|    n_updates            | 3390         |
|    policy_gradient_loss | 0.00159      |
|    std                  | 1.18         |
|    value_loss           | 1.69         |
------------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: 40.30 | Avg Reward: 40.20 | Max Reward: 45.91
Iteration: 342 | Episodes: 13900 | Median Reward: 40.02 | Avg Reward: 38.04 | Max Reward: 45.91
Iteration: 345 | Episodes: 14000 | Median Reward: 41.43 | Avg Reward: 40.57 | Max Reward: 45.91
Iteration: 347 | Episodes: 14100 | Median Reward: 42.94 | Avg Reward: 41.41 | Max Reward: 45.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 839         |
|    iterations           | 350         |
|    time_elapsed         | 1706        |
|    total_timesteps      | 1433600     |
| train/                  |             |
|    approx_kl            | 0.002110048 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -224        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -44.6       |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.00348    |
|    std                  | 1.18        |
|    value_loss           | 0.746       |
-----------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: 41.69 | Avg Reward: 41.01 | Max Reward: 45.91
Iteration: 352 | Episodes: 14300 | Median Reward: 42.16 | Avg Reward: 41.40 | Max Reward: 45.91
Iteration: 355 | Episodes: 14400 | Median Reward: 41.25 | Avg Reward: 39.26 | Max Reward: 45.93
Iteration: 357 | Episodes: 14500 | Median Reward: 41.20 | Avg Reward: 40.60 | Max Reward: 45.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 841         |
|    iterations           | 360         |
|    time_elapsed         | 1751        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.004754272 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -225        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -43.9       |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00923    |
|    std                  | 1.19        |
|    value_loss           | 4.08        |
-----------------------------------------
Iteration: 360 | Episodes: 14600 | Median Reward: 41.34 | Avg Reward: 41.30 | Max Reward: 45.93
Iteration: 362 | Episodes: 14700 | Median Reward: 41.36 | Avg Reward: 41.85 | Max Reward: 45.93
Iteration: 364 | Episodes: 14800 | Median Reward: 40.04 | Avg Reward: 40.80 | Max Reward: 45.93
Iteration: 367 | Episodes: 14900 | Median Reward: 39.32 | Avg Reward: 40.52 | Max Reward: 45.93
Iteration: 369 | Episodes: 15000 | Median Reward: 40.50 | Avg Reward: 40.09 | Max Reward: 45.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60         |
| time/                   |             |
|    fps                  | 843         |
|    iterations           | 370         |
|    time_elapsed         | 1796        |
|    total_timesteps      | 1515520     |
| train/                  |             |
|    approx_kl            | 0.011325726 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -226        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -44.6       |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.0143     |
|    std                  | 1.2         |
|    value_loss           | 1.18        |
-----------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: 40.22 | Avg Reward: 40.03 | Max Reward: 45.93
Iteration: 374 | Episodes: 15200 | Median Reward: 41.47 | Avg Reward: 40.97 | Max Reward: 45.93
Iteration: 377 | Episodes: 15300 | Median Reward: 42.19 | Avg Reward: 41.35 | Max Reward: 45.93
Iteration: 379 | Episodes: 15400 | Median Reward: 40.12 | Avg Reward: 40.23 | Max Reward: 45.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.9       |
| time/                   |             |
|    fps                  | 845         |
|    iterations           | 380         |
|    time_elapsed         | 1841        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.018641064 |
|    clip_fraction        | 0.0755      |
|    clip_range           | 0.3         |
|    entropy_loss         | -228        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -45.3       |
|    n_updates            | 3790        |
|    policy_gradient_loss | 0.00912     |
|    std                  | 1.21        |
|    value_loss           | 1.64        |
-----------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: 41.15 | Avg Reward: 41.13 | Max Reward: 45.93
Iteration: 384 | Episodes: 15600 | Median Reward: 43.81 | Avg Reward: 42.15 | Max Reward: 45.93
Iteration: 387 | Episodes: 15700 | Median Reward: 42.04 | Avg Reward: 41.00 | Max Reward: 45.93
Iteration: 389 | Episodes: 15800 | Median Reward: 42.27 | Avg Reward: 42.30 | Max Reward: 45.93
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 846          |
|    iterations           | 390          |
|    time_elapsed         | 1887         |
|    total_timesteps      | 1597440      |
| train/                  |              |
|    approx_kl            | 0.0010034016 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -231         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -45.3        |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.22         |
|    value_loss           | 1.73         |
------------------------------------------
Iteration: 392 | Episodes: 15900 | Median Reward: 43.34 | Avg Reward: 41.69 | Max Reward: 45.93
Iteration: 394 | Episodes: 16000 | Median Reward: 42.54 | Avg Reward: 42.19 | Max Reward: 45.96
Iteration: 396 | Episodes: 16100 | Median Reward: 42.63 | Avg Reward: 41.12 | Max Reward: 45.96
Iteration: 399 | Episodes: 16200 | Median Reward: 42.62 | Avg Reward: 42.91 | Max Reward: 45.96
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.3         |
| time/                   |               |
|    fps                  | 847           |
|    iterations           | 400           |
|    time_elapsed         | 1932          |
|    total_timesteps      | 1638400       |
| train/                  |               |
|    approx_kl            | 0.00014763407 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -232          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -45.4         |
|    n_updates            | 3990          |
|    policy_gradient_loss | -0.000105     |
|    std                  | 1.23          |
|    value_loss           | 2.41          |
-------------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: 41.49 | Avg Reward: 42.04 | Max Reward: 45.96
Iteration: 404 | Episodes: 16400 | Median Reward: 41.51 | Avg Reward: 41.15 | Max Reward: 45.96
Iteration: 406 | Episodes: 16500 | Median Reward: 41.47 | Avg Reward: 40.58 | Max Reward: 45.96
Iteration: 409 | Episodes: 16600 | Median Reward: 43.34 | Avg Reward: 42.31 | Max Reward: 45.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.3       |
| time/                   |             |
|    fps                  | 848         |
|    iterations           | 410         |
|    time_elapsed         | 1978        |
|    total_timesteps      | 1679360     |
| train/                  |             |
|    approx_kl            | 0.005431882 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -233        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -46.2       |
|    n_updates            | 4090        |
|    policy_gradient_loss | 0.00131     |
|    std                  | 1.24        |
|    value_loss           | 0.834       |
-----------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: 42.59 | Avg Reward: 42.73 | Max Reward: 45.98
Iteration: 414 | Episodes: 16800 | Median Reward: 44.27 | Avg Reward: 43.04 | Max Reward: 45.98
Iteration: 416 | Episodes: 16900 | Median Reward: 41.06 | Avg Reward: 40.23 | Max Reward: 45.98
Iteration: 419 | Episodes: 17000 | Median Reward: 43.29 | Avg Reward: 41.78 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.2         |
| time/                   |               |
|    fps                  | 849           |
|    iterations           | 420           |
|    time_elapsed         | 2024          |
|    total_timesteps      | 1720320       |
| train/                  |               |
|    approx_kl            | 0.00063369493 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -234          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -41.6         |
|    n_updates            | 4190          |
|    policy_gradient_loss | -0.00222      |
|    std                  | 1.25          |
|    value_loss           | 7.65          |
-------------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: 41.78 | Avg Reward: 41.22 | Max Reward: 45.98
Iteration: 424 | Episodes: 17200 | Median Reward: 40.89 | Avg Reward: 41.72 | Max Reward: 45.98
Iteration: 426 | Episodes: 17300 | Median Reward: 41.27 | Avg Reward: 41.34 | Max Reward: 45.98
Iteration: 429 | Episodes: 17400 | Median Reward: 42.83 | Avg Reward: 42.92 | Max Reward: 45.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.2       |
| time/                   |             |
|    fps                  | 851         |
|    iterations           | 430         |
|    time_elapsed         | 2069        |
|    total_timesteps      | 1761280     |
| train/                  |             |
|    approx_kl            | 0.004682699 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -235        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -44.7       |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00888    |
|    std                  | 1.25        |
|    value_loss           | 2.3         |
-----------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: 44.38 | Avg Reward: 42.46 | Max Reward: 45.98
Iteration: 433 | Episodes: 17600 | Median Reward: 43.79 | Avg Reward: 41.98 | Max Reward: 45.98
Iteration: 436 | Episodes: 17700 | Median Reward: 41.80 | Avg Reward: 40.89 | Max Reward: 45.98
Iteration: 438 | Episodes: 17800 | Median Reward: 42.48 | Avg Reward: 42.16 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.7         |
| time/                   |               |
|    fps                  | 852           |
|    iterations           | 440           |
|    time_elapsed         | 2114          |
|    total_timesteps      | 1802240       |
| train/                  |               |
|    approx_kl            | 0.00080026523 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -236          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -47           |
|    n_updates            | 4390          |
|    policy_gradient_loss | -0.000175     |
|    std                  | 1.26          |
|    value_loss           | 1.22          |
-------------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 42.14 | Avg Reward: 42.18 | Max Reward: 45.98
Iteration: 443 | Episodes: 18000 | Median Reward: 42.49 | Avg Reward: 41.53 | Max Reward: 45.98
Iteration: 446 | Episodes: 18100 | Median Reward: 41.47 | Avg Reward: 40.55 | Max Reward: 45.98
Iteration: 448 | Episodes: 18200 | Median Reward: 43.72 | Avg Reward: 42.35 | Max Reward: 46.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.3       |
| time/                   |             |
|    fps                  | 853         |
|    iterations           | 450         |
|    time_elapsed         | 2159        |
|    total_timesteps      | 1843200     |
| train/                  |             |
|    approx_kl            | 0.003206768 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -237        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -43.9       |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.00785    |
|    std                  | 1.27        |
|    value_loss           | 2.39        |
-----------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 41.73 | Avg Reward: 41.59 | Max Reward: 46.19
Iteration: 453 | Episodes: 18400 | Median Reward: 43.96 | Avg Reward: 42.85 | Max Reward: 46.19
Iteration: 456 | Episodes: 18500 | Median Reward: 41.60 | Avg Reward: 39.43 | Max Reward: 46.19
Iteration: 458 | Episodes: 18600 | Median Reward: 44.31 | Avg Reward: 43.27 | Max Reward: 46.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.6         |
| time/                   |               |
|    fps                  | 854           |
|    iterations           | 460           |
|    time_elapsed         | 2204          |
|    total_timesteps      | 1884160       |
| train/                  |               |
|    approx_kl            | 0.00011989169 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -238          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -47.2         |
|    n_updates            | 4590          |
|    policy_gradient_loss | -0.000345     |
|    std                  | 1.28          |
|    value_loss           | 1.33          |
-------------------------------------------
Iteration: 461 | Episodes: 18700 | Median Reward: 41.17 | Avg Reward: 41.25 | Max Reward: 46.19
Iteration: 463 | Episodes: 18800 | Median Reward: 42.14 | Avg Reward: 41.04 | Max Reward: 46.19
Iteration: 466 | Episodes: 18900 | Median Reward: 40.42 | Avg Reward: 41.18 | Max Reward: 46.19
Iteration: 468 | Episodes: 19000 | Median Reward: 41.39 | Avg Reward: 41.63 | Max Reward: 46.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.4       |
| time/                   |             |
|    fps                  | 855         |
|    iterations           | 470         |
|    time_elapsed         | 2249        |
|    total_timesteps      | 1925120     |
| train/                  |             |
|    approx_kl            | 0.018570535 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -238        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -47.1       |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.0189     |
|    std                  | 1.29        |
|    value_loss           | 1.34        |
-----------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 42.53 | Avg Reward: 42.19 | Max Reward: 46.19
Iteration: 473 | Episodes: 19200 | Median Reward: 41.95 | Avg Reward: 41.96 | Max Reward: 46.19
Iteration: 475 | Episodes: 19300 | Median Reward: 38.11 | Avg Reward: 38.62 | Max Reward: 46.19
Iteration: 478 | Episodes: 19400 | Median Reward: 40.54 | Avg Reward: 40.31 | Max Reward: 46.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.3        |
| time/                   |              |
|    fps                  | 856          |
|    iterations           | 480          |
|    time_elapsed         | 2294         |
|    total_timesteps      | 1966080      |
| train/                  |              |
|    approx_kl            | 0.0010371171 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -239         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -41.3        |
|    n_updates            | 4790         |
|    policy_gradient_loss | 3.05e-05     |
|    std                  | 1.3          |
|    value_loss           | 7.79         |
------------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 44.19 | Avg Reward: 42.67 | Max Reward: 46.19
Iteration: 483 | Episodes: 19600 | Median Reward: 42.79 | Avg Reward: 42.84 | Max Reward: 46.19
Iteration: 485 | Episodes: 19700 | Median Reward: 42.31 | Avg Reward: 41.35 | Max Reward: 46.19
Iteration: 488 | Episodes: 19800 | Median Reward: 41.23 | Avg Reward: 40.21 | Max Reward: 46.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.9       |
| time/                   |             |
|    fps                  | 857         |
|    iterations           | 490         |
|    time_elapsed         | 2340        |
|    total_timesteps      | 2007040     |
| train/                  |             |
|    approx_kl            | 0.006263225 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -240        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -47.8       |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.00648    |
|    std                  | 1.31        |
|    value_loss           | 1.17        |
-----------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 41.37 | Avg Reward: 41.41 | Max Reward: 46.19
Iteration: 493 | Episodes: 20000 | Median Reward: 42.05 | Avg Reward: 42.62 | Max Reward: 46.19
Iteration: 495 | Episodes: 20100 | Median Reward: 42.12 | Avg Reward: 42.48 | Max Reward: 46.19
Iteration: 498 | Episodes: 20200 | Median Reward: 41.57 | Avg Reward: 41.67 | Max Reward: 46.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.1         |
| time/                   |               |
|    fps                  | 858           |
|    iterations           | 500           |
|    time_elapsed         | 2385          |
|    total_timesteps      | 2048000       |
| train/                  |               |
|    approx_kl            | 0.00049364666 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -241          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -47.7         |
|    n_updates            | 4990          |
|    policy_gradient_loss | -0.000227     |
|    std                  | 1.32          |
|    value_loss           | 1.35          |
-------------------------------------------
Iteration: 500 | Episodes: 20300 | Median Reward: 41.75 | Avg Reward: 41.10 | Max Reward: 46.19
Iteration: 503 | Episodes: 20400 | Median Reward: 41.76 | Avg Reward: 42.03 | Max Reward: 46.19
Iteration: 505 | Episodes: 20500 | Median Reward: 44.23 | Avg Reward: 42.95 | Max Reward: 46.19
Iteration: 507 | Episodes: 20600 | Median Reward: 44.26 | Avg Reward: 43.58 | Max Reward: 46.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.5       |
| time/                   |             |
|    fps                  | 859         |
|    iterations           | 510         |
|    time_elapsed         | 2429        |
|    total_timesteps      | 2088960     |
| train/                  |             |
|    approx_kl            | 0.003332723 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -242        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -47.5       |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00457    |
|    std                  | 1.33        |
|    value_loss           | 0.872       |
-----------------------------------------
Iteration: 510 | Episodes: 20700 | Median Reward: 45.30 | Avg Reward: 42.97 | Max Reward: 46.25
Iteration: 512 | Episodes: 20800 | Median Reward: 43.75 | Avg Reward: 42.37 | Max Reward: 46.25
Iteration: 515 | Episodes: 20900 | Median Reward: 41.30 | Avg Reward: 40.84 | Max Reward: 46.25
Iteration: 517 | Episodes: 21000 | Median Reward: 41.93 | Avg Reward: 41.89 | Max Reward: 46.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 861          |
|    iterations           | 520          |
|    time_elapsed         | 2473         |
|    total_timesteps      | 2129920      |
| train/                  |              |
|    approx_kl            | 0.0033030212 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -244         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -48.1        |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.0104      |
|    std                  | 1.34         |
|    value_loss           | 1.44         |
------------------------------------------
Iteration: 520 | Episodes: 21100 | Median Reward: 44.03 | Avg Reward: 42.60 | Max Reward: 46.25
Iteration: 522 | Episodes: 21200 | Median Reward: 42.26 | Avg Reward: 42.22 | Max Reward: 46.25
Iteration: 525 | Episodes: 21300 | Median Reward: 44.22 | Avg Reward: 43.28 | Max Reward: 46.25
Iteration: 527 | Episodes: 21400 | Median Reward: 42.94 | Avg Reward: 42.76 | Max Reward: 46.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.6         |
| time/                   |               |
|    fps                  | 861           |
|    iterations           | 530           |
|    time_elapsed         | 2518          |
|    total_timesteps      | 2170880       |
| train/                  |               |
|    approx_kl            | 0.00019908638 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -246          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -48.8         |
|    n_updates            | 5290          |
|    policy_gradient_loss | -0.000124     |
|    std                  | 1.35          |
|    value_loss           | 1.18          |
-------------------------------------------
Iteration: 530 | Episodes: 21500 | Median Reward: 42.10 | Avg Reward: 42.33 | Max Reward: 46.25
Iteration: 532 | Episodes: 21600 | Median Reward: 41.46 | Avg Reward: 41.11 | Max Reward: 46.25
Iteration: 535 | Episodes: 21700 | Median Reward: 40.29 | Avg Reward: 40.73 | Max Reward: 46.25
Iteration: 537 | Episodes: 21800 | Median Reward: 44.14 | Avg Reward: 43.16 | Max Reward: 46.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.6         |
| time/                   |               |
|    fps                  | 862           |
|    iterations           | 540           |
|    time_elapsed         | 2564          |
|    total_timesteps      | 2211840       |
| train/                  |               |
|    approx_kl            | 0.00046781363 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -247          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -49.3         |
|    n_updates            | 5390          |
|    policy_gradient_loss | -0.000246     |
|    std                  | 1.37          |
|    value_loss           | 1.43          |
-------------------------------------------
Iteration: 540 | Episodes: 21900 | Median Reward: 43.53 | Avg Reward: 43.28 | Max Reward: 46.25
Iteration: 542 | Episodes: 22000 | Median Reward: 42.91 | Avg Reward: 42.72 | Max Reward: 46.25
Iteration: 544 | Episodes: 22100 | Median Reward: 44.54 | Avg Reward: 42.33 | Max Reward: 46.25
Iteration: 547 | Episodes: 22200 | Median Reward: 43.74 | Avg Reward: 43.05 | Max Reward: 46.25
Iteration: 549 | Episodes: 22300 | Median Reward: 43.26 | Avg Reward: 42.53 | Max Reward: 46.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.5       |
| time/                   |             |
|    fps                  | 863         |
|    iterations           | 550         |
|    time_elapsed         | 2609        |
|    total_timesteps      | 2252800     |
| train/                  |             |
|    approx_kl            | 0.068019055 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.3         |
|    entropy_loss         | -249        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -49.8       |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.0721     |
|    std                  | 1.38        |
|    value_loss           | 0.926       |
-----------------------------------------
Iteration: 552 | Episodes: 22400 | Median Reward: 44.26 | Avg Reward: 43.57 | Max Reward: 46.25
Iteration: 554 | Episodes: 22500 | Median Reward: 44.40 | Avg Reward: 43.56 | Max Reward: 46.25
Iteration: 557 | Episodes: 22600 | Median Reward: 43.96 | Avg Reward: 43.52 | Max Reward: 46.28
Iteration: 559 | Episodes: 22700 | Median Reward: 43.05 | Avg Reward: 42.27 | Max Reward: 46.28
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.8        |
| time/                   |              |
|    fps                  | 864          |
|    iterations           | 560          |
|    time_elapsed         | 2654         |
|    total_timesteps      | 2293760      |
| train/                  |              |
|    approx_kl            | 0.0011565068 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -250         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -48.2        |
|    n_updates            | 5590         |
|    policy_gradient_loss | -0.000963    |
|    std                  | 1.39         |
|    value_loss           | 4.8          |
------------------------------------------
Iteration: 562 | Episodes: 22800 | Median Reward: 42.64 | Avg Reward: 41.51 | Max Reward: 46.28
Iteration: 564 | Episodes: 22900 | Median Reward: 44.53 | Avg Reward: 42.73 | Max Reward: 46.28
Iteration: 567 | Episodes: 23000 | Median Reward: 43.31 | Avg Reward: 42.31 | Max Reward: 46.28
Iteration: 569 | Episodes: 23100 | Median Reward: 44.40 | Avg Reward: 42.54 | Max Reward: 46.28
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.1        |
| time/                   |              |
|    fps                  | 864          |
|    iterations           | 570          |
|    time_elapsed         | 2699         |
|    total_timesteps      | 2334720      |
| train/                  |              |
|    approx_kl            | 0.0063404874 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -49.1        |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.0146      |
|    std                  | 1.39         |
|    value_loss           | 2.17         |
------------------------------------------
Iteration: 572 | Episodes: 23200 | Median Reward: 43.23 | Avg Reward: 43.13 | Max Reward: 46.28
Iteration: 574 | Episodes: 23300 | Median Reward: 42.19 | Avg Reward: 42.41 | Max Reward: 46.28
Iteration: 577 | Episodes: 23400 | Median Reward: 42.25 | Avg Reward: 43.01 | Max Reward: 46.28
Iteration: 579 | Episodes: 23500 | Median Reward: 43.03 | Avg Reward: 41.66 | Max Reward: 46.28
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.6        |
| time/                   |              |
|    fps                  | 865          |
|    iterations           | 580          |
|    time_elapsed         | 2744         |
|    total_timesteps      | 2375680      |
| train/                  |              |
|    approx_kl            | 0.0036089197 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -251         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -49.7        |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.00448     |
|    std                  | 1.4          |
|    value_loss           | 0.482        |
------------------------------------------
Iteration: 581 | Episodes: 23600 | Median Reward: 42.33 | Avg Reward: 41.81 | Max Reward: 46.28
Iteration: 584 | Episodes: 23700 | Median Reward: 44.22 | Avg Reward: 43.38 | Max Reward: 46.28
Iteration: 586 | Episodes: 23800 | Median Reward: 41.41 | Avg Reward: 41.84 | Max Reward: 46.28
Iteration: 589 | Episodes: 23900 | Median Reward: 42.34 | Avg Reward: 41.99 | Max Reward: 46.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.8       |
| time/                   |             |
|    fps                  | 866         |
|    iterations           | 590         |
|    time_elapsed         | 2789        |
|    total_timesteps      | 2416640     |
| train/                  |             |
|    approx_kl            | 0.008255275 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -252        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -50         |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.0049     |
|    std                  | 1.41        |
|    value_loss           | 0.957       |
-----------------------------------------
Iteration: 591 | Episodes: 24000 | Median Reward: 44.54 | Avg Reward: 43.48 | Max Reward: 46.28
Iteration: 594 | Episodes: 24100 | Median Reward: 44.82 | Avg Reward: 44.02 | Max Reward: 46.28
Iteration: 596 | Episodes: 24200 | Median Reward: 42.49 | Avg Reward: 42.37 | Max Reward: 46.28
Iteration: 599 | Episodes: 24300 | Median Reward: 41.84 | Avg Reward: 41.78 | Max Reward: 46.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.3       |
| time/                   |             |
|    fps                  | 867         |
|    iterations           | 600         |
|    time_elapsed         | 2833        |
|    total_timesteps      | 2457600     |
| train/                  |             |
|    approx_kl            | 0.023184687 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.3         |
|    entropy_loss         | -252        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -50.1       |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.012      |
|    std                  | 1.42        |
|    value_loss           | 0.859       |
-----------------------------------------
Iteration: 601 | Episodes: 24400 | Median Reward: 41.86 | Avg Reward: 41.79 | Max Reward: 46.28
Iteration: 604 | Episodes: 24500 | Median Reward: 42.59 | Avg Reward: 41.39 | Max Reward: 46.28
Iteration: 606 | Episodes: 24600 | Median Reward: 44.26 | Avg Reward: 42.74 | Max Reward: 46.28
Iteration: 609 | Episodes: 24700 | Median Reward: 43.30 | Avg Reward: 42.73 | Max Reward: 46.28
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.1         |
| time/                   |               |
|    fps                  | 867           |
|    iterations           | 610           |
|    time_elapsed         | 2878          |
|    total_timesteps      | 2498560       |
| train/                  |               |
|    approx_kl            | 0.00016105868 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -254          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -50.6         |
|    n_updates            | 6090          |
|    policy_gradient_loss | 0.00049       |
|    std                  | 1.44          |
|    value_loss           | 0.821         |
-------------------------------------------
Iteration: 611 | Episodes: 24800 | Median Reward: 42.47 | Avg Reward: 42.39 | Max Reward: 46.28
Iteration: 613 | Episodes: 24900 | Median Reward: 43.61 | Avg Reward: 43.35 | Max Reward: 46.96
Iteration: 616 | Episodes: 25000 | Median Reward: 42.44 | Avg Reward: 41.72 | Max Reward: 46.96
Iteration: 618 | Episodes: 25100 | Median Reward: 44.15 | Avg Reward: 43.03 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 868          |
|    iterations           | 620          |
|    time_elapsed         | 2923         |
|    total_timesteps      | 2539520      |
| train/                  |              |
|    approx_kl            | 0.0010842443 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -255         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -50.7        |
|    n_updates            | 6190         |
|    policy_gradient_loss | -0.0062      |
|    std                  | 1.45         |
|    value_loss           | 3.74         |
------------------------------------------
Iteration: 621 | Episodes: 25200 | Median Reward: 45.09 | Avg Reward: 44.12 | Max Reward: 46.96
Iteration: 623 | Episodes: 25300 | Median Reward: 44.42 | Avg Reward: 43.42 | Max Reward: 46.96
Iteration: 626 | Episodes: 25400 | Median Reward: 42.43 | Avg Reward: 41.52 | Max Reward: 46.96
Iteration: 628 | Episodes: 25500 | Median Reward: 44.23 | Avg Reward: 43.35 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.5       |
| time/                   |             |
|    fps                  | 868         |
|    iterations           | 630         |
|    time_elapsed         | 2969        |
|    total_timesteps      | 2580480     |
| train/                  |             |
|    approx_kl            | 0.028176112 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -256        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -50.9       |
|    n_updates            | 6290        |
|    policy_gradient_loss | -0.0215     |
|    std                  | 1.46        |
|    value_loss           | 0.491       |
-----------------------------------------
Iteration: 631 | Episodes: 25600 | Median Reward: 44.32 | Avg Reward: 43.53 | Max Reward: 46.96
Iteration: 633 | Episodes: 25700 | Median Reward: 42.43 | Avg Reward: 42.72 | Max Reward: 46.96
Iteration: 636 | Episodes: 25800 | Median Reward: 44.57 | Avg Reward: 43.32 | Max Reward: 46.96
Iteration: 638 | Episodes: 25900 | Median Reward: 44.32 | Avg Reward: 42.56 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.3       |
| time/                   |             |
|    fps                  | 869         |
|    iterations           | 640         |
|    time_elapsed         | 3014        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.032093395 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.3         |
|    entropy_loss         | -257        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -51         |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.0252     |
|    std                  | 1.47        |
|    value_loss           | 0.594       |
-----------------------------------------
Iteration: 641 | Episodes: 26000 | Median Reward: 44.22 | Avg Reward: 43.18 | Max Reward: 46.96
Iteration: 643 | Episodes: 26100 | Median Reward: 42.55 | Avg Reward: 42.71 | Max Reward: 46.96
Iteration: 646 | Episodes: 26200 | Median Reward: 43.86 | Avg Reward: 42.39 | Max Reward: 46.96
Iteration: 648 | Episodes: 26300 | Median Reward: 44.29 | Avg Reward: 42.48 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 870          |
|    iterations           | 650          |
|    time_elapsed         | 3059         |
|    total_timesteps      | 2662400      |
| train/                  |              |
|    approx_kl            | 0.0147531275 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -258         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -51.4        |
|    n_updates            | 6490         |
|    policy_gradient_loss | -0.0179      |
|    std                  | 1.49         |
|    value_loss           | 0.784        |
------------------------------------------
Iteration: 650 | Episodes: 26400 | Median Reward: 43.39 | Avg Reward: 41.77 | Max Reward: 46.96
Iteration: 653 | Episodes: 26500 | Median Reward: 43.34 | Avg Reward: 42.45 | Max Reward: 46.96
Iteration: 655 | Episodes: 26600 | Median Reward: 44.30 | Avg Reward: 43.22 | Max Reward: 46.96
Iteration: 658 | Episodes: 26700 | Median Reward: 43.96 | Avg Reward: 42.34 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.3       |
| time/                   |             |
|    fps                  | 870         |
|    iterations           | 660         |
|    time_elapsed         | 3104        |
|    total_timesteps      | 2703360     |
| train/                  |             |
|    approx_kl            | 0.009728281 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -260        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -51.8       |
|    n_updates            | 6590        |
|    policy_gradient_loss | -0.0188     |
|    std                  | 1.5         |
|    value_loss           | 0.612       |
-----------------------------------------
Iteration: 660 | Episodes: 26800 | Median Reward: 41.51 | Avg Reward: 41.27 | Max Reward: 46.96
Iteration: 663 | Episodes: 26900 | Median Reward: 44.51 | Avg Reward: 44.03 | Max Reward: 46.96
Iteration: 665 | Episodes: 27000 | Median Reward: 42.84 | Avg Reward: 41.26 | Max Reward: 46.96
Iteration: 668 | Episodes: 27100 | Median Reward: 44.06 | Avg Reward: 43.07 | Max Reward: 46.96
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.1         |
| time/                   |               |
|    fps                  | 871           |
|    iterations           | 670           |
|    time_elapsed         | 3149          |
|    total_timesteps      | 2744320       |
| train/                  |               |
|    approx_kl            | 0.00044549926 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -261          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -52           |
|    n_updates            | 6690          |
|    policy_gradient_loss | -0.00127      |
|    std                  | 1.51          |
|    value_loss           | 1.3           |
-------------------------------------------
Iteration: 670 | Episodes: 27200 | Median Reward: 44.21 | Avg Reward: 42.95 | Max Reward: 46.96
Iteration: 673 | Episodes: 27300 | Median Reward: 44.52 | Avg Reward: 44.47 | Max Reward: 46.96
Iteration: 675 | Episodes: 27400 | Median Reward: 41.17 | Avg Reward: 41.31 | Max Reward: 46.96
Iteration: 678 | Episodes: 27500 | Median Reward: 43.37 | Avg Reward: 42.62 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.2        |
| time/                   |              |
|    fps                  | 872          |
|    iterations           | 680          |
|    time_elapsed         | 3194         |
|    total_timesteps      | 2785280      |
| train/                  |              |
|    approx_kl            | 0.0012863707 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -262         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -51.5        |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.00366     |
|    std                  | 1.53         |
|    value_loss           | 0.708        |
------------------------------------------
Iteration: 680 | Episodes: 27600 | Median Reward: 44.22 | Avg Reward: 42.37 | Max Reward: 46.96
Iteration: 683 | Episodes: 27700 | Median Reward: 45.14 | Avg Reward: 43.05 | Max Reward: 46.96
Iteration: 685 | Episodes: 27800 | Median Reward: 44.44 | Avg Reward: 43.70 | Max Reward: 46.96
Iteration: 687 | Episodes: 27900 | Median Reward: 43.45 | Avg Reward: 42.38 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.7       |
| time/                   |             |
|    fps                  | 872         |
|    iterations           | 690         |
|    time_elapsed         | 3239        |
|    total_timesteps      | 2826240     |
| train/                  |             |
|    approx_kl            | 0.004157845 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -263        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -52.3       |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.00169    |
|    std                  | 1.54        |
|    value_loss           | 0.575       |
-----------------------------------------
Iteration: 690 | Episodes: 28000 | Median Reward: 41.84 | Avg Reward: 42.42 | Max Reward: 46.96
Iteration: 692 | Episodes: 28100 | Median Reward: 42.30 | Avg Reward: 42.37 | Max Reward: 46.96
Iteration: 695 | Episodes: 28200 | Median Reward: 42.52 | Avg Reward: 41.84 | Max Reward: 46.96
Iteration: 697 | Episodes: 28300 | Median Reward: 41.44 | Avg Reward: 41.40 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 872         |
|    iterations           | 700         |
|    time_elapsed         | 3284        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.003710853 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -265        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -52.9       |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.00263    |
|    std                  | 1.56        |
|    value_loss           | 0.374       |
-----------------------------------------
Iteration: 700 | Episodes: 28400 | Median Reward: 41.14 | Avg Reward: 41.51 | Max Reward: 46.96
Iteration: 702 | Episodes: 28500 | Median Reward: 42.09 | Avg Reward: 42.01 | Max Reward: 46.96
Iteration: 705 | Episodes: 28600 | Median Reward: 43.37 | Avg Reward: 43.48 | Max Reward: 46.96
Iteration: 707 | Episodes: 28700 | Median Reward: 45.43 | Avg Reward: 44.17 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.5       |
| time/                   |             |
|    fps                  | 873         |
|    iterations           | 710         |
|    time_elapsed         | 3330        |
|    total_timesteps      | 2908160     |
| train/                  |             |
|    approx_kl            | 0.024833469 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -266        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -52.8       |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.0156     |
|    std                  | 1.57        |
|    value_loss           | 0.411       |
-----------------------------------------
Iteration: 710 | Episodes: 28800 | Median Reward: 44.27 | Avg Reward: 42.28 | Max Reward: 46.96
Iteration: 712 | Episodes: 28900 | Median Reward: 44.51 | Avg Reward: 43.52 | Max Reward: 46.96
Iteration: 715 | Episodes: 29000 | Median Reward: 43.71 | Avg Reward: 43.14 | Max Reward: 46.96
Iteration: 717 | Episodes: 29100 | Median Reward: 42.39 | Avg Reward: 42.80 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 873          |
|    iterations           | 720          |
|    time_elapsed         | 3376         |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0017759621 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -268         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -53.3        |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 1.59         |
|    value_loss           | 0.787        |
------------------------------------------
Iteration: 720 | Episodes: 29200 | Median Reward: 44.57 | Avg Reward: 44.62 | Max Reward: 46.96
Iteration: 722 | Episodes: 29300 | Median Reward: 43.95 | Avg Reward: 43.09 | Max Reward: 46.96
Iteration: 724 | Episodes: 29400 | Median Reward: 43.37 | Avg Reward: 43.13 | Max Reward: 46.96
Iteration: 727 | Episodes: 29500 | Median Reward: 42.42 | Avg Reward: 42.98 | Max Reward: 46.96
Iteration: 729 | Episodes: 29600 | Median Reward: 44.29 | Avg Reward: 43.36 | Max Reward: 46.96
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.6         |
| time/                   |               |
|    fps                  | 873           |
|    iterations           | 730           |
|    time_elapsed         | 3421          |
|    total_timesteps      | 2990080       |
| train/                  |               |
|    approx_kl            | 0.00072754885 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -269          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -53.4         |
|    n_updates            | 7290          |
|    policy_gradient_loss | -0.000255     |
|    std                  | 1.6           |
|    value_loss           | 0.701         |
-------------------------------------------
Iteration: 732 | Episodes: 29700 | Median Reward: 44.28 | Avg Reward: 43.94 | Max Reward: 46.96
Iteration: 734 | Episodes: 29800 | Median Reward: 42.30 | Avg Reward: 42.60 | Max Reward: 46.96
Iteration: 737 | Episodes: 29900 | Median Reward: 43.18 | Avg Reward: 42.51 | Max Reward: 46.96
Iteration: 739 | Episodes: 30000 | Median Reward: 43.65 | Avg Reward: 42.88 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.1       |
| time/                   |             |
|    fps                  | 874         |
|    iterations           | 740         |
|    time_elapsed         | 3466        |
|    total_timesteps      | 3031040     |
| train/                  |             |
|    approx_kl            | 0.013479556 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -270        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -53.7       |
|    n_updates            | 7390        |
|    policy_gradient_loss | -0.00952    |
|    std                  | 1.61        |
|    value_loss           | 0.777       |
-----------------------------------------
Iteration: 742 | Episodes: 30100 | Median Reward: 44.26 | Avg Reward: 43.27 | Max Reward: 46.96
Iteration: 744 | Episodes: 30200 | Median Reward: 44.61 | Avg Reward: 43.83 | Max Reward: 46.96
Iteration: 747 | Episodes: 30300 | Median Reward: 43.41 | Avg Reward: 43.10 | Max Reward: 46.96
Iteration: 749 | Episodes: 30400 | Median Reward: 42.36 | Avg Reward: 42.65 | Max Reward: 46.96
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 874           |
|    iterations           | 750           |
|    time_elapsed         | 3512          |
|    total_timesteps      | 3072000       |
| train/                  |               |
|    approx_kl            | 0.00017161852 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -270          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -51.7         |
|    n_updates            | 7490          |
|    policy_gradient_loss | 9.27e-05      |
|    std                  | 1.63          |
|    value_loss           | 1.94          |
-------------------------------------------
Iteration: 752 | Episodes: 30500 | Median Reward: 43.69 | Avg Reward: 43.50 | Max Reward: 46.96
Iteration: 754 | Episodes: 30600 | Median Reward: 43.70 | Avg Reward: 43.07 | Max Reward: 46.96
Iteration: 757 | Episodes: 30700 | Median Reward: 41.41 | Avg Reward: 42.07 | Max Reward: 46.96
Iteration: 759 | Episodes: 30800 | Median Reward: 43.79 | Avg Reward: 42.56 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 874          |
|    iterations           | 760          |
|    time_elapsed         | 3557         |
|    total_timesteps      | 3112960      |
| train/                  |              |
|    approx_kl            | 0.0005123011 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -271         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -54          |
|    n_updates            | 7590         |
|    policy_gradient_loss | -0.00231     |
|    std                  | 1.64         |
|    value_loss           | 0.962        |
------------------------------------------
Iteration: 761 | Episodes: 30900 | Median Reward: 44.30 | Avg Reward: 43.27 | Max Reward: 46.96
Iteration: 764 | Episodes: 31000 | Median Reward: 42.34 | Avg Reward: 42.89 | Max Reward: 46.96
Iteration: 766 | Episodes: 31100 | Median Reward: 44.26 | Avg Reward: 43.24 | Max Reward: 46.96
Iteration: 769 | Episodes: 31200 | Median Reward: 42.00 | Avg Reward: 42.61 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.8        |
| time/                   |              |
|    fps                  | 875          |
|    iterations           | 770          |
|    time_elapsed         | 3602         |
|    total_timesteps      | 3153920      |
| train/                  |              |
|    approx_kl            | 0.0010597804 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -271         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -54          |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 1.65         |
|    value_loss           | 1.28         |
------------------------------------------
Iteration: 771 | Episodes: 31300 | Median Reward: 41.10 | Avg Reward: 42.17 | Max Reward: 46.96
Iteration: 774 | Episodes: 31400 | Median Reward: 43.65 | Avg Reward: 42.82 | Max Reward: 46.96
Iteration: 776 | Episodes: 31500 | Median Reward: 43.77 | Avg Reward: 43.20 | Max Reward: 46.96
Iteration: 779 | Episodes: 31600 | Median Reward: 43.65 | Avg Reward: 42.71 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.1        |
| time/                   |              |
|    fps                  | 875          |
|    iterations           | 780          |
|    time_elapsed         | 3647         |
|    total_timesteps      | 3194880      |
| train/                  |              |
|    approx_kl            | 0.0051406575 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -272         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -54          |
|    n_updates            | 7790         |
|    policy_gradient_loss | -0.0214      |
|    std                  | 1.67         |
|    value_loss           | 2.47         |
------------------------------------------
Iteration: 781 | Episodes: 31700 | Median Reward: 41.87 | Avg Reward: 41.39 | Max Reward: 46.96
Iteration: 784 | Episodes: 31800 | Median Reward: 44.03 | Avg Reward: 42.50 | Max Reward: 46.96
Iteration: 786 | Episodes: 31900 | Median Reward: 43.84 | Avg Reward: 41.58 | Max Reward: 46.96
Iteration: 789 | Episodes: 32000 | Median Reward: 44.56 | Avg Reward: 43.76 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 876          |
|    iterations           | 790          |
|    time_elapsed         | 3692         |
|    total_timesteps      | 3235840      |
| train/                  |              |
|    approx_kl            | 0.0015463785 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -273         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -54.6        |
|    n_updates            | 7890         |
|    policy_gradient_loss | -0.00179     |
|    std                  | 1.68         |
|    value_loss           | 0.36         |
------------------------------------------
Iteration: 791 | Episodes: 32100 | Median Reward: 43.08 | Avg Reward: 42.92 | Max Reward: 46.96
Iteration: 793 | Episodes: 32200 | Median Reward: 42.52 | Avg Reward: 42.42 | Max Reward: 46.96
Iteration: 796 | Episodes: 32300 | Median Reward: 43.40 | Avg Reward: 43.15 | Max Reward: 46.96
Iteration: 798 | Episodes: 32400 | Median Reward: 44.57 | Avg Reward: 44.19 | Max Reward: 46.96
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.8         |
| time/                   |               |
|    fps                  | 876           |
|    iterations           | 800           |
|    time_elapsed         | 3737          |
|    total_timesteps      | 3276800       |
| train/                  |               |
|    approx_kl            | 0.00023322225 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -274          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -53.1         |
|    n_updates            | 7990          |
|    policy_gradient_loss | -0.000618     |
|    std                  | 1.7           |
|    value_loss           | 0.888         |
-------------------------------------------
Iteration: 801 | Episodes: 32500 | Median Reward: 41.91 | Avg Reward: 42.26 | Max Reward: 46.96
Iteration: 803 | Episodes: 32600 | Median Reward: 43.93 | Avg Reward: 43.66 | Max Reward: 46.96
Iteration: 806 | Episodes: 32700 | Median Reward: 43.90 | Avg Reward: 43.20 | Max Reward: 46.96
Iteration: 808 | Episodes: 32800 | Median Reward: 44.49 | Avg Reward: 43.77 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.6        |
| time/                   |              |
|    fps                  | 876          |
|    iterations           | 810          |
|    time_elapsed         | 3783         |
|    total_timesteps      | 3317760      |
| train/                  |              |
|    approx_kl            | 0.0010532831 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -275         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -54.7        |
|    n_updates            | 8090         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 1.71         |
|    value_loss           | 0.901        |
------------------------------------------
Iteration: 811 | Episodes: 32900 | Median Reward: 44.42 | Avg Reward: 42.65 | Max Reward: 46.96
Iteration: 813 | Episodes: 33000 | Median Reward: 44.19 | Avg Reward: 43.44 | Max Reward: 46.96
Iteration: 816 | Episodes: 33100 | Median Reward: 42.24 | Avg Reward: 42.70 | Max Reward: 46.96
Iteration: 818 | Episodes: 33200 | Median Reward: 44.53 | Avg Reward: 43.76 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.1        |
| time/                   |              |
|    fps                  | 877          |
|    iterations           | 820          |
|    time_elapsed         | 3828         |
|    total_timesteps      | 3358720      |
| train/                  |              |
|    approx_kl            | 0.0014575843 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -275         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -54.9        |
|    n_updates            | 8190         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 1.72         |
|    value_loss           | 0.397        |
------------------------------------------
Iteration: 821 | Episodes: 33300 | Median Reward: 44.02 | Avg Reward: 43.48 | Max Reward: 46.96
Iteration: 823 | Episodes: 33400 | Median Reward: 44.45 | Avg Reward: 44.05 | Max Reward: 46.96
Iteration: 826 | Episodes: 33500 | Median Reward: 42.50 | Avg Reward: 42.49 | Max Reward: 46.96
Iteration: 828 | Episodes: 33600 | Median Reward: 42.77 | Avg Reward: 42.51 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 877          |
|    iterations           | 830          |
|    time_elapsed         | 3873         |
|    total_timesteps      | 3399680      |
| train/                  |              |
|    approx_kl            | 0.0016553898 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -276         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -55          |
|    n_updates            | 8290         |
|    policy_gradient_loss | -0.00402     |
|    std                  | 1.74         |
|    value_loss           | 0.975        |
------------------------------------------
Iteration: 830 | Episodes: 33700 | Median Reward: 44.42 | Avg Reward: 44.27 | Max Reward: 46.96
Iteration: 833 | Episodes: 33800 | Median Reward: 43.30 | Avg Reward: 43.09 | Max Reward: 46.96
Iteration: 835 | Episodes: 33900 | Median Reward: 42.49 | Avg Reward: 41.98 | Max Reward: 46.96
Iteration: 838 | Episodes: 34000 | Median Reward: 44.15 | Avg Reward: 42.13 | Max Reward: 46.96
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.8         |
| time/                   |               |
|    fps                  | 877           |
|    iterations           | 840           |
|    time_elapsed         | 3920          |
|    total_timesteps      | 3440640       |
| train/                  |               |
|    approx_kl            | 0.00030991796 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -276          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -54.9         |
|    n_updates            | 8390          |
|    policy_gradient_loss | -0.000319     |
|    std                  | 1.76          |
|    value_loss           | 0.409         |
-------------------------------------------
Iteration: 840 | Episodes: 34100 | Median Reward: 44.58 | Avg Reward: 43.23 | Max Reward: 46.96
Iteration: 843 | Episodes: 34200 | Median Reward: 43.33 | Avg Reward: 43.03 | Max Reward: 46.96
Iteration: 845 | Episodes: 34300 | Median Reward: 44.30 | Avg Reward: 43.88 | Max Reward: 46.96
Iteration: 848 | Episodes: 34400 | Median Reward: 42.13 | Avg Reward: 42.12 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 877          |
|    iterations           | 850          |
|    time_elapsed         | 3965         |
|    total_timesteps      | 3481600      |
| train/                  |              |
|    approx_kl            | 0.0007740993 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -277         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -54.8        |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.00449     |
|    std                  | 1.78         |
|    value_loss           | 1.62         |
------------------------------------------
Iteration: 850 | Episodes: 34500 | Median Reward: 42.92 | Avg Reward: 42.67 | Max Reward: 46.96
Iteration: 853 | Episodes: 34600 | Median Reward: 44.25 | Avg Reward: 43.19 | Max Reward: 46.96
Iteration: 855 | Episodes: 34700 | Median Reward: 44.51 | Avg Reward: 42.56 | Max Reward: 46.96
Iteration: 858 | Episodes: 34800 | Median Reward: 43.64 | Avg Reward: 42.88 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 878         |
|    iterations           | 860         |
|    time_elapsed         | 4011        |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.001223772 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -278        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -55.2       |
|    n_updates            | 8590        |
|    policy_gradient_loss | -0.000852   |
|    std                  | 1.79        |
|    value_loss           | 0.785       |
-----------------------------------------
Iteration: 860 | Episodes: 34900 | Median Reward: 43.09 | Avg Reward: 42.76 | Max Reward: 46.96
Iteration: 863 | Episodes: 35000 | Median Reward: 44.25 | Avg Reward: 43.27 | Max Reward: 46.96
Iteration: 865 | Episodes: 35100 | Median Reward: 45.01 | Avg Reward: 43.82 | Max Reward: 46.96
Iteration: 867 | Episodes: 35200 | Median Reward: 44.43 | Avg Reward: 43.21 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.3       |
| time/                   |             |
|    fps                  | 878         |
|    iterations           | 870         |
|    time_elapsed         | 4057        |
|    total_timesteps      | 3563520     |
| train/                  |             |
|    approx_kl            | 0.001606661 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -278        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -55.5       |
|    n_updates            | 8690        |
|    policy_gradient_loss | -0.00214    |
|    std                  | 1.81        |
|    value_loss           | 0.91        |
-----------------------------------------
Iteration: 870 | Episodes: 35300 | Median Reward: 42.43 | Avg Reward: 42.75 | Max Reward: 46.96
Iteration: 872 | Episodes: 35400 | Median Reward: 44.48 | Avg Reward: 43.74 | Max Reward: 46.96
Iteration: 875 | Episodes: 35500 | Median Reward: 42.75 | Avg Reward: 42.64 | Max Reward: 46.96
Iteration: 877 | Episodes: 35600 | Median Reward: 44.25 | Avg Reward: 43.75 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.3       |
| time/                   |             |
|    fps                  | 878         |
|    iterations           | 880         |
|    time_elapsed         | 4102        |
|    total_timesteps      | 3604480     |
| train/                  |             |
|    approx_kl            | 0.043984573 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.3         |
|    entropy_loss         | -278        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -55.6       |
|    n_updates            | 8790        |
|    policy_gradient_loss | -0.0387     |
|    std                  | 1.83        |
|    value_loss           | 0.6         |
-----------------------------------------
Iteration: 880 | Episodes: 35700 | Median Reward: 41.55 | Avg Reward: 40.35 | Max Reward: 46.96
Iteration: 882 | Episodes: 35800 | Median Reward: 44.01 | Avg Reward: 42.97 | Max Reward: 46.96
Iteration: 885 | Episodes: 35900 | Median Reward: 42.78 | Avg Reward: 42.43 | Max Reward: 46.96
Iteration: 887 | Episodes: 36000 | Median Reward: 43.35 | Avg Reward: 41.97 | Max Reward: 46.96
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -57.6    |
| time/                   |          |
|    fps                  | 878      |
|    iterations           | 890      |
|    time_elapsed         | 4148     |
|    total_timesteps      | 3645440  |
| train/                  |          |
|    approx_kl            | 0.037054 |
|    clip_fraction        | 0.101    |
|    clip_range           | 0.3      |
|    entropy_loss         | -280     |
|    explained_variance   | 1        |
|    learning_rate        | 0.0005   |
|    loss                 | -55.8    |
|    n_updates            | 8890     |
|    policy_gradient_loss | -0.0503  |
|    std                  | 1.85     |
|    value_loss           | 0.578    |
--------------------------------------
Iteration: 890 | Episodes: 36100 | Median Reward: 42.03 | Avg Reward: 42.46 | Max Reward: 46.96
Iteration: 892 | Episodes: 36200 | Median Reward: 43.26 | Avg Reward: 43.42 | Max Reward: 46.96
Iteration: 895 | Episodes: 36300 | Median Reward: 42.66 | Avg Reward: 42.76 | Max Reward: 46.96
Iteration: 897 | Episodes: 36400 | Median Reward: 42.55 | Avg Reward: 42.20 | Max Reward: 46.96
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.4         |
| time/                   |               |
|    fps                  | 878           |
|    iterations           | 900           |
|    time_elapsed         | 4194          |
|    total_timesteps      | 3686400       |
| train/                  |               |
|    approx_kl            | 0.00024989014 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -280          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -56           |
|    n_updates            | 8990          |
|    policy_gradient_loss | -0.00142      |
|    std                  | 1.86          |
|    value_loss           | 0.714         |
-------------------------------------------
Iteration: 900 | Episodes: 36500 | Median Reward: 42.55 | Avg Reward: 42.65 | Max Reward: 46.96
Iteration: 902 | Episodes: 36600 | Median Reward: 44.97 | Avg Reward: 44.59 | Max Reward: 46.96
Iteration: 904 | Episodes: 36700 | Median Reward: 43.35 | Avg Reward: 42.70 | Max Reward: 46.96
Iteration: 907 | Episodes: 36800 | Median Reward: 45.47 | Avg Reward: 44.58 | Max Reward: 46.96
Iteration: 909 | Episodes: 36900 | Median Reward: 42.04 | Avg Reward: 42.27 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.8       |
| time/                   |             |
|    fps                  | 879         |
|    iterations           | 910         |
|    time_elapsed         | 4239        |
|    total_timesteps      | 3727360     |
| train/                  |             |
|    approx_kl            | 0.004837188 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -281        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -54         |
|    n_updates            | 9090        |
|    policy_gradient_loss | -0.0107     |
|    std                  | 1.88        |
|    value_loss           | 1.79        |
-----------------------------------------
Iteration: 912 | Episodes: 37000 | Median Reward: 44.98 | Avg Reward: 43.35 | Max Reward: 46.96
Iteration: 914 | Episodes: 37100 | Median Reward: 44.93 | Avg Reward: 44.52 | Max Reward: 46.96
Iteration: 917 | Episodes: 37200 | Median Reward: 44.55 | Avg Reward: 43.07 | Max Reward: 46.96
Iteration: 919 | Episodes: 37300 | Median Reward: 43.70 | Avg Reward: 42.08 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.8       |
| time/                   |             |
|    fps                  | 879         |
|    iterations           | 920         |
|    time_elapsed         | 4285        |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.032733664 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.3         |
|    entropy_loss         | -282        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -56         |
|    n_updates            | 9190        |
|    policy_gradient_loss | -0.0327     |
|    std                  | 1.91        |
|    value_loss           | 0.761       |
-----------------------------------------
Iteration: 922 | Episodes: 37400 | Median Reward: 43.98 | Avg Reward: 43.25 | Max Reward: 46.96
Iteration: 924 | Episodes: 37500 | Median Reward: 44.32 | Avg Reward: 43.25 | Max Reward: 46.96
Iteration: 927 | Episodes: 37600 | Median Reward: 41.60 | Avg Reward: 42.23 | Max Reward: 46.96
Iteration: 929 | Episodes: 37700 | Median Reward: 44.48 | Avg Reward: 42.58 | Max Reward: 46.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.8       |
| time/                   |             |
|    fps                  | 879         |
|    iterations           | 930         |
|    time_elapsed         | 4330        |
|    total_timesteps      | 3809280     |
| train/                  |             |
|    approx_kl            | 0.001250016 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -283        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -55.7       |
|    n_updates            | 9290        |
|    policy_gradient_loss | -0.00403    |
|    std                  | 1.93        |
|    value_loss           | 0.825       |
-----------------------------------------
Iteration: 932 | Episodes: 37800 | Median Reward: 41.19 | Avg Reward: 42.11 | Max Reward: 46.96
Iteration: 934 | Episodes: 37900 | Median Reward: 44.57 | Avg Reward: 43.28 | Max Reward: 46.96
Iteration: 937 | Episodes: 38000 | Median Reward: 44.23 | Avg Reward: 42.71 | Max Reward: 46.96
Iteration: 939 | Episodes: 38100 | Median Reward: 43.49 | Avg Reward: 42.79 | Max Reward: 46.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 879          |
|    iterations           | 940          |
|    time_elapsed         | 4376         |
|    total_timesteps      | 3850240      |
| train/                  |              |
|    approx_kl            | 0.0007787424 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -284         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -56.7        |
|    n_updates            | 9390         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 1.95         |
|    value_loss           | 0.192        |
------------------------------------------
Iteration: 941 | Episodes: 38200 | Median Reward: 41.38 | Avg Reward: 41.81 | Max Reward: 46.96
Iteration: 944 | Episodes: 38300 | Median Reward: 44.62 | Avg Reward: 43.53 | Max Reward: 46.96
Iteration: 946 | Episodes: 38400 | Median Reward: 44.19 | Avg Reward: 43.24 | Max Reward: 46.96
Iteration: 949 | Episodes: 38500 | Median Reward: 44.56 | Avg Reward: 44.07 | Max Reward: 46.96
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.9         |
| time/                   |               |
|    fps                  | 879           |
|    iterations           | 950           |
|    time_elapsed         | 4422          |
|    total_timesteps      | 3891200       |
| train/                  |               |
|    approx_kl            | 0.00017810389 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -285          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -56.7         |
|    n_updates            | 9490          |
|    policy_gradient_loss | 0.000514      |
|    std                  | 1.97          |
|    value_loss           | 0.496         |
-------------------------------------------
Iteration: 951 | Episodes: 38600 | Median Reward: 43.80 | Avg Reward: 43.89 | Max Reward: 46.96
Iteration: 954 | Episodes: 38700 | Median Reward: 45.01 | Avg Reward: 43.84 | Max Reward: 46.96
Iteration: 956 | Episodes: 38800 | Median Reward: 44.61 | Avg Reward: 44.21 | Max Reward: 46.96
Iteration: 959 | Episodes: 38900 | Median Reward: 44.03 | Avg Reward: 43.64 | Max Reward: 47.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 880          |
|    iterations           | 960          |
|    time_elapsed         | 4467         |
|    total_timesteps      | 3932160      |
| train/                  |              |
|    approx_kl            | 0.0035964008 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -285         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -56.3        |
|    n_updates            | 9590         |
|    policy_gradient_loss | -0.00492     |
|    std                  | 1.99         |
|    value_loss           | 0.886        |
------------------------------------------
Iteration: 961 | Episodes: 39000 | Median Reward: 42.43 | Avg Reward: 43.01 | Max Reward: 47.16
Iteration: 964 | Episodes: 39100 | Median Reward: 44.10 | Avg Reward: 42.59 | Max Reward: 47.16
Iteration: 966 | Episodes: 39200 | Median Reward: 42.28 | Avg Reward: 42.02 | Max Reward: 47.16
Iteration: 969 | Episodes: 39300 | Median Reward: 44.18 | Avg Reward: 43.56 | Max Reward: 47.16
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.3         |
| time/                   |               |
|    fps                  | 880           |
|    iterations           | 970           |
|    time_elapsed         | 4512          |
|    total_timesteps      | 3973120       |
| train/                  |               |
|    approx_kl            | 1.6580641e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -286          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -57.1         |
|    n_updates            | 9690          |
|    policy_gradient_loss | 4.51e-05      |
|    std                  | 2             |
|    value_loss           | 1.9           |
-------------------------------------------
Iteration: 971 | Episodes: 39400 | Median Reward: 42.47 | Avg Reward: 42.78 | Max Reward: 47.16
Iteration: 973 | Episodes: 39500 | Median Reward: 44.75 | Avg Reward: 43.12 | Max Reward: 47.16
Iteration: 976 | Episodes: 39600 | Median Reward: 42.77 | Avg Reward: 41.54 | Max Reward: 47.16
Iteration: 978 | Episodes: 39700 | Median Reward: 42.98 | Avg Reward: 43.39 | Max Reward: 47.16
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.3         |
| time/                   |               |
|    fps                  | 880           |
|    iterations           | 980           |
|    time_elapsed         | 4558          |
|    total_timesteps      | 4014080       |
| train/                  |               |
|    approx_kl            | 0.00063322694 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -286          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -57.2         |
|    n_updates            | 9790          |
|    policy_gradient_loss | -0.00163      |
|    std                  | 2.02          |
|    value_loss           | 0.418         |
-------------------------------------------
Iteration: 981 | Episodes: 39800 | Median Reward: 44.77 | Avg Reward: 43.92 | Max Reward: 47.16
Iteration: 983 | Episodes: 39900 | Median Reward: 44.25 | Avg Reward: 41.48 | Max Reward: 47.16
Iteration: 986 | Episodes: 40000 | Median Reward: 41.84 | Avg Reward: 41.56 | Max Reward: 47.16
Iteration: 988 | Episodes: 40100 | Median Reward: 44.48 | Avg Reward: 43.84 | Max Reward: 47.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.3        |
| time/                   |              |
|    fps                  | 880          |
|    iterations           | 990          |
|    time_elapsed         | 4603         |
|    total_timesteps      | 4055040      |
| train/                  |              |
|    approx_kl            | 2.670569e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -287         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -56          |
|    n_updates            | 9890         |
|    policy_gradient_loss | -0.000193    |
|    std                  | 2.03         |
|    value_loss           | 1.81         |
------------------------------------------
Iteration: 991 | Episodes: 40200 | Median Reward: 41.28 | Avg Reward: 41.45 | Max Reward: 47.16
Iteration: 993 | Episodes: 40300 | Median Reward: 39.31 | Avg Reward: 40.81 | Max Reward: 47.16
Iteration: 996 | Episodes: 40400 | Median Reward: 44.28 | Avg Reward: 42.69 | Max Reward: 47.16
Iteration: 998 | Episodes: 40500 | Median Reward: 41.08 | Avg Reward: 42.22 | Max Reward: 47.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.1        |
| time/                   |              |
|    fps                  | 881          |
|    iterations           | 1000         |
|    time_elapsed         | 4648         |
|    total_timesteps      | 4096000      |
| train/                  |              |
|    approx_kl            | 0.0018248805 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -288         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -57          |
|    n_updates            | 9990         |
|    policy_gradient_loss | -0.00227     |
|    std                  | 2.05         |
|    value_loss           | 0.499        |
------------------------------------------
Iteration: 1001 | Episodes: 40600 | Median Reward: 45.30 | Avg Reward: 44.35 | Max Reward: 47.16
Iteration: 1003 | Episodes: 40700 | Median Reward: 45.14 | Avg Reward: 42.88 | Max Reward: 47.16
Iteration: 1006 | Episodes: 40800 | Median Reward: 44.25 | Avg Reward: 42.95 | Max Reward: 47.16
Iteration: 1008 | Episodes: 40900 | Median Reward: 44.42 | Avg Reward: 43.70 | Max Reward: 47.16
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.4         |
| time/                   |               |
|    fps                  | 881           |
|    iterations           | 1010          |
|    time_elapsed         | 4693          |
|    total_timesteps      | 4136960       |
| train/                  |               |
|    approx_kl            | 2.2392996e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -289          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -56.5         |
|    n_updates            | 10090         |
|    policy_gradient_loss | -3.21e-05     |
|    std                  | 2.07          |
|    value_loss           | 1.19          |
-------------------------------------------
Iteration: 1010 | Episodes: 41000 | Median Reward: 43.39 | Avg Reward: 42.61 | Max Reward: 47.16
Iteration: 1013 | Episodes: 41100 | Median Reward: 44.46 | Avg Reward: 43.63 | Max Reward: 47.16
Iteration: 1015 | Episodes: 41200 | Median Reward: 43.42 | Avg Reward: 42.10 | Max Reward: 47.16
Iteration: 1018 | Episodes: 41300 | Median Reward: 44.52 | Avg Reward: 43.63 | Max Reward: 47.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 881          |
|    iterations           | 1020         |
|    time_elapsed         | 4738         |
|    total_timesteps      | 4177920      |
| train/                  |              |
|    approx_kl            | 0.0010525133 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -289         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -57.5        |
|    n_updates            | 10190        |
|    policy_gradient_loss | -0.0028      |
|    std                  | 2.09         |
|    value_loss           | 0.364        |
------------------------------------------
Iteration: 1020 | Episodes: 41400 | Median Reward: 42.20 | Avg Reward: 41.89 | Max Reward: 47.16
Iteration: 1023 | Episodes: 41500 | Median Reward: 42.12 | Avg Reward: 42.57 | Max Reward: 47.16
Iteration: 1025 | Episodes: 41600 | Median Reward: 42.37 | Avg Reward: 43.00 | Max Reward: 47.16
Iteration: 1028 | Episodes: 41700 | Median Reward: 44.53 | Avg Reward: 43.75 | Max Reward: 47.16
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.9         |
| time/                   |               |
|    fps                  | 881           |
|    iterations           | 1030          |
|    time_elapsed         | 4784          |
|    total_timesteps      | 4218880       |
| train/                  |               |
|    approx_kl            | 0.00030953935 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -290          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -57.9         |
|    n_updates            | 10290         |
|    policy_gradient_loss | -0.000767     |
|    std                  | 2.12          |
|    value_loss           | 0.652         |
-------------------------------------------
Iteration: 1030 | Episodes: 41800 | Median Reward: 41.84 | Avg Reward: 41.45 | Max Reward: 47.16
Iteration: 1033 | Episodes: 41900 | Median Reward: 45.40 | Avg Reward: 41.92 | Max Reward: 47.16
Iteration: 1035 | Episodes: 42000 | Median Reward: 43.01 | Avg Reward: 42.84 | Max Reward: 47.16
Iteration: 1038 | Episodes: 42100 | Median Reward: 42.39 | Avg Reward: 42.21 | Max Reward: 47.16
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.8         |
| time/                   |               |
|    fps                  | 882           |
|    iterations           | 1040          |
|    time_elapsed         | 4828          |
|    total_timesteps      | 4259840       |
| train/                  |               |
|    approx_kl            | 0.00085878593 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -290          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -57.8         |
|    n_updates            | 10390         |
|    policy_gradient_loss | -0.00212      |
|    std                  | 2.13          |
|    value_loss           | 0.294         |
-------------------------------------------
Iteration: 1040 | Episodes: 42200 | Median Reward: 41.93 | Avg Reward: 42.60 | Max Reward: 47.16
Iteration: 1043 | Episodes: 42300 | Median Reward: 42.41 | Avg Reward: 41.47 | Max Reward: 47.16
Iteration: 1045 | Episodes: 42400 | Median Reward: 42.71 | Avg Reward: 43.21 | Max Reward: 47.16
Iteration: 1047 | Episodes: 42500 | Median Reward: 44.61 | Avg Reward: 44.08 | Max Reward: 47.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 882          |
|    iterations           | 1050         |
|    time_elapsed         | 4873         |
|    total_timesteps      | 4300800      |
| train/                  |              |
|    approx_kl            | 0.0040036635 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -291         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -57.9        |
|    n_updates            | 10490        |
|    policy_gradient_loss | -0.0191      |
|    std                  | 2.15         |
|    value_loss           | 0.8          |
------------------------------------------
Iteration: 1050 | Episodes: 42600 | Median Reward: 43.01 | Avg Reward: 42.14 | Max Reward: 47.16
Iteration: 1052 | Episodes: 42700 | Median Reward: 44.54 | Avg Reward: 42.06 | Max Reward: 47.16
Iteration: 1055 | Episodes: 42800 | Median Reward: 44.23 | Avg Reward: 43.61 | Max Reward: 47.16
Iteration: 1057 | Episodes: 42900 | Median Reward: 43.70 | Avg Reward: 43.05 | Max Reward: 47.16
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59           |
| time/                   |               |
|    fps                  | 882           |
|    iterations           | 1060          |
|    time_elapsed         | 4919          |
|    total_timesteps      | 4341760       |
| train/                  |               |
|    approx_kl            | 0.00027847686 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -291          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -57.7         |
|    n_updates            | 10590         |
|    policy_gradient_loss | -0.00162      |
|    std                  | 2.18          |
|    value_loss           | 1.75          |
-------------------------------------------
Iteration: 1060 | Episodes: 43000 | Median Reward: 41.74 | Avg Reward: 41.10 | Max Reward: 47.16
Iteration: 1062 | Episodes: 43100 | Median Reward: 40.47 | Avg Reward: 40.51 | Max Reward: 47.16
Iteration: 1065 | Episodes: 43200 | Median Reward: 42.47 | Avg Reward: 42.25 | Max Reward: 47.16
Iteration: 1067 | Episodes: 43300 | Median Reward: 44.41 | Avg Reward: 43.20 | Max Reward: 47.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 882          |
|    iterations           | 1070         |
|    time_elapsed         | 4964         |
|    total_timesteps      | 4382720      |
| train/                  |              |
|    approx_kl            | 0.0002748667 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -292         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -56.4        |
|    n_updates            | 10690        |
|    policy_gradient_loss | -0.0005      |
|    std                  | 2.19         |
|    value_loss           | 1.54         |
------------------------------------------
Iteration: 1070 | Episodes: 43400 | Median Reward: 44.79 | Avg Reward: 42.87 | Max Reward: 47.16
Iteration: 1072 | Episodes: 43500 | Median Reward: 42.80 | Avg Reward: 41.88 | Max Reward: 47.16
Iteration: 1075 | Episodes: 43600 | Median Reward: 44.40 | Avg Reward: 43.36 | Max Reward: 47.16
Iteration: 1077 | Episodes: 43700 | Median Reward: 43.25 | Avg Reward: 43.10 | Max Reward: 47.16
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.4         |
| time/                   |               |
|    fps                  | 882           |
|    iterations           | 1080          |
|    time_elapsed         | 5010          |
|    total_timesteps      | 4423680       |
| train/                  |               |
|    approx_kl            | 0.00044423822 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -292          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -58.1         |
|    n_updates            | 10790         |
|    policy_gradient_loss | -0.00196      |
|    std                  | 2.2           |
|    value_loss           | 1.16          |
-------------------------------------------
Iteration: 1080 | Episodes: 43800 | Median Reward: 43.69 | Avg Reward: 43.55 | Max Reward: 47.16
Iteration: 1082 | Episodes: 43900 | Median Reward: 42.69 | Avg Reward: 42.33 | Max Reward: 47.16
Iteration: 1084 | Episodes: 44000 | Median Reward: 43.25 | Avg Reward: 40.42 | Max Reward: 47.16
Iteration: 1087 | Episodes: 44100 | Median Reward: 42.58 | Avg Reward: 42.59 | Max Reward: 47.16
Iteration: 1089 | Episodes: 44200 | Median Reward: 42.25 | Avg Reward: 40.78 | Max Reward: 47.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.3       |
| time/                   |             |
|    fps                  | 883         |
|    iterations           | 1090        |
|    time_elapsed         | 5055        |
|    total_timesteps      | 4464640     |
| train/                  |             |
|    approx_kl            | 0.011895509 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -293        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -58.3       |
|    n_updates            | 10890       |
|    policy_gradient_loss | -0.00571    |
|    std                  | 2.22        |
|    value_loss           | 0.809       |
-----------------------------------------
Iteration: 1092 | Episodes: 44300 | Median Reward: 41.92 | Avg Reward: 41.58 | Max Reward: 47.16
Iteration: 1094 | Episodes: 44400 | Median Reward: 44.49 | Avg Reward: 43.46 | Max Reward: 47.16
Iteration: 1097 | Episodes: 44500 | Median Reward: 42.19 | Avg Reward: 42.15 | Max Reward: 47.20
Iteration: 1099 | Episodes: 44600 | Median Reward: 42.54 | Avg Reward: 42.74 | Max Reward: 47.20
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.3         |
| time/                   |               |
|    fps                  | 883           |
|    iterations           | 1100          |
|    time_elapsed         | 5101          |
|    total_timesteps      | 4505600       |
| train/                  |               |
|    approx_kl            | 0.00050928834 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -294          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -58.7         |
|    n_updates            | 10990         |
|    policy_gradient_loss | -0.00164      |
|    std                  | 2.25          |
|    value_loss           | 1.02          |
-------------------------------------------
Iteration: 1102 | Episodes: 44700 | Median Reward: 42.11 | Avg Reward: 41.58 | Max Reward: 47.20
Iteration: 1104 | Episodes: 44800 | Median Reward: 42.44 | Avg Reward: 42.34 | Max Reward: 47.20
Iteration: 1107 | Episodes: 44900 | Median Reward: 43.99 | Avg Reward: 42.25 | Max Reward: 47.20
Iteration: 1109 | Episodes: 45000 | Median Reward: 44.54 | Avg Reward: 43.20 | Max Reward: 47.20
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.1         |
| time/                   |               |
|    fps                  | 883           |
|    iterations           | 1110          |
|    time_elapsed         | 5147          |
|    total_timesteps      | 4546560       |
| train/                  |               |
|    approx_kl            | 0.00040336454 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -295          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -58.5         |
|    n_updates            | 11090         |
|    policy_gradient_loss | -0.00148      |
|    std                  | 2.27          |
|    value_loss           | 0.791         |
-------------------------------------------
Iteration: 1112 | Episodes: 45100 | Median Reward: 42.14 | Avg Reward: 42.14 | Max Reward: 47.20
Iteration: 1114 | Episodes: 45200 | Median Reward: 42.50 | Avg Reward: 41.84 | Max Reward: 47.20
Iteration: 1117 | Episodes: 45300 | Median Reward: 41.13 | Avg Reward: 41.31 | Max Reward: 47.20
Iteration: 1119 | Episodes: 45400 | Median Reward: 41.53 | Avg Reward: 41.72 | Max Reward: 47.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.9        |
| time/                   |              |
|    fps                  | 883          |
|    iterations           | 1120         |
|    time_elapsed         | 5192         |
|    total_timesteps      | 4587520      |
| train/                  |              |
|    approx_kl            | 0.0006098891 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -295         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -58.9        |
|    n_updates            | 11190        |
|    policy_gradient_loss | 0.00108      |
|    std                  | 2.29         |
|    value_loss           | 0.912        |
------------------------------------------
Iteration: 1121 | Episodes: 45500 | Median Reward: 43.70 | Avg Reward: 42.89 | Max Reward: 47.20
Iteration: 1124 | Episodes: 45600 | Median Reward: 42.90 | Avg Reward: 42.30 | Max Reward: 47.20
Iteration: 1126 | Episodes: 45700 | Median Reward: 43.38 | Avg Reward: 42.11 | Max Reward: 47.20
Iteration: 1129 | Episodes: 45800 | Median Reward: 42.79 | Avg Reward: 42.26 | Max Reward: 47.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.5        |
| time/                   |              |
|    fps                  | 883          |
|    iterations           | 1130         |
|    time_elapsed         | 5238         |
|    total_timesteps      | 4628480      |
| train/                  |              |
|    approx_kl            | 0.0003714054 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -296         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -58.8        |
|    n_updates            | 11290        |
|    policy_gradient_loss | -0.000643    |
|    std                  | 2.31         |
|    value_loss           | 1.54         |
------------------------------------------
Iteration: 1131 | Episodes: 45900 | Median Reward: 44.29 | Avg Reward: 43.49 | Max Reward: 47.20
Iteration: 1134 | Episodes: 46000 | Median Reward: 42.08 | Avg Reward: 42.57 | Max Reward: 47.20
Iteration: 1136 | Episodes: 46100 | Median Reward: 40.49 | Avg Reward: 40.72 | Max Reward: 47.20
Iteration: 1139 | Episodes: 46200 | Median Reward: 44.53 | Avg Reward: 43.20 | Max Reward: 47.20
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 883           |
|    iterations           | 1140          |
|    time_elapsed         | 5283          |
|    total_timesteps      | 4669440       |
| train/                  |               |
|    approx_kl            | 0.00044595415 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -296          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -59.1         |
|    n_updates            | 11390         |
|    policy_gradient_loss | -0.000653     |
|    std                  | 2.34          |
|    value_loss           | 0.646         |
-------------------------------------------
Iteration: 1141 | Episodes: 46300 | Median Reward: 42.63 | Avg Reward: 42.04 | Max Reward: 47.20
Iteration: 1144 | Episodes: 46400 | Median Reward: 41.37 | Avg Reward: 40.70 | Max Reward: 47.20
Iteration: 1146 | Episodes: 46500 | Median Reward: 41.26 | Avg Reward: 41.48 | Max Reward: 47.20
Iteration: 1149 | Episodes: 46600 | Median Reward: 42.35 | Avg Reward: 42.79 | Max Reward: 47.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.8       |
| time/                   |             |
|    fps                  | 883         |
|    iterations           | 1150        |
|    time_elapsed         | 5328        |
|    total_timesteps      | 4710400     |
| train/                  |             |
|    approx_kl            | 0.007214597 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -297        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -57.7       |
|    n_updates            | 11490       |
|    policy_gradient_loss | -0.0115     |
|    std                  | 2.36        |
|    value_loss           | 1.8         |
-----------------------------------------
Iteration: 1151 | Episodes: 46700 | Median Reward: 44.93 | Avg Reward: 43.40 | Max Reward: 47.20
Iteration: 1154 | Episodes: 46800 | Median Reward: 44.76 | Avg Reward: 43.52 | Max Reward: 47.20
Iteration: 1156 | Episodes: 46900 | Median Reward: 42.64 | Avg Reward: 43.26 | Max Reward: 47.20
Iteration: 1158 | Episodes: 47000 | Median Reward: 42.98 | Avg Reward: 42.97 | Max Reward: 47.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 884          |
|    iterations           | 1160         |
|    time_elapsed         | 5374         |
|    total_timesteps      | 4751360      |
| train/                  |              |
|    approx_kl            | 3.491195e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -297         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -59.1        |
|    n_updates            | 11590        |
|    policy_gradient_loss | -0.0004      |
|    std                  | 2.38         |
|    value_loss           | 1.11         |
------------------------------------------
Iteration: 1161 | Episodes: 47100 | Median Reward: 43.75 | Avg Reward: 42.01 | Max Reward: 47.62
Iteration: 1163 | Episodes: 47200 | Median Reward: 42.25 | Avg Reward: 43.52 | Max Reward: 47.62
Iteration: 1166 | Episodes: 47300 | Median Reward: 40.78 | Avg Reward: 40.06 | Max Reward: 47.62
Iteration: 1168 | Episodes: 47400 | Median Reward: 40.29 | Avg Reward: 41.07 | Max Reward: 47.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.3       |
| time/                   |             |
|    fps                  | 884         |
|    iterations           | 1170        |
|    time_elapsed         | 5420        |
|    total_timesteps      | 4792320     |
| train/                  |             |
|    approx_kl            | 0.002437231 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -298        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -59.4       |
|    n_updates            | 11690       |
|    policy_gradient_loss | -0.00481    |
|    std                  | 2.41        |
|    value_loss           | 0.646       |
-----------------------------------------
Iteration: 1171 | Episodes: 47500 | Median Reward: 43.98 | Avg Reward: 43.50 | Max Reward: 47.62
Iteration: 1173 | Episodes: 47600 | Median Reward: 44.46 | Avg Reward: 43.01 | Max Reward: 47.62
Iteration: 1176 | Episodes: 47700 | Median Reward: 44.61 | Avg Reward: 42.43 | Max Reward: 47.62
Iteration: 1178 | Episodes: 47800 | Median Reward: 44.48 | Avg Reward: 42.08 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 884           |
|    iterations           | 1180          |
|    time_elapsed         | 5465          |
|    total_timesteps      | 4833280       |
| train/                  |               |
|    approx_kl            | 0.00060811493 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -299          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -59.7         |
|    n_updates            | 11790         |
|    policy_gradient_loss | -0.000242     |
|    std                  | 2.43          |
|    value_loss           | 0.429         |
-------------------------------------------
Iteration: 1181 | Episodes: 47900 | Median Reward: 44.51 | Avg Reward: 42.55 | Max Reward: 47.62
Iteration: 1183 | Episodes: 48000 | Median Reward: 41.48 | Avg Reward: 41.91 | Max Reward: 47.62
Iteration: 1186 | Episodes: 48100 | Median Reward: 42.42 | Avg Reward: 42.76 | Max Reward: 47.62
Iteration: 1188 | Episodes: 48200 | Median Reward: 44.91 | Avg Reward: 42.96 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.2         |
| time/                   |               |
|    fps                  | 884           |
|    iterations           | 1190          |
|    time_elapsed         | 5510          |
|    total_timesteps      | 4874240       |
| train/                  |               |
|    approx_kl            | 0.00012979945 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -300          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -59.6         |
|    n_updates            | 11890         |
|    policy_gradient_loss | -0.000574     |
|    std                  | 2.46          |
|    value_loss           | 0.642         |
-------------------------------------------
Iteration: 1190 | Episodes: 48300 | Median Reward: 43.30 | Avg Reward: 42.74 | Max Reward: 47.62
Iteration: 1193 | Episodes: 48400 | Median Reward: 41.74 | Avg Reward: 42.11 | Max Reward: 47.62
Iteration: 1195 | Episodes: 48500 | Median Reward: 41.75 | Avg Reward: 41.22 | Max Reward: 47.62
Iteration: 1198 | Episodes: 48600 | Median Reward: 44.56 | Avg Reward: 43.06 | Max Reward: 47.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.4        |
| time/                   |              |
|    fps                  | 884          |
|    iterations           | 1200         |
|    time_elapsed         | 5556         |
|    total_timesteps      | 4915200      |
| train/                  |              |
|    approx_kl            | 0.0007537119 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -300         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -59.9        |
|    n_updates            | 11990        |
|    policy_gradient_loss | -0.00437     |
|    std                  | 2.49         |
|    value_loss           | 2.31         |
------------------------------------------
Iteration: 1200 | Episodes: 48700 | Median Reward: 41.40 | Avg Reward: 41.08 | Max Reward: 47.62
Iteration: 1203 | Episodes: 48800 | Median Reward: 44.75 | Avg Reward: 43.54 | Max Reward: 47.62
Iteration: 1205 | Episodes: 48900 | Median Reward: 44.53 | Avg Reward: 42.64 | Max Reward: 47.62
Iteration: 1208 | Episodes: 49000 | Median Reward: 41.63 | Avg Reward: 40.79 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.6         |
| time/                   |               |
|    fps                  | 884           |
|    iterations           | 1210          |
|    time_elapsed         | 5601          |
|    total_timesteps      | 4956160       |
| train/                  |               |
|    approx_kl            | 0.00018194132 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -301          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -60.1         |
|    n_updates            | 12090         |
|    policy_gradient_loss | -0.000169     |
|    std                  | 2.51          |
|    value_loss           | 0.599         |
-------------------------------------------
Iteration: 1210 | Episodes: 49100 | Median Reward: 43.23 | Avg Reward: 42.92 | Max Reward: 47.62
Iteration: 1213 | Episodes: 49200 | Median Reward: 42.96 | Avg Reward: 42.88 | Max Reward: 47.62
Iteration: 1215 | Episodes: 49300 | Median Reward: 42.23 | Avg Reward: 41.44 | Max Reward: 47.62
Iteration: 1218 | Episodes: 49400 | Median Reward: 41.63 | Avg Reward: 40.64 | Max Reward: 47.62
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -58.3      |
| time/                   |            |
|    fps                  | 884        |
|    iterations           | 1220       |
|    time_elapsed         | 5647       |
|    total_timesteps      | 4997120    |
| train/                  |            |
|    approx_kl            | 0.00618564 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -302       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -60.2      |
|    n_updates            | 12190      |
|    policy_gradient_loss | -0.00334   |
|    std                  | 2.55       |
|    value_loss           | 0.596      |
----------------------------------------
Iteration: 1220 | Episodes: 49500 | Median Reward: 42.55 | Avg Reward: 41.96 | Max Reward: 47.62
Iteration: 1223 | Episodes: 49600 | Median Reward: 42.53 | Avg Reward: 42.65 | Max Reward: 47.62
Iteration: 1225 | Episodes: 49700 | Median Reward: 41.62 | Avg Reward: 40.96 | Max Reward: 47.62
Iteration: 1227 | Episodes: 49800 | Median Reward: 42.73 | Avg Reward: 42.75 | Max Reward: 47.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58          |
| time/                   |              |
|    fps                  | 884          |
|    iterations           | 1230         |
|    time_elapsed         | 5692         |
|    total_timesteps      | 5038080      |
| train/                  |              |
|    approx_kl            | 0.0029170937 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -303         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -60.4        |
|    n_updates            | 12290        |
|    policy_gradient_loss | -0.00787     |
|    std                  | 2.58         |
|    value_loss           | 0.681        |
------------------------------------------
Iteration: 1230 | Episodes: 49900 | Median Reward: 42.55 | Avg Reward: 41.92 | Max Reward: 47.62
Iteration: 1232 | Episodes: 50000 | Median Reward: 44.58 | Avg Reward: 43.70 | Max Reward: 47.62
Iteration: 1235 | Episodes: 50100 | Median Reward: 42.39 | Avg Reward: 42.48 | Max Reward: 47.62
Iteration: 1237 | Episodes: 50200 | Median Reward: 42.40 | Avg Reward: 41.44 | Max Reward: 47.62
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -57.1          |
| time/                   |                |
|    fps                  | 885            |
|    iterations           | 1240           |
|    time_elapsed         | 5737           |
|    total_timesteps      | 5079040        |
| train/                  |                |
|    approx_kl            | 0.000113116286 |
|    clip_fraction        | 0              |
|    clip_range           | 0.3            |
|    entropy_loss         | -304           |
|    explained_variance   | 0.999          |
|    learning_rate        | 0.0005         |
|    loss                 | -60.6          |
|    n_updates            | 12390          |
|    policy_gradient_loss | -0.000566      |
|    std                  | 2.6            |
|    value_loss           | 0.752          |
--------------------------------------------
Iteration: 1240 | Episodes: 50300 | Median Reward: 42.46 | Avg Reward: 42.88 | Max Reward: 47.62
Iteration: 1242 | Episodes: 50400 | Median Reward: 44.58 | Avg Reward: 43.24 | Max Reward: 47.62
Iteration: 1245 | Episodes: 50500 | Median Reward: 43.74 | Avg Reward: 42.87 | Max Reward: 47.62
Iteration: 1247 | Episodes: 50600 | Median Reward: 41.67 | Avg Reward: 41.22 | Max Reward: 47.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.5        |
| time/                   |              |
|    fps                  | 885          |
|    iterations           | 1250         |
|    time_elapsed         | 5783         |
|    total_timesteps      | 5120000      |
| train/                  |              |
|    approx_kl            | 0.0007812896 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -305         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -60.8        |
|    n_updates            | 12490        |
|    policy_gradient_loss | -0.00364     |
|    std                  | 2.63         |
|    value_loss           | 0.461        |
------------------------------------------
Iteration: 1250 | Episodes: 50700 | Median Reward: 41.77 | Avg Reward: 41.72 | Max Reward: 47.62
Iteration: 1252 | Episodes: 50800 | Median Reward: 42.10 | Avg Reward: 41.59 | Max Reward: 47.62
Iteration: 1255 | Episodes: 50900 | Median Reward: 44.90 | Avg Reward: 43.04 | Max Reward: 47.62
Iteration: 1257 | Episodes: 51000 | Median Reward: 41.59 | Avg Reward: 41.43 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 885           |
|    iterations           | 1260          |
|    time_elapsed         | 5827          |
|    total_timesteps      | 5160960       |
| train/                  |               |
|    approx_kl            | 2.7809583e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -305          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -60.9         |
|    n_updates            | 12590         |
|    policy_gradient_loss | 0.000127      |
|    std                  | 2.65          |
|    value_loss           | 1.12          |
-------------------------------------------
Iteration: 1260 | Episodes: 51100 | Median Reward: 41.72 | Avg Reward: 42.60 | Max Reward: 47.62
Iteration: 1262 | Episodes: 51200 | Median Reward: 42.10 | Avg Reward: 41.82 | Max Reward: 47.62
Iteration: 1264 | Episodes: 51300 | Median Reward: 39.53 | Avg Reward: 39.20 | Max Reward: 47.62
Iteration: 1267 | Episodes: 51400 | Median Reward: 42.28 | Avg Reward: 40.63 | Max Reward: 47.62
Iteration: 1269 | Episodes: 51500 | Median Reward: 41.52 | Avg Reward: 40.82 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.1         |
| time/                   |               |
|    fps                  | 885           |
|    iterations           | 1270          |
|    time_elapsed         | 5872          |
|    total_timesteps      | 5201920       |
| train/                  |               |
|    approx_kl            | 6.0842984e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -306          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -60.8         |
|    n_updates            | 12690         |
|    policy_gradient_loss | -6.26e-06     |
|    std                  | 2.68          |
|    value_loss           | 1.64          |
-------------------------------------------
Iteration: 1272 | Episodes: 51600 | Median Reward: 44.54 | Avg Reward: 43.55 | Max Reward: 47.62
Iteration: 1274 | Episodes: 51700 | Median Reward: 44.23 | Avg Reward: 42.78 | Max Reward: 47.62
Iteration: 1277 | Episodes: 51800 | Median Reward: 42.50 | Avg Reward: 42.76 | Max Reward: 47.62
Iteration: 1279 | Episodes: 51900 | Median Reward: 42.12 | Avg Reward: 42.00 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.9         |
| time/                   |               |
|    fps                  | 885           |
|    iterations           | 1280          |
|    time_elapsed         | 5918          |
|    total_timesteps      | 5242880       |
| train/                  |               |
|    approx_kl            | 4.8056798e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -306          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -60.4         |
|    n_updates            | 12790         |
|    policy_gradient_loss | -0.000179     |
|    std                  | 2.7           |
|    value_loss           | 0.862         |
-------------------------------------------
Iteration: 1282 | Episodes: 52000 | Median Reward: 42.83 | Avg Reward: 43.00 | Max Reward: 47.62
Iteration: 1284 | Episodes: 52100 | Median Reward: 44.53 | Avg Reward: 42.53 | Max Reward: 47.62
Iteration: 1287 | Episodes: 52200 | Median Reward: 42.51 | Avg Reward: 43.00 | Max Reward: 47.62
Iteration: 1289 | Episodes: 52300 | Median Reward: 44.44 | Avg Reward: 42.91 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.4         |
| time/                   |               |
|    fps                  | 886           |
|    iterations           | 1290          |
|    time_elapsed         | 5963          |
|    total_timesteps      | 5283840       |
| train/                  |               |
|    approx_kl            | 5.0629504e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -307          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -60.3         |
|    n_updates            | 12890         |
|    policy_gradient_loss | -4.38e-05     |
|    std                  | 2.73          |
|    value_loss           | 1.05          |
-------------------------------------------
Iteration: 1292 | Episodes: 52400 | Median Reward: 43.33 | Avg Reward: 43.20 | Max Reward: 47.62
Iteration: 1294 | Episodes: 52500 | Median Reward: 44.19 | Avg Reward: 42.42 | Max Reward: 47.62
Iteration: 1297 | Episodes: 52600 | Median Reward: 41.39 | Avg Reward: 40.91 | Max Reward: 47.62
Iteration: 1299 | Episodes: 52700 | Median Reward: 42.66 | Avg Reward: 42.15 | Max Reward: 47.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.2       |
| time/                   |             |
|    fps                  | 886         |
|    iterations           | 1300        |
|    time_elapsed         | 6008        |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.002878022 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -308        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -61.4       |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.000503   |
|    std                  | 2.76        |
|    value_loss           | 0.524       |
-----------------------------------------
Iteration: 1301 | Episodes: 52800 | Median Reward: 43.91 | Avg Reward: 42.20 | Max Reward: 47.62
Iteration: 1304 | Episodes: 52900 | Median Reward: 40.15 | Avg Reward: 39.00 | Max Reward: 47.62
Iteration: 1306 | Episodes: 53000 | Median Reward: 43.96 | Avg Reward: 43.30 | Max Reward: 47.62
Iteration: 1309 | Episodes: 53100 | Median Reward: 44.59 | Avg Reward: 42.75 | Max Reward: 47.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58          |
| time/                   |              |
|    fps                  | 886          |
|    iterations           | 1310         |
|    time_elapsed         | 6054         |
|    total_timesteps      | 5365760      |
| train/                  |              |
|    approx_kl            | 4.546401e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -308         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -61.4        |
|    n_updates            | 13090        |
|    policy_gradient_loss | -8.79e-06    |
|    std                  | 2.79         |
|    value_loss           | 0.851        |
------------------------------------------
Iteration: 1311 | Episodes: 53200 | Median Reward: 44.82 | Avg Reward: 42.88 | Max Reward: 47.62
Iteration: 1314 | Episodes: 53300 | Median Reward: 42.29 | Avg Reward: 41.28 | Max Reward: 47.62
Iteration: 1316 | Episodes: 53400 | Median Reward: 43.66 | Avg Reward: 42.39 | Max Reward: 47.62
Iteration: 1319 | Episodes: 53500 | Median Reward: 41.67 | Avg Reward: 41.63 | Max Reward: 47.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.2        |
| time/                   |              |
|    fps                  | 886          |
|    iterations           | 1320         |
|    time_elapsed         | 6099         |
|    total_timesteps      | 5406720      |
| train/                  |              |
|    approx_kl            | 0.0004686877 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -309         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -61.5        |
|    n_updates            | 13190        |
|    policy_gradient_loss | -0.00104     |
|    std                  | 2.81         |
|    value_loss           | 0.612        |
------------------------------------------
Iteration: 1321 | Episodes: 53600 | Median Reward: 42.84 | Avg Reward: 41.31 | Max Reward: 47.62
Iteration: 1324 | Episodes: 53700 | Median Reward: 42.28 | Avg Reward: 42.22 | Max Reward: 47.62
Iteration: 1326 | Episodes: 53800 | Median Reward: 39.74 | Avg Reward: 41.14 | Max Reward: 47.62
Iteration: 1329 | Episodes: 53900 | Median Reward: 41.81 | Avg Reward: 42.18 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.9         |
| time/                   |               |
|    fps                  | 886           |
|    iterations           | 1330          |
|    time_elapsed         | 6144          |
|    total_timesteps      | 5447680       |
| train/                  |               |
|    approx_kl            | 0.00023531422 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -309          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -61.7         |
|    n_updates            | 13290         |
|    policy_gradient_loss | -0.0024       |
|    std                  | 2.83          |
|    value_loss           | 1.39          |
-------------------------------------------
Iteration: 1331 | Episodes: 54000 | Median Reward: 42.41 | Avg Reward: 42.28 | Max Reward: 47.62
Iteration: 1334 | Episodes: 54100 | Median Reward: 44.58 | Avg Reward: 42.55 | Max Reward: 47.62
Iteration: 1336 | Episodes: 54200 | Median Reward: 39.65 | Avg Reward: 40.07 | Max Reward: 47.62
Iteration: 1338 | Episodes: 54300 | Median Reward: 42.01 | Avg Reward: 41.58 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.6         |
| time/                   |               |
|    fps                  | 886           |
|    iterations           | 1340          |
|    time_elapsed         | 6188          |
|    total_timesteps      | 5488640       |
| train/                  |               |
|    approx_kl            | 0.00018330225 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -310          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -61.5         |
|    n_updates            | 13390         |
|    policy_gradient_loss | -0.00103      |
|    std                  | 2.86          |
|    value_loss           | 0.748         |
-------------------------------------------
Iteration: 1341 | Episodes: 54400 | Median Reward: 42.61 | Avg Reward: 41.60 | Max Reward: 47.62
Iteration: 1343 | Episodes: 54500 | Median Reward: 41.40 | Avg Reward: 41.07 | Max Reward: 47.62
Iteration: 1346 | Episodes: 54600 | Median Reward: 42.39 | Avg Reward: 42.11 | Max Reward: 47.62
Iteration: 1348 | Episodes: 54700 | Median Reward: 41.94 | Avg Reward: 41.73 | Max Reward: 47.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.2        |
| time/                   |              |
|    fps                  | 886          |
|    iterations           | 1350         |
|    time_elapsed         | 6234         |
|    total_timesteps      | 5529600      |
| train/                  |              |
|    approx_kl            | 0.0006401426 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -311         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -60.4        |
|    n_updates            | 13490        |
|    policy_gradient_loss | 0.00209      |
|    std                  | 2.89         |
|    value_loss           | 0.997        |
------------------------------------------
Iteration: 1351 | Episodes: 54800 | Median Reward: 43.98 | Avg Reward: 43.01 | Max Reward: 47.62
Iteration: 1353 | Episodes: 54900 | Median Reward: 42.28 | Avg Reward: 42.19 | Max Reward: 47.62
Iteration: 1356 | Episodes: 55000 | Median Reward: 41.01 | Avg Reward: 41.78 | Max Reward: 47.62
Iteration: 1358 | Episodes: 55100 | Median Reward: 44.46 | Avg Reward: 43.33 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.4         |
| time/                   |               |
|    fps                  | 887           |
|    iterations           | 1360          |
|    time_elapsed         | 6279          |
|    total_timesteps      | 5570560       |
| train/                  |               |
|    approx_kl            | 0.00026954937 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -311          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -61.7         |
|    n_updates            | 13590         |
|    policy_gradient_loss | 0.000244      |
|    std                  | 2.93          |
|    value_loss           | 0.648         |
-------------------------------------------
Iteration: 1361 | Episodes: 55200 | Median Reward: 40.66 | Avg Reward: 40.70 | Max Reward: 47.62
Iteration: 1363 | Episodes: 55300 | Median Reward: 39.63 | Avg Reward: 39.51 | Max Reward: 47.62
Iteration: 1366 | Episodes: 55400 | Median Reward: 42.52 | Avg Reward: 42.17 | Max Reward: 47.62
Iteration: 1368 | Episodes: 55500 | Median Reward: 41.62 | Avg Reward: 40.24 | Max Reward: 47.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.4        |
| time/                   |              |
|    fps                  | 887          |
|    iterations           | 1370         |
|    time_elapsed         | 6324         |
|    total_timesteps      | 5611520      |
| train/                  |              |
|    approx_kl            | 0.0025889596 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -312         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -62.1        |
|    n_updates            | 13690        |
|    policy_gradient_loss | -0.0146      |
|    std                  | 2.96         |
|    value_loss           | 1.95         |
------------------------------------------
Iteration: 1370 | Episodes: 55600 | Median Reward: 39.51 | Avg Reward: 40.16 | Max Reward: 47.62
Iteration: 1373 | Episodes: 55700 | Median Reward: 43.62 | Avg Reward: 42.75 | Max Reward: 47.62
Iteration: 1375 | Episodes: 55800 | Median Reward: 42.29 | Avg Reward: 40.78 | Max Reward: 47.62
Iteration: 1378 | Episodes: 55900 | Median Reward: 40.61 | Avg Reward: 40.39 | Max Reward: 47.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.7        |
| time/                   |              |
|    fps                  | 887          |
|    iterations           | 1380         |
|    time_elapsed         | 6370         |
|    total_timesteps      | 5652480      |
| train/                  |              |
|    approx_kl            | 0.0012956604 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -312         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -62.2        |
|    n_updates            | 13790        |
|    policy_gradient_loss | -0.0107      |
|    std                  | 2.98         |
|    value_loss           | 0.746        |
------------------------------------------
Iteration: 1380 | Episodes: 56000 | Median Reward: 42.54 | Avg Reward: 42.09 | Max Reward: 47.62
Iteration: 1383 | Episodes: 56100 | Median Reward: 43.57 | Avg Reward: 43.35 | Max Reward: 47.62
Iteration: 1385 | Episodes: 56200 | Median Reward: 41.10 | Avg Reward: 41.73 | Max Reward: 47.62
Iteration: 1388 | Episodes: 56300 | Median Reward: 41.74 | Avg Reward: 41.90 | Max Reward: 47.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.2       |
| time/                   |             |
|    fps                  | 887         |
|    iterations           | 1390        |
|    time_elapsed         | 6415        |
|    total_timesteps      | 5693440     |
| train/                  |             |
|    approx_kl            | 0.007412439 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -313        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -62.4       |
|    n_updates            | 13890       |
|    policy_gradient_loss | -0.0199     |
|    std                  | 3.01        |
|    value_loss           | 0.405       |
-----------------------------------------
Iteration: 1390 | Episodes: 56400 | Median Reward: 43.38 | Avg Reward: 42.64 | Max Reward: 47.62
Iteration: 1393 | Episodes: 56500 | Median Reward: 43.38 | Avg Reward: 42.83 | Max Reward: 47.62
Iteration: 1395 | Episodes: 56600 | Median Reward: 40.96 | Avg Reward: 40.34 | Max Reward: 47.62
Iteration: 1398 | Episodes: 56700 | Median Reward: 38.51 | Avg Reward: 38.75 | Max Reward: 47.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.7       |
| time/                   |             |
|    fps                  | 887         |
|    iterations           | 1400        |
|    time_elapsed         | 6460        |
|    total_timesteps      | 5734400     |
| train/                  |             |
|    approx_kl            | 6.94632e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -314        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -61.9       |
|    n_updates            | 13990       |
|    policy_gradient_loss | 0.000707    |
|    std                  | 3.04        |
|    value_loss           | 1.16        |
-----------------------------------------
Iteration: 1400 | Episodes: 56800 | Median Reward: 42.42 | Avg Reward: 42.77 | Max Reward: 47.62
Iteration: 1403 | Episodes: 56900 | Median Reward: 41.31 | Avg Reward: 40.86 | Max Reward: 47.62
Iteration: 1405 | Episodes: 57000 | Median Reward: 42.20 | Avg Reward: 40.33 | Max Reward: 47.62
Iteration: 1407 | Episodes: 57100 | Median Reward: 42.93 | Avg Reward: 42.50 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 887           |
|    iterations           | 1410          |
|    time_elapsed         | 6505          |
|    total_timesteps      | 5775360       |
| train/                  |               |
|    approx_kl            | 0.00011193377 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -314          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -61.4         |
|    n_updates            | 14090         |
|    policy_gradient_loss | -0.000344     |
|    std                  | 3.06          |
|    value_loss           | 2.13          |
-------------------------------------------
Iteration: 1410 | Episodes: 57200 | Median Reward: 43.95 | Avg Reward: 42.14 | Max Reward: 47.62
Iteration: 1412 | Episodes: 57300 | Median Reward: 40.65 | Avg Reward: 40.63 | Max Reward: 47.62
Iteration: 1415 | Episodes: 57400 | Median Reward: 42.05 | Avg Reward: 41.71 | Max Reward: 47.62
Iteration: 1417 | Episodes: 57500 | Median Reward: 44.48 | Avg Reward: 43.01 | Max Reward: 47.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 887          |
|    iterations           | 1420         |
|    time_elapsed         | 6550         |
|    total_timesteps      | 5816320      |
| train/                  |              |
|    approx_kl            | 0.0022358135 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -315         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -63          |
|    n_updates            | 14190        |
|    policy_gradient_loss | -0.00518     |
|    std                  | 3.1          |
|    value_loss           | 0.186        |
------------------------------------------
Iteration: 1420 | Episodes: 57600 | Median Reward: 41.81 | Avg Reward: 42.07 | Max Reward: 47.62
Iteration: 1422 | Episodes: 57700 | Median Reward: 40.90 | Avg Reward: 40.04 | Max Reward: 47.62
Iteration: 1425 | Episodes: 57800 | Median Reward: 41.63 | Avg Reward: 42.52 | Max Reward: 47.62
Iteration: 1427 | Episodes: 57900 | Median Reward: 42.76 | Avg Reward: 42.41 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.2         |
| time/                   |               |
|    fps                  | 888           |
|    iterations           | 1430          |
|    time_elapsed         | 6595          |
|    total_timesteps      | 5857280       |
| train/                  |               |
|    approx_kl            | 0.00017203312 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -316          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -62.3         |
|    n_updates            | 14290         |
|    policy_gradient_loss | -0.00096      |
|    std                  | 3.13          |
|    value_loss           | 1.45          |
-------------------------------------------
Iteration: 1430 | Episodes: 58000 | Median Reward: 42.39 | Avg Reward: 41.85 | Max Reward: 47.62
Iteration: 1432 | Episodes: 58100 | Median Reward: 41.42 | Avg Reward: 41.12 | Max Reward: 47.62
Iteration: 1435 | Episodes: 58200 | Median Reward: 40.52 | Avg Reward: 39.98 | Max Reward: 47.62
Iteration: 1437 | Episodes: 58300 | Median Reward: 39.35 | Avg Reward: 39.42 | Max Reward: 47.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.5       |
| time/                   |             |
|    fps                  | 888         |
|    iterations           | 1440        |
|    time_elapsed         | 6640        |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.015039087 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -316        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -63         |
|    n_updates            | 14390       |
|    policy_gradient_loss | -0.0179     |
|    std                  | 3.16        |
|    value_loss           | 0.389       |
-----------------------------------------
Iteration: 1440 | Episodes: 58400 | Median Reward: 41.17 | Avg Reward: 41.52 | Max Reward: 47.62
Iteration: 1442 | Episodes: 58500 | Median Reward: 41.08 | Avg Reward: 40.61 | Max Reward: 47.62
Iteration: 1444 | Episodes: 58600 | Median Reward: 40.40 | Avg Reward: 40.42 | Max Reward: 47.62
Iteration: 1447 | Episodes: 58700 | Median Reward: 41.72 | Avg Reward: 41.63 | Max Reward: 47.62
Iteration: 1449 | Episodes: 58800 | Median Reward: 40.07 | Avg Reward: 41.01 | Max Reward: 47.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59          |
| time/                   |              |
|    fps                  | 888          |
|    iterations           | 1450         |
|    time_elapsed         | 6685         |
|    total_timesteps      | 5939200      |
| train/                  |              |
|    approx_kl            | 0.0006861225 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -317         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -63.1        |
|    n_updates            | 14490        |
|    policy_gradient_loss | -0.00169     |
|    std                  | 3.19         |
|    value_loss           | 0.638        |
------------------------------------------
Iteration: 1452 | Episodes: 58900 | Median Reward: 43.99 | Avg Reward: 42.10 | Max Reward: 47.62
Iteration: 1454 | Episodes: 59000 | Median Reward: 41.22 | Avg Reward: 41.18 | Max Reward: 47.62
Iteration: 1457 | Episodes: 59100 | Median Reward: 42.31 | Avg Reward: 41.68 | Max Reward: 47.62
Iteration: 1459 | Episodes: 59200 | Median Reward: 41.21 | Avg Reward: 41.58 | Max Reward: 47.62
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.5        |
| time/                   |              |
|    fps                  | 888          |
|    iterations           | 1460         |
|    time_elapsed         | 6730         |
|    total_timesteps      | 5980160      |
| train/                  |              |
|    approx_kl            | 4.028903e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -318         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -63.3        |
|    n_updates            | 14590        |
|    policy_gradient_loss | 0.000237     |
|    std                  | 3.22         |
|    value_loss           | 0.484        |
------------------------------------------
Iteration: 1462 | Episodes: 59300 | Median Reward: 41.63 | Avg Reward: 41.32 | Max Reward: 47.62
Iteration: 1464 | Episodes: 59400 | Median Reward: 42.04 | Avg Reward: 41.28 | Max Reward: 47.62
Iteration: 1467 | Episodes: 59500 | Median Reward: 41.54 | Avg Reward: 41.62 | Max Reward: 47.62
Iteration: 1469 | Episodes: 59600 | Median Reward: 41.85 | Avg Reward: 41.92 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.1         |
| time/                   |               |
|    fps                  | 888           |
|    iterations           | 1470          |
|    time_elapsed         | 6775          |
|    total_timesteps      | 6021120       |
| train/                  |               |
|    approx_kl            | 0.00085244933 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -318          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -63.5         |
|    n_updates            | 14690         |
|    policy_gradient_loss | -0.00451      |
|    std                  | 3.25          |
|    value_loss           | 0.447         |
-------------------------------------------
Iteration: 1472 | Episodes: 59700 | Median Reward: 42.24 | Avg Reward: 42.96 | Max Reward: 47.62
Iteration: 1474 | Episodes: 59800 | Median Reward: 40.85 | Avg Reward: 40.88 | Max Reward: 47.62
Iteration: 1477 | Episodes: 59900 | Median Reward: 42.34 | Avg Reward: 43.04 | Max Reward: 47.62
Iteration: 1479 | Episodes: 60000 | Median Reward: 42.37 | Avg Reward: 41.77 | Max Reward: 47.62
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.9       |
| time/                   |             |
|    fps                  | 888         |
|    iterations           | 1480        |
|    time_elapsed         | 6820        |
|    total_timesteps      | 6062080     |
| train/                  |             |
|    approx_kl            | 0.003437705 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -319        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -63.6       |
|    n_updates            | 14790       |
|    policy_gradient_loss | -0.0179     |
|    std                  | 3.28        |
|    value_loss           | 0.336       |
-----------------------------------------
Iteration: 1481 | Episodes: 60100 | Median Reward: 42.32 | Avg Reward: 41.02 | Max Reward: 47.62
Iteration: 1484 | Episodes: 60200 | Median Reward: 41.84 | Avg Reward: 41.90 | Max Reward: 47.62
Iteration: 1486 | Episodes: 60300 | Median Reward: 42.01 | Avg Reward: 40.92 | Max Reward: 47.62
Iteration: 1489 | Episodes: 60400 | Median Reward: 42.04 | Avg Reward: 41.72 | Max Reward: 47.62
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.1         |
| time/                   |               |
|    fps                  | 888           |
|    iterations           | 1490          |
|    time_elapsed         | 6865          |
|    total_timesteps      | 6103040       |
| train/                  |               |
|    approx_kl            | 5.5967364e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -319          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -63.7         |
|    n_updates            | 14890         |
|    policy_gradient_loss | -0.00066      |
|    std                  | 3.32          |
|    value_loss           | 0.743         |
-------------------------------------------
Iteration: 1491 | Episodes: 60500 | Median Reward: 41.63 | Avg Reward: 41.71 | Max Reward: 47.62
Iteration: 1494 | Episodes: 60600 | Median Reward: 41.05 | Avg Reward: 41.23 | Max Reward: 47.62
Iteration: 1496 | Episodes: 60700 | Median Reward: 41.54 | Avg Reward: 42.39 | Max Reward: 47.62
Iteration: 1499 | Episodes: 60800 | Median Reward: 42.03 | Avg Reward: 42.53 | Max Reward: 47.62