Script started on 2024-10-24 20:59:51-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/27" COLUMNS="203" LINES="53"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> cpd[K[Konda activate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python coe[Kde1_1resi[K[K[K[K_residual_e.py
GPU 3: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
Iteration: 2 | Episodes: 100 | Median Reward: 21.82 | Avg Reward: 21.60 | Max Reward: 27.55
Iteration: 4 | Episodes: 200 | Median Reward: 20.86 | Avg Reward: 21.43 | Max Reward: 31.04
Iteration: 7 | Episodes: 300 | Median Reward: 24.80 | Avg Reward: 24.26 | Max Reward: 35.07
Iteration: 9 | Episodes: 400 | Median Reward: 23.08 | Avg Reward: 23.03 | Max Reward: 35.07
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -77.3      |
| time/                   |            |
|    fps                  | 903        |
|    iterations           | 10         |
|    time_elapsed         | 45         |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.07417905 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.3        |
|    entropy_loss         | -67.9      |
|    explained_variance   | -0.00797   |
|    learning_rate        | 0.0005     |
|    loss                 | 99.9       |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.0135     |
|    std                  | 1.01       |
|    value_loss           | 251        |
----------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 24.92 | Avg Reward: 23.74 | Max Reward: 35.07
Iteration: 14 | Episodes: 600 | Median Reward: 23.19 | Avg Reward: 22.82 | Max Reward: 35.27
Iteration: 17 | Episodes: 700 | Median Reward: 25.52 | Avg Reward: 26.03 | Max Reward: 35.94
Iteration: 19 | Episodes: 800 | Median Reward: 22.84 | Avg Reward: 23.31 | Max Reward: 35.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76.8       |
| time/                   |             |
|    fps                  | 931         |
|    iterations           | 20          |
|    time_elapsed         | 87          |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.030798469 |
|    clip_fraction        | 0.0816      |
|    clip_range           | 0.3         |
|    entropy_loss         | -64.6       |
|    explained_variance   | 0.623       |
|    learning_rate        | 0.0005      |
|    loss                 | 39          |
|    n_updates            | 190         |
|    policy_gradient_loss | 0.00299     |
|    std                  | 1.01        |
|    value_loss           | 110         |
-----------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 28.00 | Avg Reward: 26.41 | Max Reward: 35.94
Iteration: 24 | Episodes: 1000 | Median Reward: 24.25 | Avg Reward: 25.08 | Max Reward: 35.94
Iteration: 27 | Episodes: 1100 | Median Reward: 26.80 | Avg Reward: 25.40 | Max Reward: 35.94
Iteration: 29 | Episodes: 1200 | Median Reward: 27.34 | Avg Reward: 27.20 | Max Reward: 35.94
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -72.6      |
| time/                   |            |
|    fps                  | 934        |
|    iterations           | 30         |
|    time_elapsed         | 131        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.00849983 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -71.7      |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0005     |
|    loss                 | 14.9       |
|    n_updates            | 290        |
|    policy_gradient_loss | 0.00343    |
|    std                  | 1.01       |
|    value_loss           | 57.3       |
----------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 27.34 | Avg Reward: 25.65 | Max Reward: 35.94
Iteration: 34 | Episodes: 1400 | Median Reward: 22.59 | Avg Reward: 23.53 | Max Reward: 35.94
Iteration: 36 | Episodes: 1500 | Median Reward: 25.25 | Avg Reward: 24.12 | Max Reward: 35.94
Iteration: 39 | Episodes: 1600 | Median Reward: 24.55 | Avg Reward: 25.13 | Max Reward: 35.94
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -74.8      |
| time/                   |            |
|    fps                  | 930        |
|    iterations           | 40         |
|    time_elapsed         | 175        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.02618399 |
|    clip_fraction        | 0.0118     |
|    clip_range           | 0.3        |
|    entropy_loss         | -74.8      |
|    explained_variance   | 0.967      |
|    learning_rate        | 0.0005     |
|    loss                 | -3.38      |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0118    |
|    std                  | 1.01       |
|    value_loss           | 55.3       |
----------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 25.09 | Avg Reward: 25.66 | Max Reward: 35.94
Iteration: 44 | Episodes: 1800 | Median Reward: 23.66 | Avg Reward: 23.78 | Max Reward: 39.20
Iteration: 46 | Episodes: 1900 | Median Reward: 22.72 | Avg Reward: 25.77 | Max Reward: 39.20
Iteration: 49 | Episodes: 2000 | Median Reward: 26.75 | Avg Reward: 26.32 | Max Reward: 39.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -73.8        |
| time/                   |              |
|    fps                  | 927          |
|    iterations           | 50           |
|    time_elapsed         | 220          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0035218233 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -79.4        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0005       |
|    loss                 | -2.57        |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00492     |
|    std                  | 1.01         |
|    value_loss           | 16.8         |
------------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 25.04 | Avg Reward: 25.54 | Max Reward: 39.20
Iteration: 54 | Episodes: 2200 | Median Reward: 27.09 | Avg Reward: 26.35 | Max Reward: 39.20
Iteration: 56 | Episodes: 2300 | Median Reward: 26.33 | Avg Reward: 26.50 | Max Reward: 39.20
Iteration: 59 | Episodes: 2400 | Median Reward: 23.88 | Avg Reward: 21.86 | Max Reward: 39.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.2        |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 60           |
|    time_elapsed         | 266          |
|    total_timesteps      | 245760       |
| train/                  |              |
|    approx_kl            | 0.0018820029 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -89.1        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.1         |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00213     |
|    std                  | 1.02         |
|    value_loss           | 13           |
------------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 27.45 | Avg Reward: 25.20 | Max Reward: 39.20
Iteration: 64 | Episodes: 2600 | Median Reward: 21.44 | Avg Reward: 21.73 | Max Reward: 39.20
Iteration: 66 | Episodes: 2700 | Median Reward: 26.82 | Avg Reward: 25.81 | Max Reward: 39.20
Iteration: 69 | Episodes: 2800 | Median Reward: 24.15 | Avg Reward: 25.13 | Max Reward: 39.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -75.4       |
| time/                   |             |
|    fps                  | 924         |
|    iterations           | 70          |
|    time_elapsed         | 310         |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.024538657 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.3         |
|    entropy_loss         | -95.4       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0005      |
|    loss                 | -12         |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00627    |
|    std                  | 1.02        |
|    value_loss           | 9.59        |
-----------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 25.56 | Avg Reward: 25.12 | Max Reward: 39.20
Iteration: 73 | Episodes: 3000 | Median Reward: 26.90 | Avg Reward: 24.71 | Max Reward: 39.20
Iteration: 76 | Episodes: 3100 | Median Reward: 28.86 | Avg Reward: 28.37 | Max Reward: 39.20
Iteration: 78 | Episodes: 3200 | Median Reward: 27.15 | Avg Reward: 26.74 | Max Reward: 39.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.2       |
| time/                   |             |
|    fps                  | 920         |
|    iterations           | 80          |
|    time_elapsed         | 355         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.031989194 |
|    clip_fraction        | 0.0587      |
|    clip_range           | 0.3         |
|    entropy_loss         | -108        |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0005      |
|    loss                 | -14.5       |
|    n_updates            | 790         |
|    policy_gradient_loss | 0.000575    |
|    std                  | 1.02        |
|    value_loss           | 6.61        |
-----------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 26.02 | Avg Reward: 26.37 | Max Reward: 39.20
Iteration: 83 | Episodes: 3400 | Median Reward: 26.72 | Avg Reward: 27.57 | Max Reward: 39.20
Iteration: 86 | Episodes: 3500 | Median Reward: 26.16 | Avg Reward: 27.13 | Max Reward: 39.20
Iteration: 88 | Episodes: 3600 | Median Reward: 28.56 | Avg Reward: 27.81 | Max Reward: 39.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74         |
| time/                   |             |
|    fps                  | 922         |
|    iterations           | 90          |
|    time_elapsed         | 399         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.026033618 |
|    clip_fraction        | 0.077       |
|    clip_range           | 0.3         |
|    entropy_loss         | -118        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.24       |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0244     |
|    std                  | 1.02        |
|    value_loss           | 14.1        |
-----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 24.59 | Avg Reward: 24.89 | Max Reward: 39.20
Iteration: 93 | Episodes: 3800 | Median Reward: 26.85 | Avg Reward: 28.13 | Max Reward: 39.20
Iteration: 96 | Episodes: 3900 | Median Reward: 27.21 | Avg Reward: 27.43 | Max Reward: 39.20
Iteration: 98 | Episodes: 4000 | Median Reward: 27.55 | Avg Reward: 26.54 | Max Reward: 39.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.9       |
| time/                   |             |
|    fps                  | 920         |
|    iterations           | 100         |
|    time_elapsed         | 445         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.016952053 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.3         |
|    entropy_loss         | -126        |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0005      |
|    loss                 | -14.4       |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0132     |
|    std                  | 1.02        |
|    value_loss           | 20.5        |
-----------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 31.01 | Avg Reward: 29.90 | Max Reward: 39.20
Iteration: 103 | Episodes: 4200 | Median Reward: 26.08 | Avg Reward: 26.57 | Max Reward: 39.20
Iteration: 106 | Episodes: 4300 | Median Reward: 26.44 | Avg Reward: 23.92 | Max Reward: 39.20
Iteration: 108 | Episodes: 4400 | Median Reward: 28.65 | Avg Reward: 28.19 | Max Reward: 39.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.5       |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 110         |
|    time_elapsed         | 490         |
|    total_timesteps      | 450560      |
| train/                  |             |
|    approx_kl            | 0.008744612 |
|    clip_fraction        | 0.000537    |
|    clip_range           | 0.3         |
|    entropy_loss         | -134        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -19         |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00932    |
|    std                  | 1.03        |
|    value_loss           | 4.73        |
-----------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 26.75 | Avg Reward: 25.52 | Max Reward: 39.20
Iteration: 113 | Episodes: 4600 | Median Reward: 26.87 | Avg Reward: 27.32 | Max Reward: 39.20
Iteration: 115 | Episodes: 4700 | Median Reward: 29.39 | Avg Reward: 28.21 | Max Reward: 39.77
Iteration: 118 | Episodes: 4800 | Median Reward: 29.81 | Avg Reward: 29.45 | Max Reward: 39.77
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -72.7     |
| time/                   |           |
|    fps                  | 919       |
|    iterations           | 120       |
|    time_elapsed         | 534       |
|    total_timesteps      | 491520    |
| train/                  |           |
|    approx_kl            | 0.0290352 |
|    clip_fraction        | 0.00806   |
|    clip_range           | 0.3       |
|    entropy_loss         | -140      |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0005    |
|    loss                 | -16.2     |
|    n_updates            | 1190      |
|    policy_gradient_loss | -0.0396   |
|    std                  | 1.03      |
|    value_loss           | 7.34      |
---------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 27.29 | Avg Reward: 27.60 | Max Reward: 39.77
Iteration: 123 | Episodes: 5000 | Median Reward: 25.97 | Avg Reward: 26.56 | Max Reward: 39.77
Iteration: 125 | Episodes: 5100 | Median Reward: 29.55 | Avg Reward: 27.39 | Max Reward: 39.77
Iteration: 128 | Episodes: 5200 | Median Reward: 26.77 | Avg Reward: 28.34 | Max Reward: 39.77
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -71.1      |
| time/                   |            |
|    fps                  | 919        |
|    iterations           | 130        |
|    time_elapsed         | 579        |
|    total_timesteps      | 532480     |
| train/                  |            |
|    approx_kl            | 0.05158769 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.3        |
|    entropy_loss         | -148       |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0005     |
|    loss                 | -21.4      |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.0366    |
|    std                  | 1.03       |
|    value_loss           | 3.82       |
----------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 28.98 | Avg Reward: 29.11 | Max Reward: 39.77
Iteration: 133 | Episodes: 5400 | Median Reward: 29.19 | Avg Reward: 27.84 | Max Reward: 39.77
Iteration: 135 | Episodes: 5500 | Median Reward: 27.84 | Avg Reward: 28.15 | Max Reward: 39.77
Iteration: 138 | Episodes: 5600 | Median Reward: 34.70 | Avg Reward: 33.21 | Max Reward: 39.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.9        |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 140          |
|    time_elapsed         | 625          |
|    total_timesteps      | 573440       |
| train/                  |              |
|    approx_kl            | 0.0009556654 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -154         |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0005       |
|    loss                 | -21.2        |
|    n_updates            | 1390         |
|    policy_gradient_loss | 4.15e-05     |
|    std                  | 1.04         |
|    value_loss           | 3.67         |
------------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 29.44 | Avg Reward: 30.12 | Max Reward: 39.77
Iteration: 143 | Episodes: 5800 | Median Reward: 32.18 | Avg Reward: 31.29 | Max Reward: 39.77
Iteration: 145 | Episodes: 5900 | Median Reward: 30.69 | Avg Reward: 31.13 | Max Reward: 39.77
Iteration: 147 | Episodes: 6000 | Median Reward: 29.42 | Avg Reward: 29.73 | Max Reward: 39.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.2       |
| time/                   |             |
|    fps                  | 912         |
|    iterations           | 150         |
|    time_elapsed         | 673         |
|    total_timesteps      | 614400      |
| train/                  |             |
|    approx_kl            | 0.037376948 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.3         |
|    entropy_loss         | -160        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.2       |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0169     |
|    std                  | 1.04        |
|    value_loss           | 1.81        |
-----------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 30.11 | Avg Reward: 31.51 | Max Reward: 43.13
Iteration: 152 | Episodes: 6200 | Median Reward: 30.10 | Avg Reward: 31.80 | Max Reward: 43.13
Iteration: 155 | Episodes: 6300 | Median Reward: 31.49 | Avg Reward: 31.70 | Max Reward: 43.13
Iteration: 157 | Episodes: 6400 | Median Reward: 30.17 | Avg Reward: 29.95 | Max Reward: 43.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.1       |
| time/                   |             |
|    fps                  | 911         |
|    iterations           | 160         |
|    time_elapsed         | 719         |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.011130627 |
|    clip_fraction        | 0.00645     |
|    clip_range           | 0.3         |
|    entropy_loss         | -163        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.5       |
|    n_updates            | 1590        |
|    policy_gradient_loss | 0.00288     |
|    std                  | 1.04        |
|    value_loss           | 5.18        |
-----------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 29.73 | Avg Reward: 30.57 | Max Reward: 43.13
Iteration: 162 | Episodes: 6600 | Median Reward: 31.73 | Avg Reward: 31.51 | Max Reward: 43.95
Iteration: 165 | Episodes: 6700 | Median Reward: 31.96 | Avg Reward: 31.25 | Max Reward: 43.95
Iteration: 167 | Episodes: 6800 | Median Reward: 31.38 | Avg Reward: 31.72 | Max Reward: 43.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.5       |
| time/                   |             |
|    fps                  | 911         |
|    iterations           | 170         |
|    time_elapsed         | 763         |
|    total_timesteps      | 696320      |
| train/                  |             |
|    approx_kl            | 0.019752352 |
|    clip_fraction        | 0.00342     |
|    clip_range           | 0.3         |
|    entropy_loss         | -168        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -24.3       |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0194     |
|    std                  | 1.04        |
|    value_loss           | 2.91        |
-----------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 29.60 | Avg Reward: 29.64 | Max Reward: 43.95
Iteration: 172 | Episodes: 7000 | Median Reward: 33.53 | Avg Reward: 32.28 | Max Reward: 43.95
Iteration: 175 | Episodes: 7100 | Median Reward: 34.97 | Avg Reward: 32.12 | Max Reward: 43.95
Iteration: 177 | Episodes: 7200 | Median Reward: 36.25 | Avg Reward: 33.62 | Max Reward: 43.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.4       |
| time/                   |             |
|    fps                  | 909         |
|    iterations           | 180         |
|    time_elapsed         | 810         |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.002243353 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -177        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -24.7       |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.000934   |
|    std                  | 1.05        |
|    value_loss           | 3.04        |
-----------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 35.08 | Avg Reward: 33.74 | Max Reward: 43.95
Iteration: 182 | Episodes: 7400 | Median Reward: 33.78 | Avg Reward: 31.88 | Max Reward: 43.95
Iteration: 184 | Episodes: 7500 | Median Reward: 32.27 | Avg Reward: 32.87 | Max Reward: 43.95
Iteration: 187 | Episodes: 7600 | Median Reward: 36.24 | Avg Reward: 34.30 | Max Reward: 43.95
Iteration: 189 | Episodes: 7700 | Median Reward: 36.09 | Avg Reward: 34.61 | Max Reward: 43.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.4         |
| time/                   |               |
|    fps                  | 907           |
|    iterations           | 190           |
|    time_elapsed         | 857           |
|    total_timesteps      | 778240        |
| train/                  |               |
|    approx_kl            | 0.00039792922 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -179          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -26.6         |
|    n_updates            | 1890          |
|    policy_gradient_loss | -0.000161     |
|    std                  | 1.05          |
|    value_loss           | 4.42          |
-------------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 32.66 | Avg Reward: 31.61 | Max Reward: 43.95
Iteration: 194 | Episodes: 7900 | Median Reward: 33.65 | Avg Reward: 33.68 | Max Reward: 43.95
Iteration: 197 | Episodes: 8000 | Median Reward: 33.45 | Avg Reward: 33.52 | Max Reward: 43.95
Iteration: 199 | Episodes: 8100 | Median Reward: 36.03 | Avg Reward: 33.61 | Max Reward: 43.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.1        |
| time/                   |              |
|    fps                  | 903          |
|    iterations           | 200          |
|    time_elapsed         | 906          |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0058245063 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.3          |
|    entropy_loss         | -183         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.9        |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.0047      |
|    std                  | 1.05         |
|    value_loss           | 3.13         |
------------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 33.11 | Avg Reward: 33.46 | Max Reward: 43.95
Iteration: 204 | Episodes: 8300 | Median Reward: 33.54 | Avg Reward: 31.79 | Max Reward: 43.95
Iteration: 207 | Episodes: 8400 | Median Reward: 34.14 | Avg Reward: 34.51 | Max Reward: 43.95
Iteration: 209 | Episodes: 8500 | Median Reward: 33.24 | Avg Reward: 33.73 | Max Reward: 43.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.2       |
| time/                   |             |
|    fps                  | 904         |
|    iterations           | 210         |
|    time_elapsed         | 951         |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.025356462 |
|    clip_fraction        | 0.0255      |
|    clip_range           | 0.3         |
|    entropy_loss         | -185        |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0005      |
|    loss                 | -26.9       |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00971    |
|    std                  | 1.06        |
|    value_loss           | 3.08        |
-----------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 34.37 | Avg Reward: 34.11 | Max Reward: 43.95
Iteration: 214 | Episodes: 8700 | Median Reward: 33.58 | Avg Reward: 34.28 | Max Reward: 44.13
Iteration: 216 | Episodes: 8800 | Median Reward: 36.39 | Avg Reward: 35.29 | Max Reward: 44.13
Iteration: 219 | Episodes: 8900 | Median Reward: 36.09 | Avg Reward: 35.43 | Max Reward: 44.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.8        |
| time/                   |              |
|    fps                  | 903          |
|    iterations           | 220          |
|    time_elapsed         | 997          |
|    total_timesteps      | 901120       |
| train/                  |              |
|    approx_kl            | 0.0016386749 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -186         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.5        |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00232     |
|    std                  | 1.06         |
|    value_loss           | 3.16         |
------------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 36.62 | Avg Reward: 36.45 | Max Reward: 44.13
Iteration: 224 | Episodes: 9100 | Median Reward: 37.57 | Avg Reward: 35.82 | Max Reward: 44.13
Iteration: 226 | Episodes: 9200 | Median Reward: 34.60 | Avg Reward: 35.64 | Max Reward: 44.13
Iteration: 229 | Episodes: 9300 | Median Reward: 34.17 | Avg Reward: 34.11 | Max Reward: 44.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.8       |
| time/                   |             |
|    fps                  | 901         |
|    iterations           | 230         |
|    time_elapsed         | 1044        |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.007378357 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.3         |
|    entropy_loss         | -189        |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0005      |
|    loss                 | -27.1       |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00424    |
|    std                  | 1.07        |
|    value_loss           | 2.3         |
-----------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 33.73 | Avg Reward: 32.24 | Max Reward: 44.13
Iteration: 234 | Episodes: 9500 | Median Reward: 36.05 | Avg Reward: 34.15 | Max Reward: 44.13
Iteration: 236 | Episodes: 9600 | Median Reward: 35.26 | Avg Reward: 35.43 | Max Reward: 44.13
Iteration: 239 | Episodes: 9700 | Median Reward: 35.99 | Avg Reward: 36.04 | Max Reward: 44.13
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -62            |
| time/                   |                |
|    fps                  | 900            |
|    iterations           | 240            |
|    time_elapsed         | 1091           |
|    total_timesteps      | 983040         |
| train/                  |                |
|    approx_kl            | 0.000120063196 |
|    clip_fraction        | 0              |
|    clip_range           | 0.3            |
|    entropy_loss         | -194           |
|    explained_variance   | 0.988          |
|    learning_rate        | 0.0005         |
|    loss                 | -21.2          |
|    n_updates            | 2390           |
|    policy_gradient_loss | -0.000247      |
|    std                  | 1.07           |
|    value_loss           | 7.01           |
--------------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 37.41 | Avg Reward: 36.80 | Max Reward: 44.13
Iteration: 244 | Episodes: 9900 | Median Reward: 36.61 | Avg Reward: 35.75 | Max Reward: 44.13
Iteration: 246 | Episodes: 10000 | Median Reward: 34.69 | Avg Reward: 34.85 | Max Reward: 44.13
Iteration: 249 | Episodes: 10100 | Median Reward: 33.73 | Avg Reward: 34.19 | Max Reward: 44.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.9         |
| time/                   |               |
|    fps                  | 900           |
|    iterations           | 250           |
|    time_elapsed         | 1136          |
|    total_timesteps      | 1024000       |
| train/                  |               |
|    approx_kl            | 0.00087635516 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -196          |
|    explained_variance   | 0.99          |
|    learning_rate        | 0.0005        |
|    loss                 | -28.2         |
|    n_updates            | 2490          |
|    policy_gradient_loss | -0.000449     |
|    std                  | 1.07          |
|    value_loss           | 4.75          |
-------------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 35.27 | Avg Reward: 35.10 | Max Reward: 44.13
Iteration: 253 | Episodes: 10300 | Median Reward: 36.23 | Avg Reward: 34.36 | Max Reward: 44.13
Iteration: 256 | Episodes: 10400 | Median Reward: 34.35 | Avg Reward: 34.34 | Max Reward: 44.13
Iteration: 258 | Episodes: 10500 | Median Reward: 36.25 | Avg Reward: 34.77 | Max Reward: 44.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65         |
| time/                   |             |
|    fps                  | 900         |
|    iterations           | 260         |
|    time_elapsed         | 1183        |
|    total_timesteps      | 1064960     |
| train/                  |             |
|    approx_kl            | 0.037172966 |
|    clip_fraction        | 0.0735      |
|    clip_range           | 0.3         |
|    entropy_loss         | -198        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -29.2       |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0237     |
|    std                  | 1.08        |
|    value_loss           | 1.7         |
-----------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 37.49 | Avg Reward: 37.74 | Max Reward: 44.13
Iteration: 263 | Episodes: 10700 | Median Reward: 36.84 | Avg Reward: 36.98 | Max Reward: 44.13
Iteration: 266 | Episodes: 10800 | Median Reward: 39.05 | Avg Reward: 37.46 | Max Reward: 44.13
Iteration: 268 | Episodes: 10900 | Median Reward: 35.04 | Avg Reward: 35.28 | Max Reward: 44.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.6        |
| time/                   |              |
|    fps                  | 898          |
|    iterations           | 270          |
|    time_elapsed         | 1230         |
|    total_timesteps      | 1105920      |
| train/                  |              |
|    approx_kl            | 0.0024904623 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -199         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0005       |
|    loss                 | -28.1        |
|    n_updates            | 2690         |
|    policy_gradient_loss | 0.00337      |
|    std                  | 1.08         |
|    value_loss           | 2.81         |
------------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 34.79 | Avg Reward: 33.95 | Max Reward: 44.13
Iteration: 273 | Episodes: 11100 | Median Reward: 36.44 | Avg Reward: 36.58 | Max Reward: 44.13
Iteration: 276 | Episodes: 11200 | Median Reward: 34.95 | Avg Reward: 36.19 | Max Reward: 44.13
Iteration: 278 | Episodes: 11300 | Median Reward: 35.98 | Avg Reward: 36.07 | Max Reward: 44.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64         |
| time/                   |             |
|    fps                  | 897         |
|    iterations           | 280         |
|    time_elapsed         | 1278        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.010109048 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -201        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -29.4       |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.0098     |
|    std                  | 1.09        |
|    value_loss           | 1.2         |
-----------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 35.50 | Avg Reward: 36.46 | Max Reward: 44.13
Iteration: 283 | Episodes: 11500 | Median Reward: 34.06 | Avg Reward: 35.35 | Max Reward: 44.13
Iteration: 286 | Episodes: 11600 | Median Reward: 32.54 | Avg Reward: 33.56 | Max Reward: 44.13
Iteration: 288 | Episodes: 11700 | Median Reward: 37.41 | Avg Reward: 36.63 | Max Reward: 44.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.6       |
| time/                   |             |
|    fps                  | 896         |
|    iterations           | 290         |
|    time_elapsed         | 1325        |
|    total_timesteps      | 1187840     |
| train/                  |             |
|    approx_kl            | 0.029755397 |
|    clip_fraction        | 0.0769      |
|    clip_range           | 0.3         |
|    entropy_loss         | -203        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -25.6       |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.035      |
|    std                  | 1.09        |
|    value_loss           | 3.97        |
-----------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 36.18 | Avg Reward: 36.09 | Max Reward: 44.13
Iteration: 293 | Episodes: 11900 | Median Reward: 36.87 | Avg Reward: 36.41 | Max Reward: 44.13
Iteration: 295 | Episodes: 12000 | Median Reward: 36.97 | Avg Reward: 36.36 | Max Reward: 44.13
Iteration: 298 | Episodes: 12100 | Median Reward: 36.58 | Avg Reward: 37.45 | Max Reward: 44.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.7       |
| time/                   |             |
|    fps                  | 895         |
|    iterations           | 300         |
|    time_elapsed         | 1371        |
|    total_timesteps      | 1228800     |
| train/                  |             |
|    approx_kl            | 0.029559316 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -204        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -30.5       |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.0141     |
|    std                  | 1.09        |
|    value_loss           | 1.2         |
-----------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 40.06 | Avg Reward: 40.07 | Max Reward: 44.13
Iteration: 303 | Episodes: 12300 | Median Reward: 38.50 | Avg Reward: 37.69 | Max Reward: 44.13
Iteration: 305 | Episodes: 12400 | Median Reward: 37.44 | Avg Reward: 37.43 | Max Reward: 44.13
Iteration: 308 | Episodes: 12500 | Median Reward: 38.83 | Avg Reward: 38.73 | Max Reward: 44.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64         |
| time/                   |             |
|    fps                  | 896         |
|    iterations           | 310         |
|    time_elapsed         | 1416        |
|    total_timesteps      | 1269760     |
| train/                  |             |
|    approx_kl            | 0.003115316 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -206        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -30         |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.0036     |
|    std                  | 1.1         |
|    value_loss           | 1.51        |
-----------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: 36.43 | Avg Reward: 36.11 | Max Reward: 44.13
Iteration: 313 | Episodes: 12700 | Median Reward: 35.75 | Avg Reward: 36.55 | Max Reward: 44.13
Iteration: 315 | Episodes: 12800 | Median Reward: 38.51 | Avg Reward: 37.35 | Max Reward: 44.13
Iteration: 318 | Episodes: 12900 | Median Reward: 39.94 | Avg Reward: 38.86 | Max Reward: 44.13
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -63.8      |
| time/                   |            |
|    fps                  | 896        |
|    iterations           | 320        |
|    time_elapsed         | 1461       |
|    total_timesteps      | 1310720    |
| train/                  |            |
|    approx_kl            | 0.16826983 |
|    clip_fraction        | 0.279      |
|    clip_range           | 0.3        |
|    entropy_loss         | -207       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -30.2      |
|    n_updates            | 3190       |
|    policy_gradient_loss | -0.0735    |
|    std                  | 1.1        |
|    value_loss           | 1.33       |
----------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: 38.00 | Avg Reward: 36.10 | Max Reward: 44.13
Iteration: 323 | Episodes: 13100 | Median Reward: 38.01 | Avg Reward: 37.30 | Max Reward: 44.13
Iteration: 325 | Episodes: 13200 | Median Reward: 37.50 | Avg Reward: 37.09 | Max Reward: 44.13
Iteration: 327 | Episodes: 13300 | Median Reward: 37.65 | Avg Reward: 37.99 | Max Reward: 44.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.6        |
| time/                   |              |
|    fps                  | 895          |
|    iterations           | 330          |
|    time_elapsed         | 1508         |
|    total_timesteps      | 1351680      |
| train/                  |              |
|    approx_kl            | 0.0026660648 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -208         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -30.3        |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.00269     |
|    std                  | 1.11         |
|    value_loss           | 0.941        |
------------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: 38.80 | Avg Reward: 38.04 | Max Reward: 44.13
Iteration: 332 | Episodes: 13500 | Median Reward: 37.59 | Avg Reward: 36.79 | Max Reward: 44.13
Iteration: 335 | Episodes: 13600 | Median Reward: 36.34 | Avg Reward: 36.41 | Max Reward: 44.13
Iteration: 337 | Episodes: 13700 | Median Reward: 39.75 | Avg Reward: 38.42 | Max Reward: 44.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.2       |
| time/                   |             |
|    fps                  | 894         |
|    iterations           | 340         |
|    time_elapsed         | 1556        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.013135975 |
|    clip_fraction        | 0.0022      |
|    clip_range           | 0.3         |
|    entropy_loss         | -210        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -31.2       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.0086     |
|    std                  | 1.11        |
|    value_loss           | 1.5         |
-----------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: 38.45 | Avg Reward: 37.58 | Max Reward: 44.13
Iteration: 342 | Episodes: 13900 | Median Reward: 36.65 | Avg Reward: 36.87 | Max Reward: 44.13
Iteration: 345 | Episodes: 14000 | Median Reward: 36.75 | Avg Reward: 36.65 | Max Reward: 44.13
Iteration: 347 | Episodes: 14100 | Median Reward: 39.14 | Avg Reward: 38.13 | Max Reward: 44.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.3       |
| time/                   |             |
|    fps                  | 893         |
|    iterations           | 350         |
|    time_elapsed         | 1603        |
|    total_timesteps      | 1433600     |
| train/                  |             |
|    approx_kl            | 0.000454259 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -210        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.6       |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.000574   |
|    std                  | 1.12        |
|    value_loss           | 6.93        |
-----------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: 39.24 | Avg Reward: 38.38 | Max Reward: 44.13
Iteration: 352 | Episodes: 14300 | Median Reward: 37.05 | Avg Reward: 36.09 | Max Reward: 44.13
Iteration: 355 | Episodes: 14400 | Median Reward: 39.47 | Avg Reward: 38.98 | Max Reward: 44.13
Iteration: 357 | Episodes: 14500 | Median Reward: 39.60 | Avg Reward: 37.21 | Max Reward: 44.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.9       |
| time/                   |             |
|    fps                  | 892         |
|    iterations           | 360         |
|    time_elapsed         | 1652        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.007820311 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -213        |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0005      |
|    loss                 | -30.8       |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00188    |
|    std                  | 1.12        |
|    value_loss           | 3.84        |
-----------------------------------------
Iteration: 360 | Episodes: 14600 | Median Reward: 38.85 | Avg Reward: 38.21 | Max Reward: 44.13
Iteration: 362 | Episodes: 14700 | Median Reward: 38.91 | Avg Reward: 39.04 | Max Reward: 44.13
Iteration: 364 | Episodes: 14800 | Median Reward: 38.45 | Avg Reward: 38.53 | Max Reward: 44.13
Iteration: 367 | Episodes: 14900 | Median Reward: 38.48 | Avg Reward: 38.08 | Max Reward: 44.13
Iteration: 369 | Episodes: 15000 | Median Reward: 39.28 | Avg Reward: 39.34 | Max Reward: 44.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.6       |
| time/                   |             |
|    fps                  | 892         |
|    iterations           | 370         |
|    time_elapsed         | 1697        |
|    total_timesteps      | 1515520     |
| train/                  |             |
|    approx_kl            | 0.015627757 |
|    clip_fraction        | 0.000513    |
|    clip_range           | 0.3         |
|    entropy_loss         | -213        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -31.3       |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.00796    |
|    std                  | 1.13        |
|    value_loss           | 0.959       |
-----------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: 37.54 | Avg Reward: 37.78 | Max Reward: 44.13
Iteration: 374 | Episodes: 15200 | Median Reward: 36.96 | Avg Reward: 37.77 | Max Reward: 44.13
Iteration: 377 | Episodes: 15300 | Median Reward: 38.84 | Avg Reward: 38.51 | Max Reward: 44.13
Iteration: 379 | Episodes: 15400 | Median Reward: 38.23 | Avg Reward: 38.95 | Max Reward: 44.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.2       |
| time/                   |             |
|    fps                  | 893         |
|    iterations           | 380         |
|    time_elapsed         | 1742        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.015993008 |
|    clip_fraction        | 0.0515      |
|    clip_range           | 0.3         |
|    entropy_loss         | -214        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -28.9       |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.0142     |
|    std                  | 1.13        |
|    value_loss           | 3.57        |
-----------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: 39.15 | Avg Reward: 39.70 | Max Reward: 44.69
Iteration: 384 | Episodes: 15600 | Median Reward: 37.69 | Avg Reward: 37.56 | Max Reward: 44.69
Iteration: 387 | Episodes: 15700 | Median Reward: 37.39 | Avg Reward: 36.69 | Max Reward: 44.69
Iteration: 389 | Episodes: 15800 | Median Reward: 37.36 | Avg Reward: 36.61 | Max Reward: 44.69
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63          |
| time/                   |              |
|    fps                  | 892          |
|    iterations           | 390          |
|    time_elapsed         | 1788         |
|    total_timesteps      | 1597440      |
| train/                  |              |
|    approx_kl            | 0.0012669871 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -217         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -31.9        |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.00268     |
|    std                  | 1.14         |
|    value_loss           | 2.67         |
------------------------------------------
Iteration: 392 | Episodes: 15900 | Median Reward: 39.89 | Avg Reward: 39.07 | Max Reward: 44.69
Iteration: 394 | Episodes: 16000 | Median Reward: 40.70 | Avg Reward: 39.97 | Max Reward: 44.69
Iteration: 396 | Episodes: 16100 | Median Reward: 39.56 | Avg Reward: 39.23 | Max Reward: 44.69
Iteration: 399 | Episodes: 16200 | Median Reward: 40.43 | Avg Reward: 40.31 | Max Reward: 44.69
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.3        |
| time/                   |              |
|    fps                  | 893          |
|    iterations           | 400          |
|    time_elapsed         | 1834         |
|    total_timesteps      | 1638400      |
| train/                  |              |
|    approx_kl            | 0.0032619052 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -219         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -32.7        |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.00621     |
|    std                  | 1.14         |
|    value_loss           | 2.1          |
------------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: 38.77 | Avg Reward: 39.45 | Max Reward: 44.69
Iteration: 404 | Episodes: 16400 | Median Reward: 41.51 | Avg Reward: 40.48 | Max Reward: 44.69
Iteration: 406 | Episodes: 16500 | Median Reward: 36.39 | Avg Reward: 36.36 | Max Reward: 44.69
Iteration: 409 | Episodes: 16600 | Median Reward: 37.78 | Avg Reward: 38.58 | Max Reward: 44.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61         |
| time/                   |             |
|    fps                  | 892         |
|    iterations           | 410         |
|    time_elapsed         | 1881        |
|    total_timesteps      | 1679360     |
| train/                  |             |
|    approx_kl            | 0.006123681 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -219        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -32.6       |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.011      |
|    std                  | 1.15        |
|    value_loss           | 1.95        |
-----------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: 39.85 | Avg Reward: 38.97 | Max Reward: 44.69
Iteration: 414 | Episodes: 16800 | Median Reward: 38.63 | Avg Reward: 38.10 | Max Reward: 44.69
Iteration: 416 | Episodes: 16900 | Median Reward: 38.09 | Avg Reward: 39.07 | Max Reward: 44.69
Iteration: 419 | Episodes: 17000 | Median Reward: 40.24 | Avg Reward: 40.59 | Max Reward: 44.69
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 892          |
|    iterations           | 420          |
|    time_elapsed         | 1927         |
|    total_timesteps      | 1720320      |
| train/                  |              |
|    approx_kl            | 0.0009562675 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -221         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -31.6        |
|    n_updates            | 4190         |
|    policy_gradient_loss | -0.000851    |
|    std                  | 1.15         |
|    value_loss           | 2.19         |
------------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: 39.61 | Avg Reward: 40.26 | Max Reward: 44.69
Iteration: 424 | Episodes: 17200 | Median Reward: 41.16 | Avg Reward: 39.83 | Max Reward: 44.69
Iteration: 426 | Episodes: 17300 | Median Reward: 39.81 | Avg Reward: 38.90 | Max Reward: 44.69
Iteration: 429 | Episodes: 17400 | Median Reward: 39.22 | Avg Reward: 39.35 | Max Reward: 44.69
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.2        |
| time/                   |              |
|    fps                  | 892          |
|    iterations           | 430          |
|    time_elapsed         | 1974         |
|    total_timesteps      | 1761280      |
| train/                  |              |
|    approx_kl            | 0.0026843348 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -223         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -32.7        |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.00209     |
|    std                  | 1.16         |
|    value_loss           | 2.07         |
------------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: 39.84 | Avg Reward: 39.46 | Max Reward: 44.69
Iteration: 433 | Episodes: 17600 | Median Reward: 38.86 | Avg Reward: 38.92 | Max Reward: 44.69
Iteration: 436 | Episodes: 17700 | Median Reward: 37.82 | Avg Reward: 38.00 | Max Reward: 44.69
Iteration: 438 | Episodes: 17800 | Median Reward: 40.31 | Avg Reward: 39.32 | Max Reward: 44.69
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.3        |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 440          |
|    time_elapsed         | 2020         |
|    total_timesteps      | 1802240      |
| train/                  |              |
|    approx_kl            | 0.0028496943 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -223         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -31.8        |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.00491     |
|    std                  | 1.16         |
|    value_loss           | 3.44         |
------------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 39.52 | Avg Reward: 38.61 | Max Reward: 44.69
Iteration: 443 | Episodes: 18000 | Median Reward: 40.09 | Avg Reward: 39.41 | Max Reward: 44.69
Iteration: 446 | Episodes: 18100 | Median Reward: 39.67 | Avg Reward: 38.72 | Max Reward: 44.69
Iteration: 448 | Episodes: 18200 | Median Reward: 40.00 | Avg Reward: 38.41 | Max Reward: 44.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.4       |
| time/                   |             |
|    fps                  | 891         |
|    iterations           | 450         |
|    time_elapsed         | 2068        |
|    total_timesteps      | 1843200     |
| train/                  |             |
|    approx_kl            | 0.003526287 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -227        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -33.8       |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.00718    |
|    std                  | 1.17        |
|    value_loss           | 1.67        |
-----------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 39.50 | Avg Reward: 38.85 | Max Reward: 44.69
Iteration: 453 | Episodes: 18400 | Median Reward: 41.16 | Avg Reward: 40.69 | Max Reward: 44.69
Iteration: 456 | Episodes: 18500 | Median Reward: 37.35 | Avg Reward: 38.24 | Max Reward: 44.69
Iteration: 458 | Episodes: 18600 | Median Reward: 42.67 | Avg Reward: 40.51 | Max Reward: 44.69
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.1        |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 460          |
|    time_elapsed         | 2114         |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 0.0049734777 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -228         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29.4        |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.0112      |
|    std                  | 1.17         |
|    value_loss           | 3.19         |
------------------------------------------
Iteration: 461 | Episodes: 18700 | Median Reward: 40.87 | Avg Reward: 39.48 | Max Reward: 44.72
Iteration: 463 | Episodes: 18800 | Median Reward: 40.04 | Avg Reward: 40.16 | Max Reward: 44.72
Iteration: 466 | Episodes: 18900 | Median Reward: 40.13 | Avg Reward: 40.78 | Max Reward: 44.72
Iteration: 468 | Episodes: 19000 | Median Reward: 37.99 | Avg Reward: 38.81 | Max Reward: 44.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.6        |
| time/                   |              |
|    fps                  | 892          |
|    iterations           | 470          |
|    time_elapsed         | 2157         |
|    total_timesteps      | 1925120      |
| train/                  |              |
|    approx_kl            | 0.0060583674 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -228         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -33.8        |
|    n_updates            | 4690         |
|    policy_gradient_loss | -0.00367     |
|    std                  | 1.18         |
|    value_loss           | 2.02         |
------------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 39.79 | Avg Reward: 39.94 | Max Reward: 44.72
Iteration: 473 | Episodes: 19200 | Median Reward: 39.64 | Avg Reward: 38.98 | Max Reward: 44.72
Iteration: 475 | Episodes: 19300 | Median Reward: 39.89 | Avg Reward: 39.74 | Max Reward: 44.72
Iteration: 478 | Episodes: 19400 | Median Reward: 37.56 | Avg Reward: 38.19 | Max Reward: 44.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.9        |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 480          |
|    time_elapsed         | 2205         |
|    total_timesteps      | 1966080      |
| train/                  |              |
|    approx_kl            | 0.0053789914 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -229         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27.9        |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.0136      |
|    std                  | 1.18         |
|    value_loss           | 4.62         |
------------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 39.68 | Avg Reward: 39.42 | Max Reward: 44.72
Iteration: 483 | Episodes: 19600 | Median Reward: 40.15 | Avg Reward: 39.24 | Max Reward: 44.72
Iteration: 485 | Episodes: 19700 | Median Reward: 39.78 | Avg Reward: 40.15 | Max Reward: 44.72
Iteration: 488 | Episodes: 19800 | Median Reward: 38.94 | Avg Reward: 38.16 | Max Reward: 44.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.5        |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 490          |
|    time_elapsed         | 2252         |
|    total_timesteps      | 2007040      |
| train/                  |              |
|    approx_kl            | 0.0012848714 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -231         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -32.8        |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.00258     |
|    std                  | 1.19         |
|    value_loss           | 3.23         |
------------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 40.15 | Avg Reward: 39.92 | Max Reward: 44.72
Iteration: 493 | Episodes: 20000 | Median Reward: 40.46 | Avg Reward: 40.62 | Max Reward: 44.72
Iteration: 495 | Episodes: 20100 | Median Reward: 40.49 | Avg Reward: 40.45 | Max Reward: 44.72
Iteration: 498 | Episodes: 20200 | Median Reward: 40.18 | Avg Reward: 38.66 | Max Reward: 44.72
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.1         |
| time/                   |               |
|    fps                  | 891           |
|    iterations           | 500           |
|    time_elapsed         | 2297          |
|    total_timesteps      | 2048000       |
| train/                  |               |
|    approx_kl            | 0.00061658624 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -232          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -34.1         |
|    n_updates            | 4990          |
|    policy_gradient_loss | 0.00264       |
|    std                  | 1.2           |
|    value_loss           | 1.33          |
-------------------------------------------
Iteration: 500 | Episodes: 20300 | Median Reward: 41.56 | Avg Reward: 41.23 | Max Reward: 44.72
Iteration: 503 | Episodes: 20400 | Median Reward: 40.31 | Avg Reward: 40.66 | Max Reward: 44.72
Iteration: 505 | Episodes: 20500 | Median Reward: 40.57 | Avg Reward: 39.91 | Max Reward: 44.72
Iteration: 507 | Episodes: 20600 | Median Reward: 40.63 | Avg Reward: 40.14 | Max Reward: 44.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.4       |
| time/                   |             |
|    fps                  | 890         |
|    iterations           | 510         |
|    time_elapsed         | 2344        |
|    total_timesteps      | 2088960     |
| train/                  |             |
|    approx_kl            | 0.020159654 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.3         |
|    entropy_loss         | -234        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -35.1       |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 1.2         |
|    value_loss           | 0.749       |
-----------------------------------------
Iteration: 510 | Episodes: 20700 | Median Reward: 39.37 | Avg Reward: 40.00 | Max Reward: 44.72
Iteration: 512 | Episodes: 20800 | Median Reward: 40.88 | Avg Reward: 40.24 | Max Reward: 44.72
Iteration: 515 | Episodes: 20900 | Median Reward: 43.27 | Avg Reward: 42.52 | Max Reward: 44.72
Iteration: 517 | Episodes: 21000 | Median Reward: 40.37 | Avg Reward: 40.47 | Max Reward: 44.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.4        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 520          |
|    time_elapsed         | 2391         |
|    total_timesteps      | 2129920      |
| train/                  |              |
|    approx_kl            | 0.0023134346 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -236         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -34.8        |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.0036      |
|    std                  | 1.21         |
|    value_loss           | 1.74         |
------------------------------------------
Iteration: 520 | Episodes: 21100 | Median Reward: 39.00 | Avg Reward: 38.98 | Max Reward: 44.72
Iteration: 522 | Episodes: 21200 | Median Reward: 41.97 | Avg Reward: 41.90 | Max Reward: 44.72
Iteration: 525 | Episodes: 21300 | Median Reward: 41.15 | Avg Reward: 40.82 | Max Reward: 44.72
Iteration: 527 | Episodes: 21400 | Median Reward: 42.54 | Avg Reward: 41.97 | Max Reward: 44.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.6        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 530          |
|    time_elapsed         | 2436         |
|    total_timesteps      | 2170880      |
| train/                  |              |
|    approx_kl            | 0.0027240412 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -236         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -35.3        |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.00459     |
|    std                  | 1.22         |
|    value_loss           | 1.01         |
------------------------------------------
Iteration: 530 | Episodes: 21500 | Median Reward: 41.85 | Avg Reward: 41.20 | Max Reward: 44.72
Iteration: 532 | Episodes: 21600 | Median Reward: 39.93 | Avg Reward: 39.39 | Max Reward: 44.72
Iteration: 535 | Episodes: 21700 | Median Reward: 41.74 | Avg Reward: 40.71 | Max Reward: 44.72
Iteration: 537 | Episodes: 21800 | Median Reward: 41.50 | Avg Reward: 40.73 | Max Reward: 44.72
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.1         |
| time/                   |               |
|    fps                  | 890           |
|    iterations           | 540           |
|    time_elapsed         | 2482          |
|    total_timesteps      | 2211840       |
| train/                  |               |
|    approx_kl            | 0.00013307783 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -238          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -31.4         |
|    n_updates            | 5390          |
|    policy_gradient_loss | -0.000377     |
|    std                  | 1.22          |
|    value_loss           | 2.64          |
-------------------------------------------
Iteration: 540 | Episodes: 21900 | Median Reward: 39.04 | Avg Reward: 38.93 | Max Reward: 44.72
Iteration: 542 | Episodes: 22000 | Median Reward: 40.96 | Avg Reward: 41.30 | Max Reward: 44.72
Iteration: 544 | Episodes: 22100 | Median Reward: 39.54 | Avg Reward: 40.06 | Max Reward: 44.72
Iteration: 547 | Episodes: 22200 | Median Reward: 41.86 | Avg Reward: 41.31 | Max Reward: 44.72
Iteration: 549 | Episodes: 22300 | Median Reward: 40.33 | Avg Reward: 40.35 | Max Reward: 44.72
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.6         |
| time/                   |               |
|    fps                  | 890           |
|    iterations           | 550           |
|    time_elapsed         | 2528          |
|    total_timesteps      | 2252800       |
| train/                  |               |
|    approx_kl            | 0.00014447344 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -238          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -34.9         |
|    n_updates            | 5490          |
|    policy_gradient_loss | -3.72e-05     |
|    std                  | 1.23          |
|    value_loss           | 4.31          |
-------------------------------------------
Iteration: 552 | Episodes: 22400 | Median Reward: 40.66 | Avg Reward: 41.41 | Max Reward: 44.72
Iteration: 554 | Episodes: 22500 | Median Reward: 40.52 | Avg Reward: 40.98 | Max Reward: 44.72
Iteration: 557 | Episodes: 22600 | Median Reward: 41.63 | Avg Reward: 41.76 | Max Reward: 44.72
Iteration: 559 | Episodes: 22700 | Median Reward: 41.72 | Avg Reward: 40.55 | Max Reward: 44.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.5       |
| time/                   |             |
|    fps                  | 891         |
|    iterations           | 560         |
|    time_elapsed         | 2572        |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.002387037 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -239        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -35.3       |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.00528    |
|    std                  | 1.24        |
|    value_loss           | 2.25        |
-----------------------------------------
Iteration: 562 | Episodes: 22800 | Median Reward: 42.62 | Avg Reward: 41.98 | Max Reward: 44.72
Iteration: 564 | Episodes: 22900 | Median Reward: 41.11 | Avg Reward: 40.24 | Max Reward: 44.72
Iteration: 567 | Episodes: 23000 | Median Reward: 40.99 | Avg Reward: 40.13 | Max Reward: 44.72
Iteration: 569 | Episodes: 23100 | Median Reward: 41.14 | Avg Reward: 41.22 | Max Reward: 44.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.7        |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 570          |
|    time_elapsed         | 2617         |
|    total_timesteps      | 2334720      |
| train/                  |              |
|    approx_kl            | 0.0006272551 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -240         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -32.3        |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.00248     |
|    std                  | 1.24         |
|    value_loss           | 6.67         |
------------------------------------------
Iteration: 572 | Episodes: 23200 | Median Reward: 41.36 | Avg Reward: 40.80 | Max Reward: 44.72
Iteration: 574 | Episodes: 23300 | Median Reward: 39.67 | Avg Reward: 39.81 | Max Reward: 44.72
Iteration: 577 | Episodes: 23400 | Median Reward: 42.69 | Avg Reward: 41.56 | Max Reward: 44.72
Iteration: 579 | Episodes: 23500 | Median Reward: 42.41 | Avg Reward: 42.49 | Max Reward: 44.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 892          |
|    iterations           | 580          |
|    time_elapsed         | 2662         |
|    total_timesteps      | 2375680      |
| train/                  |              |
|    approx_kl            | 0.0014676864 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -241         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -35.8        |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 1.25         |
|    value_loss           | 1.17         |
------------------------------------------
Iteration: 581 | Episodes: 23600 | Median Reward: 40.22 | Avg Reward: 40.20 | Max Reward: 44.72
Iteration: 584 | Episodes: 23700 | Median Reward: 41.49 | Avg Reward: 41.42 | Max Reward: 44.72
Iteration: 586 | Episodes: 23800 | Median Reward: 42.26 | Avg Reward: 41.96 | Max Reward: 44.72
Iteration: 589 | Episodes: 23900 | Median Reward: 40.64 | Avg Reward: 39.90 | Max Reward: 44.72
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -59.4      |
| time/                   |            |
|    fps                  | 892        |
|    iterations           | 590        |
|    time_elapsed         | 2709       |
|    total_timesteps      | 2416640    |
| train/                  |            |
|    approx_kl            | 0.17941357 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.3        |
|    entropy_loss         | -242       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -36.2      |
|    n_updates            | 5890       |
|    policy_gradient_loss | -0.104     |
|    std                  | 1.25       |
|    value_loss           | 4.35       |
----------------------------------------
Iteration: 591 | Episodes: 24000 | Median Reward: 41.86 | Avg Reward: 40.56 | Max Reward: 44.72
Iteration: 594 | Episodes: 24100 | Median Reward: 40.31 | Avg Reward: 40.91 | Max Reward: 44.72
Iteration: 596 | Episodes: 24200 | Median Reward: 40.19 | Avg Reward: 40.34 | Max Reward: 44.72
Iteration: 599 | Episodes: 24300 | Median Reward: 43.34 | Avg Reward: 41.80 | Max Reward: 44.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 600          |
|    time_elapsed         | 2757         |
|    total_timesteps      | 2457600      |
| train/                  |              |
|    approx_kl            | 0.0015624099 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -243         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -35.2        |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.00509     |
|    std                  | 1.26         |
|    value_loss           | 1.56         |
------------------------------------------
Iteration: 601 | Episodes: 24400 | Median Reward: 42.59 | Avg Reward: 41.17 | Max Reward: 44.72
Iteration: 604 | Episodes: 24500 | Median Reward: 41.85 | Avg Reward: 41.15 | Max Reward: 44.72
Iteration: 606 | Episodes: 24600 | Median Reward: 40.40 | Avg Reward: 39.45 | Max Reward: 44.72
Iteration: 609 | Episodes: 24700 | Median Reward: 42.56 | Avg Reward: 40.60 | Max Reward: 44.72
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.7         |
| time/                   |               |
|    fps                  | 891           |
|    iterations           | 610           |
|    time_elapsed         | 2802          |
|    total_timesteps      | 2498560       |
| train/                  |               |
|    approx_kl            | 0.00063283276 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -243          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -35.3         |
|    n_updates            | 6090          |
|    policy_gradient_loss | -0.000446     |
|    std                  | 1.27          |
|    value_loss           | 1.29          |
-------------------------------------------
Iteration: 611 | Episodes: 24800 | Median Reward: 40.32 | Avg Reward: 40.64 | Max Reward: 44.72
Iteration: 613 | Episodes: 24900 | Median Reward: 40.98 | Avg Reward: 41.18 | Max Reward: 44.72
Iteration: 616 | Episodes: 25000 | Median Reward: 41.90 | Avg Reward: 41.01 | Max Reward: 44.72
Iteration: 618 | Episodes: 25100 | Median Reward: 40.59 | Avg Reward: 40.60 | Max Reward: 44.72
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.6         |
| time/                   |               |
|    fps                  | 891           |
|    iterations           | 620           |
|    time_elapsed         | 2848          |
|    total_timesteps      | 2539520       |
| train/                  |               |
|    approx_kl            | 0.00033676458 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -244          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -33.4         |
|    n_updates            | 6190          |
|    policy_gradient_loss | 1.51e-05      |
|    std                  | 1.28          |
|    value_loss           | 2.56          |
-------------------------------------------
Iteration: 621 | Episodes: 25200 | Median Reward: 40.35 | Avg Reward: 40.09 | Max Reward: 44.72
Iteration: 623 | Episodes: 25300 | Median Reward: 40.01 | Avg Reward: 40.59 | Max Reward: 44.72
Iteration: 626 | Episodes: 25400 | Median Reward: 39.50 | Avg Reward: 39.20 | Max Reward: 44.72
Iteration: 628 | Episodes: 25500 | Median Reward: 40.80 | Avg Reward: 40.67 | Max Reward: 44.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61          |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 630          |
|    time_elapsed         | 2893         |
|    total_timesteps      | 2580480      |
| train/                  |              |
|    approx_kl            | 0.0018748397 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -246         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -36.4        |
|    n_updates            | 6290         |
|    policy_gradient_loss | -0.00961     |
|    std                  | 1.28         |
|    value_loss           | 1.44         |
------------------------------------------
Iteration: 631 | Episodes: 25600 | Median Reward: 39.30 | Avg Reward: 38.90 | Max Reward: 44.72
Iteration: 633 | Episodes: 25700 | Median Reward: 41.51 | Avg Reward: 41.29 | Max Reward: 44.72
Iteration: 636 | Episodes: 25800 | Median Reward: 41.48 | Avg Reward: 40.20 | Max Reward: 44.72
Iteration: 638 | Episodes: 25900 | Median Reward: 42.85 | Avg Reward: 41.73 | Max Reward: 44.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59          |
| time/                   |              |
|    fps                  | 892          |
|    iterations           | 640          |
|    time_elapsed         | 2938         |
|    total_timesteps      | 2621440      |
| train/                  |              |
|    approx_kl            | 0.0047168704 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -247         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -34.9        |
|    n_updates            | 6390         |
|    policy_gradient_loss | -0.0169      |
|    std                  | 1.29         |
|    value_loss           | 2.85         |
------------------------------------------
Iteration: 641 | Episodes: 26000 | Median Reward: 40.53 | Avg Reward: 40.24 | Max Reward: 44.72
Iteration: 643 | Episodes: 26100 | Median Reward: 40.50 | Avg Reward: 39.68 | Max Reward: 44.72
Iteration: 646 | Episodes: 26200 | Median Reward: 40.23 | Avg Reward: 40.39 | Max Reward: 44.72
Iteration: 648 | Episodes: 26300 | Median Reward: 42.42 | Avg Reward: 40.62 | Max Reward: 44.72
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.1       |
| time/                   |             |
|    fps                  | 892         |
|    iterations           | 650         |
|    time_elapsed         | 2984        |
|    total_timesteps      | 2662400     |
| train/                  |             |
|    approx_kl            | 0.001042947 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -248        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -36.9       |
|    n_updates            | 6490        |
|    policy_gradient_loss | -0.00295    |
|    std                  | 1.3         |
|    value_loss           | 1.22        |
-----------------------------------------
Iteration: 650 | Episodes: 26400 | Median Reward: 42.63 | Avg Reward: 41.82 | Max Reward: 44.72
Iteration: 653 | Episodes: 26500 | Median Reward: 40.68 | Avg Reward: 40.72 | Max Reward: 44.72
Iteration: 655 | Episodes: 26600 | Median Reward: 41.91 | Avg Reward: 41.51 | Max Reward: 44.72
Iteration: 658 | Episodes: 26700 | Median Reward: 40.80 | Avg Reward: 41.19 | Max Reward: 45.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.4         |
| time/                   |               |
|    fps                  | 892           |
|    iterations           | 660           |
|    time_elapsed         | 3028          |
|    total_timesteps      | 2703360       |
| train/                  |               |
|    approx_kl            | 0.00063837063 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -249          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -36.9         |
|    n_updates            | 6590          |
|    policy_gradient_loss | 0.0009        |
|    std                  | 1.31          |
|    value_loss           | 0.961         |
-------------------------------------------
Iteration: 660 | Episodes: 26800 | Median Reward: 40.82 | Avg Reward: 40.61 | Max Reward: 45.71
Iteration: 663 | Episodes: 26900 | Median Reward: 42.63 | Avg Reward: 41.92 | Max Reward: 45.71
Iteration: 665 | Episodes: 27000 | Median Reward: 40.65 | Avg Reward: 40.67 | Max Reward: 45.71
Iteration: 668 | Episodes: 27100 | Median Reward: 41.14 | Avg Reward: 40.88 | Max Reward: 45.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.7       |
| time/                   |             |
|    fps                  | 892         |
|    iterations           | 670         |
|    time_elapsed         | 3075        |
|    total_timesteps      | 2744320     |
| train/                  |             |
|    approx_kl            | 0.013047181 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -250        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -36         |
|    n_updates            | 6690        |
|    policy_gradient_loss | -0.0271     |
|    std                  | 1.31        |
|    value_loss           | 3.63        |
-----------------------------------------
Iteration: 670 | Episodes: 27200 | Median Reward: 41.44 | Avg Reward: 40.79 | Max Reward: 45.71
Iteration: 673 | Episodes: 27300 | Median Reward: 41.89 | Avg Reward: 41.56 | Max Reward: 45.71
Iteration: 675 | Episodes: 27400 | Median Reward: 42.51 | Avg Reward: 41.40 | Max Reward: 45.71
Iteration: 678 | Episodes: 27500 | Median Reward: 40.82 | Avg Reward: 40.64 | Max Reward: 45.71
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -58.7      |
| time/                   |            |
|    fps                  | 891        |
|    iterations           | 680        |
|    time_elapsed         | 3122       |
|    total_timesteps      | 2785280    |
| train/                  |            |
|    approx_kl            | 0.01616721 |
|    clip_fraction        | 0.025      |
|    clip_range           | 0.3        |
|    entropy_loss         | -251       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -36.6      |
|    n_updates            | 6790       |
|    policy_gradient_loss | -0.0239    |
|    std                  | 1.32       |
|    value_loss           | 2.56       |
----------------------------------------
Iteration: 680 | Episodes: 27600 | Median Reward: 40.80 | Avg Reward: 40.95 | Max Reward: 45.71
Iteration: 683 | Episodes: 27700 | Median Reward: 41.13 | Avg Reward: 40.33 | Max Reward: 45.71
Iteration: 685 | Episodes: 27800 | Median Reward: 42.61 | Avg Reward: 40.85 | Max Reward: 46.56
Iteration: 687 | Episodes: 27900 | Median Reward: 42.52 | Avg Reward: 41.78 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.4        |
| time/                   |              |
|    fps                  | 892          |
|    iterations           | 690          |
|    time_elapsed         | 3167         |
|    total_timesteps      | 2826240      |
| train/                  |              |
|    approx_kl            | 0.0044747572 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -252         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -37.7        |
|    n_updates            | 6890         |
|    policy_gradient_loss | -0.000398    |
|    std                  | 1.33         |
|    value_loss           | 0.635        |
------------------------------------------
Iteration: 690 | Episodes: 28000 | Median Reward: 40.77 | Avg Reward: 40.48 | Max Reward: 46.56
Iteration: 692 | Episodes: 28100 | Median Reward: 42.25 | Avg Reward: 41.80 | Max Reward: 46.56
Iteration: 695 | Episodes: 28200 | Median Reward: 41.17 | Avg Reward: 40.99 | Max Reward: 46.56
Iteration: 697 | Episodes: 28300 | Median Reward: 42.16 | Avg Reward: 41.59 | Max Reward: 46.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.6       |
| time/                   |             |
|    fps                  | 891         |
|    iterations           | 700         |
|    time_elapsed         | 3217        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.029127695 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -253        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -37.8       |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.0314     |
|    std                  | 1.34        |
|    value_loss           | 1.4         |
-----------------------------------------
Iteration: 700 | Episodes: 28400 | Median Reward: 42.64 | Avg Reward: 41.70 | Max Reward: 46.56
Iteration: 702 | Episodes: 28500 | Median Reward: 41.99 | Avg Reward: 41.29 | Max Reward: 46.56
Iteration: 705 | Episodes: 28600 | Median Reward: 43.71 | Avg Reward: 42.26 | Max Reward: 46.56
Iteration: 707 | Episodes: 28700 | Median Reward: 42.59 | Avg Reward: 41.69 | Max Reward: 46.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.6       |
| time/                   |             |
|    fps                  | 891         |
|    iterations           | 710         |
|    time_elapsed         | 3263        |
|    total_timesteps      | 2908160     |
| train/                  |             |
|    approx_kl            | 9.22731e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -254        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -32.2       |
|    n_updates            | 7090        |
|    policy_gradient_loss | -2.26e-05   |
|    std                  | 1.35        |
|    value_loss           | 7.34        |
-----------------------------------------
Iteration: 710 | Episodes: 28800 | Median Reward: 39.99 | Avg Reward: 41.28 | Max Reward: 46.56
Iteration: 712 | Episodes: 28900 | Median Reward: 42.52 | Avg Reward: 41.57 | Max Reward: 46.56
Iteration: 715 | Episodes: 29000 | Median Reward: 40.64 | Avg Reward: 40.43 | Max Reward: 46.56
Iteration: 717 | Episodes: 29100 | Median Reward: 42.54 | Avg Reward: 40.75 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.2        |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 720          |
|    time_elapsed         | 3308         |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0006632935 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -253         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -37.9        |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.000894    |
|    std                  | 1.35         |
|    value_loss           | 0.609        |
------------------------------------------
Iteration: 720 | Episodes: 29200 | Median Reward: 43.04 | Avg Reward: 41.75 | Max Reward: 46.56
Iteration: 722 | Episodes: 29300 | Median Reward: 38.66 | Avg Reward: 39.69 | Max Reward: 46.56
Iteration: 724 | Episodes: 29400 | Median Reward: 41.96 | Avg Reward: 40.91 | Max Reward: 46.56
Iteration: 727 | Episodes: 29500 | Median Reward: 41.15 | Avg Reward: 41.05 | Max Reward: 46.56
Iteration: 729 | Episodes: 29600 | Median Reward: 41.51 | Avg Reward: 41.60 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.4        |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 730          |
|    time_elapsed         | 3354         |
|    total_timesteps      | 2990080      |
| train/                  |              |
|    approx_kl            | 0.0004364502 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -254         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -37.4        |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 1.36         |
|    value_loss           | 1.17         |
------------------------------------------
Iteration: 732 | Episodes: 29700 | Median Reward: 41.67 | Avg Reward: 41.34 | Max Reward: 46.56
Iteration: 734 | Episodes: 29800 | Median Reward: 41.02 | Avg Reward: 41.55 | Max Reward: 46.56
Iteration: 737 | Episodes: 29900 | Median Reward: 41.51 | Avg Reward: 41.37 | Max Reward: 46.56
Iteration: 739 | Episodes: 30000 | Median Reward: 41.83 | Avg Reward: 40.87 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.3        |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 740          |
|    time_elapsed         | 3399         |
|    total_timesteps      | 3031040      |
| train/                  |              |
|    approx_kl            | 0.0021141486 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -255         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -37.8        |
|    n_updates            | 7390         |
|    policy_gradient_loss | -0.00444     |
|    std                  | 1.37         |
|    value_loss           | 0.521        |
------------------------------------------
Iteration: 742 | Episodes: 30100 | Median Reward: 42.55 | Avg Reward: 41.80 | Max Reward: 46.56
Iteration: 744 | Episodes: 30200 | Median Reward: 41.46 | Avg Reward: 41.29 | Max Reward: 46.56
Iteration: 747 | Episodes: 30300 | Median Reward: 42.72 | Avg Reward: 41.09 | Max Reward: 46.56
Iteration: 749 | Episodes: 30400 | Median Reward: 42.84 | Avg Reward: 42.25 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.7         |
| time/                   |               |
|    fps                  | 891           |
|    iterations           | 750           |
|    time_elapsed         | 3445          |
|    total_timesteps      | 3072000       |
| train/                  |               |
|    approx_kl            | 0.00028721266 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -255          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -37.7         |
|    n_updates            | 7490          |
|    policy_gradient_loss | -0.000506     |
|    std                  | 1.38          |
|    value_loss           | 1.86          |
-------------------------------------------
Iteration: 752 | Episodes: 30500 | Median Reward: 41.99 | Avg Reward: 40.79 | Max Reward: 46.56
Iteration: 754 | Episodes: 30600 | Median Reward: 40.90 | Avg Reward: 40.68 | Max Reward: 46.56
Iteration: 757 | Episodes: 30700 | Median Reward: 42.87 | Avg Reward: 42.23 | Max Reward: 46.56
Iteration: 759 | Episodes: 30800 | Median Reward: 39.56 | Avg Reward: 40.39 | Max Reward: 46.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.5       |
| time/                   |             |
|    fps                  | 891         |
|    iterations           | 760         |
|    time_elapsed         | 3491        |
|    total_timesteps      | 3112960     |
| train/                  |             |
|    approx_kl            | 0.038499106 |
|    clip_fraction        | 0.074       |
|    clip_range           | 0.3         |
|    entropy_loss         | -256        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -38.1       |
|    n_updates            | 7590        |
|    policy_gradient_loss | -0.0406     |
|    std                  | 1.38        |
|    value_loss           | 2.76        |
-----------------------------------------
Iteration: 761 | Episodes: 30900 | Median Reward: 41.04 | Avg Reward: 41.09 | Max Reward: 46.56
Iteration: 764 | Episodes: 31000 | Median Reward: 40.36 | Avg Reward: 40.75 | Max Reward: 46.56
Iteration: 766 | Episodes: 31100 | Median Reward: 41.28 | Avg Reward: 40.96 | Max Reward: 46.56
Iteration: 769 | Episodes: 31200 | Median Reward: 42.62 | Avg Reward: 41.42 | Max Reward: 46.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.3       |
| time/                   |             |
|    fps                  | 891         |
|    iterations           | 770         |
|    time_elapsed         | 3537        |
|    total_timesteps      | 3153920     |
| train/                  |             |
|    approx_kl            | 0.003487623 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -256        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -38.3       |
|    n_updates            | 7690        |
|    policy_gradient_loss | -0.00222    |
|    std                  | 1.39        |
|    value_loss           | 0.957       |
-----------------------------------------
Iteration: 771 | Episodes: 31300 | Median Reward: 42.25 | Avg Reward: 41.41 | Max Reward: 46.56
Iteration: 774 | Episodes: 31400 | Median Reward: 42.17 | Avg Reward: 41.41 | Max Reward: 46.56
Iteration: 776 | Episodes: 31500 | Median Reward: 41.97 | Avg Reward: 41.51 | Max Reward: 46.56
Iteration: 779 | Episodes: 31600 | Median Reward: 42.64 | Avg Reward: 41.11 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.8        |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 780          |
|    time_elapsed         | 3585         |
|    total_timesteps      | 3194880      |
| train/                  |              |
|    approx_kl            | 0.0012269749 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -257         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -36.5        |
|    n_updates            | 7790         |
|    policy_gradient_loss | -0.000543    |
|    std                  | 1.4          |
|    value_loss           | 1.83         |
------------------------------------------
Iteration: 781 | Episodes: 31700 | Median Reward: 41.55 | Avg Reward: 40.73 | Max Reward: 46.56
Iteration: 784 | Episodes: 31800 | Median Reward: 42.23 | Avg Reward: 40.27 | Max Reward: 46.56
Iteration: 786 | Episodes: 31900 | Median Reward: 42.64 | Avg Reward: 42.43 | Max Reward: 46.56
Iteration: 789 | Episodes: 32000 | Median Reward: 43.01 | Avg Reward: 42.73 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.6         |
| time/                   |               |
|    fps                  | 890           |
|    iterations           | 790           |
|    time_elapsed         | 3632          |
|    total_timesteps      | 3235840       |
| train/                  |               |
|    approx_kl            | 5.7514975e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -258          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -38.2         |
|    n_updates            | 7890          |
|    policy_gradient_loss | -0.000285     |
|    std                  | 1.41          |
|    value_loss           | 1.75          |
-------------------------------------------
Iteration: 791 | Episodes: 32100 | Median Reward: 42.62 | Avg Reward: 41.90 | Max Reward: 46.56
Iteration: 793 | Episodes: 32200 | Median Reward: 42.59 | Avg Reward: 41.23 | Max Reward: 46.56
Iteration: 796 | Episodes: 32300 | Median Reward: 42.37 | Avg Reward: 41.61 | Max Reward: 46.56
Iteration: 798 | Episodes: 32400 | Median Reward: 42.82 | Avg Reward: 41.92 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 800          |
|    time_elapsed         | 3677         |
|    total_timesteps      | 3276800      |
| train/                  |              |
|    approx_kl            | 0.0010410487 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -259         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -37.1        |
|    n_updates            | 7990         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 1.42         |
|    value_loss           | 1.2          |
------------------------------------------
Iteration: 801 | Episodes: 32500 | Median Reward: 40.60 | Avg Reward: 40.96 | Max Reward: 46.56
Iteration: 803 | Episodes: 32600 | Median Reward: 42.61 | Avg Reward: 40.84 | Max Reward: 46.56
Iteration: 806 | Episodes: 32700 | Median Reward: 39.66 | Avg Reward: 39.92 | Max Reward: 46.56
Iteration: 808 | Episodes: 32800 | Median Reward: 42.27 | Avg Reward: 40.97 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.3         |
| time/                   |               |
|    fps                  | 890           |
|    iterations           | 810           |
|    time_elapsed         | 3726          |
|    total_timesteps      | 3317760       |
| train/                  |               |
|    approx_kl            | 0.00016097144 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -259          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -36.2         |
|    n_updates            | 8090          |
|    policy_gradient_loss | -0.00166      |
|    std                  | 1.43          |
|    value_loss           | 5.81          |
-------------------------------------------
Iteration: 811 | Episodes: 32900 | Median Reward: 40.08 | Avg Reward: 40.27 | Max Reward: 46.56
Iteration: 813 | Episodes: 33000 | Median Reward: 42.20 | Avg Reward: 41.37 | Max Reward: 46.56
Iteration: 816 | Episodes: 33100 | Median Reward: 42.75 | Avg Reward: 41.27 | Max Reward: 46.56
Iteration: 818 | Episodes: 33200 | Median Reward: 43.05 | Avg Reward: 42.75 | Max Reward: 46.56
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -57.7      |
| time/                   |            |
|    fps                  | 890        |
|    iterations           | 820        |
|    time_elapsed         | 3772       |
|    total_timesteps      | 3358720    |
| train/                  |            |
|    approx_kl            | 0.07043858 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.3        |
|    entropy_loss         | -260       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -38.8      |
|    n_updates            | 8190       |
|    policy_gradient_loss | -0.0552    |
|    std                  | 1.44       |
|    value_loss           | 0.353      |
----------------------------------------
Iteration: 821 | Episodes: 33300 | Median Reward: 41.47 | Avg Reward: 40.57 | Max Reward: 46.56
Iteration: 823 | Episodes: 33400 | Median Reward: 42.81 | Avg Reward: 41.89 | Max Reward: 46.56
Iteration: 826 | Episodes: 33500 | Median Reward: 41.99 | Avg Reward: 40.85 | Max Reward: 46.56
Iteration: 828 | Episodes: 33600 | Median Reward: 42.41 | Avg Reward: 41.86 | Max Reward: 46.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.4       |
| time/                   |             |
|    fps                  | 890         |
|    iterations           | 830         |
|    time_elapsed         | 3818        |
|    total_timesteps      | 3399680     |
| train/                  |             |
|    approx_kl            | 0.014988314 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.3         |
|    entropy_loss         | -261        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -38.8       |
|    n_updates            | 8290        |
|    policy_gradient_loss | -0.0104     |
|    std                  | 1.45        |
|    value_loss           | 0.569       |
-----------------------------------------
Iteration: 830 | Episodes: 33700 | Median Reward: 41.51 | Avg Reward: 41.26 | Max Reward: 46.56
Iteration: 833 | Episodes: 33800 | Median Reward: 40.64 | Avg Reward: 40.62 | Max Reward: 46.56
Iteration: 835 | Episodes: 33900 | Median Reward: 41.80 | Avg Reward: 41.30 | Max Reward: 46.56
Iteration: 838 | Episodes: 34000 | Median Reward: 42.89 | Avg Reward: 42.35 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.2        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 840          |
|    time_elapsed         | 3864         |
|    total_timesteps      | 3440640      |
| train/                  |              |
|    approx_kl            | 0.0065675783 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -262         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -37.8        |
|    n_updates            | 8390         |
|    policy_gradient_loss | -0.0056      |
|    std                  | 1.46         |
|    value_loss           | 1.2          |
------------------------------------------
Iteration: 840 | Episodes: 34100 | Median Reward: 42.57 | Avg Reward: 40.93 | Max Reward: 46.56
Iteration: 843 | Episodes: 34200 | Median Reward: 40.61 | Avg Reward: 41.24 | Max Reward: 46.56
Iteration: 845 | Episodes: 34300 | Median Reward: 40.54 | Avg Reward: 40.43 | Max Reward: 46.56
Iteration: 848 | Episodes: 34400 | Median Reward: 42.63 | Avg Reward: 41.21 | Max Reward: 46.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.8       |
| time/                   |             |
|    fps                  | 890         |
|    iterations           | 850         |
|    time_elapsed         | 3910        |
|    total_timesteps      | 3481600     |
| train/                  |             |
|    approx_kl            | 0.013639596 |
|    clip_fraction        | 0.000488    |
|    clip_range           | 0.3         |
|    entropy_loss         | -263        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -39.3       |
|    n_updates            | 8490        |
|    policy_gradient_loss | -0.00571    |
|    std                  | 1.47        |
|    value_loss           | 0.348       |
-----------------------------------------
Iteration: 850 | Episodes: 34500 | Median Reward: 42.61 | Avg Reward: 42.43 | Max Reward: 46.56
Iteration: 853 | Episodes: 34600 | Median Reward: 41.15 | Avg Reward: 41.16 | Max Reward: 46.56
Iteration: 855 | Episodes: 34700 | Median Reward: 41.46 | Avg Reward: 41.07 | Max Reward: 46.56
Iteration: 858 | Episodes: 34800 | Median Reward: 41.74 | Avg Reward: 41.24 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.8        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 860          |
|    time_elapsed         | 3956         |
|    total_timesteps      | 3522560      |
| train/                  |              |
|    approx_kl            | 0.0012047122 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -265         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -39.5        |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.00694     |
|    std                  | 1.48         |
|    value_loss           | 1.04         |
------------------------------------------
Iteration: 860 | Episodes: 34900 | Median Reward: 41.83 | Avg Reward: 41.35 | Max Reward: 46.56
Iteration: 863 | Episodes: 35000 | Median Reward: 41.86 | Avg Reward: 42.09 | Max Reward: 46.56
Iteration: 865 | Episodes: 35100 | Median Reward: 41.35 | Avg Reward: 40.91 | Max Reward: 46.56
Iteration: 867 | Episodes: 35200 | Median Reward: 41.31 | Avg Reward: 41.69 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.8        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 870          |
|    time_elapsed         | 4002         |
|    total_timesteps      | 3563520      |
| train/                  |              |
|    approx_kl            | 1.078684e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -265         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -39.2        |
|    n_updates            | 8690         |
|    policy_gradient_loss | 4.2e-05      |
|    std                  | 1.48         |
|    value_loss           | 2.77         |
------------------------------------------
Iteration: 870 | Episodes: 35300 | Median Reward: 43.48 | Avg Reward: 42.30 | Max Reward: 46.56
Iteration: 872 | Episodes: 35400 | Median Reward: 42.68 | Avg Reward: 41.61 | Max Reward: 46.56
Iteration: 875 | Episodes: 35500 | Median Reward: 42.62 | Avg Reward: 42.22 | Max Reward: 46.56
Iteration: 877 | Episodes: 35600 | Median Reward: 42.58 | Avg Reward: 41.58 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.3        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 880          |
|    time_elapsed         | 4049         |
|    total_timesteps      | 3604480      |
| train/                  |              |
|    approx_kl            | 0.0077404417 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -266         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -39.7        |
|    n_updates            | 8790         |
|    policy_gradient_loss | -0.0224      |
|    std                  | 1.49         |
|    value_loss           | 0.373        |
------------------------------------------
Iteration: 880 | Episodes: 35700 | Median Reward: 40.54 | Avg Reward: 40.70 | Max Reward: 46.56
Iteration: 882 | Episodes: 35800 | Median Reward: 43.05 | Avg Reward: 41.69 | Max Reward: 46.56
Iteration: 885 | Episodes: 35900 | Median Reward: 41.93 | Avg Reward: 41.91 | Max Reward: 46.56
Iteration: 887 | Episodes: 36000 | Median Reward: 43.08 | Avg Reward: 42.25 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 889           |
|    iterations           | 890           |
|    time_elapsed         | 4096          |
|    total_timesteps      | 3645440       |
| train/                  |               |
|    approx_kl            | 0.00015551364 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -267          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -39.7         |
|    n_updates            | 8890          |
|    policy_gradient_loss | -0.000429     |
|    std                  | 1.5           |
|    value_loss           | 0.844         |
-------------------------------------------
Iteration: 890 | Episodes: 36100 | Median Reward: 43.45 | Avg Reward: 42.39 | Max Reward: 46.56
Iteration: 892 | Episodes: 36200 | Median Reward: 42.30 | Avg Reward: 42.09 | Max Reward: 46.56
Iteration: 895 | Episodes: 36300 | Median Reward: 42.88 | Avg Reward: 41.54 | Max Reward: 46.56
Iteration: 897 | Episodes: 36400 | Median Reward: 41.91 | Avg Reward: 40.93 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.2         |
| time/                   |               |
|    fps                  | 890           |
|    iterations           | 900           |
|    time_elapsed         | 4141          |
|    total_timesteps      | 3686400       |
| train/                  |               |
|    approx_kl            | 0.00047730946 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -268          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -39.1         |
|    n_updates            | 8990          |
|    policy_gradient_loss | -0.000813     |
|    std                  | 1.51          |
|    value_loss           | 1.26          |
-------------------------------------------
Iteration: 900 | Episodes: 36500 | Median Reward: 42.80 | Avg Reward: 41.84 | Max Reward: 46.56
Iteration: 902 | Episodes: 36600 | Median Reward: 40.80 | Avg Reward: 41.26 | Max Reward: 46.56
Iteration: 904 | Episodes: 36700 | Median Reward: 41.30 | Avg Reward: 41.45 | Max Reward: 46.56
Iteration: 907 | Episodes: 36800 | Median Reward: 41.08 | Avg Reward: 41.56 | Max Reward: 46.56
Iteration: 909 | Episodes: 36900 | Median Reward: 40.80 | Avg Reward: 40.85 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.2         |
| time/                   |               |
|    fps                  | 890           |
|    iterations           | 910           |
|    time_elapsed         | 4187          |
|    total_timesteps      | 3727360       |
| train/                  |               |
|    approx_kl            | 0.00016282561 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -268          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -39.9         |
|    n_updates            | 9090          |
|    policy_gradient_loss | -0.000146     |
|    std                  | 1.52          |
|    value_loss           | 0.822         |
-------------------------------------------
Iteration: 912 | Episodes: 37000 | Median Reward: 41.10 | Avg Reward: 41.03 | Max Reward: 46.56
Iteration: 914 | Episodes: 37100 | Median Reward: 42.70 | Avg Reward: 41.81 | Max Reward: 46.56
Iteration: 917 | Episodes: 37200 | Median Reward: 42.39 | Avg Reward: 42.29 | Max Reward: 46.56
Iteration: 919 | Episodes: 37300 | Median Reward: 43.06 | Avg Reward: 42.49 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 920          |
|    time_elapsed         | 4233         |
|    total_timesteps      | 3768320      |
| train/                  |              |
|    approx_kl            | 0.0010769609 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -268         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -40          |
|    n_updates            | 9190         |
|    policy_gradient_loss | -0.00337     |
|    std                  | 1.52         |
|    value_loss           | 2.26         |
------------------------------------------
Iteration: 922 | Episodes: 37400 | Median Reward: 42.62 | Avg Reward: 42.23 | Max Reward: 46.56
Iteration: 924 | Episodes: 37500 | Median Reward: 41.11 | Avg Reward: 41.01 | Max Reward: 46.56
Iteration: 927 | Episodes: 37600 | Median Reward: 42.98 | Avg Reward: 41.83 | Max Reward: 46.56
Iteration: 929 | Episodes: 37700 | Median Reward: 39.81 | Avg Reward: 39.85 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.2        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 930          |
|    time_elapsed         | 4279         |
|    total_timesteps      | 3809280      |
| train/                  |              |
|    approx_kl            | 0.0017908206 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -269         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -40.3        |
|    n_updates            | 9290         |
|    policy_gradient_loss | -0.00388     |
|    std                  | 1.53         |
|    value_loss           | 0.542        |
------------------------------------------
Iteration: 932 | Episodes: 37800 | Median Reward: 41.10 | Avg Reward: 41.35 | Max Reward: 46.56
Iteration: 934 | Episodes: 37900 | Median Reward: 42.25 | Avg Reward: 41.93 | Max Reward: 46.56
Iteration: 937 | Episodes: 38000 | Median Reward: 43.02 | Avg Reward: 42.44 | Max Reward: 46.56
Iteration: 939 | Episodes: 38100 | Median Reward: 41.86 | Avg Reward: 40.76 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 940          |
|    time_elapsed         | 4324         |
|    total_timesteps      | 3850240      |
| train/                  |              |
|    approx_kl            | 0.0015414031 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -270         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -39          |
|    n_updates            | 9390         |
|    policy_gradient_loss | -0.008       |
|    std                  | 1.54         |
|    value_loss           | 1.97         |
------------------------------------------
Iteration: 941 | Episodes: 38200 | Median Reward: 42.87 | Avg Reward: 42.52 | Max Reward: 46.56
Iteration: 944 | Episodes: 38300 | Median Reward: 43.03 | Avg Reward: 41.88 | Max Reward: 46.56
Iteration: 946 | Episodes: 38400 | Median Reward: 42.39 | Avg Reward: 41.32 | Max Reward: 46.56
Iteration: 949 | Episodes: 38500 | Median Reward: 43.07 | Avg Reward: 42.56 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.5        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 950          |
|    time_elapsed         | 4371         |
|    total_timesteps      | 3891200      |
| train/                  |              |
|    approx_kl            | 0.0004860329 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -271         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -40.4        |
|    n_updates            | 9490         |
|    policy_gradient_loss | -0.000783    |
|    std                  | 1.55         |
|    value_loss           | 0.917        |
------------------------------------------
Iteration: 951 | Episodes: 38600 | Median Reward: 42.68 | Avg Reward: 41.48 | Max Reward: 46.56
Iteration: 954 | Episodes: 38700 | Median Reward: 40.86 | Avg Reward: 40.76 | Max Reward: 46.56
Iteration: 956 | Episodes: 38800 | Median Reward: 40.45 | Avg Reward: 40.29 | Max Reward: 46.56
Iteration: 959 | Episodes: 38900 | Median Reward: 43.37 | Avg Reward: 42.56 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 890           |
|    iterations           | 960           |
|    time_elapsed         | 4415          |
|    total_timesteps      | 3932160       |
| train/                  |               |
|    approx_kl            | 0.00022853061 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -271          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -39.5         |
|    n_updates            | 9590          |
|    policy_gradient_loss | -0.000832     |
|    std                  | 1.56          |
|    value_loss           | 2.1           |
-------------------------------------------
Iteration: 961 | Episodes: 39000 | Median Reward: 40.98 | Avg Reward: 40.74 | Max Reward: 46.56
Iteration: 964 | Episodes: 39100 | Median Reward: 42.87 | Avg Reward: 42.17 | Max Reward: 46.56
Iteration: 966 | Episodes: 39200 | Median Reward: 43.09 | Avg Reward: 41.65 | Max Reward: 46.56
Iteration: 969 | Episodes: 39300 | Median Reward: 42.01 | Avg Reward: 41.66 | Max Reward: 46.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.9       |
| time/                   |             |
|    fps                  | 890         |
|    iterations           | 970         |
|    time_elapsed         | 4460        |
|    total_timesteps      | 3973120     |
| train/                  |             |
|    approx_kl            | 0.008815462 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -272        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -39.7       |
|    n_updates            | 9690        |
|    policy_gradient_loss | -0.00946    |
|    std                  | 1.57        |
|    value_loss           | 1.2         |
-----------------------------------------
Iteration: 971 | Episodes: 39400 | Median Reward: 43.73 | Avg Reward: 42.33 | Max Reward: 46.56
Iteration: 973 | Episodes: 39500 | Median Reward: 42.30 | Avg Reward: 41.87 | Max Reward: 46.56
Iteration: 976 | Episodes: 39600 | Median Reward: 42.00 | Avg Reward: 40.67 | Max Reward: 46.56
Iteration: 978 | Episodes: 39700 | Median Reward: 42.35 | Avg Reward: 41.54 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 890          |
|    iterations           | 980          |
|    time_elapsed         | 4508         |
|    total_timesteps      | 4014080      |
| train/                  |              |
|    approx_kl            | 0.0063152136 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -273         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -40.8        |
|    n_updates            | 9790         |
|    policy_gradient_loss | -0.00716     |
|    std                  | 1.58         |
|    value_loss           | 0.733        |
------------------------------------------
Iteration: 981 | Episodes: 39800 | Median Reward: 42.25 | Avg Reward: 41.54 | Max Reward: 46.56
Iteration: 983 | Episodes: 39900 | Median Reward: 40.46 | Avg Reward: 40.91 | Max Reward: 46.56
Iteration: 986 | Episodes: 40000 | Median Reward: 40.20 | Avg Reward: 40.46 | Max Reward: 46.56
Iteration: 988 | Episodes: 40100 | Median Reward: 40.55 | Avg Reward: 40.96 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.9         |
| time/                   |               |
|    fps                  | 890           |
|    iterations           | 990           |
|    time_elapsed         | 4554          |
|    total_timesteps      | 4055040       |
| train/                  |               |
|    approx_kl            | 2.6336114e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -274          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -40.6         |
|    n_updates            | 9890          |
|    policy_gradient_loss | 4.36e-05      |
|    std                  | 1.6           |
|    value_loss           | 1.7           |
-------------------------------------------
Iteration: 991 | Episodes: 40200 | Median Reward: 41.12 | Avg Reward: 40.48 | Max Reward: 46.56
Iteration: 993 | Episodes: 40300 | Median Reward: 42.84 | Avg Reward: 42.45 | Max Reward: 46.56
Iteration: 996 | Episodes: 40400 | Median Reward: 42.33 | Avg Reward: 41.90 | Max Reward: 46.56
Iteration: 998 | Episodes: 40500 | Median Reward: 40.23 | Avg Reward: 40.18 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.8         |
| time/                   |               |
|    fps                  | 890           |
|    iterations           | 1000          |
|    time_elapsed         | 4599          |
|    total_timesteps      | 4096000       |
| train/                  |               |
|    approx_kl            | 0.00061425235 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -274          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -40.3         |
|    n_updates            | 9990          |
|    policy_gradient_loss | 0.000188      |
|    std                  | 1.61          |
|    value_loss           | 1.73          |
-------------------------------------------
Training End | Episodes: 40552 | Median Reward: 39.62 | Avg Reward: 40.21 | Max Reward: 46.56
Plot saved as fig_code1_1_residual_e.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> nano code1_1_red[Ksidual_e.py
[?2004h[?1049h[22;0;0t[1;53r(B[m[4l[?7h[39;49m[?1h=[?1h=[?25l[39;49m(B[m[H[2J[51;95H(B[0;7m[ Reading... ](B[m[51;93H(B[0;7m[ Read 420 lines ](B[m[H(B[0;7m  GNU nano 4.8                                                                               code1_1_residual_e.py                                                                                         [1;202H(B[m[52d(B[0;7m^G(B[m Get Help     (B[0;7m^O(B[m Write Out    (B[0;7m^W(B[m Where Is     (B[0;7m^K(B[m Cut Text     (B[0;7m^J(B[m Justify[81G(B[0;7m^C(B[m Cur Pos[97G(B[0;7mM-U(B[m Undo[52;113H(B[0;7mM-A(B[m Mark Text   (B[0;7mM-](B[m To Bracket  (B[0;7mM-Q(B[m Previous    (B[0;7m^B(B[m Back[52;177H(B[0;7m^◀(B[m Prev Word[53d(B[0;7m^X(B[m Exit[53;17H(B[0;7m^R(B[m Read File    (B[0;7m^\(B[m Replace[49G(B[0;7m^U(B[m Paste Text   (B[0;7m^T(B[m To Spell     (B[0;7m^_(B[m Go To Line   (B[0;7mM-E(B[m Redo[53;113H(B[0;7mM-6(B[m Copy Text   (B[0;7m^Q(B[m Where Was    (B[0;7mM-W(B[m Next[53;161H(B[0;7m^F(B[m Forward[177G(B[0;7m^▶(B[m Next Word[51d[2d(B[0;1m[36mimport[39m(B[m gymnasium (B[0;1m[36mas[39m(B[m gym[3d(B[0;1m[36mfrom[39m(B[m gymnasium (B[0;1m[36mimport[39m(B[m spaces[4d(B[0;1m[36mfrom[39m(B[m statistics (B[0;1m[36mimport[39m(B[m median[5d(B[0;1m[36mimport[39m(B[m numpy (B[0;1m[36mas[39m(B[m np[6d(B[0;1m[36mimport[39m(B[m mujoco_py[7d(B[0;1m[36mimport[39m(B[m os[8d(B[0;1m[36mimport[39m(B[m torch[9d(B[0;1m[36mfrom[39m(B[m torch (B[0;1m[36mimport[39m(B[m nn[10d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.vec_env (B[0;1m[36mimport[39m(B[m DummyVecEnv, VecMonitor[11d(B[0;1m[36mfrom[39m(B[m sb3_contrib (B[0;1m[36mimport[39m(B[m RecurrentPPO[12d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.callbacks (B[0;1m[36mimport[39m(B[m BaseCallback[13d(B[0;1m[36mfrom[39m(B[m sb3_contrib.ppo_recurrent.policies (B[0;1m[36mimport[39m(B[m MlpLstmPolicy[14d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.torch_layers (B[0;1m[36mimport[39m(B[m BaseFeaturesExtractor[15d(B[0;1m[36mimport[39m(B[m logging[16d(B[0;1m[36mimport[39m(B[m matplotlib.pyplot (B[0;1m[36mas[39m(B[m plt[42m  [17d(B[0;1m[36mimport[39m(B[m math[19d(B[0;1m[31m# Set up logging[20d[39m(B[mlogging.basicConfig(level=logging.INFO)[21d(B[0;1m[36mclass[39m(B[m ResidualBlock(nn.Module):[22;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self, size):[23;9Hsuper(ResidualBlock, self).__init__()[42m   [24;9H[49m(B[mself.fc1 = nn.Linear(size, size)[25;9Hself.relu = nn.ReLU()[26;9Hself.fc2 = nn.Linear(size, size)[27;9Hself.layer_norm = nn.LayerNorm(size)[29;5H(B[0;1m[36mdef[34m forward[39m(B[m(self, x):[30;9Hresidual = x[31;9Hout = self.fc1(x)[32;9Hout = self.relu(out)[33;9Hout = self.fc2(out)[34;9Hout += residual (B[0;1m[31m # Residual connection[35;9H[39m(B[mout = self.layer_norm(out)[36;9Hout = self.relu(out)[37;9H(B[0;1m[36mreturn[39m(B[m out[39d(B[0;1m[36mclass[39m(B[m CustomFE(BaseFeaturesExtractor):[40;5H(B[0;1m[32m"""[41d    Custom feature extractor for HandEnv.[42d    Increases network depth and incorporates layer normalization.[43d    """[45;5H[36mdef[34m __init__[39m(B[m(self, observation_space: gym.Space, features_dim: int = 256):[46;8H(B[0;1m[31m # Ensure the observation space is as expected[47;9H[36massert[39m(B[m isinstance(observation_space, spaces.Box), (B[0;1m[32m"Observation space must be of type Box"[49;9H[39m(B[msuper(CustomFE, self).__init__(observation_space, features_dim)[2d[?12l[?25h[?25l[8d[?12l[?25h[?25l[14d[?12l[?25h[?25l[20d[?12l[?25h[?25l[26d[?12l[?25h[?25l[51d[K[32d[?12l[?25h[?25l[38d[?12l[?25h[?25l[44d[?12l[?25h[?25l[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.initial_net = nn.Sequential([46;13Hnn.Linear(observation_space.shape[0], 128),[47;13Hnn.ReLU(),[48;13Hnn.LayerNorm(128)[49;9H)[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;8H(B[0;1m[31m # Add residual blocks[46;9H[39m(B[mself.residual_blocks = nn.Sequential([47;13HResidualBlock(128),[48;13HResidualBlock(128)[49;9H)[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.final_net = nn.Sequential([46;13Hnn.Linear(128, features_dim),[47;13Hnn.ReLU(),[48;13Hnn.LayerNorm(features_dim)[49;9H)[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5H(B[0;1m[36mdef[34m forward[39m(B[m(self, observations: torch.Tensor) -> torch.Tensor:[46;9Hx = self.initial_net(observations)[47;9Hx = self.residual_blocks(x)[48;9Hx = self.final_net(x)[49;9H(B[0;1m[36mreturn[39m(B[m x[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H(B[0;1m[36mclass[39m(B[m CustomACLstmPolicy(MlpLstmPolicy):[46;5H(B[0;1m[32m"""[47d    Custom Actor-Critic Policy with LSTM for RecurrentPPO.[48d    Integrates the CustomFeaturesExtractor.[49d    """[50d[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self, *args, **kwargs):[46;9Hsuper(CustomACLstmPolicy, self).__init__([47;13H*args,[48;13H**kwargs,[49;13Hfeatures_extractor_class=CustomFE,[50;13Hfeatures_extractor_kwargs=dict(features_dim=256),[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Hnet_arch=[dict(pi=[256, 256], vf=[256, 256])], (B[0;1m[31m # Corrected bracket[46;13H[39m(B[mactivation_fn=nn.ReLU,[47;13Hlstm_hidden_size=128 (B[0;1m[31m # Size of LSTM hidden state[48;9H[39m(B[m)[50d(B[0;1m[36mclass[39m(B[m HandEnv(gym.Env):[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self):[46;9Hsuper(HandEnv, self).__init__()[47;9Hxml_path = os.path.expanduser((B[0;1m[32m'/home/easgrad/dgusain/Bot_hand/bot_hand.xml'[39m(B[m)[48;9Hlogging.info(f(B[0;1m[32m"Attempting to load Mujoco model from: {xml_path}"[39m(B[m)[50;9H(B[0;1m[36mif[39m(B[m (B[0;1m[36mnot[39m(B[m os.path.isfile(xml_path):[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Hlogging.error(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[46;13H(B[0;1m[36mraise[39m(B[m FileNotFoundError(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[48;9H(B[0;1m[36mtry[39m(B[m:[49dself.model = mujoco_py.load_model_from_path(xml_path)[50;13Hself.sim = mujoco_py.MjSim(self.model)[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Hlogging.info((B[0;1m[32m"Mujoco model loaded successfully."[39m(B[m)[46;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[47;13Hlogging.error(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[48;13H(B[0;1m[36mraise[39m(B[m RuntimeError(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[50;9Hjoint_weights = [[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H0.5, (B[0;1m[31m # ID 0: WristJoint1[46;13H[39m(B[m0.5, (B[0;1m[31m # ID 1: WristJoint0[47;13H[39m(B[m1.5, (B[0;1m[31m # ID 2: ForeFingerJoint3 (MCP)[48;13H[39m(B[m2.5, (B[0;1m[31m # ID 3: ForeFingerJoint2 (MCP proximal)[49;13H[39m(B[m2.0, (B[0;1m[31m # ID 4: ForeFingerJoint1 (PIP)[50;13H[39m(B[m1.0, (B[0;1m[31m # ID 5: ForeFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H1.5, (B[0;1m[31m # ID 6: MiddleFingerJoint3 (MCP)[46;13H[39m(B[m2.5, (B[0;1m[31m # ID 7: MiddleFingerJoint2 (MCP proximal)[47;13H[39m(B[m2.0, (B[0;1m[31m # ID 8: MiddleFingerJoint1 (PIP)[48;13H[39m(B[m1.0, (B[0;1m[31m # ID 9: MiddleFingerJoint0 (DIP)[49;13H[39m(B[m1.5, (B[0;1m[31m # ID 10: RingFingerJoint3 (MCP)[50;13H[39m(B[m2.5, (B[0;1m[31m # ID 11: RingFingerJoint2 (MCP proximal)[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H2.0, (B[0;1m[31m # ID 12: RingFingerJoint1 (PIP)[46;13H[39m(B[m1.0, (B[0;1m[31m # ID 13: RingFingerJoint0 (DIP)[47;13H[39m(B[m1.5, (B[0;1m[31m # ID 14: LittleFingerJoint4 (CMC Joint rotation)[48;13H[39m(B[m1.5, (B[0;1m[31m # ID 15: LittleFingerJoint3 (MCP)[49;13H[39m(B[m2.5, (B[0;1m[31m # ID 16: LittleFingerJoint2 (MCP proximal)[50;13H[39m(B[m2.0, (B[0;1m[31m # ID 17: LittleFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H1.0, (B[0;1m[31m # ID 18: LittleFingerJoint0 (DIP)[46;13H[39m(B[m1.5, (B[0;1m[31m # ID 19: ThumbJoint4 (CMC Joint abduction/adduction)[47;13H[39m(B[m1.5, (B[0;1m[31m # ID 20: ThumbJoint3 (CMC Joint flexion/extension)[48;13H[39m(B[m1.5, (B[0;1m[31m # ID 21: ThumbJoint2 (CMC Joint rotation)[49;13H[39m(B[m3.0, (B[0;1m[31m # ID 22: ThumbJoint1 (MCP)[50;13H[39m(B[m0.5, (B[0;1m[31m # ID 23: ThumbJoint0 (IP)[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H0.0  (B[0;1m[31m # ID 24: None[46;9H[39m(B[m][47d(B[0;1m[31m # Converting joint weights to a tensor[48;9H[39m(B[mself.joint_weights = torch.tensor(joint_weights, dtype=torch.float32)[49;8H(B[0;1m[31m # Normalize the weights so that their sum equals 1[50;9H[39m(B[mself.joint_weights /= self.joint_weights.sum()[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;9Hself.initial_state = self.sim.get_state()[47;8H(B[0;1m[31m # Define observation and action spaces[48;9H[39m(B[mself.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(24,), dtype=np.float32)[49;9Hself.target_threshold = 99[50;9Hself.max_steps = 100[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.steps_taken = 0[46;9Hself.sim_coeff = 0.5[47;9Hself.ang_coeff = 1 - self.sim_coeff[49;8H(B[0;1m[31m # Initialize actuator ranges[50;9H[39m(B[mactuator_ranges = self.sim.model.actuator_ctrlrange[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.actuator_min = torch.tensor(actuator_ranges[:, 0], dtype=torch.float32)[46;9Hself.actuator_max = torch.tensor(actuator_ranges[:, 1], dtype=torch.float32)[48;9Hself.action_space = spaces.Box(low=-1, high=1, shape=(self.actuator_min.shape[0],), dtype=np.float32)[49;9Hself.ground_truth_quats = torch.tensor(self.get_ground_truth_quaternion(), dtype=torch.float32)[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5H(B[0;1m[36mdef[34m reset[39m(B[m(self, seed=(B[0;1m[35mNone[39m(B[m, options=(B[0;1m[35mNone[39m(B[m):[46;9Hsuper().reset(seed=seed)[47;8H(B[0;1m[31m #self.sim.reset()[48;9H[39m(B[mself.sim.set_state(self.initial_state)[49;9Hself.sim.forward()[50;9Hself.steps_taken = 0[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hobs = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[46;9H(B[0;1m[36mreturn[39m(B[m obs, {}[48;5H(B[0;1m[36mdef[34m step[39m(B[m(self, action: np.ndarray):[49;9Haction_tensor = torch.from_numpy(action).float()[50;9Hrescaled_action = self.actuator_min + (action_tensor + 1) * (self.actuator_max - self.actuator_min) / 2[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.sim.data.ctrl[:] = rescaled_action.numpy()[46;9Hself.sim.step()[48;8H(B[0;1m[31m # Fetch state as a tensor[49;9H[39m(B[mstate = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[50d[42m        [49m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;8H(B[0;1m[31m # Calculate done condition[46;9H[39m(B[mdone = self.calculate_done(state, flag=(B[0;1m[35mFalse[39m(B[m) (B[0;1m[36mor[39m(B[m self.steps_taken >= self.max_steps[47;9H(B[0;1m[36mif[39m(B[m done:[48;13Hreward = self.calculate_reward(state, flag=(B[0;1m[35mTrue[39m(B[m).item()[49;9H(B[0;1m[36melse[39m(B[m:[50dreward = -1.0 (B[0;1m[31m # Penalize extra steps[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;9Hself.steps_taken += 1[47;9Htruncated = (B[0;1m[35mFalse[48;9H[39m(B[minfo = {}[50;9H(B[0;1m[36mreturn[39m(B[m state.numpy(), reward, done, truncated, info[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;5H(B[0;1m[36mdef[34m calculate_done[39m(B[m(self, state: torch.Tensor, flag: bool) -> bool:[47;9H(B[0;1m[36mif[39m(B[m flag:[48;13H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mFalse[49;9H[39m(B[mconfidence = self.calculate_confidence(state)[50;9H(B[0;1m[36mreturn[39m(B[m confidence >= self.target_threshold[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;5H(B[0;1m[36mdef[34m calculate_reward[39m(B[m(self, state: torch.Tensor, flag: bool) -> torch.Tensor:[47;9Hconfidence = self.calculate_confidence(state)[48;9H(B[0;1m[36mif[39m(B[m flag:[49;13H(B[0;1m[36mreturn[39m(B[m confidence - 50 (B[0;1m[31m # Reward calculation[50;9H[36melse[39m(B[m:[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H(B[0;1m[36mif[39m(B[m confidence > 85:[46;17H(B[0;1m[36mreturn[39m(B[m (confidence - 50) / 2.0 (B[0;1m[31m # Encouraging the model in the right direction[47;13H[36melse[39m(B[m:[48d(B[0;1m[36mreturn[39m(B[m (confidence - 50) / 4.0 (B[0;1m[31m # Lesser reward[49d(B[0m[42m    [50d[49m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;5H(B[0;1m[36mdef[34m calculate_confidence[39m(B[m(self, state: torch.Tensor) -> torch.Tensor:[47;8H(B[0;1m[31m # Retrieve and normalize quaternions[48;9H[39m(B[mrendered_quat = self.get_rendered_pose_quaternion()[49;9Hrendered_quat = self.normalize_quaternion(rendered_quat)[50;9Hgt_quat = self.normalize_quaternion(self.ground_truth_quats)[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H[42m        [46d(B[0;1m[31m # Compute dot product between rendered and ground truth quaternions for each joint[47;9H[39m(B[mdot_product = torch.sum(rendered_quat * gt_quat, dim=1) (B[0;1m[31m # Shape: [24][48d(B[0m[42m        [49d(B[0;1m[31m # Calculate Angular Displacement in radians[50;9H[39m(B[mangular_displacement = 2 * torch.acos(torch.clamp(dot_product, -1.0, 1.0)) (B[0;1m[31m # [24], radians[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H[42m        [46d(B[0;1m[31m # Scale angular displacement from [0, 2π] to [0, 100][47;9H[39m(B[mscaled_displacement = (angular_displacement / (2 * math.pi)) * 100 (B[0;1m[31m # [24], 0-100[48;9H[39m(B[mscaled_displacement = torch.clamp(scaled_displacement, 0.0, 100.0) (B[0;1m[31m # Ensure values are within [0, 100][49d(B[0m[42m        [50d(B[0;1m[31m # Apply joint weights[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hweighted_angular_disp = scaled_displacement * self.joint_weights (B[0;1m[31m # [24][46d(B[0m[42m        [47d(B[0;1m[31m # Aggregate weighted angular displacements to compute average confidence[48;8H # Since higher displacement indicates lower confidence, invert the scaled displacement[49;9H[39m(B[mangular_disp_score = 100 - weighted_angular_disp.sum() (B[0;1m[31m # Scalar, 0-100[50d(B[0m[42m        [49m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9H(B[0;1m[36mreturn[39m(B[m angular_disp_score[46d[42m        [48;5H(B[0;1m[36mdef[34m get_rendered_pose_quaternion[39m(B[m(self) -> torch.Tensor:[49;9Hquaternions = [][50;9H(B[0;1m[36mfor[39m(B[m joint (B[0;1m[36min[39m(B[m range(self.model.njnt):[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Hq = self.sim.data.qpos[self.model.jnt_qposadr[joint]:self.model.jnt_qposadr[joint] + 4][46;13Hquaternions.append(q)[47;8H(B[0;1m[31m # Convert list of arrays to a single NumPy array[48;9H[39m(B[mquaternions_np = np.array(quaternions, dtype=np.float32)[49;8H(B[0;1m[31m # Convert NumPy array to PyTorch tensor[50;9H[36mreturn[39m(B[m torch.from_numpy(quaternions_np)[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;5H(B[0;1m[36mdef[34m normalize_quaternion[39m(B[m(self, q: torch.Tensor) -> torch.Tensor:[47;9Hnorm = torch.norm(q, dim=1, keepdim=(B[0;1m[35mTrue[39m(B[m)[48;9H(B[0;1m[36mreturn[39m(B[m q / norm.clamp(min=1e-8) (B[0;1m[31m # Prevent division by zero[50;5H[36mdef[34m get_ground_truth_quaternion[39m(B[m(self) -> np.ndarray:[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hground_truth_quats = [[46;13H[6.32658406e-04,  1.25473697e-03, -9.99718591e-03,  1.56702220e+00],[47;13H[1.25473697e-03, -9.99718591e-03,  1.56702220e+00,  1.56730258e+00],[48;13H[-0.00999719,  1.5670222,   1.56730258,  1.5674143],[49;13H[1.5670222,   1.56730258,  1.5674143,  -0.00999801],[50;13H[1.56730258,  1.5674143,  -0.00999801,  1.56701926],[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H[1.5674143,  -0.00999801,  1.56701926,  1.56730194],[46;13H[-0.00999801,  1.56701926,  1.56730194,  1.5674143],[47;13H[1.56701926,  1.56730194,  1.5674143,  -0.00999756],[48;13H[1.56730194,  1.5674143,  -0.00999756,  1.56701717],[49;13H[1.5674143,  -0.00999756,  1.56701717,  1.56730177],[50;13H[-0.00999756,  1.56701717,  1.56730177,  1.56741429],[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H[1.56701717, 1.56730177, 1.56741429, 0.00868252],[46;13H[1.56730177,  1.56741429,  0.00868252, -0.01000805],[47;13H[1.56741429,  0.00868252, -0.01000805,  1.56708349],[48;13H[0.00868252, -0.01000805,  1.56708349,  1.56730911],[49;13H[-0.01000805,  1.56708349,  1.56730911,  1.56741508],[50;13H[1.56708349, 1.56730911, 1.56741508, 0.49916711],[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H[1.56730911, 1.56741508, 0.49916711, 0.50022545],[46;13H[1.56741508, 0.49916711, 0.50022545, 0.25330455],[47;13H[4.99167109e-01, 5.00225452e-01, 2.53304550e-01, 4.08440608e-04],[48;13H[5.00225452e-01,  2.53304550e-01,  4.08440608e-04, -9.00000689e-01],[49;13H[2.53304550e-01,  4.08440608e-04, -9.00000689e-01,  1.00000000e-01],[50;13H[4.08440608e-04, -9.00000689e-01,  1.00000000e-01, -1.00000000e-01],[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13H[-0.90000069,  0.1,[45;40H-0.1,[45;54H0.01463168],[46;13H[0.1,[46;26H-0.1,[46;40H0.01463168,  1.0][47;9H][48d(B[0;1m[36mreturn[39m(B[m np.array(ground_truth_quats, dtype=np.float32)[50d(B[0;1m[36mclass[39m(B[m RewardCallback(BaseCallback):[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5H(B[0;1m[32m"""[46d    Custom callback for logging average rewards over every 100 episodes, episode numbers, and iteration numbers.[47d    Also stores average rewards for plotting.[48d    """[50;5H[36mdef[34m __init__[39m(B[m(self, avg_interval=100):[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hsuper(RewardCallback, self).__init__()[46;9Hself.episode_num = 0[47;9Hself.max_reward = -np.inf[48;9Hself.avg_interval = avg_interval (B[0;1m[31m # Number of episodes per average - not hyperparameter[49;9H[39m(B[mself.sum_rewards = 0.0[50;9Hself.count_rewards = 0[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hself.iteration_num = 0[42m     [46;9H[49m(B[mself.rewards_list = [][47;9Hself.median_rewards = {}[48;9Hself.avg_rewards = {} (B[0;1m[31m # New dictionary to store average rewards[50;5H[36mdef[34m _on_rollout_end[39m(B[m(self) -> bool:[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9H(B[0;1m[32m"""[46d        Called at the end of a rollout.[47d        Used to track the number of iterations.[48d        """[49;9H[39m(B[mself.iteration_num += 1[50;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;5H(B[0;1m[36mdef[34m _on_step[39m(B[m(self) -> bool:[47;9H(B[0;1m[32m"""[48d        Called at every step. Checks if any episode has finished and logs the reward.[49d        """[50;8H[31m # Retrieve 'dones' and 'rewards' from the current step[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hdones = self.locals.get((B[0;1m[32m'dones'[39m(B[m, [])[46;9Hrewards = self.locals.get((B[0;1m[32m'rewards'[39m(B[m, [])[48;8H(B[0;1m[31m # Check if any of the environments are done[49;9H[36mif[39m(B[m np.any(dones):[50;13H(B[0;1m[36mfor[39m(B[m done, reward (B[0;1m[36min[39m(B[m zip(dones, rewards):[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;17H(B[0;1m[36mif[39m(B[m done:[46;21Hself.episode_num += 1[47;21H(B[0;1m[36mif[39m(B[m reward > self.max_reward:[48;25Hself.max_reward = reward[49;21Hself.rewards_list.append(reward)[50;21H(B[0;1m[36mif[39m(B[m len(self.rewards_list) == self.avg_interval:[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;25Hmedian_reward = median(self.rewards_list)[46;25Havg_reward = np.mean(self.rewards_list)[47;25Hblock_num = self.episode_num // self.avg_interval[48;25Hself.median_rewards[block_num] = median_reward[49;25Hself.avg_rewards[block_num] = avg_reward[42m [50;25H[49m(B[mprint(f(B[0;1m[32m"Iteration: {self.iteration_num} | Episodes: {block_num * self.avg_interval} | Median Reward: {median_reward:.2f} | Avg Reward: {avg_reward:.2f} | Max Reward: {self.max_re[39m(B[0;7m>[50;1H(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;24H(B[0;1m[31m # Reset sum and count[46;25H[39m(B[mself.rewards_list = [][47;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[49;5H[36mdef[34m _on_training_end[39m(B[m(self) -> (B[0;1m[35mNone[39m(B[m:[50;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H(B[0;1m[32m        Called at the end of training. If there are remaining episodes, compute and store the average.[46d        """[47;9H[36mif[39m(B[m len(self.rewards_list) > 0:[48;13Hmedian_reward = median(self.rewards_list)[49;13Havg_reward = np.mean(self.rewards_list)[50;13Hblock_num = self.episode_num // self.avg_interval + 1[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Hself.median_rewards[block_num] = median_reward[46;13Hself.avg_rewards[block_num] = avg_reward[47;13Hcurrent_max = max(self.rewards_list)[48;13H(B[0;1m[36mif[39m(B[m current_max > self.max_reward:[49;17Hself.max_reward = current_max[42m           [50;13H[49m(B[mprint(f(B[0;1m[32m"Training End | Episodes: {self.episode_num} | Median Reward: {median_reward:.2f} | Avg Reward: {avg_reward:.2f} | Max Reward: {self.max_reward:.2f}"[39m(B[m)[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Hself.rewards_list = [][47d(B[0;1m[36mdef[34m make_env[39m(B[m():[48;5H(B[0;1m[32m"""Utility function to create a HandEnv without Monitor."""[49;5H[36mdef[34m _init[39m(B[m():[50;9H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;13Henv = HandEnv()[46;13H(B[0;1m[36mreturn[39m(B[m env[47;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[48;13Hlogging.error(f(B[0;1m[32m"Failed to initialize HandEnv: {e}"[39m(B[m)[49;13H(B[0;1m[36mraise[39m(B[m e[50;5H(B[0;1m[36mreturn[39m(B[m _init[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[46;1H(B[0;1m[36mdef[34m train_on_gpu[39m(B[m(gpu_id: int, num_envs: int, total_timesteps: int, num_steps: int, save_interval: int):[47;5Hos.environ[(B[0;1m[32m'CUDA_VISIBLE_DEVICES'[39m(B[m] = str(gpu_id)[48;5Hdevice = (B[0;1m[32m'cuda'[39m(B[m (B[0;1m[36mif[39m(B[m torch.cuda.is_available() (B[0;1m[36melse[39m(B[m (B[0;1m[32m'cpu'[49;5H[39m(B[mprint(f(B[0;1m[32m"GPU {gpu_id}: Using {device} device"[39m(B[m)[50;5Henv = DummyVecEnv([make_env() (B[0;1m[36mfor[39m(B[m _ (B[0;1m[36min[39m(B[m range(num_envs)])[42m  [49m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5Henv = VecMonitor(env) (B[0;1m[31m # Use VecMonitor instead of individual Monitor wrappers[47;5H[39m(B[mmodel = RecurrentPPO([48;9HCustomACLstmPolicy, (B[0;1m[31m # Use the custom policy[49;9H[39m(B[menv,[50;9Hverbose=1,[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hdevice=device,[46;9Hent_coef=0.15,[46;36H(B[0;1m[31m # Adjusted entropy coefficient[47;9H[39m(B[mlearning_rate=0.0005,[48;9Hclip_range=0.3,[49;9Hn_steps=num_steps,[49;36H(B[0;1m[31m # Steps per environment per update[50;9H[39m(B[mbatch_size=4096,[50;36H(B[0;1m[31m # Adjusted batch size[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9Hgamma=0.99,[46;9Hgae_lambda=0.95,[47;9Hmax_grad_norm=0.5,[48;9Hvf_coef=0.5,[49;9Huse_sde=(B[0;1m[35mTrue[39m(B[m,[49;36H(B[0;1m[31m # Use State Dependent Exploration for better exploration[50;8H # Removed lstm_hidden_size=128[39m(B[m[?12l[?25h[?25l[44d[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25le[?12l[?25h[?25ln[?12l[?25h[?25lt[?12l[?25h[?25l_[?12l[?25h[?25lc[?12l[?25h[?25lo[?12l[?25h[?25le[?12l[?25h[?25lf[?12l[?25h[?25l=[?12l[?25h[?25l0[?12l[?25h[?25l.[?12l[?25h[?25l1[?12l[?25h[?25l5[?12l[?25h[?25l[1;194H(B[0;7mModified(B[m[40;21H[1P[?12l[?25h[?25l[1P[?12l[?25h[?25l2,[40;34H (B[0;1m[31m # Adjusted entropy coefficient[40;21H[39m(B[m[?12l[?25h[?25l[46d[?12l[?25h[?25l[2;51r[51;1H[2S[1;53r[49;5H)[50d[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5Hcallback = RewardCallback(avg_interval=100)[47;5H(B[0;1m[36mtry[39m(B[m:[48dmodel.learn(total_timesteps=total_timesteps, callback=callback, log_interval=10)[49;5H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[50;9Hlogging.error(f(B[0;1m[32m"Error during training: {e}"[39m(B[m)[50;21H[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;9H(B[0;1m[36mraise[39m(B[m e[47;5Hmodel.save(f(B[0;1m[32m"/home/easgrad/dgusain/Bot_hand/agents/agent_code1_1_residual_e_gpu{gpu_id}"[39m(B[m)[49;5H(B[0;1m[36mreturn[39m(B[m callback (B[0;1m[31m # Return the callback to access episode_rewards[50d[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;1H(B[0;1m[36mdef[34m main[39m(B[m():[46;5Hgpu_id = 3[42m          [47;5H[49m(B[mnum_envs = 4[42m        [48;5H[49m(B[mn_iter = 1000[42m         [49;5H[49m(B[mn_steps = 1024     (B[0;1m[31m # Steps per environment per update[50;5H[39m(B[mtotal_timesteps = n_iter * n_steps * num_envs (B[0;1m[31m # Total training timesteps[50;21H[39m(B[m[?12l[?25h[?25l7[2;51r8[51d[6S[1;53r[45;5Hsave_interval = 25[42m  [47;5H[49m(B[mcallback = train_on_gpu(gpu_id, num_envs, total_timesteps, n_steps, save_interval)[48;5Hblock_numbers = list(callback.median_rewards.keys())[49;5Hblock_median_rewards = list(callback.median_rewards.values())[50;5Hblock_avg_rewards = list(callback.avg_rewards.values())[50;21H[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[45;21H[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[1P[?12l[?25h[?25l500[42;26H[42m [42;16H[49m(B[m[?12l[?25h[?25l[51d(B[0;7mSave modified buffer?                                                                                                                                                                                      [52;1H Y(B[m Yes[K[53d(B[0;7m N(B[m No  [53;18H(B[0;7mC(B[m Cancel[K[51;23H[?12l[?25h[?25l[52d(B[0;7m^G(B[m Get Help[52;51H(B[0;7mM-D(B[m DOS Format[52;101H(B[0;7mM-A(B[m Append[52;151H(B[0;7mM-B(B[m Backup File[53d(B[0;7m^C(B[m Cancel[17G         [53;51H(B[0;7mM-M(B[m Mac Format[53;101H(B[0;7mM-P(B[m Prepend[53;151H(B[0;7m^T(B[m To Files[51d(B[0;7mFile Name to Write: code1_1_residual_e.py(B[m[51;42H[?12l[?25h[?25l[51;94H[1K (B[0;7m[ Writing... ](B[m[K[1;194H(B[0;7m        (B[m[51;93H(B[0;7m[ Wrote 420 lines ](B[m[J[53d[?12l[?25h[53;1H[?1049l[23;0;0t[?1l>[?2004l(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> clear
[H[2J(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python code1_1_residual_e.py
GPU 3: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
Iteration: 2 | Episodes: 100 | Median Reward: 23.02 | Avg Reward: 22.01 | Max Reward: 31.59
Iteration: 4 | Episodes: 200 | Median Reward: 22.45 | Avg Reward: 22.44 | Max Reward: 31.59
Iteration: 7 | Episodes: 300 | Median Reward: 24.00 | Avg Reward: 24.94 | Max Reward: 35.11
Iteration: 9 | Episodes: 400 | Median Reward: 24.99 | Avg Reward: 23.93 | Max Reward: 35.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76         |
| time/                   |             |
|    fps                  | 599         |
|    iterations           | 10          |
|    time_elapsed         | 68          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.029411923 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.3         |
|    entropy_loss         | -71.8       |
|    explained_variance   | -0.00766    |
|    learning_rate        | 0.0005      |
|    loss                 | 107         |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.00198     |
|    std                  | 1.01        |
|    value_loss           | 250         |
-----------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 24.16 | Avg Reward: 24.20 | Max Reward: 35.11
Iteration: 14 | Episodes: 600 | Median Reward: 22.73 | Avg Reward: 21.62 | Max Reward: 35.11
Iteration: 17 | Episodes: 700 | Median Reward: 22.08 | Avg Reward: 23.13 | Max Reward: 35.11
Iteration: 19 | Episodes: 800 | Median Reward: 22.46 | Avg Reward: 22.43 | Max Reward: 35.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.8        |
| time/                   |              |
|    fps                  | 596          |
|    iterations           | 20           |
|    time_elapsed         | 137          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0043810783 |
|    clip_fraction        | 0.00388      |
|    clip_range           | 0.3          |
|    entropy_loss         | -75.6        |
|    explained_variance   | 0.395        |
|    learning_rate        | 0.0005       |
|    loss                 | 43.3         |
|    n_updates            | 190          |
|    policy_gradient_loss | 0.00183      |
|    std                  | 1.01         |
|    value_loss           | 154          |
------------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 25.96 | Avg Reward: 24.70 | Max Reward: 38.01
Iteration: 24 | Episodes: 1000 | Median Reward: 23.39 | Avg Reward: 25.50 | Max Reward: 38.01
Iteration: 27 | Episodes: 1100 | Median Reward: 23.30 | Avg Reward: 24.22 | Max Reward: 38.01
Iteration: 29 | Episodes: 1200 | Median Reward: 23.72 | Avg Reward: 24.18 | Max Reward: 38.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76         |
| time/                   |             |
|    fps                  | 587         |
|    iterations           | 30          |
|    time_elapsed         | 209         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.013950946 |
|    clip_fraction        | 0.00259     |
|    clip_range           | 0.3         |
|    entropy_loss         | -75         |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.24       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0203     |
|    std                  | 1.01        |
|    value_loss           | 36.4        |
-----------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 24.19 | Avg Reward: 23.94 | Max Reward: 38.01
Iteration: 34 | Episodes: 1400 | Median Reward: 25.25 | Avg Reward: 24.40 | Max Reward: 38.01
Iteration: 36 | Episodes: 1500 | Median Reward: 24.30 | Avg Reward: 23.75 | Max Reward: 38.01
Iteration: 39 | Episodes: 1600 | Median Reward: 23.26 | Avg Reward: 24.31 | Max Reward: 38.01
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -74.8     |
| time/                   |           |
|    fps                  | 590       |
|    iterations           | 40        |
|    time_elapsed         | 277       |
|    total_timesteps      | 163840    |
| train/                  |           |
|    approx_kl            | 0.3878979 |
|    clip_fraction        | 0.656     |
|    clip_range           | 0.3       |
|    entropy_loss         | -89.1     |
|    explained_variance   | 0.961     |
|    learning_rate        | 0.0005    |
|    loss                 | -12.6     |
|    n_updates            | 390       |
|    policy_gradient_loss | 0.0424    |
|    std                  | 1.02      |
|    value_loss           | 18.1      |
---------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 23.77 | Avg Reward: 25.78 | Max Reward: 38.01
Iteration: 44 | Episodes: 1800 | Median Reward: 27.54 | Avg Reward: 25.52 | Max Reward: 38.01
Iteration: 46 | Episodes: 1900 | Median Reward: 22.29 | Avg Reward: 22.85 | Max Reward: 38.01
Iteration: 49 | Episodes: 2000 | Median Reward: 24.01 | Avg Reward: 25.68 | Max Reward: 38.01
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -72.8      |
| time/                   |            |
|    fps                  | 609        |
|    iterations           | 50         |
|    time_elapsed         | 336        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.16697928 |
|    clip_fraction        | 0.607      |
|    clip_range           | 0.3        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.974      |
|    learning_rate        | 0.0005     |
|    loss                 | -19.1      |
|    n_updates            | 490        |
|    policy_gradient_loss | 0.0109     |
|    std                  | 1.02       |
|    value_loss           | 9.07       |
----------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 26.23 | Avg Reward: 27.11 | Max Reward: 42.11
Iteration: 54 | Episodes: 2200 | Median Reward: 23.01 | Avg Reward: 23.02 | Max Reward: 42.11
Iteration: 56 | Episodes: 2300 | Median Reward: 23.78 | Avg Reward: 25.45 | Max Reward: 42.11
Iteration: 59 | Episodes: 2400 | Median Reward: 25.37 | Avg Reward: 25.36 | Max Reward: 42.11
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -76.4      |
| time/                   |            |
|    fps                  | 626        |
|    iterations           | 60         |
|    time_elapsed         | 392        |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.14195007 |
|    clip_fraction        | 0.361      |
|    clip_range           | 0.3        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.979      |
|    learning_rate        | 0.0005     |
|    loss                 | -23.2      |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0146    |
|    std                  | 1.02       |
|    value_loss           | 9.92       |
----------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 23.86 | Avg Reward: 24.01 | Max Reward: 42.11
Iteration: 64 | Episodes: 2600 | Median Reward: 26.58 | Avg Reward: 25.32 | Max Reward: 42.11
Iteration: 66 | Episodes: 2700 | Median Reward: 28.56 | Avg Reward: 29.11 | Max Reward: 42.11
Iteration: 69 | Episodes: 2800 | Median Reward: 24.38 | Avg Reward: 24.94 | Max Reward: 42.11
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -73.6      |
| time/                   |            |
|    fps                  | 650        |
|    iterations           | 70         |
|    time_elapsed         | 440        |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.23083393 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.3        |
|    entropy_loss         | -139       |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0005     |
|    loss                 | -27        |
|    n_updates            | 690        |
|    policy_gradient_loss | 0.032      |
|    std                  | 1.02       |
|    value_loss           | 3.66       |
----------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 30.78 | Avg Reward: 29.36 | Max Reward: 42.11
Iteration: 73 | Episodes: 3000 | Median Reward: 26.71 | Avg Reward: 27.91 | Max Reward: 42.11
Iteration: 76 | Episodes: 3100 | Median Reward: 29.94 | Avg Reward: 29.17 | Max Reward: 42.11
Iteration: 78 | Episodes: 3200 | Median Reward: 28.27 | Avg Reward: 28.03 | Max Reward: 42.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.9       |
| time/                   |             |
|    fps                  | 665         |
|    iterations           | 80          |
|    time_elapsed         | 492         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.028525777 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.3         |
|    entropy_loss         | -150        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -26.5       |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0105     |
|    std                  | 1.03        |
|    value_loss           | 3           |
-----------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 28.57 | Avg Reward: 28.74 | Max Reward: 42.11
Iteration: 83 | Episodes: 3400 | Median Reward: 25.74 | Avg Reward: 26.96 | Max Reward: 43.56
Iteration: 86 | Episodes: 3500 | Median Reward: 29.63 | Avg Reward: 30.00 | Max Reward: 43.56
Iteration: 88 | Episodes: 3600 | Median Reward: 28.96 | Avg Reward: 27.34 | Max Reward: 43.56
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -73.9     |
| time/                   |           |
|    fps                  | 681       |
|    iterations           | 90        |
|    time_elapsed         | 541       |
|    total_timesteps      | 368640    |
| train/                  |           |
|    approx_kl            | 0.2070114 |
|    clip_fraction        | 0.473     |
|    clip_range           | 0.3       |
|    entropy_loss         | -159      |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0005    |
|    loss                 | -29.1     |
|    n_updates            | 890       |
|    policy_gradient_loss | -0.0297   |
|    std                  | 1.03      |
|    value_loss           | 7.05      |
---------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 24.58 | Avg Reward: 26.42 | Max Reward: 43.56
Iteration: 93 | Episodes: 3800 | Median Reward: 27.75 | Avg Reward: 28.57 | Max Reward: 43.56
Iteration: 96 | Episodes: 3900 | Median Reward: 28.19 | Avg Reward: 26.72 | Max Reward: 43.56
Iteration: 98 | Episodes: 4000 | Median Reward: 30.35 | Avg Reward: 29.13 | Max Reward: 43.56
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.1     |
| time/                   |           |
|    fps                  | 694       |
|    iterations           | 100       |
|    time_elapsed         | 590       |
|    total_timesteps      | 409600    |
| train/                  |           |
|    approx_kl            | 0.0644364 |
|    clip_fraction        | 0.147     |
|    clip_range           | 0.3       |
|    entropy_loss         | -163      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.0005    |
|    loss                 | -32.3     |
|    n_updates            | 990       |
|    policy_gradient_loss | -0.0387   |
|    std                  | 1.04      |
|    value_loss           | 3.34      |
---------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 29.70 | Avg Reward: 29.15 | Max Reward: 43.56
Iteration: 103 | Episodes: 4200 | Median Reward: 33.83 | Avg Reward: 33.65 | Max Reward: 43.56
Iteration: 106 | Episodes: 4300 | Median Reward: 34.26 | Avg Reward: 32.68 | Max Reward: 43.56
Iteration: 108 | Episodes: 4400 | Median Reward: 28.70 | Avg Reward: 27.85 | Max Reward: 43.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.9        |
| time/                   |              |
|    fps                  | 708          |
|    iterations           | 110          |
|    time_elapsed         | 636          |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 0.0063375873 |
|    clip_fraction        | 0.000806     |
|    clip_range           | 0.3          |
|    entropy_loss         | -171         |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0005       |
|    loss                 | -31.7        |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.0028      |
|    std                  | 1.05         |
|    value_loss           | 3.86         |
------------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 31.83 | Avg Reward: 31.96 | Max Reward: 43.56
Iteration: 113 | Episodes: 4600 | Median Reward: 33.45 | Avg Reward: 32.16 | Max Reward: 43.56
Iteration: 115 | Episodes: 4700 | Median Reward: 30.24 | Avg Reward: 30.05 | Max Reward: 43.56
Iteration: 118 | Episodes: 4800 | Median Reward: 31.54 | Avg Reward: 29.86 | Max Reward: 43.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70          |
| time/                   |              |
|    fps                  | 722          |
|    iterations           | 120          |
|    time_elapsed         | 680          |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0027708986 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.3          |
|    entropy_loss         | -174         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -31.2        |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00414     |
|    std                  | 1.05         |
|    value_loss           | 4.82         |
------------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 31.74 | Avg Reward: 30.48 | Max Reward: 43.56
Iteration: 123 | Episodes: 5000 | Median Reward: 32.49 | Avg Reward: 32.93 | Max Reward: 43.56
Iteration: 125 | Episodes: 5100 | Median Reward: 31.62 | Avg Reward: 31.59 | Max Reward: 43.56
Iteration: 128 | Episodes: 5200 | Median Reward: 29.57 | Avg Reward: 30.05 | Max Reward: 43.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.6        |
| time/                   |              |
|    fps                  | 733          |
|    iterations           | 130          |
|    time_elapsed         | 726          |
|    total_timesteps      | 532480       |
| train/                  |              |
|    approx_kl            | 0.0010412631 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.3          |
|    entropy_loss         | -177         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -33.4        |
|    n_updates            | 1290         |
|    policy_gradient_loss | 0.00137      |
|    std                  | 1.06         |
|    value_loss           | 4.32         |
------------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 33.15 | Avg Reward: 33.32 | Max Reward: 43.56
Iteration: 133 | Episodes: 5400 | Median Reward: 32.86 | Avg Reward: 32.02 | Max Reward: 43.56
Iteration: 135 | Episodes: 5500 | Median Reward: 32.55 | Avg Reward: 33.03 | Max Reward: 43.56
Iteration: 138 | Episodes: 5600 | Median Reward: 31.93 | Avg Reward: 32.91 | Max Reward: 43.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.2        |
| time/                   |              |
|    fps                  | 743          |
|    iterations           | 140          |
|    time_elapsed         | 771          |
|    total_timesteps      | 573440       |
| train/                  |              |
|    approx_kl            | 0.0015482256 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.3          |
|    entropy_loss         | -180         |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0005       |
|    loss                 | -35          |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 1.06         |
|    value_loss           | 6.1          |
------------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 32.23 | Avg Reward: 32.36 | Max Reward: 43.56
Iteration: 143 | Episodes: 5800 | Median Reward: 33.95 | Avg Reward: 33.87 | Max Reward: 43.70
Iteration: 145 | Episodes: 5900 | Median Reward: 29.59 | Avg Reward: 30.85 | Max Reward: 43.70
Iteration: 147 | Episodes: 6000 | Median Reward: 32.55 | Avg Reward: 31.23 | Max Reward: 43.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.3       |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 150         |
|    time_elapsed         | 816         |
|    total_timesteps      | 614400      |
| train/                  |             |
|    approx_kl            | 0.025991425 |
|    clip_fraction        | 0.0309      |
|    clip_range           | 0.3         |
|    entropy_loss         | -184        |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0005      |
|    loss                 | -35.9       |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00469    |
|    std                  | 1.07        |
|    value_loss           | 2.94        |
-----------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 31.74 | Avg Reward: 31.45 | Max Reward: 43.70
Iteration: 152 | Episodes: 6200 | Median Reward: 36.88 | Avg Reward: 33.95 | Max Reward: 43.70
Iteration: 155 | Episodes: 6300 | Median Reward: 29.83 | Avg Reward: 30.08 | Max Reward: 43.70
Iteration: 157 | Episodes: 6400 | Median Reward: 33.59 | Avg Reward: 32.59 | Max Reward: 43.70
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.1        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 160          |
|    time_elapsed         | 862          |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 0.0021724848 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -189         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -37.1        |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.00172     |
|    std                  | 1.07         |
|    value_loss           | 2.69         |
------------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 33.84 | Avg Reward: 32.02 | Max Reward: 43.70
Iteration: 162 | Episodes: 6600 | Median Reward: 35.17 | Avg Reward: 34.52 | Max Reward: 43.70
Iteration: 165 | Episodes: 6700 | Median Reward: 34.03 | Avg Reward: 33.53 | Max Reward: 43.70
Iteration: 167 | Episodes: 6800 | Median Reward: 32.13 | Avg Reward: 33.42 | Max Reward: 43.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.2       |
| time/                   |             |
|    fps                  | 765         |
|    iterations           | 170         |
|    time_elapsed         | 909         |
|    total_timesteps      | 696320      |
| train/                  |             |
|    approx_kl            | 0.013736526 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -191        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -37.5       |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0113     |
|    std                  | 1.08        |
|    value_loss           | 4.52        |
-----------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 35.78 | Avg Reward: 34.83 | Max Reward: 43.70
Iteration: 172 | Episodes: 7000 | Median Reward: 34.34 | Avg Reward: 33.39 | Max Reward: 43.70
Iteration: 175 | Episodes: 7100 | Median Reward: 34.00 | Avg Reward: 33.53 | Max Reward: 43.70
Iteration: 177 | Episodes: 7200 | Median Reward: 34.06 | Avg Reward: 33.58 | Max Reward: 43.70
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66          |
| time/                   |              |
|    fps                  | 771          |
|    iterations           | 180          |
|    time_elapsed         | 955          |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.0025800928 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -195         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -38.7        |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.00424     |
|    std                  | 1.08         |
|    value_loss           | 1.89         |
------------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 34.43 | Avg Reward: 34.01 | Max Reward: 43.70
Iteration: 182 | Episodes: 7400 | Median Reward: 33.33 | Avg Reward: 32.73 | Max Reward: 43.70
Iteration: 184 | Episodes: 7500 | Median Reward: 34.40 | Avg Reward: 32.96 | Max Reward: 43.70
Iteration: 187 | Episodes: 7600 | Median Reward: 33.22 | Avg Reward: 34.06 | Max Reward: 43.70
Iteration: 189 | Episodes: 7700 | Median Reward: 34.70 | Avg Reward: 34.99 | Max Reward: 43.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.9       |
| time/                   |             |
|    fps                  | 778         |
|    iterations           | 190         |
|    time_elapsed         | 999         |
|    total_timesteps      | 778240      |
| train/                  |             |
|    approx_kl            | 0.041091405 |
|    clip_fraction        | 0.0527      |
|    clip_range           | 0.3         |
|    entropy_loss         | -197        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -38.5       |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0171     |
|    std                  | 1.09        |
|    value_loss           | 1.55        |
-----------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 37.72 | Avg Reward: 36.74 | Max Reward: 43.74
Iteration: 194 | Episodes: 7900 | Median Reward: 31.20 | Avg Reward: 31.71 | Max Reward: 43.74
Iteration: 197 | Episodes: 8000 | Median Reward: 34.14 | Avg Reward: 33.29 | Max Reward: 43.74
Iteration: 199 | Episodes: 8100 | Median Reward: 33.47 | Avg Reward: 33.20 | Max Reward: 43.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.4       |
| time/                   |             |
|    fps                  | 784         |
|    iterations           | 200         |
|    time_elapsed         | 1044        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.002811009 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -198        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0005      |
|    loss                 | -36.2       |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00178    |
|    std                  | 1.09        |
|    value_loss           | 5.27        |
-----------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 33.69 | Avg Reward: 34.12 | Max Reward: 43.74
Iteration: 204 | Episodes: 8300 | Median Reward: 32.01 | Avg Reward: 30.93 | Max Reward: 43.74
Iteration: 207 | Episodes: 8400 | Median Reward: 33.02 | Avg Reward: 33.67 | Max Reward: 43.74
Iteration: 209 | Episodes: 8500 | Median Reward: 35.87 | Avg Reward: 35.84 | Max Reward: 43.74
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.3        |
| time/                   |              |
|    fps                  | 788          |
|    iterations           | 210          |
|    time_elapsed         | 1090         |
|    total_timesteps      | 860160       |
| train/                  |              |
|    approx_kl            | 0.0016394355 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -200         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -36.1        |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00207     |
|    std                  | 1.1          |
|    value_loss           | 5.39         |
------------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 35.82 | Avg Reward: 35.98 | Max Reward: 43.74
Iteration: 214 | Episodes: 8700 | Median Reward: 34.06 | Avg Reward: 33.61 | Max Reward: 43.74
Iteration: 216 | Episodes: 8800 | Median Reward: 35.67 | Avg Reward: 36.26 | Max Reward: 43.74
Iteration: 219 | Episodes: 8900 | Median Reward: 34.85 | Avg Reward: 35.38 | Max Reward: 43.74
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.4         |
| time/                   |               |
|    fps                  | 794           |
|    iterations           | 220           |
|    time_elapsed         | 1134          |
|    total_timesteps      | 901120        |
| train/                  |               |
|    approx_kl            | 0.00030957785 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -202          |
|    explained_variance   | 0.991         |
|    learning_rate        | 0.0005        |
|    loss                 | -38.1         |
|    n_updates            | 2190          |
|    policy_gradient_loss | -0.000453     |
|    std                  | 1.11          |
|    value_loss           | 4.47          |
-------------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 34.56 | Avg Reward: 34.07 | Max Reward: 43.74
Iteration: 224 | Episodes: 9100 | Median Reward: 33.04 | Avg Reward: 33.64 | Max Reward: 43.74
Iteration: 226 | Episodes: 9200 | Median Reward: 33.34 | Avg Reward: 35.58 | Max Reward: 43.74
Iteration: 229 | Episodes: 9300 | Median Reward: 36.55 | Avg Reward: 36.12 | Max Reward: 43.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.8       |
| time/                   |             |
|    fps                  | 798         |
|    iterations           | 230         |
|    time_elapsed         | 1179        |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.009671911 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.3         |
|    entropy_loss         | -204        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0005      |
|    loss                 | -38.7       |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.000294   |
|    std                  | 1.11        |
|    value_loss           | 3.45        |
-----------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 36.04 | Avg Reward: 35.91 | Max Reward: 43.74
Iteration: 234 | Episodes: 9500 | Median Reward: 34.89 | Avg Reward: 34.96 | Max Reward: 43.74
Iteration: 236 | Episodes: 9600 | Median Reward: 35.12 | Avg Reward: 35.89 | Max Reward: 43.74
Iteration: 239 | Episodes: 9700 | Median Reward: 33.79 | Avg Reward: 33.55 | Max Reward: 43.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.8       |
| time/                   |             |
|    fps                  | 802         |
|    iterations           | 240         |
|    time_elapsed         | 1225        |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.002071652 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -206        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -37.4       |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00295    |
|    std                  | 1.12        |
|    value_loss           | 4.77        |
-----------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 33.74 | Avg Reward: 34.13 | Max Reward: 43.74
Iteration: 244 | Episodes: 9900 | Median Reward: 33.62 | Avg Reward: 34.19 | Max Reward: 43.74
Iteration: 246 | Episodes: 10000 | Median Reward: 33.97 | Avg Reward: 34.04 | Max Reward: 43.74
Iteration: 249 | Episodes: 10100 | Median Reward: 34.93 | Avg Reward: 34.58 | Max Reward: 43.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.8       |
| time/                   |             |
|    fps                  | 805         |
|    iterations           | 250         |
|    time_elapsed         | 1271        |
|    total_timesteps      | 1024000     |
| train/                  |             |
|    approx_kl            | 0.004654332 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -208        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -40.9       |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00977    |
|    std                  | 1.13        |
|    value_loss           | 1.86        |
-----------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 34.00 | Avg Reward: 33.46 | Max Reward: 43.74
Iteration: 253 | Episodes: 10300 | Median Reward: 37.50 | Avg Reward: 36.71 | Max Reward: 43.74
Iteration: 256 | Episodes: 10400 | Median Reward: 36.69 | Avg Reward: 37.22 | Max Reward: 43.74
Iteration: 258 | Episodes: 10500 | Median Reward: 34.76 | Avg Reward: 35.58 | Max Reward: 43.74
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.2       |
| time/                   |             |
|    fps                  | 809         |
|    iterations           | 260         |
|    time_elapsed         | 1316        |
|    total_timesteps      | 1064960     |
| train/                  |             |
|    approx_kl            | 0.041686002 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.3         |
|    entropy_loss         | -211        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -39.1       |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0344     |
|    std                  | 1.13        |
|    value_loss           | 1.78        |
-----------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 34.28 | Avg Reward: 35.22 | Max Reward: 43.96
Iteration: 263 | Episodes: 10700 | Median Reward: 35.44 | Avg Reward: 35.82 | Max Reward: 43.96
Iteration: 266 | Episodes: 10800 | Median Reward: 34.63 | Avg Reward: 33.84 | Max Reward: 43.96
Iteration: 268 | Episodes: 10900 | Median Reward: 35.68 | Avg Reward: 34.84 | Max Reward: 43.96
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.3       |
| time/                   |             |
|    fps                  | 811         |
|    iterations           | 270         |
|    time_elapsed         | 1362        |
|    total_timesteps      | 1105920     |
| train/                  |             |
|    approx_kl            | 0.014589561 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -214        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0005      |
|    loss                 | -42.7       |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.0029     |
|    std                  | 1.14        |
|    value_loss           | 1.76        |
-----------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 37.66 | Avg Reward: 37.99 | Max Reward: 43.96
Iteration: 273 | Episodes: 11100 | Median Reward: 37.82 | Avg Reward: 37.26 | Max Reward: 43.96
Iteration: 276 | Episodes: 11200 | Median Reward: 36.95 | Avg Reward: 37.03 | Max Reward: 43.96
Iteration: 278 | Episodes: 11300 | Median Reward: 34.38 | Avg Reward: 36.52 | Max Reward: 43.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.8        |
| time/                   |              |
|    fps                  | 814          |
|    iterations           | 280          |
|    time_elapsed         | 1408         |
|    total_timesteps      | 1146880      |
| train/                  |              |
|    approx_kl            | 0.0070883906 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -216         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -41.1        |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.00972     |
|    std                  | 1.15         |
|    value_loss           | 2.26         |
------------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 39.77 | Avg Reward: 36.95 | Max Reward: 43.96
Iteration: 283 | Episodes: 11500 | Median Reward: 35.20 | Avg Reward: 35.93 | Max Reward: 43.96
Iteration: 286 | Episodes: 11600 | Median Reward: 37.66 | Avg Reward: 38.01 | Max Reward: 43.96
Iteration: 288 | Episodes: 11700 | Median Reward: 34.51 | Avg Reward: 35.27 | Max Reward: 44.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.4       |
| time/                   |             |
|    fps                  | 817         |
|    iterations           | 290         |
|    time_elapsed         | 1453        |
|    total_timesteps      | 1187840     |
| train/                  |             |
|    approx_kl            | 0.013052527 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -219        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -43.5       |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00635    |
|    std                  | 1.16        |
|    value_loss           | 1.78        |
-----------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 36.99 | Avg Reward: 36.82 | Max Reward: 44.08
Iteration: 293 | Episodes: 11900 | Median Reward: 36.37 | Avg Reward: 36.84 | Max Reward: 44.08
Iteration: 295 | Episodes: 12000 | Median Reward: 35.40 | Avg Reward: 36.40 | Max Reward: 44.08
Iteration: 298 | Episodes: 12100 | Median Reward: 40.12 | Avg Reward: 39.14 | Max Reward: 44.08
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.8         |
| time/                   |               |
|    fps                  | 819           |
|    iterations           | 300           |
|    time_elapsed         | 1499          |
|    total_timesteps      | 1228800       |
| train/                  |               |
|    approx_kl            | 0.00024214882 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -220          |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.0005        |
|    loss                 | -43.1         |
|    n_updates            | 2990          |
|    policy_gradient_loss | 7.46e-05      |
|    std                  | 1.16          |
|    value_loss           | 6.13          |
-------------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 38.30 | Avg Reward: 38.26 | Max Reward: 44.08
Iteration: 303 | Episodes: 12300 | Median Reward: 36.44 | Avg Reward: 36.80 | Max Reward: 44.08
Iteration: 305 | Episodes: 12400 | Median Reward: 36.93 | Avg Reward: 37.17 | Max Reward: 44.08
Iteration: 308 | Episodes: 12500 | Median Reward: 37.17 | Avg Reward: 36.49 | Max Reward: 44.08
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.8        |
| time/                   |              |
|    fps                  | 796          |
|    iterations           | 310          |
|    time_elapsed         | 1594         |
|    total_timesteps      | 1269760      |
| train/                  |              |
|    approx_kl            | 0.0147292465 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -221         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -44          |
|    n_updates            | 3090         |
|    policy_gradient_loss | 0.00168      |
|    std                  | 1.17         |
|    value_loss           | 0.747        |
------------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: 37.86 | Avg Reward: 38.08 | Max Reward: 44.08
Iteration: 313 | Episodes: 12700 | Median Reward: 37.55 | Avg Reward: 37.83 | Max Reward: 44.08
Iteration: 315 | Episodes: 12800 | Median Reward: 35.14 | Avg Reward: 35.64 | Max Reward: 44.08
Iteration: 318 | Episodes: 12900 | Median Reward: 37.87 | Avg Reward: 38.74 | Max Reward: 44.08
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.4         |
| time/                   |               |
|    fps                  | 783           |
|    iterations           | 320           |
|    time_elapsed         | 1672          |
|    total_timesteps      | 1310720       |
| train/                  |               |
|    approx_kl            | 0.00062255835 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -223          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -43.2         |
|    n_updates            | 3190          |
|    policy_gradient_loss | -0.000671     |
|    std                  | 1.18          |
|    value_loss           | 3.82          |
-------------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: 39.77 | Avg Reward: 39.35 | Max Reward: 44.08
Iteration: 323 | Episodes: 13100 | Median Reward: 37.66 | Avg Reward: 38.07 | Max Reward: 44.08
Iteration: 325 | Episodes: 13200 | Median Reward: 37.62 | Avg Reward: 36.98 | Max Reward: 44.08
Iteration: 327 | Episodes: 13300 | Median Reward: 40.03 | Avg Reward: 38.73 | Max Reward: 44.08
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.4        |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 330          |
|    time_elapsed         | 1780         |
|    total_timesteps      | 1351680      |
| train/                  |              |
|    approx_kl            | 0.0018947509 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -224         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -43.1        |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.00397     |
|    std                  | 1.18         |
|    value_loss           | 7.58         |
------------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: 38.04 | Avg Reward: 36.99 | Max Reward: 44.08
Iteration: 332 | Episodes: 13500 | Median Reward: 38.45 | Avg Reward: 38.00 | Max Reward: 44.08
Iteration: 335 | Episodes: 13600 | Median Reward: 39.58 | Avg Reward: 38.01 | Max Reward: 44.08
Iteration: 337 | Episodes: 13700 | Median Reward: 37.85 | Avg Reward: 38.98 | Max Reward: 44.08
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.8       |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 340         |
|    time_elapsed         | 1861        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.003960187 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -228        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -44.9       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.00396    |
|    std                  | 1.19        |
|    value_loss           | 0.72        |
-----------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: 39.64 | Avg Reward: 38.52 | Max Reward: 44.08
Iteration: 342 | Episodes: 13900 | Median Reward: 37.86 | Avg Reward: 38.80 | Max Reward: 44.08
Iteration: 345 | Episodes: 14000 | Median Reward: 39.56 | Avg Reward: 39.28 | Max Reward: 44.08
Iteration: 347 | Episodes: 14100 | Median Reward: 39.63 | Avg Reward: 39.34 | Max Reward: 44.10
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.1         |
| time/                   |               |
|    fps                  | 730           |
|    iterations           | 350           |
|    time_elapsed         | 1962          |
|    total_timesteps      | 1433600       |
| train/                  |               |
|    approx_kl            | 6.9663234e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -231          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -45.8         |
|    n_updates            | 3490          |
|    policy_gradient_loss | -0.000205     |
|    std                  | 1.2           |
|    value_loss           | 1.31          |
-------------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: 37.61 | Avg Reward: 37.04 | Max Reward: 44.10
Iteration: 352 | Episodes: 14300 | Median Reward: 39.21 | Avg Reward: 37.37 | Max Reward: 44.10
Iteration: 355 | Episodes: 14400 | Median Reward: 39.76 | Avg Reward: 39.46 | Max Reward: 44.26
Iteration: 357 | Episodes: 14500 | Median Reward: 39.25 | Avg Reward: 38.04 | Max Reward: 44.26
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.8        |
| time/                   |              |
|    fps                  | 717          |
|    iterations           | 360          |
|    time_elapsed         | 2055         |
|    total_timesteps      | 1474560      |
| train/                  |              |
|    approx_kl            | 0.0005646625 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -234         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -46.6        |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.00084     |
|    std                  | 1.21         |
|    value_loss           | 1.28         |
------------------------------------------
Iteration: 360 | Episodes: 14600 | Median Reward: 38.42 | Avg Reward: 38.26 | Max Reward: 44.26
Iteration: 362 | Episodes: 14700 | Median Reward: 40.07 | Avg Reward: 39.89 | Max Reward: 44.26
Iteration: 364 | Episodes: 14800 | Median Reward: 40.28 | Avg Reward: 40.08 | Max Reward: 44.26
Iteration: 367 | Episodes: 14900 | Median Reward: 39.51 | Avg Reward: 38.78 | Max Reward: 44.26
Iteration: 369 | Episodes: 15000 | Median Reward: 39.34 | Avg Reward: 38.96 | Max Reward: 44.26
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.2         |
| time/                   |               |
|    fps                  | 721           |
|    iterations           | 370           |
|    time_elapsed         | 2101          |
|    total_timesteps      | 1515520       |
| train/                  |               |
|    approx_kl            | 4.4045955e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -235          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -46           |
|    n_updates            | 3690          |
|    policy_gradient_loss | 2.57e-05      |
|    std                  | 1.22          |
|    value_loss           | 3.92          |
-------------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: 40.16 | Avg Reward: 39.76 | Max Reward: 44.26
Iteration: 374 | Episodes: 15200 | Median Reward: 40.62 | Avg Reward: 40.72 | Max Reward: 44.26
Iteration: 377 | Episodes: 15300 | Median Reward: 40.19 | Avg Reward: 39.53 | Max Reward: 44.26
Iteration: 379 | Episodes: 15400 | Median Reward: 41.07 | Avg Reward: 40.99 | Max Reward: 44.26
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 709          |
|    iterations           | 380          |
|    time_elapsed         | 2194         |
|    total_timesteps      | 1556480      |
| train/                  |              |
|    approx_kl            | 0.0006186667 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -235         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -39.8        |
|    n_updates            | 3790         |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.22         |
|    value_loss           | 9.95         |
------------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: 40.87 | Avg Reward: 40.42 | Max Reward: 44.73
Iteration: 384 | Episodes: 15600 | Median Reward: 38.21 | Avg Reward: 39.38 | Max Reward: 44.73
Iteration: 387 | Episodes: 15700 | Median Reward: 40.16 | Avg Reward: 39.43 | Max Reward: 44.73
Iteration: 389 | Episodes: 15800 | Median Reward: 41.27 | Avg Reward: 41.40 | Max Reward: 44.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.5        |
| time/                   |              |
|    fps                  | 694          |
|    iterations           | 390          |
|    time_elapsed         | 2299         |
|    total_timesteps      | 1597440      |
| train/                  |              |
|    approx_kl            | 0.0001196612 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -236         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -45.8        |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.000319    |
|    std                  | 1.23         |
|    value_loss           | 3.7          |
------------------------------------------
Iteration: 392 | Episodes: 15900 | Median Reward: 39.68 | Avg Reward: 39.47 | Max Reward: 44.73
Iteration: 394 | Episodes: 16000 | Median Reward: 40.27 | Avg Reward: 39.44 | Max Reward: 44.73
Iteration: 396 | Episodes: 16100 | Median Reward: 39.80 | Avg Reward: 39.92 | Max Reward: 44.73
Iteration: 399 | Episodes: 16200 | Median Reward: 37.01 | Avg Reward: 38.57 | Max Reward: 44.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.4        |
| time/                   |              |
|    fps                  | 698          |
|    iterations           | 400          |
|    time_elapsed         | 2344         |
|    total_timesteps      | 1638400      |
| train/                  |              |
|    approx_kl            | 0.0042803576 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -237         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -47.2        |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.00293     |
|    std                  | 1.24         |
|    value_loss           | 0.682        |
------------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: 40.92 | Avg Reward: 40.34 | Max Reward: 44.73
Iteration: 404 | Episodes: 16400 | Median Reward: 39.99 | Avg Reward: 39.61 | Max Reward: 44.73
Iteration: 406 | Episodes: 16500 | Median Reward: 40.63 | Avg Reward: 39.87 | Max Reward: 44.73
Iteration: 409 | Episodes: 16600 | Median Reward: 40.49 | Avg Reward: 38.50 | Max Reward: 44.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.1       |
| time/                   |             |
|    fps                  | 702         |
|    iterations           | 410         |
|    time_elapsed         | 2390        |
|    total_timesteps      | 1679360     |
| train/                  |             |
|    approx_kl            | 0.017736875 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.3         |
|    entropy_loss         | -239        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -47.3       |
|    n_updates            | 4090        |
|    policy_gradient_loss | 0.00795     |
|    std                  | 1.25        |
|    value_loss           | 1.96        |
-----------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: 40.66 | Avg Reward: 39.91 | Max Reward: 44.73
Iteration: 414 | Episodes: 16800 | Median Reward: 40.55 | Avg Reward: 40.76 | Max Reward: 44.73
Iteration: 416 | Episodes: 16900 | Median Reward: 39.74 | Avg Reward: 39.57 | Max Reward: 44.73
Iteration: 419 | Episodes: 17000 | Median Reward: 40.07 | Avg Reward: 39.91 | Max Reward: 44.73
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.2         |
| time/                   |               |
|    fps                  | 691           |
|    iterations           | 420           |
|    time_elapsed         | 2488          |
|    total_timesteps      | 1720320       |
| train/                  |               |
|    approx_kl            | 0.00083645526 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -240          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -47.4         |
|    n_updates            | 4190          |
|    policy_gradient_loss | -0.00129      |
|    std                  | 1.26          |
|    value_loss           | 1.35          |
-------------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: 39.99 | Avg Reward: 40.40 | Max Reward: 44.73
Iteration: 424 | Episodes: 17200 | Median Reward: 41.93 | Avg Reward: 40.82 | Max Reward: 44.73
Iteration: 426 | Episodes: 17300 | Median Reward: 39.61 | Avg Reward: 39.59 | Max Reward: 44.73
Iteration: 429 | Episodes: 17400 | Median Reward: 39.83 | Avg Reward: 39.19 | Max Reward: 44.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.6        |
| time/                   |              |
|    fps                  | 690          |
|    iterations           | 430          |
|    time_elapsed         | 2551         |
|    total_timesteps      | 1761280      |
| train/                  |              |
|    approx_kl            | 0.0063854223 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -241         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -47.3        |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.00596     |
|    std                  | 1.27         |
|    value_loss           | 1.64         |
------------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: 40.62 | Avg Reward: 40.42 | Max Reward: 44.73
Iteration: 433 | Episodes: 17600 | Median Reward: 40.65 | Avg Reward: 40.68 | Max Reward: 44.73
Iteration: 436 | Episodes: 17700 | Median Reward: 39.35 | Avg Reward: 39.11 | Max Reward: 44.73
Iteration: 438 | Episodes: 17800 | Median Reward: 40.67 | Avg Reward: 39.48 | Max Reward: 44.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.5       |
| time/                   |             |
|    fps                  | 687         |
|    iterations           | 440         |
|    time_elapsed         | 2621        |
|    total_timesteps      | 1802240     |
| train/                  |             |
|    approx_kl            | 0.010989446 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -242        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -44.7       |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0012     |
|    std                  | 1.28        |
|    value_loss           | 3.26        |
-----------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 41.12 | Avg Reward: 41.07 | Max Reward: 44.73
Iteration: 443 | Episodes: 18000 | Median Reward: 40.87 | Avg Reward: 40.58 | Max Reward: 44.73
Iteration: 446 | Episodes: 18100 | Median Reward: 40.50 | Avg Reward: 40.47 | Max Reward: 44.73
Iteration: 448 | Episodes: 18200 | Median Reward: 40.49 | Avg Reward: 41.07 | Max Reward: 44.73
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.8         |
| time/                   |               |
|    fps                  | 687           |
|    iterations           | 450           |
|    time_elapsed         | 2682          |
|    total_timesteps      | 1843200       |
| train/                  |               |
|    approx_kl            | 0.00013541713 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -243          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -47.8         |
|    n_updates            | 4490          |
|    policy_gradient_loss | -0.000447     |
|    std                  | 1.29          |
|    value_loss           | 3.32          |
-------------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 40.26 | Avg Reward: 40.89 | Max Reward: 44.73
Iteration: 453 | Episodes: 18400 | Median Reward: 39.82 | Avg Reward: 38.85 | Max Reward: 44.73
Iteration: 456 | Episodes: 18500 | Median Reward: 40.43 | Avg Reward: 40.74 | Max Reward: 44.73
Iteration: 458 | Episodes: 18600 | Median Reward: 43.16 | Avg Reward: 41.72 | Max Reward: 44.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.9        |
| time/                   |              |
|    fps                  | 675          |
|    iterations           | 460          |
|    time_elapsed         | 2791         |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 0.0011936161 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -245         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -48.8        |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.00054     |
|    std                  | 1.3          |
|    value_loss           | 0.859        |
------------------------------------------
Iteration: 461 | Episodes: 18700 | Median Reward: 41.31 | Avg Reward: 40.45 | Max Reward: 44.73
Iteration: 463 | Episodes: 18800 | Median Reward: 41.65 | Avg Reward: 40.97 | Max Reward: 44.73
Iteration: 466 | Episodes: 18900 | Median Reward: 40.70 | Avg Reward: 40.68 | Max Reward: 44.73
Iteration: 468 | Episodes: 19000 | Median Reward: 41.25 | Avg Reward: 40.32 | Max Reward: 44.73
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.1         |
| time/                   |               |
|    fps                  | 667           |
|    iterations           | 470           |
|    time_elapsed         | 2883          |
|    total_timesteps      | 1925120       |
| train/                  |               |
|    approx_kl            | 0.00010007582 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -245          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -46.6         |
|    n_updates            | 4690          |
|    policy_gradient_loss | -0.00111      |
|    std                  | 1.31          |
|    value_loss           | 7.28          |
-------------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 40.29 | Avg Reward: 39.86 | Max Reward: 44.73
Iteration: 473 | Episodes: 19200 | Median Reward: 39.99 | Avg Reward: 40.48 | Max Reward: 44.73
Iteration: 475 | Episodes: 19300 | Median Reward: 40.32 | Avg Reward: 40.40 | Max Reward: 44.73
Iteration: 478 | Episodes: 19400 | Median Reward: 40.70 | Avg Reward: 40.61 | Max Reward: 44.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.1       |
| time/                   |             |
|    fps                  | 666         |
|    iterations           | 480         |
|    time_elapsed         | 2951        |
|    total_timesteps      | 1966080     |
| train/                  |             |
|    approx_kl            | 0.011323973 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -246        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -49.1       |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.0292     |
|    std                  | 1.32        |
|    value_loss           | 1.25        |
-----------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 41.76 | Avg Reward: 40.61 | Max Reward: 44.73
Iteration: 483 | Episodes: 19600 | Median Reward: 41.03 | Avg Reward: 40.87 | Max Reward: 44.79
Iteration: 485 | Episodes: 19700 | Median Reward: 40.51 | Avg Reward: 41.05 | Max Reward: 44.79
Iteration: 488 | Episodes: 19800 | Median Reward: 43.50 | Avg Reward: 41.56 | Max Reward: 44.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 656          |
|    iterations           | 490          |
|    time_elapsed         | 3059         |
|    total_timesteps      | 2007040      |
| train/                  |              |
|    approx_kl            | 0.0012675563 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -248         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -49.3        |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.000726    |
|    std                  | 1.33         |
|    value_loss           | 1.46         |
------------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 40.68 | Avg Reward: 40.94 | Max Reward: 44.79
Iteration: 493 | Episodes: 20000 | Median Reward: 43.03 | Avg Reward: 41.85 | Max Reward: 44.79
Iteration: 495 | Episodes: 20100 | Median Reward: 41.93 | Avg Reward: 41.10 | Max Reward: 44.79
Iteration: 498 | Episodes: 20200 | Median Reward: 40.09 | Avg Reward: 39.14 | Max Reward: 44.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.4         |
| time/                   |               |
|    fps                  | 654           |
|    iterations           | 500           |
|    time_elapsed         | 3130          |
|    total_timesteps      | 2048000       |
| train/                  |               |
|    approx_kl            | 0.00012371152 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -249          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -47.9         |
|    n_updates            | 4990          |
|    policy_gradient_loss | 0.000832      |
|    std                  | 1.34          |
|    value_loss           | 0.996         |
-------------------------------------------
Iteration: 500 | Episodes: 20300 | Median Reward: 39.78 | Avg Reward: 39.77 | Max Reward: 44.79
Iteration: 503 | Episodes: 20400 | Median Reward: 38.69 | Avg Reward: 38.95 | Max Reward: 44.79
Iteration: 505 | Episodes: 20500 | Median Reward: 40.03 | Avg Reward: 39.65 | Max Reward: 44.79
Iteration: 507 | Episodes: 20600 | Median Reward: 39.56 | Avg Reward: 39.66 | Max Reward: 44.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.5        |
| time/                   |              |
|    fps                  | 646          |
|    iterations           | 510          |
|    time_elapsed         | 3232         |
|    total_timesteps      | 2088960      |
| train/                  |              |
|    approx_kl            | 0.0011010678 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -49.6        |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.00184     |
|    std                  | 1.35         |
|    value_loss           | 1.03         |
------------------------------------------
Iteration: 510 | Episodes: 20700 | Median Reward: 42.58 | Avg Reward: 41.67 | Max Reward: 44.79
Iteration: 512 | Episodes: 20800 | Median Reward: 41.49 | Avg Reward: 41.60 | Max Reward: 44.79
Iteration: 515 | Episodes: 20900 | Median Reward: 38.78 | Avg Reward: 39.54 | Max Reward: 44.79
Iteration: 517 | Episodes: 21000 | Median Reward: 39.99 | Avg Reward: 38.87 | Max Reward: 44.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.1        |
| time/                   |              |
|    fps                  | 640          |
|    iterations           | 520          |
|    time_elapsed         | 3325         |
|    total_timesteps      | 2129920      |
| train/                  |              |
|    approx_kl            | 0.0061757974 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -251         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -49.3        |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00443     |
|    std                  | 1.36         |
|    value_loss           | 0.628        |
------------------------------------------
Iteration: 520 | Episodes: 21100 | Median Reward: 40.20 | Avg Reward: 40.23 | Max Reward: 44.87
Iteration: 522 | Episodes: 21200 | Median Reward: 40.38 | Avg Reward: 39.45 | Max Reward: 44.87
Iteration: 525 | Episodes: 21300 | Median Reward: 40.48 | Avg Reward: 40.65 | Max Reward: 44.87
Iteration: 527 | Episodes: 21400 | Median Reward: 43.03 | Avg Reward: 41.58 | Max Reward: 44.87
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.2        |
| time/                   |              |
|    fps                  | 634          |
|    iterations           | 530          |
|    time_elapsed         | 3422         |
|    total_timesteps      | 2170880      |
| train/                  |              |
|    approx_kl            | 0.0015593625 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -252         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -48.4        |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.00716     |
|    std                  | 1.38         |
|    value_loss           | 2.41         |
------------------------------------------
Iteration: 530 | Episodes: 21500 | Median Reward: 39.06 | Avg Reward: 38.47 | Max Reward: 44.87
Iteration: 532 | Episodes: 21600 | Median Reward: 40.45 | Avg Reward: 40.11 | Max Reward: 44.87
Iteration: 535 | Episodes: 21700 | Median Reward: 40.19 | Avg Reward: 39.98 | Max Reward: 44.87
Iteration: 537 | Episodes: 21800 | Median Reward: 39.26 | Avg Reward: 39.56 | Max Reward: 44.87
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.8        |
| time/                   |              |
|    fps                  | 627          |
|    iterations           | 540          |
|    time_elapsed         | 3524         |
|    total_timesteps      | 2211840      |
| train/                  |              |
|    approx_kl            | 0.0011389736 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -253         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -50.4        |
|    n_updates            | 5390         |
|    policy_gradient_loss | -0.000154    |
|    std                  | 1.39         |
|    value_loss           | 1.79         |
------------------------------------------
Iteration: 540 | Episodes: 21900 | Median Reward: 40.66 | Avg Reward: 40.22 | Max Reward: 44.87
Iteration: 542 | Episodes: 22000 | Median Reward: 40.07 | Avg Reward: 40.75 | Max Reward: 44.87
Iteration: 544 | Episodes: 22100 | Median Reward: 40.19 | Avg Reward: 40.45 | Max Reward: 44.87
Iteration: 547 | Episodes: 22200 | Median Reward: 40.30 | Avg Reward: 39.29 | Max Reward: 44.87
Iteration: 549 | Episodes: 22300 | Median Reward: 42.36 | Avg Reward: 41.27 | Max Reward: 44.87
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.7        |
| time/                   |              |
|    fps                  | 628          |
|    iterations           | 550          |
|    time_elapsed         | 3584         |
|    total_timesteps      | 2252800      |
| train/                  |              |
|    approx_kl            | 0.0014336531 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -254         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -50.4        |
|    n_updates            | 5490         |
|    policy_gradient_loss | -0.00239     |
|    std                  | 1.4          |
|    value_loss           | 0.714        |
------------------------------------------
Iteration: 552 | Episodes: 22400 | Median Reward: 41.95 | Avg Reward: 41.67 | Max Reward: 44.87
Iteration: 554 | Episodes: 22500 | Median Reward: 39.80 | Avg Reward: 39.99 | Max Reward: 44.87
Iteration: 557 | Episodes: 22600 | Median Reward: 38.15 | Avg Reward: 38.32 | Max Reward: 44.87
Iteration: 559 | Episodes: 22700 | Median Reward: 39.60 | Avg Reward: 38.51 | Max Reward: 44.87
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.9       |
| time/                   |             |
|    fps                  | 621         |
|    iterations           | 560         |
|    time_elapsed         | 3692        |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.030218955 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -255        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -50.8       |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.0231     |
|    std                  | 1.42        |
|    value_loss           | 0.777       |
-----------------------------------------
Iteration: 562 | Episodes: 22800 | Median Reward: 40.40 | Avg Reward: 39.86 | Max Reward: 44.87
Iteration: 564 | Episodes: 22900 | Median Reward: 41.23 | Avg Reward: 39.66 | Max Reward: 44.87
Iteration: 567 | Episodes: 23000 | Median Reward: 39.72 | Avg Reward: 39.42 | Max Reward: 44.87
Iteration: 569 | Episodes: 23100 | Median Reward: 39.91 | Avg Reward: 39.98 | Max Reward: 44.87
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.9        |
| time/                   |              |
|    fps                  | 618          |
|    iterations           | 570          |
|    time_elapsed         | 3774         |
|    total_timesteps      | 2334720      |
| train/                  |              |
|    approx_kl            | 0.0036534485 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -257         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -50.6        |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.00038     |
|    std                  | 1.42         |
|    value_loss           | 0.686        |
------------------------------------------
Iteration: 572 | Episodes: 23200 | Median Reward: 39.69 | Avg Reward: 40.40 | Max Reward: 44.87
Iteration: 574 | Episodes: 23300 | Median Reward: 39.44 | Avg Reward: 40.01 | Max Reward: 44.87
Iteration: 577 | Episodes: 23400 | Median Reward: 40.66 | Avg Reward: 40.13 | Max Reward: 44.87
Iteration: 579 | Episodes: 23500 | Median Reward: 40.30 | Avg Reward: 40.36 | Max Reward: 44.87
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.3       |
| time/                   |             |
|    fps                  | 612         |
|    iterations           | 580         |
|    time_elapsed         | 3877        |
|    total_timesteps      | 2375680     |
| train/                  |             |
|    approx_kl            | 0.004953592 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -258        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -51.3       |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.014      |
|    std                  | 1.44        |
|    value_loss           | 1.32        |
-----------------------------------------
Iteration: 581 | Episodes: 23600 | Median Reward: 41.78 | Avg Reward: 41.91 | Max Reward: 44.87
Iteration: 584 | Episodes: 23700 | Median Reward: 41.11 | Avg Reward: 40.56 | Max Reward: 44.87
Iteration: 586 | Episodes: 23800 | Median Reward: 40.65 | Avg Reward: 40.69 | Max Reward: 44.87
Iteration: 589 | Episodes: 23900 | Median Reward: 40.31 | Avg Reward: 39.56 | Max Reward: 44.87
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.8       |
| time/                   |             |
|    fps                  | 609         |
|    iterations           | 590         |
|    time_elapsed         | 3967        |
|    total_timesteps      | 2416640     |
| train/                  |             |
|    approx_kl            | 0.039992824 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.3         |
|    entropy_loss         | -259        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -51.6       |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.0609     |
|    std                  | 1.45        |
|    value_loss           | 1.12        |
-----------------------------------------
Iteration: 591 | Episodes: 24000 | Median Reward: 40.56 | Avg Reward: 41.13 | Max Reward: 44.88
Iteration: 594 | Episodes: 24100 | Median Reward: 39.73 | Avg Reward: 39.65 | Max Reward: 44.88
Iteration: 596 | Episodes: 24200 | Median Reward: 40.29 | Avg Reward: 39.55 | Max Reward: 44.88
Iteration: 599 | Episodes: 24300 | Median Reward: 40.36 | Avg Reward: 40.37 | Max Reward: 44.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.4         |
| time/                   |               |
|    fps                  | 610           |
|    iterations           | 600           |
|    time_elapsed         | 4026          |
|    total_timesteps      | 2457600       |
| train/                  |               |
|    approx_kl            | 0.00019772274 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -260          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -49.7         |
|    n_updates            | 5990          |
|    policy_gradient_loss | -0.000258     |
|    std                  | 1.46          |
|    value_loss           | 1.72          |
-------------------------------------------
Iteration: 601 | Episodes: 24400 | Median Reward: 40.06 | Avg Reward: 40.16 | Max Reward: 44.88
Iteration: 604 | Episodes: 24500 | Median Reward: 41.76 | Avg Reward: 40.63 | Max Reward: 44.88
Iteration: 606 | Episodes: 24600 | Median Reward: 39.74 | Avg Reward: 39.15 | Max Reward: 44.88
Iteration: 609 | Episodes: 24700 | Median Reward: 41.29 | Avg Reward: 40.93 | Max Reward: 44.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.5       |
| time/                   |             |
|    fps                  | 604         |
|    iterations           | 610         |
|    time_elapsed         | 4132        |
|    total_timesteps      | 2498560     |
| train/                  |             |
|    approx_kl            | 0.012016861 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -260        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -52         |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0198     |
|    std                  | 1.48        |
|    value_loss           | 1.2         |
-----------------------------------------
Iteration: 611 | Episodes: 24800 | Median Reward: 41.25 | Avg Reward: 41.17 | Max Reward: 44.88
Iteration: 613 | Episodes: 24900 | Median Reward: 39.99 | Avg Reward: 40.18 | Max Reward: 44.88
Iteration: 616 | Episodes: 25000 | Median Reward: 40.34 | Avg Reward: 39.26 | Max Reward: 44.88
Iteration: 618 | Episodes: 25100 | Median Reward: 40.38 | Avg Reward: 40.35 | Max Reward: 44.88
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -59.5      |
| time/                   |            |
|    fps                  | 603        |
|    iterations           | 620        |
|    time_elapsed         | 4210       |
|    total_timesteps      | 2539520    |
| train/                  |            |
|    approx_kl            | 0.00578189 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -262       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -52.1      |
|    n_updates            | 6190       |
|    policy_gradient_loss | -0.0199    |
|    std                  | 1.49       |
|    value_loss           | 1.34       |
----------------------------------------
Iteration: 621 | Episodes: 25200 | Median Reward: 42.56 | Avg Reward: 41.19 | Max Reward: 45.59
Iteration: 623 | Episodes: 25300 | Median Reward: 40.32 | Avg Reward: 39.69 | Max Reward: 45.59
Iteration: 626 | Episodes: 25400 | Median Reward: 39.50 | Avg Reward: 39.54 | Max Reward: 45.59
Iteration: 628 | Episodes: 25500 | Median Reward: 42.26 | Avg Reward: 41.87 | Max Reward: 45.59
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59          |
| time/                   |              |
|    fps                  | 604          |
|    iterations           | 630          |
|    time_elapsed         | 4270         |
|    total_timesteps      | 2580480      |
| train/                  |              |
|    approx_kl            | 0.0011312529 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -263         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -51.7        |
|    n_updates            | 6290         |
|    policy_gradient_loss | -0.00278     |
|    std                  | 1.5          |
|    value_loss           | 1.52         |
------------------------------------------
Iteration: 631 | Episodes: 25600 | Median Reward: 40.45 | Avg Reward: 40.51 | Max Reward: 45.59
Iteration: 633 | Episodes: 25700 | Median Reward: 41.54 | Avg Reward: 41.49 | Max Reward: 45.59
Iteration: 636 | Episodes: 25800 | Median Reward: 40.70 | Avg Reward: 40.95 | Max Reward: 45.59
Iteration: 638 | Episodes: 25900 | Median Reward: 42.82 | Avg Reward: 40.97 | Max Reward: 45.59
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.9        |
| time/                   |              |
|    fps                  | 599          |
|    iterations           | 640          |
|    time_elapsed         | 4371         |
|    total_timesteps      | 2621440      |
| train/                  |              |
|    approx_kl            | 0.0009360432 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -265         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -52.7        |
|    n_updates            | 6390         |
|    policy_gradient_loss | 0.000659     |
|    std                  | 1.52         |
|    value_loss           | 0.373        |
------------------------------------------
Iteration: 641 | Episodes: 26000 | Median Reward: 40.54 | Avg Reward: 39.69 | Max Reward: 45.59
Iteration: 643 | Episodes: 26100 | Median Reward: 40.58 | Avg Reward: 39.53 | Max Reward: 45.59
Iteration: 646 | Episodes: 26200 | Median Reward: 40.66 | Avg Reward: 40.16 | Max Reward: 45.59
Iteration: 648 | Episodes: 26300 | Median Reward: 40.59 | Avg Reward: 40.36 | Max Reward: 45.59
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.5         |
| time/                   |               |
|    fps                  | 597           |
|    iterations           | 650           |
|    time_elapsed         | 4453          |
|    total_timesteps      | 2662400       |
| train/                  |               |
|    approx_kl            | 0.00066548807 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -266          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -53.1         |
|    n_updates            | 6490          |
|    policy_gradient_loss | -0.00266      |
|    std                  | 1.53          |
|    value_loss           | 1.24          |
-------------------------------------------
Iteration: 650 | Episodes: 26400 | Median Reward: 41.94 | Avg Reward: 41.23 | Max Reward: 45.59
Iteration: 653 | Episodes: 26500 | Median Reward: 40.81 | Avg Reward: 40.96 | Max Reward: 45.59
Iteration: 655 | Episodes: 26600 | Median Reward: 41.36 | Avg Reward: 40.93 | Max Reward: 45.59
Iteration: 658 | Episodes: 26700 | Median Reward: 40.51 | Avg Reward: 39.89 | Max Reward: 45.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.1       |
| time/                   |             |
|    fps                  | 600         |
|    iterations           | 660         |
|    time_elapsed         | 4500        |
|    total_timesteps      | 2703360     |
| train/                  |             |
|    approx_kl            | 0.003225555 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -267        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -52.6       |
|    n_updates            | 6590        |
|    policy_gradient_loss | -0.000266   |
|    std                  | 1.54        |
|    value_loss           | 2.06        |
-----------------------------------------
Iteration: 660 | Episodes: 26800 | Median Reward: 41.66 | Avg Reward: 40.60 | Max Reward: 45.59
Iteration: 663 | Episodes: 26900 | Median Reward: 41.20 | Avg Reward: 40.92 | Max Reward: 45.59
Iteration: 665 | Episodes: 27000 | Median Reward: 42.61 | Avg Reward: 40.91 | Max Reward: 45.59
Iteration: 668 | Episodes: 27100 | Median Reward: 41.02 | Avg Reward: 41.66 | Max Reward: 45.59
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.2        |
| time/                   |              |
|    fps                  | 600          |
|    iterations           | 670          |
|    time_elapsed         | 4568         |
|    total_timesteps      | 2744320      |
| train/                  |              |
|    approx_kl            | 0.0004706055 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -268         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -53          |
|    n_updates            | 6690         |
|    policy_gradient_loss | -0.000493    |
|    std                  | 1.56         |
|    value_loss           | 1.69         |
------------------------------------------
Iteration: 670 | Episodes: 27200 | Median Reward: 39.53 | Avg Reward: 39.71 | Max Reward: 45.59
Iteration: 673 | Episodes: 27300 | Median Reward: 42.17 | Avg Reward: 40.89 | Max Reward: 45.59
Iteration: 675 | Episodes: 27400 | Median Reward: 42.40 | Avg Reward: 41.64 | Max Reward: 45.59
Iteration: 678 | Episodes: 27500 | Median Reward: 40.87 | Avg Reward: 40.28 | Max Reward: 45.59
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.3        |
| time/                   |              |
|    fps                  | 596          |
|    iterations           | 680          |
|    time_elapsed         | 4672         |
|    total_timesteps      | 2785280      |
| train/                  |              |
|    approx_kl            | 0.0036519156 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -269         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -52.8        |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.00301     |
|    std                  | 1.57         |
|    value_loss           | 0.547        |
------------------------------------------
Iteration: 680 | Episodes: 27600 | Median Reward: 41.29 | Avg Reward: 41.06 | Max Reward: 45.59
Iteration: 683 | Episodes: 27700 | Median Reward: 40.92 | Avg Reward: 39.94 | Max Reward: 45.59
Iteration: 685 | Episodes: 27800 | Median Reward: 40.37 | Avg Reward: 40.28 | Max Reward: 45.59
Iteration: 687 | Episodes: 27900 | Median Reward: 42.23 | Avg Reward: 41.27 | Max Reward: 45.59
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.6        |
| time/                   |              |
|    fps                  | 594          |
|    iterations           | 690          |
|    time_elapsed         | 4751         |
|    total_timesteps      | 2826240      |
| train/                  |              |
|    approx_kl            | 0.0004681438 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -270         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -53.5        |
|    n_updates            | 6890         |
|    policy_gradient_loss | -0.00186     |
|    std                  | 1.59         |
|    value_loss           | 1.24         |
------------------------------------------
Iteration: 690 | Episodes: 28000 | Median Reward: 41.60 | Avg Reward: 41.15 | Max Reward: 45.59
Iteration: 692 | Episodes: 28100 | Median Reward: 41.48 | Avg Reward: 41.04 | Max Reward: 45.59
Iteration: 695 | Episodes: 28200 | Median Reward: 40.48 | Avg Reward: 39.23 | Max Reward: 45.59
Iteration: 697 | Episodes: 28300 | Median Reward: 40.86 | Avg Reward: 40.63 | Max Reward: 45.59
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.5        |
| time/                   |              |
|    fps                  | 590          |
|    iterations           | 700          |
|    time_elapsed         | 4857         |
|    total_timesteps      | 2867200      |
| train/                  |              |
|    approx_kl            | 0.0010189929 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -271         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -53.8        |
|    n_updates            | 6990         |
|    policy_gradient_loss | -0.00115     |
|    std                  | 1.6          |
|    value_loss           | 1.12         |
------------------------------------------
Iteration: 700 | Episodes: 28400 | Median Reward: 39.56 | Avg Reward: 39.28 | Max Reward: 45.59
Iteration: 702 | Episodes: 28500 | Median Reward: 39.99 | Avg Reward: 40.11 | Max Reward: 45.59
Iteration: 705 | Episodes: 28600 | Median Reward: 40.75 | Avg Reward: 40.91 | Max Reward: 45.59
Iteration: 707 | Episodes: 28700 | Median Reward: 41.71 | Avg Reward: 40.98 | Max Reward: 45.59
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.6       |
| time/                   |             |
|    fps                  | 588         |
|    iterations           | 710         |
|    time_elapsed         | 4944        |
|    total_timesteps      | 2908160     |
| train/                  |             |
|    approx_kl            | 0.005513708 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -272        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -53.7       |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.0134     |
|    std                  | 1.62        |
|    value_loss           | 0.739       |
-----------------------------------------
Iteration: 710 | Episodes: 28800 | Median Reward: 39.13 | Avg Reward: 39.35 | Max Reward: 45.59
Iteration: 712 | Episodes: 28900 | Median Reward: 40.35 | Avg Reward: 40.02 | Max Reward: 45.59
Iteration: 715 | Episodes: 29000 | Median Reward: 40.52 | Avg Reward: 39.61 | Max Reward: 45.59
Iteration: 717 | Episodes: 29100 | Median Reward: 39.90 | Avg Reward: 40.12 | Max Reward: 45.59
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 586          |
|    iterations           | 720          |
|    time_elapsed         | 5025         |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0023047044 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -274         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -54.1        |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.00445     |
|    std                  | 1.63         |
|    value_loss           | 0.947        |
------------------------------------------
Iteration: 720 | Episodes: 29200 | Median Reward: 40.31 | Avg Reward: 40.40 | Max Reward: 45.59
Iteration: 722 | Episodes: 29300 | Median Reward: 40.38 | Avg Reward: 40.41 | Max Reward: 45.59
Iteration: 724 | Episodes: 29400 | Median Reward: 39.66 | Avg Reward: 39.47 | Max Reward: 45.59
Iteration: 727 | Episodes: 29500 | Median Reward: 40.73 | Avg Reward: 39.92 | Max Reward: 45.59
Iteration: 729 | Episodes: 29600 | Median Reward: 41.96 | Avg Reward: 40.80 | Max Reward: 45.59
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 582          |
|    iterations           | 730          |
|    time_elapsed         | 5131         |
|    total_timesteps      | 2990080      |
| train/                  |              |
|    approx_kl            | 0.0029072475 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -276         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -53.5        |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.00941     |
|    std                  | 1.65         |
|    value_loss           | 1.45         |
------------------------------------------
Iteration: 732 | Episodes: 29700 | Median Reward: 40.10 | Avg Reward: 39.83 | Max Reward: 45.59
Iteration: 734 | Episodes: 29800 | Median Reward: 43.06 | Avg Reward: 40.84 | Max Reward: 45.59
Iteration: 737 | Episodes: 29900 | Median Reward: 43.09 | Avg Reward: 42.26 | Max Reward: 45.59
Iteration: 739 | Episodes: 30000 | Median Reward: 41.02 | Avg Reward: 41.32 | Max Reward: 45.59
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -58.7      |
| time/                   |            |
|    fps                  | 583        |
|    iterations           | 740        |
|    time_elapsed         | 5193       |
|    total_timesteps      | 3031040    |
| train/                  |            |
|    approx_kl            | 0.00900423 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -277       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -55.3      |
|    n_updates            | 7390       |
|    policy_gradient_loss | -0.00928   |
|    std                  | 1.66       |
|    value_loss           | 0.806      |
----------------------------------------
Iteration: 742 | Episodes: 30100 | Median Reward: 38.39 | Avg Reward: 39.32 | Max Reward: 45.59
Iteration: 744 | Episodes: 30200 | Median Reward: 41.48 | Avg Reward: 41.55 | Max Reward: 45.59
Iteration: 747 | Episodes: 30300 | Median Reward: 41.07 | Avg Reward: 41.01 | Max Reward: 45.59
Iteration: 749 | Episodes: 30400 | Median Reward: 39.84 | Avg Reward: 39.84 | Max Reward: 45.59
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.2         |
| time/                   |               |
|    fps                  | 581           |
|    iterations           | 750           |
|    time_elapsed         | 5280          |
|    total_timesteps      | 3072000       |
| train/                  |               |
|    approx_kl            | 0.00029849057 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -278          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -55.2         |
|    n_updates            | 7490          |
|    policy_gradient_loss | -0.000833     |
|    std                  | 1.67          |
|    value_loss           | 1.34          |
-------------------------------------------
Iteration: 752 | Episodes: 30500 | Median Reward: 41.00 | Avg Reward: 40.51 | Max Reward: 45.59
Iteration: 754 | Episodes: 30600 | Median Reward: 42.54 | Avg Reward: 41.70 | Max Reward: 45.59
Iteration: 757 | Episodes: 30700 | Median Reward: 43.05 | Avg Reward: 42.00 | Max Reward: 45.93
Iteration: 759 | Episodes: 30800 | Median Reward: 43.49 | Avg Reward: 42.68 | Max Reward: 45.93
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.9        |
| time/                   |              |
|    fps                  | 577          |
|    iterations           | 760          |
|    time_elapsed         | 5390         |
|    total_timesteps      | 3112960      |
| train/                  |              |
|    approx_kl            | 7.556984e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -278         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -54.9        |
|    n_updates            | 7590         |
|    policy_gradient_loss | -6.54e-05    |
|    std                  | 1.68         |
|    value_loss           | 2.11         |
------------------------------------------
Iteration: 761 | Episodes: 30900 | Median Reward: 41.81 | Avg Reward: 41.28 | Max Reward: 45.93
Iteration: 764 | Episodes: 31000 | Median Reward: 41.13 | Avg Reward: 41.13 | Max Reward: 45.93
Iteration: 766 | Episodes: 31100 | Median Reward: 40.97 | Avg Reward: 40.60 | Max Reward: 45.93
Iteration: 769 | Episodes: 31200 | Median Reward: 40.74 | Avg Reward: 39.40 | Max Reward: 45.93
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.3        |
| time/                   |              |
|    fps                  | 578          |
|    iterations           | 770          |
|    time_elapsed         | 5452         |
|    total_timesteps      | 3153920      |
| train/                  |              |
|    approx_kl            | 0.0005064588 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -279         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -54.9        |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.0035      |
|    std                  | 1.7          |
|    value_loss           | 1.99         |
------------------------------------------
Iteration: 771 | Episodes: 31300 | Median Reward: 41.10 | Avg Reward: 40.27 | Max Reward: 45.93
Iteration: 774 | Episodes: 31400 | Median Reward: 40.96 | Avg Reward: 41.23 | Max Reward: 45.93
Iteration: 776 | Episodes: 31500 | Median Reward: 39.88 | Avg Reward: 39.86 | Max Reward: 45.93
Iteration: 779 | Episodes: 31600 | Median Reward: 43.55 | Avg Reward: 41.44 | Max Reward: 45.93
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.7        |
| time/                   |              |
|    fps                  | 574          |
|    iterations           | 780          |
|    time_elapsed         | 5558         |
|    total_timesteps      | 3194880      |
| train/                  |              |
|    approx_kl            | 0.0011568507 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -280         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -55          |
|    n_updates            | 7790         |
|    policy_gradient_loss | -0.00344     |
|    std                  | 1.72         |
|    value_loss           | 1.1          |
------------------------------------------
Iteration: 781 | Episodes: 31700 | Median Reward: 39.79 | Avg Reward: 40.23 | Max Reward: 45.93
Iteration: 784 | Episodes: 31800 | Median Reward: 41.47 | Avg Reward: 40.75 | Max Reward: 45.93
Iteration: 786 | Episodes: 31900 | Median Reward: 42.25 | Avg Reward: 41.70 | Max Reward: 45.93
Iteration: 789 | Episodes: 32000 | Median Reward: 40.86 | Avg Reward: 40.36 | Max Reward: 45.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.9       |
| time/                   |             |
|    fps                  | 574         |
|    iterations           | 790         |
|    time_elapsed         | 5637        |
|    total_timesteps      | 3235840     |
| train/                  |             |
|    approx_kl            | 0.010446593 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -281        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -55.2       |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.0165     |
|    std                  | 1.74        |
|    value_loss           | 1.19        |
-----------------------------------------
Iteration: 791 | Episodes: 32100 | Median Reward: 42.34 | Avg Reward: 40.19 | Max Reward: 45.93
Iteration: 793 | Episodes: 32200 | Median Reward: 41.17 | Avg Reward: 41.15 | Max Reward: 45.93
Iteration: 796 | Episodes: 32300 | Median Reward: 40.87 | Avg Reward: 41.08 | Max Reward: 45.93
Iteration: 798 | Episodes: 32400 | Median Reward: 40.87 | Avg Reward: 40.36 | Max Reward: 45.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.7       |
| time/                   |             |
|    fps                  | 571         |
|    iterations           | 800         |
|    time_elapsed         | 5735        |
|    total_timesteps      | 3276800     |
| train/                  |             |
|    approx_kl            | 0.008671757 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -282        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -56.3       |
|    n_updates            | 7990        |
|    policy_gradient_loss | -0.0183     |
|    std                  | 1.75        |
|    value_loss           | 0.534       |
-----------------------------------------
Iteration: 801 | Episodes: 32500 | Median Reward: 41.13 | Avg Reward: 41.00 | Max Reward: 45.93
Iteration: 803 | Episodes: 32600 | Median Reward: 42.57 | Avg Reward: 41.45 | Max Reward: 45.93
Iteration: 806 | Episodes: 32700 | Median Reward: 41.53 | Avg Reward: 40.66 | Max Reward: 45.93
Iteration: 808 | Episodes: 32800 | Median Reward: 37.90 | Avg Reward: 39.16 | Max Reward: 45.93
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.6       |
| time/                   |             |
|    fps                  | 569         |
|    iterations           | 810         |
|    time_elapsed         | 5824        |
|    total_timesteps      | 3317760     |
| train/                  |             |
|    approx_kl            | 0.038971845 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.3         |
|    entropy_loss         | -283        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -56         |
|    n_updates            | 8090        |
|    policy_gradient_loss | -0.0446     |
|    std                  | 1.77        |
|    value_loss           | 0.438       |
-----------------------------------------
Iteration: 811 | Episodes: 32900 | Median Reward: 40.13 | Avg Reward: 40.26 | Max Reward: 45.93
Iteration: 813 | Episodes: 33000 | Median Reward: 41.88 | Avg Reward: 41.04 | Max Reward: 45.93
Iteration: 816 | Episodes: 33100 | Median Reward: 40.68 | Avg Reward: 40.60 | Max Reward: 45.93
Iteration: 818 | Episodes: 33200 | Median Reward: 41.01 | Avg Reward: 40.43 | Max Reward: 45.93
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.1         |
| time/                   |               |
|    fps                  | 568           |
|    iterations           | 820           |
|    time_elapsed         | 5903          |
|    total_timesteps      | 3358720       |
| train/                  |               |
|    approx_kl            | 5.8356673e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -284          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -55.9         |
|    n_updates            | 8190          |
|    policy_gradient_loss | 9.01e-05      |
|    std                  | 1.78          |
|    value_loss           | 1.08          |
-------------------------------------------
Iteration: 821 | Episodes: 33300 | Median Reward: 40.54 | Avg Reward: 40.73 | Max Reward: 45.93
Iteration: 823 | Episodes: 33400 | Median Reward: 40.97 | Avg Reward: 41.35 | Max Reward: 45.98
Iteration: 826 | Episodes: 33500 | Median Reward: 41.04 | Avg Reward: 41.12 | Max Reward: 45.98
Iteration: 828 | Episodes: 33600 | Median Reward: 38.03 | Avg Reward: 39.08 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.7         |
| time/                   |               |
|    fps                  | 565           |
|    iterations           | 830           |
|    time_elapsed         | 6008          |
|    total_timesteps      | 3399680       |
| train/                  |               |
|    approx_kl            | 5.4751028e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -285          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -55.2         |
|    n_updates            | 8290          |
|    policy_gradient_loss | -0.0004       |
|    std                  | 1.79          |
|    value_loss           | 3.52          |
-------------------------------------------
Iteration: 830 | Episodes: 33700 | Median Reward: 40.51 | Avg Reward: 41.23 | Max Reward: 45.98
Iteration: 833 | Episodes: 33800 | Median Reward: 39.59 | Avg Reward: 39.27 | Max Reward: 45.98
Iteration: 835 | Episodes: 33900 | Median Reward: 41.23 | Avg Reward: 40.24 | Max Reward: 45.98
Iteration: 838 | Episodes: 34000 | Median Reward: 41.46 | Avg Reward: 40.77 | Max Reward: 45.98
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -60.3      |
| time/                   |            |
|    fps                  | 566        |
|    iterations           | 840        |
|    time_elapsed         | 6077       |
|    total_timesteps      | 3440640    |
| train/                  |            |
|    approx_kl            | 0.00872547 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -285       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -56.8      |
|    n_updates            | 8390       |
|    policy_gradient_loss | -0.00754   |
|    std                  | 1.8        |
|    value_loss           | 0.684      |
----------------------------------------
Iteration: 840 | Episodes: 34100 | Median Reward: 39.35 | Avg Reward: 39.25 | Max Reward: 45.98
Iteration: 843 | Episodes: 34200 | Median Reward: 40.53 | Avg Reward: 40.57 | Max Reward: 45.98
Iteration: 845 | Episodes: 34300 | Median Reward: 42.21 | Avg Reward: 41.44 | Max Reward: 45.98
Iteration: 848 | Episodes: 34400 | Median Reward: 40.37 | Avg Reward: 40.72 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.5         |
| time/                   |               |
|    fps                  | 565           |
|    iterations           | 850           |
|    time_elapsed         | 6153          |
|    total_timesteps      | 3481600       |
| train/                  |               |
|    approx_kl            | 2.8397422e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -286          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -56           |
|    n_updates            | 8490          |
|    policy_gradient_loss | -0.000125     |
|    std                  | 1.82          |
|    value_loss           | 1.6           |
-------------------------------------------
Iteration: 850 | Episodes: 34500 | Median Reward: 39.34 | Avg Reward: 39.86 | Max Reward: 45.98
Iteration: 853 | Episodes: 34600 | Median Reward: 39.55 | Avg Reward: 39.15 | Max Reward: 45.98
Iteration: 855 | Episodes: 34700 | Median Reward: 41.07 | Avg Reward: 40.61 | Max Reward: 45.98
Iteration: 858 | Episodes: 34800 | Median Reward: 40.60 | Avg Reward: 39.51 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.8        |
| time/                   |              |
|    fps                  | 565          |
|    iterations           | 860          |
|    time_elapsed         | 6229         |
|    total_timesteps      | 3522560      |
| train/                  |              |
|    approx_kl            | 0.0014375807 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -287         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -56.9        |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.00592     |
|    std                  | 1.83         |
|    value_loss           | 1.39         |
------------------------------------------
Iteration: 860 | Episodes: 34900 | Median Reward: 41.44 | Avg Reward: 40.86 | Max Reward: 45.98
Iteration: 863 | Episodes: 35000 | Median Reward: 40.52 | Avg Reward: 41.12 | Max Reward: 45.98
Iteration: 865 | Episodes: 35100 | Median Reward: 40.95 | Avg Reward: 40.05 | Max Reward: 45.98
Iteration: 867 | Episodes: 35200 | Median Reward: 43.38 | Avg Reward: 42.44 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.6         |
| time/                   |               |
|    fps                  | 564           |
|    iterations           | 870           |
|    time_elapsed         | 6312          |
|    total_timesteps      | 3563520       |
| train/                  |               |
|    approx_kl            | 0.00016891505 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -288          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -54.6         |
|    n_updates            | 8690          |
|    policy_gradient_loss | 0.000159      |
|    std                  | 1.85          |
|    value_loss           | 2.51          |
-------------------------------------------
Iteration: 870 | Episodes: 35300 | Median Reward: 42.20 | Avg Reward: 41.45 | Max Reward: 45.98
Iteration: 872 | Episodes: 35400 | Median Reward: 40.18 | Avg Reward: 40.27 | Max Reward: 45.98
Iteration: 875 | Episodes: 35500 | Median Reward: 40.44 | Avg Reward: 40.55 | Max Reward: 45.98
Iteration: 877 | Episodes: 35600 | Median Reward: 41.09 | Avg Reward: 41.05 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.6         |
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 880           |
|    time_elapsed         | 6368          |
|    total_timesteps      | 3604480       |
| train/                  |               |
|    approx_kl            | 0.00076216925 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -289          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -56.1         |
|    n_updates            | 8790          |
|    policy_gradient_loss | 7.73e-05      |
|    std                  | 1.87          |
|    value_loss           | 1.55          |
-------------------------------------------
Iteration: 880 | Episodes: 35700 | Median Reward: 40.64 | Avg Reward: 40.14 | Max Reward: 45.98
Iteration: 882 | Episodes: 35800 | Median Reward: 40.48 | Avg Reward: 40.00 | Max Reward: 45.98
Iteration: 885 | Episodes: 35900 | Median Reward: 40.45 | Avg Reward: 40.44 | Max Reward: 45.98
Iteration: 887 | Episodes: 36000 | Median Reward: 41.03 | Avg Reward: 40.59 | Max Reward: 45.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.5       |
| time/                   |             |
|    fps                  | 567         |
|    iterations           | 890         |
|    time_elapsed         | 6424        |
|    total_timesteps      | 3645440     |
| train/                  |             |
|    approx_kl            | 0.008171507 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -290        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -56.3       |
|    n_updates            | 8890        |
|    policy_gradient_loss | -0.00469    |
|    std                  | 1.89        |
|    value_loss           | 2.84        |
-----------------------------------------
Iteration: 890 | Episodes: 36100 | Median Reward: 38.76 | Avg Reward: 39.60 | Max Reward: 45.98
Iteration: 892 | Episodes: 36200 | Median Reward: 40.63 | Avg Reward: 40.78 | Max Reward: 45.98
Iteration: 895 | Episodes: 36300 | Median Reward: 42.93 | Avg Reward: 42.15 | Max Reward: 45.98
Iteration: 897 | Episodes: 36400 | Median Reward: 39.69 | Avg Reward: 39.48 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.5         |
| time/                   |               |
|    fps                  | 566           |
|    iterations           | 900           |
|    time_elapsed         | 6503          |
|    total_timesteps      | 3686400       |
| train/                  |               |
|    approx_kl            | 0.00076998235 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -291          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -55.9         |
|    n_updates            | 8990          |
|    policy_gradient_loss | 0.000537      |
|    std                  | 1.91          |
|    value_loss           | 1.75          |
-------------------------------------------
Iteration: 900 | Episodes: 36500 | Median Reward: 42.49 | Avg Reward: 41.47 | Max Reward: 45.98
Iteration: 902 | Episodes: 36600 | Median Reward: 40.46 | Avg Reward: 40.09 | Max Reward: 45.98
Iteration: 904 | Episodes: 36700 | Median Reward: 37.77 | Avg Reward: 38.05 | Max Reward: 45.98
Iteration: 907 | Episodes: 36800 | Median Reward: 42.27 | Avg Reward: 40.66 | Max Reward: 45.98
Iteration: 909 | Episodes: 36900 | Median Reward: 40.35 | Avg Reward: 40.41 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 563          |
|    iterations           | 910          |
|    time_elapsed         | 6610         |
|    total_timesteps      | 3727360      |
| train/                  |              |
|    approx_kl            | 0.0033034058 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -291         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -57.7        |
|    n_updates            | 9090         |
|    policy_gradient_loss | -0.00431     |
|    std                  | 1.92         |
|    value_loss           | 0.905        |
------------------------------------------
Iteration: 912 | Episodes: 37000 | Median Reward: 41.71 | Avg Reward: 40.96 | Max Reward: 45.98
Iteration: 914 | Episodes: 37100 | Median Reward: 40.29 | Avg Reward: 40.13 | Max Reward: 45.98
Iteration: 917 | Episodes: 37200 | Median Reward: 40.79 | Avg Reward: 40.70 | Max Reward: 45.98
Iteration: 919 | Episodes: 37300 | Median Reward: 41.54 | Avg Reward: 40.96 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.1         |
| time/                   |               |
|    fps                  | 563           |
|    iterations           | 920           |
|    time_elapsed         | 6687          |
|    total_timesteps      | 3768320       |
| train/                  |               |
|    approx_kl            | 0.00064179604 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -292          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -57.8         |
|    n_updates            | 9190          |
|    policy_gradient_loss | -0.000428     |
|    std                  | 1.94          |
|    value_loss           | 2.25          |
-------------------------------------------
Iteration: 922 | Episodes: 37400 | Median Reward: 39.76 | Avg Reward: 40.10 | Max Reward: 45.98
Iteration: 924 | Episodes: 37500 | Median Reward: 40.32 | Avg Reward: 39.15 | Max Reward: 45.98
Iteration: 927 | Episodes: 37600 | Median Reward: 41.04 | Avg Reward: 40.35 | Max Reward: 45.98
Iteration: 929 | Episodes: 37700 | Median Reward: 41.41 | Avg Reward: 40.69 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.7         |
| time/                   |               |
|    fps                  | 560           |
|    iterations           | 930           |
|    time_elapsed         | 6794          |
|    total_timesteps      | 3809280       |
| train/                  |               |
|    approx_kl            | 7.5485266e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -293          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -57.5         |
|    n_updates            | 9290          |
|    policy_gradient_loss | -0.000577     |
|    std                  | 1.95          |
|    value_loss           | 1.52          |
-------------------------------------------
Iteration: 932 | Episodes: 37800 | Median Reward: 41.51 | Avg Reward: 41.38 | Max Reward: 45.98
Iteration: 934 | Episodes: 37900 | Median Reward: 40.09 | Avg Reward: 40.18 | Max Reward: 45.98
Iteration: 937 | Episodes: 38000 | Median Reward: 41.07 | Avg Reward: 40.97 | Max Reward: 45.98
Iteration: 939 | Episodes: 38100 | Median Reward: 41.38 | Avg Reward: 40.64 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.6        |
| time/                   |              |
|    fps                  | 560          |
|    iterations           | 940          |
|    time_elapsed         | 6863         |
|    total_timesteps      | 3850240      |
| train/                  |              |
|    approx_kl            | 0.0012515823 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -293         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -58.5        |
|    n_updates            | 9390         |
|    policy_gradient_loss | -0.000825    |
|    std                  | 1.96         |
|    value_loss           | 0.445        |
------------------------------------------
Iteration: 941 | Episodes: 38200 | Median Reward: 40.38 | Avg Reward: 40.75 | Max Reward: 45.98
Iteration: 944 | Episodes: 38300 | Median Reward: 40.70 | Avg Reward: 39.10 | Max Reward: 45.98
Iteration: 946 | Episodes: 38400 | Median Reward: 39.10 | Avg Reward: 39.33 | Max Reward: 45.98
Iteration: 949 | Episodes: 38500 | Median Reward: 40.04 | Avg Reward: 39.42 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61           |
| time/                   |               |
|    fps                  | 558           |
|    iterations           | 950           |
|    time_elapsed         | 6969          |
|    total_timesteps      | 3891200       |
| train/                  |               |
|    approx_kl            | 0.00018826478 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -294          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -56.7         |
|    n_updates            | 9490          |
|    policy_gradient_loss | -0.00273      |
|    std                  | 1.97          |
|    value_loss           | 3.64          |
-------------------------------------------
Iteration: 951 | Episodes: 38600 | Median Reward: 39.86 | Avg Reward: 39.62 | Max Reward: 45.98
Iteration: 954 | Episodes: 38700 | Median Reward: 40.77 | Avg Reward: 40.60 | Max Reward: 45.98
Iteration: 956 | Episodes: 38800 | Median Reward: 40.07 | Avg Reward: 40.16 | Max Reward: 45.98
Iteration: 959 | Episodes: 38900 | Median Reward: 39.76 | Avg Reward: 38.90 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.2        |
| time/                   |              |
|    fps                  | 557          |
|    iterations           | 960          |
|    time_elapsed         | 7053         |
|    total_timesteps      | 3932160      |
| train/                  |              |
|    approx_kl            | 0.0002130263 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -294         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -56.5        |
|    n_updates            | 9590         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 1.98         |
|    value_loss           | 3.7          |
------------------------------------------
Iteration: 961 | Episodes: 39000 | Median Reward: 40.04 | Avg Reward: 40.68 | Max Reward: 45.98
Iteration: 964 | Episodes: 39100 | Median Reward: 41.10 | Avg Reward: 41.17 | Max Reward: 45.98
Iteration: 966 | Episodes: 39200 | Median Reward: 41.08 | Avg Reward: 40.53 | Max Reward: 45.98
Iteration: 969 | Episodes: 39300 | Median Reward: 40.61 | Avg Reward: 40.62 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 555          |
|    iterations           | 970          |
|    time_elapsed         | 7147         |
|    total_timesteps      | 3973120      |
| train/                  |              |
|    approx_kl            | 0.0009505829 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -294         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -57.6        |
|    n_updates            | 9690         |
|    policy_gradient_loss | -0.00679     |
|    std                  | 1.98         |
|    value_loss           | 3.68         |
------------------------------------------
Iteration: 971 | Episodes: 39400 | Median Reward: 42.61 | Avg Reward: 41.34 | Max Reward: 45.98
Iteration: 973 | Episodes: 39500 | Median Reward: 38.96 | Avg Reward: 39.77 | Max Reward: 45.98
Iteration: 976 | Episodes: 39600 | Median Reward: 40.72 | Avg Reward: 40.38 | Max Reward: 45.98
Iteration: 978 | Episodes: 39700 | Median Reward: 42.67 | Avg Reward: 40.84 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59           |
| time/                   |               |
|    fps                  | 553           |
|    iterations           | 980           |
|    time_elapsed         | 7250          |
|    total_timesteps      | 4014080       |
| train/                  |               |
|    approx_kl            | 3.7491656e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -295          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -56.4         |
|    n_updates            | 9790          |
|    policy_gradient_loss | -0.000336     |
|    std                  | 1.99          |
|    value_loss           | 3.93          |
-------------------------------------------
Iteration: 981 | Episodes: 39800 | Median Reward: 39.34 | Avg Reward: 39.62 | Max Reward: 45.98
Iteration: 983 | Episodes: 39900 | Median Reward: 42.46 | Avg Reward: 41.41 | Max Reward: 45.98
Iteration: 986 | Episodes: 40000 | Median Reward: 41.64 | Avg Reward: 41.64 | Max Reward: 45.98
Iteration: 988 | Episodes: 40100 | Median Reward: 41.12 | Avg Reward: 39.61 | Max Reward: 45.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.2       |
| time/                   |             |
|    fps                  | 553         |
|    iterations           | 990         |
|    time_elapsed         | 7324        |
|    total_timesteps      | 4055040     |
| train/                  |             |
|    approx_kl            | 0.014888868 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -295        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -57.1       |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.0307     |
|    std                  | 2.01        |
|    value_loss           | 2.4         |
-----------------------------------------
Iteration: 991 | Episodes: 40200 | Median Reward: 38.06 | Avg Reward: 38.19 | Max Reward: 45.98
Iteration: 993 | Episodes: 40300 | Median Reward: 37.81 | Avg Reward: 39.00 | Max Reward: 45.98
Iteration: 996 | Episodes: 40400 | Median Reward: 40.90 | Avg Reward: 41.09 | Max Reward: 45.98
Iteration: 998 | Episodes: 40500 | Median Reward: 40.54 | Avg Reward: 41.07 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.1         |
| time/                   |               |
|    fps                  | 551           |
|    iterations           | 1000          |
|    time_elapsed         | 7433          |
|    total_timesteps      | 4096000       |
| train/                  |               |
|    approx_kl            | 0.00043481056 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -296          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -59.2         |
|    n_updates            | 9990          |
|    policy_gradient_loss | -0.00249      |
|    std                  | 2.03          |
|    value_loss           | 0.826         |
-------------------------------------------
Iteration: 1001 | Episodes: 40600 | Median Reward: 39.62 | Avg Reward: 40.49 | Max Reward: 45.98
Iteration: 1003 | Episodes: 40700 | Median Reward: 40.69 | Avg Reward: 39.51 | Max Reward: 45.98
Iteration: 1006 | Episodes: 40800 | Median Reward: 40.68 | Avg Reward: 40.72 | Max Reward: 45.98
Iteration: 1008 | Episodes: 40900 | Median Reward: 38.94 | Avg Reward: 39.54 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.1         |
| time/                   |               |
|    fps                  | 551           |
|    iterations           | 1010          |
|    time_elapsed         | 7499          |
|    total_timesteps      | 4136960       |
| train/                  |               |
|    approx_kl            | 0.00060862076 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -297          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -58.9         |
|    n_updates            | 10090         |
|    policy_gradient_loss | -0.00107      |
|    std                  | 2.06          |
|    value_loss           | 1.76          |
-------------------------------------------
Iteration: 1010 | Episodes: 41000 | Median Reward: 39.98 | Avg Reward: 40.21 | Max Reward: 45.98
Iteration: 1013 | Episodes: 41100 | Median Reward: 40.05 | Avg Reward: 39.64 | Max Reward: 45.98
Iteration: 1015 | Episodes: 41200 | Median Reward: 40.84 | Avg Reward: 41.16 | Max Reward: 45.98
Iteration: 1018 | Episodes: 41300 | Median Reward: 39.79 | Avg Reward: 39.76 | Max Reward: 45.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.5       |
| time/                   |             |
|    fps                  | 550         |
|    iterations           | 1020        |
|    time_elapsed         | 7585        |
|    total_timesteps      | 4177920     |
| train/                  |             |
|    approx_kl            | 0.005443071 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -298        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -58.3       |
|    n_updates            | 10190       |
|    policy_gradient_loss | -0.00782    |
|    std                  | 2.08        |
|    value_loss           | 0.771       |
-----------------------------------------
Iteration: 1020 | Episodes: 41400 | Median Reward: 40.66 | Avg Reward: 39.55 | Max Reward: 45.98
Iteration: 1023 | Episodes: 41500 | Median Reward: 41.07 | Avg Reward: 40.44 | Max Reward: 45.98
Iteration: 1025 | Episodes: 41600 | Median Reward: 39.83 | Avg Reward: 39.44 | Max Reward: 45.98
Iteration: 1028 | Episodes: 41700 | Median Reward: 41.11 | Avg Reward: 41.11 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 1030         |
|    time_elapsed         | 7691         |
|    total_timesteps      | 4218880      |
| train/                  |              |
|    approx_kl            | 0.0026545236 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -299         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -59.3        |
|    n_updates            | 10290        |
|    policy_gradient_loss | -0.00372     |
|    std                  | 2.1          |
|    value_loss           | 0.32         |
------------------------------------------
Iteration: 1030 | Episodes: 41800 | Median Reward: 38.69 | Avg Reward: 40.28 | Max Reward: 45.98
Iteration: 1033 | Episodes: 41900 | Median Reward: 39.41 | Avg Reward: 39.47 | Max Reward: 45.98
Iteration: 1035 | Episodes: 42000 | Median Reward: 40.12 | Avg Reward: 39.69 | Max Reward: 45.98
Iteration: 1038 | Episodes: 42100 | Median Reward: 41.16 | Avg Reward: 41.08 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.7         |
| time/                   |               |
|    fps                  | 548           |
|    iterations           | 1040          |
|    time_elapsed         | 7764          |
|    total_timesteps      | 4259840       |
| train/                  |               |
|    approx_kl            | 0.00093329384 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -299          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -58.8         |
|    n_updates            | 10390         |
|    policy_gradient_loss | -0.00271      |
|    std                  | 2.12          |
|    value_loss           | 2.14          |
-------------------------------------------
Iteration: 1040 | Episodes: 42200 | Median Reward: 40.44 | Avg Reward: 40.38 | Max Reward: 45.98
Iteration: 1043 | Episodes: 42300 | Median Reward: 42.62 | Avg Reward: 41.94 | Max Reward: 45.98
Iteration: 1045 | Episodes: 42400 | Median Reward: 40.61 | Avg Reward: 40.70 | Max Reward: 45.98
Iteration: 1047 | Episodes: 42500 | Median Reward: 39.79 | Avg Reward: 39.46 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.2         |
| time/                   |               |
|    fps                  | 546           |
|    iterations           | 1050          |
|    time_elapsed         | 7871          |
|    total_timesteps      | 4300800       |
| train/                  |               |
|    approx_kl            | 0.00024385638 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -300          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -59.8         |
|    n_updates            | 10490         |
|    policy_gradient_loss | -0.000918     |
|    std                  | 2.13          |
|    value_loss           | 0.437         |
-------------------------------------------
Iteration: 1050 | Episodes: 42600 | Median Reward: 40.48 | Avg Reward: 39.90 | Max Reward: 45.98
Iteration: 1052 | Episodes: 42700 | Median Reward: 40.28 | Avg Reward: 40.22 | Max Reward: 45.98
Iteration: 1055 | Episodes: 42800 | Median Reward: 43.38 | Avg Reward: 41.85 | Max Reward: 45.98
Iteration: 1057 | Episodes: 42900 | Median Reward: 41.30 | Avg Reward: 40.76 | Max Reward: 45.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.9       |
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 1060        |
|    time_elapsed         | 7940        |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.011413448 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -300        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -59.3       |
|    n_updates            | 10590       |
|    policy_gradient_loss | -0.0196     |
|    std                  | 2.15        |
|    value_loss           | 0.825       |
-----------------------------------------
Iteration: 1060 | Episodes: 43000 | Median Reward: 39.96 | Avg Reward: 39.40 | Max Reward: 45.98
Iteration: 1062 | Episodes: 43100 | Median Reward: 40.19 | Avg Reward: 40.19 | Max Reward: 45.98
Iteration: 1065 | Episodes: 43200 | Median Reward: 39.82 | Avg Reward: 39.45 | Max Reward: 45.98
Iteration: 1067 | Episodes: 43300 | Median Reward: 38.19 | Avg Reward: 38.84 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.7        |
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 1070         |
|    time_elapsed         | 7997         |
|    total_timesteps      | 4382720      |
| train/                  |              |
|    approx_kl            | 0.0012527341 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -301         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -60          |
|    n_updates            | 10690        |
|    policy_gradient_loss | -0.00191     |
|    std                  | 2.18         |
|    value_loss           | 0.837        |
------------------------------------------
Iteration: 1070 | Episodes: 43400 | Median Reward: 40.93 | Avg Reward: 40.25 | Max Reward: 45.98
Iteration: 1072 | Episodes: 43500 | Median Reward: 39.75 | Avg Reward: 39.83 | Max Reward: 45.98
Iteration: 1075 | Episodes: 43600 | Median Reward: 39.72 | Avg Reward: 40.65 | Max Reward: 45.98
Iteration: 1077 | Episodes: 43700 | Median Reward: 43.34 | Avg Reward: 42.29 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.5         |
| time/                   |               |
|    fps                  | 545           |
|    iterations           | 1080          |
|    time_elapsed         | 8103          |
|    total_timesteps      | 4423680       |
| train/                  |               |
|    approx_kl            | 0.00013476847 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -302          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -59.3         |
|    n_updates            | 10790         |
|    policy_gradient_loss | 2.21e-05      |
|    std                  | 2.2           |
|    value_loss           | 1.78          |
-------------------------------------------
Iteration: 1080 | Episodes: 43800 | Median Reward: 40.66 | Avg Reward: 40.43 | Max Reward: 45.98
Iteration: 1082 | Episodes: 43900 | Median Reward: 38.82 | Avg Reward: 38.77 | Max Reward: 45.98
Iteration: 1084 | Episodes: 44000 | Median Reward: 39.64 | Avg Reward: 38.99 | Max Reward: 45.98
Iteration: 1087 | Episodes: 44100 | Median Reward: 40.61 | Avg Reward: 40.38 | Max Reward: 45.98
Iteration: 1089 | Episodes: 44200 | Median Reward: 39.47 | Avg Reward: 39.88 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.2         |
| time/                   |               |
|    fps                  | 547           |
|    iterations           | 1090          |
|    time_elapsed         | 8155          |
|    total_timesteps      | 4464640       |
| train/                  |               |
|    approx_kl            | 0.00021437253 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -302          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -60.2         |
|    n_updates            | 10890         |
|    policy_gradient_loss | -0.00158      |
|    std                  | 2.21          |
|    value_loss           | 1.09          |
-------------------------------------------
Iteration: 1092 | Episodes: 44300 | Median Reward: 37.67 | Avg Reward: 37.53 | Max Reward: 45.98
Iteration: 1094 | Episodes: 44400 | Median Reward: 39.95 | Avg Reward: 39.10 | Max Reward: 45.98
Iteration: 1097 | Episodes: 44500 | Median Reward: 40.15 | Avg Reward: 40.03 | Max Reward: 45.98
Iteration: 1099 | Episodes: 44600 | Median Reward: 39.19 | Avg Reward: 39.44 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.6         |
| time/                   |               |
|    fps                  | 547           |
|    iterations           | 1100          |
|    time_elapsed         | 8224          |
|    total_timesteps      | 4505600       |
| train/                  |               |
|    approx_kl            | 0.00047016743 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -303          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -60.3         |
|    n_updates            | 10990         |
|    policy_gradient_loss | -0.000907     |
|    std                  | 2.25          |
|    value_loss           | 0.257         |
-------------------------------------------
Iteration: 1102 | Episodes: 44700 | Median Reward: 40.28 | Avg Reward: 40.12 | Max Reward: 45.98
Iteration: 1104 | Episodes: 44800 | Median Reward: 40.04 | Avg Reward: 40.09 | Max Reward: 45.98
Iteration: 1107 | Episodes: 44900 | Median Reward: 40.40 | Avg Reward: 40.56 | Max Reward: 45.98
Iteration: 1109 | Episodes: 45000 | Median Reward: 41.52 | Avg Reward: 40.96 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.2        |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 1110         |
|    time_elapsed         | 8270         |
|    total_timesteps      | 4546560      |
| train/                  |              |
|    approx_kl            | 0.0009567902 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -304         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -60.7        |
|    n_updates            | 11090        |
|    policy_gradient_loss | -0.00347     |
|    std                  | 2.27         |
|    value_loss           | 0.63         |
------------------------------------------
Iteration: 1112 | Episodes: 45100 | Median Reward: 39.17 | Avg Reward: 38.59 | Max Reward: 45.98
Iteration: 1114 | Episodes: 45200 | Median Reward: 40.11 | Avg Reward: 40.28 | Max Reward: 45.98
Iteration: 1117 | Episodes: 45300 | Median Reward: 37.97 | Avg Reward: 38.26 | Max Reward: 45.98
Iteration: 1119 | Episodes: 45400 | Median Reward: 40.37 | Avg Reward: 39.63 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.8        |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 1120         |
|    time_elapsed         | 8354         |
|    total_timesteps      | 4587520      |
| train/                  |              |
|    approx_kl            | 0.0074613346 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -305         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -60.9        |
|    n_updates            | 11190        |
|    policy_gradient_loss | -0.0131      |
|    std                  | 2.3          |
|    value_loss           | 0.331        |
------------------------------------------
Iteration: 1121 | Episodes: 45500 | Median Reward: 40.08 | Avg Reward: 39.24 | Max Reward: 45.98
Iteration: 1124 | Episodes: 45600 | Median Reward: 39.38 | Avg Reward: 39.04 | Max Reward: 45.98
Iteration: 1126 | Episodes: 45700 | Median Reward: 40.28 | Avg Reward: 39.15 | Max Reward: 45.98
Iteration: 1129 | Episodes: 45800 | Median Reward: 40.83 | Avg Reward: 40.93 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.3        |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 1130         |
|    time_elapsed         | 8423         |
|    total_timesteps      | 4628480      |
| train/                  |              |
|    approx_kl            | 7.719587e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -305         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -60.6        |
|    n_updates            | 11290        |
|    policy_gradient_loss | -5.41e-05    |
|    std                  | 2.31         |
|    value_loss           | 1.85         |
------------------------------------------
Iteration: 1131 | Episodes: 45900 | Median Reward: 39.32 | Avg Reward: 39.34 | Max Reward: 45.98
Iteration: 1134 | Episodes: 46000 | Median Reward: 40.59 | Avg Reward: 40.53 | Max Reward: 45.98
Iteration: 1136 | Episodes: 46100 | Median Reward: 42.26 | Avg Reward: 41.43 | Max Reward: 45.98
Iteration: 1139 | Episodes: 46200 | Median Reward: 39.54 | Avg Reward: 39.00 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.1        |
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 1140         |
|    time_elapsed         | 8512         |
|    total_timesteps      | 4669440      |
| train/                  |              |
|    approx_kl            | 0.0013122808 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -306         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -59.9        |
|    n_updates            | 11390        |
|    policy_gradient_loss | -0.00781     |
|    std                  | 2.33         |
|    value_loss           | 1.3          |
------------------------------------------
Iteration: 1141 | Episodes: 46300 | Median Reward: 40.06 | Avg Reward: 40.81 | Max Reward: 45.98
Iteration: 1144 | Episodes: 46400 | Median Reward: 39.63 | Avg Reward: 40.10 | Max Reward: 45.98
Iteration: 1146 | Episodes: 46500 | Median Reward: 40.55 | Avg Reward: 39.61 | Max Reward: 45.98
Iteration: 1149 | Episodes: 46600 | Median Reward: 39.87 | Avg Reward: 40.03 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.7        |
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 1150         |
|    time_elapsed         | 8594         |
|    total_timesteps      | 4710400      |
| train/                  |              |
|    approx_kl            | 0.0004936749 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -306         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -60.9        |
|    n_updates            | 11490        |
|    policy_gradient_loss | -0.000998    |
|    std                  | 2.35         |
|    value_loss           | 0.766        |
------------------------------------------
Iteration: 1151 | Episodes: 46700 | Median Reward: 42.54 | Avg Reward: 41.22 | Max Reward: 45.98
Iteration: 1154 | Episodes: 46800 | Median Reward: 41.23 | Avg Reward: 40.92 | Max Reward: 45.98
Iteration: 1156 | Episodes: 46900 | Median Reward: 40.38 | Avg Reward: 40.27 | Max Reward: 45.98
Iteration: 1158 | Episodes: 47000 | Median Reward: 40.10 | Avg Reward: 39.66 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.2        |
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 1160         |
|    time_elapsed         | 8702         |
|    total_timesteps      | 4751360      |
| train/                  |              |
|    approx_kl            | 0.0055189766 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -307         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -61          |
|    n_updates            | 11590        |
|    policy_gradient_loss | -0.0296      |
|    std                  | 2.38         |
|    value_loss           | 3.08         |
------------------------------------------
Iteration: 1161 | Episodes: 47100 | Median Reward: 40.19 | Avg Reward: 39.66 | Max Reward: 45.98
Iteration: 1163 | Episodes: 47200 | Median Reward: 38.64 | Avg Reward: 38.11 | Max Reward: 45.98
Iteration: 1166 | Episodes: 47300 | Median Reward: 42.56 | Avg Reward: 40.48 | Max Reward: 45.98
Iteration: 1168 | Episodes: 47400 | Median Reward: 38.53 | Avg Reward: 39.05 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.7         |
| time/                   |               |
|    fps                  | 545           |
|    iterations           | 1170          |
|    time_elapsed         | 8780          |
|    total_timesteps      | 4792320       |
| train/                  |               |
|    approx_kl            | 0.00018924773 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -308          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -61.1         |
|    n_updates            | 11690         |
|    policy_gradient_loss | 0.00017       |
|    std                  | 2.4           |
|    value_loss           | 1.11          |
-------------------------------------------
Iteration: 1171 | Episodes: 47500 | Median Reward: 39.18 | Avg Reward: 39.70 | Max Reward: 45.98
Iteration: 1173 | Episodes: 47600 | Median Reward: 39.51 | Avg Reward: 38.83 | Max Reward: 45.98
Iteration: 1176 | Episodes: 47700 | Median Reward: 40.11 | Avg Reward: 40.32 | Max Reward: 45.98
Iteration: 1178 | Episodes: 47800 | Median Reward: 40.69 | Avg Reward: 40.43 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.2         |
| time/                   |               |
|    fps                  | 543           |
|    iterations           | 1180          |
|    time_elapsed         | 8888          |
|    total_timesteps      | 4833280       |
| train/                  |               |
|    approx_kl            | 0.00021270949 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -308          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -61.1         |
|    n_updates            | 11790         |
|    policy_gradient_loss | -0.00257      |
|    std                  | 2.43          |
|    value_loss           | 1.99          |
-------------------------------------------
Iteration: 1181 | Episodes: 47900 | Median Reward: 41.06 | Avg Reward: 39.77 | Max Reward: 45.98
Iteration: 1183 | Episodes: 48000 | Median Reward: 41.00 | Avg Reward: 39.94 | Max Reward: 45.98
Iteration: 1186 | Episodes: 48100 | Median Reward: 39.98 | Avg Reward: 39.89 | Max Reward: 45.98
Iteration: 1188 | Episodes: 48200 | Median Reward: 39.55 | Avg Reward: 39.16 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.8        |
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 1190         |
|    time_elapsed         | 8970         |
|    total_timesteps      | 4874240      |
| train/                  |              |
|    approx_kl            | 0.0045758947 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -309         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -61.6        |
|    n_updates            | 11890        |
|    policy_gradient_loss | -0.0102      |
|    std                  | 2.45         |
|    value_loss           | 0.35         |
------------------------------------------
Iteration: 1190 | Episodes: 48300 | Median Reward: 40.35 | Avg Reward: 41.00 | Max Reward: 45.98
Iteration: 1193 | Episodes: 48400 | Median Reward: 40.19 | Avg Reward: 39.17 | Max Reward: 45.98
Iteration: 1195 | Episodes: 48500 | Median Reward: 40.60 | Avg Reward: 39.65 | Max Reward: 45.98
Iteration: 1198 | Episodes: 48600 | Median Reward: 40.73 | Avg Reward: 39.62 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.6        |
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 1200         |
|    time_elapsed         | 9078         |
|    total_timesteps      | 4915200      |
| train/                  |              |
|    approx_kl            | 0.0005032741 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -309         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -61.4        |
|    n_updates            | 11990        |
|    policy_gradient_loss | -0.00185     |
|    std                  | 2.47         |
|    value_loss           | 1.22         |
------------------------------------------
Iteration: 1200 | Episodes: 48700 | Median Reward: 40.70 | Avg Reward: 40.56 | Max Reward: 45.98
Iteration: 1203 | Episodes: 48800 | Median Reward: 39.62 | Avg Reward: 39.78 | Max Reward: 45.98
Iteration: 1205 | Episodes: 48900 | Median Reward: 39.10 | Avg Reward: 38.54 | Max Reward: 45.98
Iteration: 1208 | Episodes: 49000 | Median Reward: 40.49 | Avg Reward: 39.24 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.5         |
| time/                   |               |
|    fps                  | 541           |
|    iterations           | 1210          |
|    time_elapsed         | 9156          |
|    total_timesteps      | 4956160       |
| train/                  |               |
|    approx_kl            | 0.00067645544 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -310          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -61.7         |
|    n_updates            | 12090         |
|    policy_gradient_loss | -0.00365      |
|    std                  | 2.5           |
|    value_loss           | 1.24          |
-------------------------------------------
Iteration: 1210 | Episodes: 49100 | Median Reward: 40.65 | Avg Reward: 39.91 | Max Reward: 45.98
Iteration: 1213 | Episodes: 49200 | Median Reward: 40.28 | Avg Reward: 39.78 | Max Reward: 45.98
Iteration: 1215 | Episodes: 49300 | Median Reward: 39.84 | Avg Reward: 39.83 | Max Reward: 45.98
Iteration: 1218 | Episodes: 49400 | Median Reward: 39.83 | Avg Reward: 39.68 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.2        |
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 1220         |
|    time_elapsed         | 9259         |
|    total_timesteps      | 4997120      |
| train/                  |              |
|    approx_kl            | 0.0017860815 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -310         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -61.5        |
|    n_updates            | 12190        |
|    policy_gradient_loss | -0.00651     |
|    std                  | 2.51         |
|    value_loss           | 0.586        |
------------------------------------------
Iteration: 1220 | Episodes: 49500 | Median Reward: 39.97 | Avg Reward: 40.62 | Max Reward: 45.98
Iteration: 1223 | Episodes: 49600 | Median Reward: 38.07 | Avg Reward: 37.74 | Max Reward: 45.98
Iteration: 1225 | Episodes: 49700 | Median Reward: 39.48 | Avg Reward: 37.69 | Max Reward: 45.98
Iteration: 1227 | Episodes: 49800 | Median Reward: 39.59 | Avg Reward: 39.23 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.6         |
| time/                   |               |
|    fps                  | 538           |
|    iterations           | 1230          |
|    time_elapsed         | 9355          |
|    total_timesteps      | 5038080       |
| train/                  |               |
|    approx_kl            | 0.00055964943 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -311          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -61.8         |
|    n_updates            | 12290         |
|    policy_gradient_loss | -0.00124      |
|    std                  | 2.54          |
|    value_loss           | 0.523         |
-------------------------------------------
Iteration: 1230 | Episodes: 49900 | Median Reward: 40.59 | Avg Reward: 40.36 | Max Reward: 45.98
Iteration: 1232 | Episodes: 50000 | Median Reward: 40.16 | Avg Reward: 39.93 | Max Reward: 45.98
Iteration: 1235 | Episodes: 50100 | Median Reward: 38.06 | Avg Reward: 38.26 | Max Reward: 45.98
Iteration: 1237 | Episodes: 50200 | Median Reward: 39.90 | Avg Reward: 39.65 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.2         |
| time/                   |               |
|    fps                  | 537           |
|    iterations           | 1240          |
|    time_elapsed         | 9442          |
|    total_timesteps      | 5079040       |
| train/                  |               |
|    approx_kl            | 0.00021558063 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -312          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -62.2         |
|    n_updates            | 12390         |
|    policy_gradient_loss | -0.000141     |
|    std                  | 2.57          |
|    value_loss           | 0.763         |
-------------------------------------------
Iteration: 1240 | Episodes: 50300 | Median Reward: 40.75 | Avg Reward: 40.73 | Max Reward: 45.98
Iteration: 1242 | Episodes: 50400 | Median Reward: 39.49 | Avg Reward: 39.51 | Max Reward: 45.98
Iteration: 1245 | Episodes: 50500 | Median Reward: 40.48 | Avg Reward: 39.37 | Max Reward: 45.98
Iteration: 1247 | Episodes: 50600 | Median Reward: 39.84 | Avg Reward: 38.90 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.6        |
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 1250         |
|    time_elapsed         | 9545         |
|    total_timesteps      | 5120000      |
| train/                  |              |
|    approx_kl            | 0.0007367453 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -312         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -62.4        |
|    n_updates            | 12490        |
|    policy_gradient_loss | -0.00204     |
|    std                  | 2.59         |
|    value_loss           | 0.733        |
------------------------------------------
Iteration: 1250 | Episodes: 50700 | Median Reward: 37.87 | Avg Reward: 38.16 | Max Reward: 45.98
Iteration: 1252 | Episodes: 50800 | Median Reward: 39.73 | Avg Reward: 38.74 | Max Reward: 45.98
Iteration: 1255 | Episodes: 50900 | Median Reward: 36.27 | Avg Reward: 36.47 | Max Reward: 45.98
Iteration: 1257 | Episodes: 51000 | Median Reward: 40.01 | Avg Reward: 39.75 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.6         |
| time/                   |               |
|    fps                  | 536           |
|    iterations           | 1260          |
|    time_elapsed         | 9621          |
|    total_timesteps      | 5160960       |
| train/                  |               |
|    approx_kl            | 6.6024586e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -313          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -62           |
|    n_updates            | 12590         |
|    policy_gradient_loss | -0.000341     |
|    std                  | 2.63          |
|    value_loss           | 1.5           |
-------------------------------------------
Iteration: 1260 | Episodes: 51100 | Median Reward: 40.24 | Avg Reward: 39.41 | Max Reward: 45.98
Iteration: 1262 | Episodes: 51200 | Median Reward: 40.85 | Avg Reward: 39.75 | Max Reward: 45.98
Iteration: 1264 | Episodes: 51300 | Median Reward: 39.93 | Avg Reward: 39.42 | Max Reward: 45.98
Iteration: 1267 | Episodes: 51400 | Median Reward: 40.33 | Avg Reward: 39.85 | Max Reward: 45.98
Iteration: 1269 | Episodes: 51500 | Median Reward: 39.75 | Avg Reward: 39.37 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.6        |
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 1270         |
|    time_elapsed         | 9727         |
|    total_timesteps      | 5201920      |
| train/                  |              |
|    approx_kl            | 7.813239e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -314         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -62.4        |
|    n_updates            | 12690        |
|    policy_gradient_loss | -0.000112    |
|    std                  | 2.65         |
|    value_loss           | 1.1          |
------------------------------------------
Iteration: 1272 | Episodes: 51600 | Median Reward: 37.74 | Avg Reward: 37.11 | Max Reward: 45.98
Iteration: 1274 | Episodes: 51700 | Median Reward: 38.87 | Avg Reward: 39.05 | Max Reward: 45.98
Iteration: 1277 | Episodes: 51800 | Median Reward: 38.35 | Avg Reward: 38.33 | Max Reward: 45.98
Iteration: 1279 | Episodes: 51900 | Median Reward: 38.03 | Avg Reward: 38.43 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.7        |
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 1280         |
|    time_elapsed         | 9803         |
|    total_timesteps      | 5242880      |
| train/                  |              |
|    approx_kl            | 0.0015211629 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -314         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -62.7        |
|    n_updates            | 12790        |
|    policy_gradient_loss | -0.00774     |
|    std                  | 2.67         |
|    value_loss           | 0.215        |
------------------------------------------
Iteration: 1282 | Episodes: 52000 | Median Reward: 37.84 | Avg Reward: 37.51 | Max Reward: 45.98
Iteration: 1284 | Episodes: 52100 | Median Reward: 39.76 | Avg Reward: 39.04 | Max Reward: 45.98
Iteration: 1287 | Episodes: 52200 | Median Reward: 38.21 | Avg Reward: 38.35 | Max Reward: 45.98
Iteration: 1289 | Episodes: 52300 | Median Reward: 40.65 | Avg Reward: 40.20 | Max Reward: 45.98
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -59.7     |
| time/                   |           |
|    fps                  | 534       |
|    iterations           | 1290      |
|    time_elapsed         | 9888      |
|    total_timesteps      | 5283840   |
| train/                  |           |
|    approx_kl            | 0.0057531 |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    entropy_loss         | -315      |
|    explained_variance   | 1         |
|    learning_rate        | 0.0005    |
|    loss                 | -62.7     |
|    n_updates            | 12890     |
|    policy_gradient_loss | -0.0084   |
|    std                  | 2.71      |
|    value_loss           | 0.43      |
---------------------------------------
Iteration: 1292 | Episodes: 52400 | Median Reward: 39.29 | Avg Reward: 37.91 | Max Reward: 45.98
Iteration: 1294 | Episodes: 52500 | Median Reward: 37.69 | Avg Reward: 38.11 | Max Reward: 45.98
Iteration: 1297 | Episodes: 52600 | Median Reward: 39.54 | Avg Reward: 39.35 | Max Reward: 45.98
Iteration: 1299 | Episodes: 52700 | Median Reward: 40.64 | Avg Reward: 39.97 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.5         |
| time/                   |               |
|    fps                  | 533           |
|    iterations           | 1300          |
|    time_elapsed         | 9989          |
|    total_timesteps      | 5324800       |
| train/                  |               |
|    approx_kl            | 2.9715098e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -315          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -61.7         |
|    n_updates            | 12990         |
|    policy_gradient_loss | -0.000798     |
|    std                  | 2.74          |
|    value_loss           | 3.38          |
-------------------------------------------
Iteration: 1301 | Episodes: 52800 | Median Reward: 39.95 | Avg Reward: 39.15 | Max Reward: 45.98
Iteration: 1304 | Episodes: 52900 | Median Reward: 39.09 | Avg Reward: 39.62 | Max Reward: 45.98
Iteration: 1306 | Episodes: 53000 | Median Reward: 40.16 | Avg Reward: 39.60 | Max Reward: 45.98
Iteration: 1309 | Episodes: 53100 | Median Reward: 40.29 | Avg Reward: 37.77 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.6        |
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 1310         |
|    time_elapsed         | 10042        |
|    total_timesteps      | 5365760      |
| train/                  |              |
|    approx_kl            | 0.0026276978 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -315         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -62.5        |
|    n_updates            | 13090        |
|    policy_gradient_loss | -0.0203      |
|    std                  | 2.75         |
|    value_loss           | 2.43         |
------------------------------------------
Iteration: 1311 | Episodes: 53200 | Median Reward: 37.65 | Avg Reward: 36.16 | Max Reward: 45.98
Iteration: 1314 | Episodes: 53300 | Median Reward: 39.34 | Avg Reward: 38.34 | Max Reward: 45.98
Iteration: 1316 | Episodes: 53400 | Median Reward: 40.07 | Avg Reward: 40.05 | Max Reward: 45.98
Iteration: 1319 | Episodes: 53500 | Median Reward: 35.89 | Avg Reward: 37.00 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.4        |
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 1320         |
|    time_elapsed         | 10111        |
|    total_timesteps      | 5406720      |
| train/                  |              |
|    approx_kl            | 0.0006375875 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -316         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -63.1        |
|    n_updates            | 13190        |
|    policy_gradient_loss | -0.00426     |
|    std                  | 2.77         |
|    value_loss           | 0.581        |
------------------------------------------
Iteration: 1321 | Episodes: 53600 | Median Reward: 38.42 | Avg Reward: 37.75 | Max Reward: 45.98
Iteration: 1324 | Episodes: 53700 | Median Reward: 39.75 | Avg Reward: 37.26 | Max Reward: 45.98
Iteration: 1326 | Episodes: 53800 | Median Reward: 40.13 | Avg Reward: 39.57 | Max Reward: 45.98
Iteration: 1329 | Episodes: 53900 | Median Reward: 39.66 | Avg Reward: 39.42 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.6         |
| time/                   |               |
|    fps                  | 533           |
|    iterations           | 1330          |
|    time_elapsed         | 10218         |
|    total_timesteps      | 5447680       |
| train/                  |               |
|    approx_kl            | 0.00020525011 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -317          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -63.2         |
|    n_updates            | 13290         |
|    policy_gradient_loss | -0.000555     |
|    std                  | 2.8           |
|    value_loss           | 0.537         |
-------------------------------------------
Iteration: 1331 | Episodes: 54000 | Median Reward: 37.65 | Avg Reward: 39.05 | Max Reward: 45.98
Iteration: 1334 | Episodes: 54100 | Median Reward: 37.98 | Avg Reward: 38.52 | Max Reward: 45.98
Iteration: 1336 | Episodes: 54200 | Median Reward: 37.65 | Avg Reward: 37.74 | Max Reward: 45.98
Iteration: 1338 | Episodes: 54300 | Median Reward: 37.99 | Avg Reward: 38.05 | Max Reward: 45.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.9       |
| time/                   |             |
|    fps                  | 533         |
|    iterations           | 1340        |
|    time_elapsed         | 10290       |
|    total_timesteps      | 5488640     |
| train/                  |             |
|    approx_kl            | 0.005611396 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -317        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -62.9       |
|    n_updates            | 13390       |
|    policy_gradient_loss | -0.0239     |
|    std                  | 2.84        |
|    value_loss           | 1.02        |
-----------------------------------------
Iteration: 1341 | Episodes: 54400 | Median Reward: 38.87 | Avg Reward: 37.91 | Max Reward: 45.98
Iteration: 1343 | Episodes: 54500 | Median Reward: 38.17 | Avg Reward: 38.25 | Max Reward: 45.98
Iteration: 1346 | Episodes: 54600 | Median Reward: 40.36 | Avg Reward: 39.74 | Max Reward: 45.98
Iteration: 1348 | Episodes: 54700 | Median Reward: 39.17 | Avg Reward: 38.90 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63           |
| time/                   |               |
|    fps                  | 533           |
|    iterations           | 1350          |
|    time_elapsed         | 10361         |
|    total_timesteps      | 5529600       |
| train/                  |               |
|    approx_kl            | 3.2949712e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -318          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -63.1         |
|    n_updates            | 13490         |
|    policy_gradient_loss | -0.000341     |
|    std                  | 2.87          |
|    value_loss           | 0.675         |
-------------------------------------------
Iteration: 1351 | Episodes: 54800 | Median Reward: 36.53 | Avg Reward: 36.90 | Max Reward: 45.98
Iteration: 1353 | Episodes: 54900 | Median Reward: 38.45 | Avg Reward: 38.92 | Max Reward: 45.98
Iteration: 1356 | Episodes: 55000 | Median Reward: 38.00 | Avg Reward: 37.60 | Max Reward: 45.98
Iteration: 1358 | Episodes: 55100 | Median Reward: 38.02 | Avg Reward: 38.18 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.5         |
| time/                   |               |
|    fps                  | 532           |
|    iterations           | 1360          |
|    time_elapsed         | 10467         |
|    total_timesteps      | 5570560       |
| train/                  |               |
|    approx_kl            | 4.6107176e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -318          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -62.1         |
|    n_updates            | 13590         |
|    policy_gradient_loss | -0.000339     |
|    std                  | 2.9           |
|    value_loss           | 1.27          |
-------------------------------------------
Iteration: 1361 | Episodes: 55200 | Median Reward: 40.79 | Avg Reward: 40.43 | Max Reward: 45.98
Iteration: 1363 | Episodes: 55300 | Median Reward: 40.69 | Avg Reward: 40.14 | Max Reward: 45.98
Iteration: 1366 | Episodes: 55400 | Median Reward: 36.92 | Avg Reward: 37.27 | Max Reward: 45.98
Iteration: 1368 | Episodes: 55500 | Median Reward: 37.03 | Avg Reward: 36.62 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.9         |
| time/                   |               |
|    fps                  | 532           |
|    iterations           | 1370          |
|    time_elapsed         | 10536         |
|    total_timesteps      | 5611520       |
| train/                  |               |
|    approx_kl            | 0.00015078198 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -319          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -63.6         |
|    n_updates            | 13690         |
|    policy_gradient_loss | -0.000307     |
|    std                  | 2.93          |
|    value_loss           | 2.46          |
-------------------------------------------
Iteration: 1370 | Episodes: 55600 | Median Reward: 38.64 | Avg Reward: 39.38 | Max Reward: 45.98
Iteration: 1373 | Episodes: 55700 | Median Reward: 40.24 | Avg Reward: 39.90 | Max Reward: 45.98
Iteration: 1375 | Episodes: 55800 | Median Reward: 38.11 | Avg Reward: 38.99 | Max Reward: 45.98
Iteration: 1378 | Episodes: 55900 | Median Reward: 37.66 | Avg Reward: 36.20 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.9        |
| time/                   |              |
|    fps                  | 531          |
|    iterations           | 1380         |
|    time_elapsed         | 10634        |
|    total_timesteps      | 5652480      |
| train/                  |              |
|    approx_kl            | 0.0023313724 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -320         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -62.2        |
|    n_updates            | 13790        |
|    policy_gradient_loss | -0.01        |
|    std                  | 2.97         |
|    value_loss           | 0.97         |
------------------------------------------
Iteration: 1380 | Episodes: 56000 | Median Reward: 38.47 | Avg Reward: 37.90 | Max Reward: 45.98
Iteration: 1383 | Episodes: 56100 | Median Reward: 38.75 | Avg Reward: 37.24 | Max Reward: 45.98
Iteration: 1385 | Episodes: 56200 | Median Reward: 39.50 | Avg Reward: 38.97 | Max Reward: 45.98
Iteration: 1388 | Episodes: 56300 | Median Reward: 39.65 | Avg Reward: 39.09 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.3        |
| time/                   |              |
|    fps                  | 530          |
|    iterations           | 1390         |
|    time_elapsed         | 10731        |
|    total_timesteps      | 5693440      |
| train/                  |              |
|    approx_kl            | 0.0029104794 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -320         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -62.9        |
|    n_updates            | 13890        |
|    policy_gradient_loss | -0.0107      |
|    std                  | 2.99         |
|    value_loss           | 1.4          |
------------------------------------------
Iteration: 1390 | Episodes: 56400 | Median Reward: 39.42 | Avg Reward: 38.53 | Max Reward: 45.98
Iteration: 1393 | Episodes: 56500 | Median Reward: 37.33 | Avg Reward: 37.16 | Max Reward: 45.98
Iteration: 1395 | Episodes: 56600 | Median Reward: 38.31 | Avg Reward: 38.50 | Max Reward: 45.98
Iteration: 1398 | Episodes: 56700 | Median Reward: 35.70 | Avg Reward: 36.84 | Max Reward: 45.98
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -62.3      |
| time/                   |            |
|    fps                  | 530        |
|    iterations           | 1400       |
|    time_elapsed         | 10812      |
|    total_timesteps      | 5734400    |
| train/                  |            |
|    approx_kl            | 0.00539914 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -321       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -63        |
|    n_updates            | 13990      |
|    policy_gradient_loss | -0.0222    |
|    std                  | 3.03       |
|    value_loss           | 1.65       |
----------------------------------------
Iteration: 1400 | Episodes: 56800 | Median Reward: 38.91 | Avg Reward: 38.13 | Max Reward: 45.98
Iteration: 1403 | Episodes: 56900 | Median Reward: 39.09 | Avg Reward: 37.48 | Max Reward: 45.98
Iteration: 1405 | Episodes: 57000 | Median Reward: 40.21 | Avg Reward: 39.91 | Max Reward: 45.98
Iteration: 1407 | Episodes: 57100 | Median Reward: 41.48 | Avg Reward: 41.59 | Max Reward: 45.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.1       |
| time/                   |             |
|    fps                  | 529         |
|    iterations           | 1410        |
|    time_elapsed         | 10916       |
|    total_timesteps      | 5775360     |
| train/                  |             |
|    approx_kl            | 0.000171051 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -321        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -62         |
|    n_updates            | 14090       |
|    policy_gradient_loss | -0.000917   |
|    std                  | 3.06        |
|    value_loss           | 1.71        |
-----------------------------------------
Iteration: 1410 | Episodes: 57200 | Median Reward: 35.60 | Avg Reward: 35.99 | Max Reward: 45.98
Iteration: 1412 | Episodes: 57300 | Median Reward: 40.03 | Avg Reward: 38.66 | Max Reward: 45.98
Iteration: 1415 | Episodes: 57400 | Median Reward: 36.75 | Avg Reward: 36.72 | Max Reward: 45.98
Iteration: 1417 | Episodes: 57500 | Median Reward: 39.21 | Avg Reward: 38.68 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.8         |
| time/                   |               |
|    fps                  | 529           |
|    iterations           | 1420          |
|    time_elapsed         | 10978         |
|    total_timesteps      | 5816320       |
| train/                  |               |
|    approx_kl            | 0.00018050916 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -322          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -62.9         |
|    n_updates            | 14190         |
|    policy_gradient_loss | -0.00216      |
|    std                  | 3.09          |
|    value_loss           | 1.69          |
-------------------------------------------
Iteration: 1420 | Episodes: 57600 | Median Reward: 40.01 | Avg Reward: 39.26 | Max Reward: 45.98
Iteration: 1422 | Episodes: 57700 | Median Reward: 39.47 | Avg Reward: 38.19 | Max Reward: 45.98
Iteration: 1425 | Episodes: 57800 | Median Reward: 37.83 | Avg Reward: 38.75 | Max Reward: 45.98
Iteration: 1427 | Episodes: 57900 | Median Reward: 38.49 | Avg Reward: 38.00 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.2         |
| time/                   |               |
|    fps                  | 528           |
|    iterations           | 1430          |
|    time_elapsed         | 11082         |
|    total_timesteps      | 5857280       |
| train/                  |               |
|    approx_kl            | 0.00024476653 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -322          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -63.7         |
|    n_updates            | 14290         |
|    policy_gradient_loss | -0.0018       |
|    std                  | 3.11          |
|    value_loss           | 3.15          |
-------------------------------------------
Iteration: 1430 | Episodes: 58000 | Median Reward: 37.88 | Avg Reward: 37.59 | Max Reward: 45.98
Iteration: 1432 | Episodes: 58100 | Median Reward: 39.91 | Avg Reward: 38.46 | Max Reward: 45.98
Iteration: 1435 | Episodes: 58200 | Median Reward: 37.08 | Avg Reward: 37.40 | Max Reward: 45.98
Iteration: 1437 | Episodes: 58300 | Median Reward: 37.82 | Avg Reward: 38.16 | Max Reward: 45.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.4        |
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 1440         |
|    time_elapsed         | 11169        |
|    total_timesteps      | 5898240      |
| train/                  |              |
|    approx_kl            | 0.0007499112 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -323         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -63.8        |
|    n_updates            | 14390        |
|    policy_gradient_loss | -0.00396     |
|    std                  | 3.16         |
|    value_loss           | 0.71         |
------------------------------------------
Iteration: 1440 | Episodes: 58400 | Median Reward: 36.97 | Avg Reward: 37.49 | Max Reward: 45.98
Iteration: 1442 | Episodes: 58500 | Median Reward: 39.78 | Avg Reward: 38.40 | Max Reward: 45.98
Iteration: 1444 | Episodes: 58600 | Median Reward: 40.25 | Avg Reward: 38.82 | Max Reward: 45.98
Iteration: 1447 | Episodes: 58700 | Median Reward: 35.64 | Avg Reward: 37.15 | Max Reward: 45.98
Iteration: 1449 | Episodes: 58800 | Median Reward: 37.22 | Avg Reward: 36.73 | Max Reward: 45.98
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.3       |
| time/                   |             |
|    fps                  | 527         |
|    iterations           | 1450        |
|    time_elapsed         | 11256       |
|    total_timesteps      | 5939200     |
| train/                  |             |
|    approx_kl            | 0.011852684 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.3         |
|    entropy_loss         | -324        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -64.6       |
|    n_updates            | 14490       |
|    policy_gradient_loss | -0.0168     |
|    std                  | 3.2         |
|    value_loss           | 0.76        |
-----------------------------------------
Iteration: 1452 | Episodes: 58900 | Median Reward: 39.07 | Avg Reward: 37.98 | Max Reward: 45.98
Iteration: 1454 | Episodes: 59000 | Median Reward: 40.41 | Avg Reward: 39.21 | Max Reward: 45.98
Iteration: 1457 | Episodes: 59100 | Median Reward: 39.69 | Avg Reward: 39.00 | Max Reward: 45.98
Iteration: 1459 | Episodes: 59200 | Median Reward: 37.72 | Avg Reward: 37.96 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.1         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 1460          |
|    time_elapsed         | 11340         |
|    total_timesteps      | 5980160       |
| train/                  |               |
|    approx_kl            | 0.00045560903 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -325          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -64.2         |
|    n_updates            | 14590         |
|    policy_gradient_loss | -0.0077       |
|    std                  | 3.24          |
|    value_loss           | 0.939         |
-------------------------------------------
Iteration: 1462 | Episodes: 59300 | Median Reward: 38.13 | Avg Reward: 37.04 | Max Reward: 45.98
Iteration: 1464 | Episodes: 59400 | Median Reward: 37.37 | Avg Reward: 36.82 | Max Reward: 45.98
Iteration: 1467 | Episodes: 59500 | Median Reward: 39.48 | Avg Reward: 39.13 | Max Reward: 45.98
Iteration: 1469 | Episodes: 59600 | Median Reward: 36.63 | Avg Reward: 37.22 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63           |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 1470          |
|    time_elapsed         | 11420         |
|    total_timesteps      | 6021120       |
| train/                  |               |
|    approx_kl            | 0.00030248892 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -326          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -63.7         |
|    n_updates            | 14690         |
|    policy_gradient_loss | 5.87e-05      |
|    std                  | 3.27          |
|    value_loss           | 1.56          |
-------------------------------------------
Iteration: 1472 | Episodes: 59700 | Median Reward: 39.64 | Avg Reward: 38.65 | Max Reward: 45.98
Iteration: 1474 | Episodes: 59800 | Median Reward: 38.10 | Avg Reward: 37.26 | Max Reward: 45.98
Iteration: 1477 | Episodes: 59900 | Median Reward: 37.85 | Avg Reward: 37.26 | Max Reward: 45.98
Iteration: 1479 | Episodes: 60000 | Median Reward: 39.76 | Avg Reward: 38.99 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.1         |
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 1480          |
|    time_elapsed         | 11527         |
|    total_timesteps      | 6062080       |
| train/                  |               |
|    approx_kl            | 0.00046921492 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -327          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -64.9         |
|    n_updates            | 14790         |
|    policy_gradient_loss | 0.0038        |
|    std                  | 3.31          |
|    value_loss           | 1.1           |
-------------------------------------------
Iteration: 1481 | Episodes: 60100 | Median Reward: 38.77 | Avg Reward: 38.24 | Max Reward: 45.98
Iteration: 1484 | Episodes: 60200 | Median Reward: 37.08 | Avg Reward: 36.62 | Max Reward: 45.98
Iteration: 1486 | Episodes: 60300 | Median Reward: 39.07 | Avg Reward: 38.25 | Max Reward: 45.98
Iteration: 1489 | Episodes: 60400 | Median Reward: 36.31 | Avg Reward: 35.91 | Max Reward: 45.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.7         |
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 1490          |
|    time_elapsed         | 11611         |
|    total_timesteps      | 6103040       |
| train/                  |               |
|    approx_kl            | 0.00040958374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -327          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -65.2         |
|    n_updates            | 14890         |
|    policy_gradient_loss | -0.00281      |
|    std                  | 3.34          |
|    value_loss           | 0.379         |
-------------------------------------------
Iteration: 1491 | Episodes: 60500 | Median Reward: 38.80 | Avg Reward: 37.55 | Max Reward: 45.98
Iteration: 1494 | Episodes: 60600 | Median Reward: 38.33 | Avg Reward: 38.13 | Max Reward: 45.98
Iteration: 1496 | Epi