Script started on 2024-10-24 17:16:42-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/21" COLUMNS="203" LINES="53"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> codn[K[Knda activate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python code1_1_residual_d.py
GPU 6: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
Iteration: 2 | Episodes: 100 | Median Reward: 21.59 | Avg Reward: 20.13 | Max Reward: 32.92
Iteration: 4 | Episodes: 200 | Median Reward: 19.65 | Avg Reward: 19.29 | Max Reward: 34.77
Iteration: 7 | Episodes: 300 | Median Reward: 15.55 | Avg Reward: 16.15 | Max Reward: 34.77
Iteration: 9 | Episodes: 400 | Median Reward: 22.23 | Avg Reward: 19.80 | Max Reward: 34.77
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -80.1      |
| time/                   |            |
|    fps                  | 948        |
|    iterations           | 10         |
|    time_elapsed         | 43         |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.06050464 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.3        |
|    entropy_loss         | -72.8      |
|    explained_variance   | 0.00304    |
|    learning_rate        | 0.0005     |
|    loss                 | 61.2       |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.0145     |
|    std                  | 1.01       |
|    value_loss           | 188        |
----------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 18.02 | Avg Reward: 18.07 | Max Reward: 34.77
Iteration: 14 | Episodes: 600 | Median Reward: 21.43 | Avg Reward: 21.34 | Max Reward: 39.07
Iteration: 17 | Episodes: 700 | Median Reward: 16.10 | Avg Reward: 16.48 | Max Reward: 39.07
Iteration: 19 | Episodes: 800 | Median Reward: 19.80 | Avg Reward: 18.70 | Max Reward: 39.07
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.8       |
| time/                   |             |
|    fps                  | 944         |
|    iterations           | 20          |
|    time_elapsed         | 86          |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.009418362 |
|    clip_fraction        | 0.00754     |
|    clip_range           | 0.3         |
|    entropy_loss         | -73.7       |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.0005      |
|    loss                 | 89.7        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00291    |
|    std                  | 1.01        |
|    value_loss           | 212         |
-----------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 19.80 | Avg Reward: 18.53 | Max Reward: 39.07
Iteration: 24 | Episodes: 1000 | Median Reward: 20.97 | Avg Reward: 19.41 | Max Reward: 39.07
Iteration: 27 | Episodes: 1100 | Median Reward: 17.49 | Avg Reward: 17.70 | Max Reward: 39.07
Iteration: 29 | Episodes: 1200 | Median Reward: 19.40 | Avg Reward: 19.07 | Max Reward: 39.07
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -80.5      |
| time/                   |            |
|    fps                  | 944        |
|    iterations           | 30         |
|    time_elapsed         | 130        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.06869607 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.3        |
|    entropy_loss         | -71.5      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.09       |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0209    |
|    std                  | 1.01       |
|    value_loss           | 18.9       |
----------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 16.70 | Avg Reward: 16.56 | Max Reward: 39.07
Iteration: 34 | Episodes: 1400 | Median Reward: 21.20 | Avg Reward: 21.40 | Max Reward: 39.07
Iteration: 36 | Episodes: 1500 | Median Reward: 19.15 | Avg Reward: 23.28 | Max Reward: 39.07
Iteration: 39 | Episodes: 1600 | Median Reward: 21.88 | Avg Reward: 21.01 | Max Reward: 39.07
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -79.3      |
| time/                   |            |
|    fps                  | 942        |
|    iterations           | 40         |
|    time_elapsed         | 173        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.19065794 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.3        |
|    entropy_loss         | -82.3      |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0005     |
|    loss                 | -5.07      |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0334    |
|    std                  | 1.01       |
|    value_loss           | 12.3       |
----------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 23.34 | Avg Reward: 21.48 | Max Reward: 39.07
Iteration: 44 | Episodes: 1800 | Median Reward: 19.61 | Avg Reward: 18.48 | Max Reward: 39.07
Iteration: 46 | Episodes: 1900 | Median Reward: 20.20 | Avg Reward: 20.10 | Max Reward: 39.07
Iteration: 49 | Episodes: 2000 | Median Reward: 21.00 | Avg Reward: 20.07 | Max Reward: 39.07
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.5        |
| time/                   |              |
|    fps                  | 944          |
|    iterations           | 50           |
|    time_elapsed         | 216          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0034159736 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -90.9        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0005       |
|    loss                 | -1.83        |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00485     |
|    std                  | 1.01         |
|    value_loss           | 13.3         |
------------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 19.61 | Avg Reward: 19.78 | Max Reward: 39.07
Iteration: 54 | Episodes: 2200 | Median Reward: 18.64 | Avg Reward: 16.73 | Max Reward: 39.07
Iteration: 56 | Episodes: 2300 | Median Reward: 13.04 | Avg Reward: 14.99 | Max Reward: 39.07
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[BIteration: 59 | Episodes: 2400 | Median Reward: 22.77 | Avg Reward: 20.77 | Max Reward: 39.07
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -79         |
| time/                   |             |
|    fps                  | 946         |
|    iterations           | 60          |
|    time_elapsed         | 259         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.065883026 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.3         |
|    entropy_loss         | -106        |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.44       |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.0035     |
|    std                  | 1.01        |
|    value_loss           | 6.83        |
-----------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 22.80 | Avg Reward: 23.12 | Max Reward: 39.07
Iteration: 64 | Episodes: 2600 | Median Reward: 21.10 | Avg Reward: 21.61 | Max Reward: 39.07
Iteration: 66 | Episodes: 2700 | Median Reward: 21.94 | Avg Reward: 22.83 | Max Reward: 39.07
Iteration: 69 | Episodes: 2800 | Median Reward: 26.31 | Avg Reward: 26.01 | Max Reward: 39.07
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -74.3      |
| time/                   |            |
|    fps                  | 945        |
|    iterations           | 70         |
|    time_elapsed         | 303        |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.01128569 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -118       |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0005     |
|    loss                 | -4.41      |
|    n_updates            | 690        |
|    policy_gradient_loss | 0.00704    |
|    std                  | 1.01       |
|    value_loss           | 7.7        |
----------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 24.81 | Avg Reward: 23.95 | Max Reward: 39.07
Iteration: 73 | Episodes: 3000 | Median Reward: 24.15 | Avg Reward: 24.84 | Max Reward: 39.07
Iteration: 76 | Episodes: 3100 | Median Reward: 25.19 | Avg Reward: 22.28 | Max Reward: 39.07
Iteration: 78 | Episodes: 3200 | Median Reward: 29.27 | Avg Reward: 27.42 | Max Reward: 41.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72.4        |
| time/                   |              |
|    fps                  | 944          |
|    iterations           | 80           |
|    time_elapsed         | 347          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0010273011 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.3          |
|    entropy_loss         | -130         |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.29        |
|    n_updates            | 790          |
|    policy_gradient_loss | -4.47e-05    |
|    std                  | 1.01         |
|    value_loss           | 11.5         |
------------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 23.95 | Avg Reward: 22.64 | Max Reward: 43.29
Iteration: 83 | Episodes: 3400 | Median Reward: 26.85 | Avg Reward: 24.03 | Max Reward: 43.29
Iteration: 86 | Episodes: 3500 | Median Reward: 28.63 | Avg Reward: 24.90 | Max Reward: 43.29
Iteration: 88 | Episodes: 3600 | Median Reward: 28.34 | Avg Reward: 26.81 | Max Reward: 43.29
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74.5       |
| time/                   |             |
|    fps                  | 943         |
|    iterations           | 90          |
|    time_elapsed         | 390         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.102490634 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.3         |
|    entropy_loss         | -136        |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.4       |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0687     |
|    std                  | 1.02        |
|    value_loss           | 3.16        |
-----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 25.98 | Avg Reward: 27.24 | Max Reward: 43.29
Iteration: 93 | Episodes: 3800 | Median Reward: 25.98 | Avg Reward: 23.96 | Max Reward: 43.29
Iteration: 96 | Episodes: 3900 | Median Reward: 27.49 | Avg Reward: 26.74 | Max Reward: 43.29
Iteration: 98 | Episodes: 4000 | Median Reward: 25.36 | Avg Reward: 26.04 | Max Reward: 43.29
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -77.5       |
| time/                   |             |
|    fps                  | 943         |
|    iterations           | 100         |
|    time_elapsed         | 434         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.011394294 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -145        |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.1       |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0127     |
|    std                  | 1.02        |
|    value_loss           | 4.87        |
-----------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 22.82 | Avg Reward: 22.15 | Max Reward: 43.29
Iteration: 103 | Episodes: 4200 | Median Reward: 25.94 | Avg Reward: 26.20 | Max Reward: 43.29
Iteration: 106 | Episodes: 4300 | Median Reward: 28.09 | Avg Reward: 26.42 | Max Reward: 43.65
Iteration: 108 | Episodes: 4400 | Median Reward: 26.77 | Avg Reward: 28.28 | Max Reward: 44.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.1       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 110         |
|    time_elapsed         | 478         |
|    total_timesteps      | 450560      |
| train/                  |             |
|    approx_kl            | 0.008864043 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -149        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.1       |
|    n_updates            | 1090        |
|    policy_gradient_loss | 0.00257     |
|    std                  | 1.02        |
|    value_loss           | 8.38        |
-----------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 29.35 | Avg Reward: 28.12 | Max Reward: 44.20
Iteration: 113 | Episodes: 4600 | Median Reward: 27.09 | Avg Reward: 26.72 | Max Reward: 44.20
Iteration: 115 | Episodes: 4700 | Median Reward: 27.77 | Avg Reward: 26.54 | Max Reward: 44.20
Iteration: 118 | Episodes: 4800 | Median Reward: 29.46 | Avg Reward: 31.27 | Max Reward: 44.20
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -72.9       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 120         |
|    time_elapsed         | 521         |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.023937944 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.3         |
|    entropy_loss         | -154        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -13.8       |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0408     |
|    std                  | 1.02        |
|    value_loss           | 4.47        |
-----------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 25.31 | Avg Reward: 26.38 | Max Reward: 44.89
Iteration: 123 | Episodes: 5000 | Median Reward: 31.18 | Avg Reward: 31.20 | Max Reward: 44.89
Iteration: 125 | Episodes: 5100 | Median Reward: 31.00 | Avg Reward: 28.86 | Max Reward: 44.89
Iteration: 128 | Episodes: 5200 | Median Reward: 27.93 | Avg Reward: 28.20 | Max Reward: 44.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.1       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 130         |
|    time_elapsed         | 565         |
|    total_timesteps      | 532480      |
| train/                  |             |
|    approx_kl            | 0.012939731 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.3         |
|    entropy_loss         | -163        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.6       |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0148     |
|    std                  | 1.02        |
|    value_loss           | 5.4         |
-----------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 28.86 | Avg Reward: 28.57 | Max Reward: 44.89
Iteration: 133 | Episodes: 5400 | Median Reward: 27.57 | Avg Reward: 27.23 | Max Reward: 44.89
Iteration: 135 | Episodes: 5500 | Median Reward: 27.51 | Avg Reward: 27.45 | Max Reward: 44.89
Iteration: 138 | Episodes: 5600 | Median Reward: 30.18 | Avg Reward: 31.37 | Max Reward: 44.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.7        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 140          |
|    time_elapsed         | 609          |
|    total_timesteps      | 573440       |
| train/                  |              |
|    approx_kl            | 0.0016360839 |
|    clip_fraction        | 0.000732     |
|    clip_range           | 0.3          |
|    entropy_loss         | -168         |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0005       |
|    loss                 | -15.2        |
|    n_updates            | 1390         |
|    policy_gradient_loss | -5.45e-05    |
|    std                  | 1.03         |
|    value_loss           | 5.05         |
------------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 32.48 | Avg Reward: 31.93 | Max Reward: 44.89
Iteration: 143 | Episodes: 5800 | Median Reward: 30.43 | Avg Reward: 29.90 | Max Reward: 44.89
Iteration: 145 | Episodes: 5900 | Median Reward: 33.01 | Avg Reward: 32.55 | Max Reward: 44.89
Iteration: 147 | Episodes: 6000 | Median Reward: 27.83 | Avg Reward: 28.74 | Max Reward: 44.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.9       |
| time/                   |             |
|    fps                  | 942         |
|    iterations           | 150         |
|    time_elapsed         | 651         |
|    total_timesteps      | 614400      |
| train/                  |             |
|    approx_kl            | 0.008406941 |
|    clip_fraction        | 0.000732    |
|    clip_range           | 0.3         |
|    entropy_loss         | -171        |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0005      |
|    loss                 | -13.3       |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00236    |
|    std                  | 1.03        |
|    value_loss           | 3.61        |
-----------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 33.03 | Avg Reward: 30.49 | Max Reward: 44.89
Iteration: 152 | Episodes: 6200 | Median Reward: 28.63 | Avg Reward: 27.45 | Max Reward: 44.89
Iteration: 155 | Episodes: 6300 | Median Reward: 34.33 | Avg Reward: 34.21 | Max Reward: 44.89
Iteration: 157 | Episodes: 6400 | Median Reward: 33.49 | Avg Reward: 32.26 | Max Reward: 44.89
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -66.5      |
| time/                   |            |
|    fps                  | 942        |
|    iterations           | 160        |
|    time_elapsed         | 695        |
|    total_timesteps      | 655360     |
| train/                  |            |
|    approx_kl            | 0.02940147 |
|    clip_fraction        | 0.05       |
|    clip_range           | 0.3        |
|    entropy_loss         | -174       |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0005     |
|    loss                 | -16.8      |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.027     |
|    std                  | 1.03       |
|    value_loss           | 2.26       |
----------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 33.50 | Avg Reward: 33.90 | Max Reward: 44.89
Iteration: 162 | Episodes: 6600 | Median Reward: 32.57 | Avg Reward: 33.35 | Max Reward: 44.89
Iteration: 165 | Episodes: 6700 | Median Reward: 32.41 | Avg Reward: 31.49 | Max Reward: 44.89
Iteration: 167 | Episodes: 6800 | Median Reward: 30.85 | Avg Reward: 31.88 | Max Reward: 44.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.8       |
| time/                   |             |
|    fps                  | 942         |
|    iterations           | 170         |
|    time_elapsed         | 738         |
|    total_timesteps      | 696320      |
| train/                  |             |
|    approx_kl            | 0.011597848 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -178        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0005      |
|    loss                 | -16.9       |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00503    |
|    std                  | 1.03        |
|    value_loss           | 3.88        |
-----------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 35.02 | Avg Reward: 33.27 | Max Reward: 44.89
Iteration: 172 | Episodes: 7000 | Median Reward: 35.02 | Avg Reward: 34.31 | Max Reward: 44.89
Iteration: 175 | Episodes: 7100 | Median Reward: 29.70 | Avg Reward: 31.60 | Max Reward: 44.89
Iteration: 177 | Episodes: 7200 | Median Reward: 37.88 | Avg Reward: 36.20 | Max Reward: 44.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.2       |
| time/                   |             |
|    fps                  | 942         |
|    iterations           | 180         |
|    time_elapsed         | 782         |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.006129345 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -183        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -16.6       |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00749    |
|    std                  | 1.04        |
|    value_loss           | 1.88        |
-----------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 35.66 | Avg Reward: 34.93 | Max Reward: 44.91
Iteration: 182 | Episodes: 7400 | Median Reward: 38.40 | Avg Reward: 37.34 | Max Reward: 44.91
Iteration: 184 | Episodes: 7500 | Median Reward: 36.21 | Avg Reward: 36.29 | Max Reward: 45.51
Iteration: 187 | Episodes: 7600 | Median Reward: 36.30 | Avg Reward: 34.34 | Max Reward: 45.51
Iteration: 189 | Episodes: 7700 | Median Reward: 35.10 | Avg Reward: 35.61 | Max Reward: 45.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.3       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 190         |
|    time_elapsed         | 826         |
|    total_timesteps      | 778240      |
| train/                  |             |
|    approx_kl            | 0.005087601 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -186        |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.2       |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00445    |
|    std                  | 1.04        |
|    value_loss           | 9.48        |
-----------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 36.82 | Avg Reward: 37.08 | Max Reward: 45.94
Iteration: 194 | Episodes: 7900 | Median Reward: 38.12 | Avg Reward: 37.90 | Max Reward: 45.94
Iteration: 197 | Episodes: 8000 | Median Reward: 35.43 | Avg Reward: 37.19 | Max Reward: 45.94
Iteration: 199 | Episodes: 8100 | Median Reward: 36.08 | Avg Reward: 35.04 | Max Reward: 45.94
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.9        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 200          |
|    time_elapsed         | 870          |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0036379963 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -188         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -18.5        |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00243     |
|    std                  | 1.04         |
|    value_loss           | 2.35         |
------------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 36.83 | Avg Reward: 35.20 | Max Reward: 45.94
Iteration: 204 | Episodes: 8300 | Median Reward: 37.41 | Avg Reward: 37.73 | Max Reward: 45.94
Iteration: 207 | Episodes: 8400 | Median Reward: 34.33 | Avg Reward: 34.89 | Max Reward: 45.94
Iteration: 209 | Episodes: 8500 | Median Reward: 38.50 | Avg Reward: 38.15 | Max Reward: 45.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.7       |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 210         |
|    time_elapsed         | 913         |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.014205413 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -191        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -18.6       |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00975    |
|    std                  | 1.04        |
|    value_loss           | 2.14        |
-----------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 38.18 | Avg Reward: 36.31 | Max Reward: 45.94
Iteration: 214 | Episodes: 8700 | Median Reward: 37.69 | Avg Reward: 37.09 | Max Reward: 45.94
Iteration: 216 | Episodes: 8800 | Median Reward: 33.91 | Avg Reward: 35.13 | Max Reward: 45.94
Iteration: 219 | Episodes: 8900 | Median Reward: 37.73 | Avg Reward: 37.42 | Max Reward: 45.94
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.1        |
| time/                   |              |
|    fps                  | 941          |
|    iterations           | 220          |
|    time_elapsed         | 957          |
|    total_timesteps      | 901120       |
| train/                  |              |
|    approx_kl            | 0.0027746651 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -194         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -18.5        |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00363     |
|    std                  | 1.05         |
|    value_loss           | 3.26         |
------------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 37.89 | Avg Reward: 36.32 | Max Reward: 45.94
Iteration: 224 | Episodes: 9100 | Median Reward: 37.29 | Avg Reward: 35.97 | Max Reward: 45.94
Iteration: 226 | Episodes: 9200 | Median Reward: 37.68 | Avg Reward: 37.68 | Max Reward: 45.94
Iteration: 229 | Episodes: 9300 | Median Reward: 38.41 | Avg Reward: 37.27 | Max Reward: 45.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.7       |
| time/                   |             |
|    fps                  | 940         |
|    iterations           | 230         |
|    time_elapsed         | 1001        |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.011879215 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -195        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0005      |
|    loss                 | -16.4       |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00168    |
|    std                  | 1.05        |
|    value_loss           | 3.82        |
-----------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 40.04 | Avg Reward: 37.45 | Max Reward: 45.94
Iteration: 234 | Episodes: 9500 | Median Reward: 37.21 | Avg Reward: 36.73 | Max Reward: 45.94
Iteration: 236 | Episodes: 9600 | Median Reward: 40.52 | Avg Reward: 38.81 | Max Reward: 45.94
Iteration: 239 | Episodes: 9700 | Median Reward: 40.64 | Avg Reward: 37.90 | Max Reward: 45.95
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -62.2      |
| time/                   |            |
|    fps                  | 940        |
|    iterations           | 240        |
|    time_elapsed         | 1045       |
|    total_timesteps      | 983040     |
| train/                  |            |
|    approx_kl            | 0.11732904 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.3        |
|    entropy_loss         | -198       |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0005     |
|    loss                 | -17.7      |
|    n_updates            | 2390       |
|    policy_gradient_loss | -0.0822    |
|    std                  | 1.05       |
|    value_loss           | 3.35       |
----------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 38.89 | Avg Reward: 37.73 | Max Reward: 45.95
Iteration: 244 | Episodes: 9900 | Median Reward: 39.29 | Avg Reward: 37.75 | Max Reward: 45.95
Iteration: 246 | Episodes: 10000 | Median Reward: 40.45 | Avg Reward: 39.13 | Max Reward: 45.95
Iteration: 249 | Episodes: 10100 | Median Reward: 39.41 | Avg Reward: 40.15 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60         |
| time/                   |             |
|    fps                  | 940         |
|    iterations           | 250         |
|    time_elapsed         | 1088        |
|    total_timesteps      | 1024000     |
| train/                  |             |
|    approx_kl            | 0.012918246 |
|    clip_fraction        | 0.000488    |
|    clip_range           | 0.3         |
|    entropy_loss         | -202        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -20         |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00672    |
|    std                  | 1.06        |
|    value_loss           | 1.07        |
-----------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 39.44 | Avg Reward: 38.30 | Max Reward: 45.95
Iteration: 253 | Episodes: 10300 | Median Reward: 37.26 | Avg Reward: 38.06 | Max Reward: 45.95
Iteration: 256 | Episodes: 10400 | Median Reward: 39.49 | Avg Reward: 38.17 | Max Reward: 45.95
Iteration: 258 | Episodes: 10500 | Median Reward: 39.37 | Avg Reward: 39.25 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.8       |
| time/                   |             |
|    fps                  | 940         |
|    iterations           | 260         |
|    time_elapsed         | 1132        |
|    total_timesteps      | 1064960     |
| train/                  |             |
|    approx_kl            | 0.040123083 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.3         |
|    entropy_loss         | -206        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -19.8       |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0433     |
|    std                  | 1.06        |
|    value_loss           | 1.06        |
-----------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 41.72 | Avg Reward: 41.06 | Max Reward: 45.95
Iteration: 263 | Episodes: 10700 | Median Reward: 41.28 | Avg Reward: 41.15 | Max Reward: 45.95
Iteration: 266 | Episodes: 10800 | Median Reward: 37.95 | Avg Reward: 38.48 | Max Reward: 45.95
Iteration: 268 | Episodes: 10900 | Median Reward: 40.37 | Avg Reward: 39.78 | Max Reward: 45.95
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -59.7     |
| time/                   |           |
|    fps                  | 939       |
|    iterations           | 270       |
|    time_elapsed         | 1176      |
|    total_timesteps      | 1105920   |
| train/                  |           |
|    approx_kl            | 0.0876221 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.3       |
|    entropy_loss         | -208      |
|    explained_variance   | 0.998     |
|    learning_rate        | 0.0005    |
|    loss                 | -20.3     |
|    n_updates            | 2690      |
|    policy_gradient_loss | -0.00725  |
|    std                  | 1.06      |
|    value_loss           | 1.31      |
---------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 37.41 | Avg Reward: 39.00 | Max Reward: 45.95
Iteration: 273 | Episodes: 11100 | Median Reward: 39.66 | Avg Reward: 39.97 | Max Reward: 45.95
Iteration: 276 | Episodes: 11200 | Median Reward: 41.54 | Avg Reward: 39.93 | Max Reward: 45.95
Iteration: 278 | Episodes: 11300 | Median Reward: 39.85 | Avg Reward: 40.30 | Max Reward: 45.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.1       |
| time/                   |             |
|    fps                  | 939         |
|    iterations           | 280         |
|    time_elapsed         | 1220        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.058005385 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.3         |
|    entropy_loss         | -209        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -20.8       |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.0391     |
|    std                  | 1.07        |
|    value_loss           | 0.812       |
-----------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 41.84 | Avg Reward: 40.46 | Max Reward: 46.86
Iteration: 283 | Episodes: 11500 | Median Reward: 42.43 | Avg Reward: 42.00 | Max Reward: 46.86
Iteration: 286 | Episodes: 11600 | Median Reward: 37.84 | Avg Reward: 39.14 | Max Reward: 46.86
Iteration: 288 | Episodes: 11700 | Median Reward: 39.69 | Avg Reward: 39.21 | Max Reward: 46.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 939         |
|    iterations           | 290         |
|    time_elapsed         | 1264        |
|    total_timesteps      | 1187840     |
| train/                  |             |
|    approx_kl            | 0.002552005 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -211        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -20.7       |
|    n_updates            | 2890        |
|    policy_gradient_loss | 0.00212     |
|    std                  | 1.07        |
|    value_loss           | 1.93        |
-----------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 41.44 | Avg Reward: 41.04 | Max Reward: 46.86
Iteration: 293 | Episodes: 11900 | Median Reward: 41.30 | Avg Reward: 41.17 | Max Reward: 46.86
Iteration: 295 | Episodes: 12000 | Median Reward: 39.45 | Avg Reward: 40.06 | Max Reward: 46.86
Iteration: 298 | Episodes: 12100 | Median Reward: 41.51 | Avg Reward: 40.75 | Max Reward: 46.86
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -61       |
| time/                   |           |
|    fps                  | 939       |
|    iterations           | 300       |
|    time_elapsed         | 1308      |
|    total_timesteps      | 1228800   |
| train/                  |           |
|    approx_kl            | 0.1684872 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.3       |
|    entropy_loss         | -212      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.0005    |
|    loss                 | -20.8     |
|    n_updates            | 2990      |
|    policy_gradient_loss | -0.0891   |
|    std                  | 1.07      |
|    value_loss           | 0.681     |
---------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 40.03 | Avg Reward: 39.44 | Max Reward: 46.86
Iteration: 303 | Episodes: 12300 | Median Reward: 41.87 | Avg Reward: 42.03 | Max Reward: 46.86
Iteration: 305 | Episodes: 12400 | Median Reward: 41.98 | Avg Reward: 42.29 | Max Reward: 46.86
Iteration: 308 | Episodes: 12500 | Median Reward: 41.96 | Avg Reward: 41.83 | Max Reward: 46.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.6       |
| time/                   |             |
|    fps                  | 939         |
|    iterations           | 310         |
|    time_elapsed         | 1351        |
|    total_timesteps      | 1269760     |
| train/                  |             |
|    approx_kl            | 0.043577414 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.3         |
|    entropy_loss         | -213        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -19.5       |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.0734     |
|    std                  | 1.07        |
|    value_loss           | 11.1        |
-----------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: 37.23 | Avg Reward: 39.50 | Max Reward: 47.01
Iteration: 313 | Episodes: 12700 | Median Reward: 40.47 | Avg Reward: 39.58 | Max Reward: 47.01
Iteration: 315 | Episodes: 12800 | Median Reward: 39.34 | Avg Reward: 39.63 | Max Reward: 47.01
Iteration: 318 | Episodes: 12900 | Median Reward: 40.88 | Avg Reward: 39.36 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.3        |
| time/                   |              |
|    fps                  | 939          |
|    iterations           | 320          |
|    time_elapsed         | 1395         |
|    total_timesteps      | 1310720      |
| train/                  |              |
|    approx_kl            | 0.0048583085 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -216         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.6        |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.00403     |
|    std                  | 1.08         |
|    value_loss           | 5.65         |
------------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: 39.59 | Avg Reward: 39.13 | Max Reward: 47.01
Iteration: 323 | Episodes: 13100 | Median Reward: 43.98 | Avg Reward: 42.22 | Max Reward: 47.01
Iteration: 325 | Episodes: 13200 | Median Reward: 40.40 | Avg Reward: 41.08 | Max Reward: 47.01
Iteration: 327 | Episodes: 13300 | Median Reward: 43.48 | Avg Reward: 42.74 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 939          |
|    iterations           | 330          |
|    time_elapsed         | 1439         |
|    total_timesteps      | 1351680      |
| train/                  |              |
|    approx_kl            | 0.0011355925 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -217         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.5        |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.00247     |
|    std                  | 1.08         |
|    value_loss           | 4.02         |
------------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: 43.97 | Avg Reward: 42.71 | Max Reward: 47.01
Iteration: 332 | Episodes: 13500 | Median Reward: 41.77 | Avg Reward: 42.64 | Max Reward: 47.01
Iteration: 335 | Episodes: 13600 | Median Reward: 40.29 | Avg Reward: 40.47 | Max Reward: 47.01
Iteration: 337 | Episodes: 13700 | Median Reward: 43.79 | Avg Reward: 42.22 | Max Reward: 47.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.3       |
| time/                   |             |
|    fps                  | 939         |
|    iterations           | 340         |
|    time_elapsed         | 1482        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.020543769 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -218        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.3       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.0106     |
|    std                  | 1.08        |
|    value_loss           | 0.903       |
-----------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: 40.19 | Avg Reward: 40.74 | Max Reward: 47.01
Iteration: 342 | Episodes: 13900 | Median Reward: 41.70 | Avg Reward: 41.81 | Max Reward: 47.01
Iteration: 345 | Episodes: 14000 | Median Reward: 41.30 | Avg Reward: 39.95 | Max Reward: 47.01
Iteration: 347 | Episodes: 14100 | Median Reward: 41.12 | Avg Reward: 40.76 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.7        |
| time/                   |              |
|    fps                  | 939          |
|    iterations           | 350          |
|    time_elapsed         | 1526         |
|    total_timesteps      | 1433600      |
| train/                  |              |
|    approx_kl            | 0.0005162729 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -219         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -19.2        |
|    n_updates            | 3490         |
|    policy_gradient_loss | -0.000548    |
|    std                  | 1.09         |
|    value_loss           | 2.79         |
------------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: 41.13 | Avg Reward: 40.13 | Max Reward: 47.01
Iteration: 352 | Episodes: 14300 | Median Reward: 41.07 | Avg Reward: 41.18 | Max Reward: 47.01
Iteration: 355 | Episodes: 14400 | Median Reward: 43.39 | Avg Reward: 42.91 | Max Reward: 47.01
Iteration: 357 | Episodes: 14500 | Median Reward: 42.13 | Avg Reward: 41.84 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58          |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 360          |
|    time_elapsed         | 1570         |
|    total_timesteps      | 1474560      |
| train/                  |              |
|    approx_kl            | 0.0054381583 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -220         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -21.8        |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.00661     |
|    std                  | 1.09         |
|    value_loss           | 1.09         |
------------------------------------------
Iteration: 360 | Episodes: 14600 | Median Reward: 42.50 | Avg Reward: 42.09 | Max Reward: 47.01
Iteration: 362 | Episodes: 14700 | Median Reward: 44.49 | Avg Reward: 42.79 | Max Reward: 47.01
Iteration: 364 | Episodes: 14800 | Median Reward: 42.80 | Avg Reward: 42.55 | Max Reward: 47.01
Iteration: 367 | Episodes: 14900 | Median Reward: 42.70 | Avg Reward: 42.58 | Max Reward: 47.01
Iteration: 369 | Episodes: 15000 | Median Reward: 42.01 | Avg Reward: 42.28 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.8        |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 370          |
|    time_elapsed         | 1614         |
|    total_timesteps      | 1515520      |
| train/                  |              |
|    approx_kl            | 0.0033564614 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -221         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.5        |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00189     |
|    std                  | 1.09         |
|    value_loss           | 1.28         |
------------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: 42.32 | Avg Reward: 42.06 | Max Reward: 47.01
Iteration: 374 | Episodes: 15200 | Median Reward: 43.24 | Avg Reward: 42.34 | Max Reward: 47.01
Iteration: 377 | Episodes: 15300 | Median Reward: 42.22 | Avg Reward: 42.47 | Max Reward: 47.01
Iteration: 379 | Episodes: 15400 | Median Reward: 42.15 | Avg Reward: 41.68 | Max Reward: 47.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 938         |
|    iterations           | 380         |
|    time_elapsed         | 1657        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.051613137 |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.3         |
|    entropy_loss         | -222        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.5       |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.0227     |
|    std                  | 1.1         |
|    value_loss           | 0.753       |
-----------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: 41.81 | Avg Reward: 41.59 | Max Reward: 47.01
Iteration: 384 | Episodes: 15600 | Median Reward: 43.00 | Avg Reward: 42.90 | Max Reward: 47.01
Iteration: 387 | Episodes: 15700 | Median Reward: 43.85 | Avg Reward: 42.78 | Max Reward: 47.01
Iteration: 389 | Episodes: 15800 | Median Reward: 43.41 | Avg Reward: 43.11 | Max Reward: 47.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57         |
| time/                   |             |
|    fps                  | 938         |
|    iterations           | 390         |
|    time_elapsed         | 1701        |
|    total_timesteps      | 1597440     |
| train/                  |             |
|    approx_kl            | 0.011772663 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -223        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.4       |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.0122     |
|    std                  | 1.1         |
|    value_loss           | 7.86        |
-----------------------------------------
Iteration: 392 | Episodes: 15900 | Median Reward: 42.59 | Avg Reward: 41.99 | Max Reward: 47.01
Iteration: 394 | Episodes: 16000 | Median Reward: 44.87 | Avg Reward: 43.70 | Max Reward: 47.01
Iteration: 396 | Episodes: 16100 | Median Reward: 42.83 | Avg Reward: 43.60 | Max Reward: 47.01
Iteration: 399 | Episodes: 16200 | Median Reward: 42.27 | Avg Reward: 42.04 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58          |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 400          |
|    time_elapsed         | 1745         |
|    total_timesteps      | 1638400      |
| train/                  |              |
|    approx_kl            | 0.0021985148 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -225         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -19.1        |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 1.11         |
|    value_loss           | 3.22         |
------------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: 44.97 | Avg Reward: 42.08 | Max Reward: 47.01
Iteration: 404 | Episodes: 16400 | Median Reward: 43.67 | Avg Reward: 43.69 | Max Reward: 47.01
Iteration: 406 | Episodes: 16500 | Median Reward: 42.55 | Avg Reward: 43.15 | Max Reward: 47.01
Iteration: 409 | Episodes: 16600 | Median Reward: 44.46 | Avg Reward: 43.24 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 410          |
|    time_elapsed         | 1789         |
|    total_timesteps      | 1679360      |
| train/                  |              |
|    approx_kl            | 0.0018031418 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -226         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.7        |
|    n_updates            | 4090         |
|    policy_gradient_loss | -0.00327     |
|    std                  | 1.11         |
|    value_loss           | 2.21         |
------------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: 41.47 | Avg Reward: 41.46 | Max Reward: 47.01
Iteration: 414 | Episodes: 16800 | Median Reward: 41.70 | Avg Reward: 42.23 | Max Reward: 47.01
Iteration: 416 | Episodes: 16900 | Median Reward: 43.40 | Avg Reward: 42.78 | Max Reward: 47.01
Iteration: 419 | Episodes: 17000 | Median Reward: 42.61 | Avg Reward: 43.07 | Max Reward: 47.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.5       |
| time/                   |             |
|    fps                  | 938         |
|    iterations           | 420         |
|    time_elapsed         | 1833        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.007608851 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -228        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -22.7       |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00961    |
|    std                  | 1.11        |
|    value_loss           | 1.47        |
-----------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: 42.78 | Avg Reward: 42.76 | Max Reward: 47.01
Iteration: 424 | Episodes: 17200 | Median Reward: 44.21 | Avg Reward: 43.10 | Max Reward: 47.01
Iteration: 426 | Episodes: 17300 | Median Reward: 41.72 | Avg Reward: 40.97 | Max Reward: 47.01
Iteration: 429 | Episodes: 17400 | Median Reward: 42.46 | Avg Reward: 41.59 | Max Reward: 47.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.7       |
| time/                   |             |
|    fps                  | 938         |
|    iterations           | 430         |
|    time_elapsed         | 1876        |
|    total_timesteps      | 1761280     |
| train/                  |             |
|    approx_kl            | 0.008643905 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -230        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.3       |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00135    |
|    std                  | 1.12        |
|    value_loss           | 1.57        |
-----------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: 45.08 | Avg Reward: 43.75 | Max Reward: 47.01
Iteration: 433 | Episodes: 17600 | Median Reward: 44.24 | Avg Reward: 43.03 | Max Reward: 47.01
Iteration: 436 | Episodes: 17700 | Median Reward: 44.75 | Avg Reward: 42.62 | Max Reward: 47.01
Iteration: 438 | Episodes: 17800 | Median Reward: 45.11 | Avg Reward: 43.84 | Max Reward: 47.01
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -56.8      |
| time/                   |            |
|    fps                  | 938        |
|    iterations           | 440        |
|    time_elapsed         | 1919       |
|    total_timesteps      | 1802240    |
| train/                  |            |
|    approx_kl            | 0.08562456 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.3        |
|    entropy_loss         | -231       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -22.7      |
|    n_updates            | 4390       |
|    policy_gradient_loss | -0.0565    |
|    std                  | 1.12       |
|    value_loss           | 0.7        |
----------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 44.46 | Avg Reward: 43.38 | Max Reward: 47.01
Iteration: 443 | Episodes: 18000 | Median Reward: 44.32 | Avg Reward: 43.16 | Max Reward: 47.01
Iteration: 446 | Episodes: 18100 | Median Reward: 44.61 | Avg Reward: 44.14 | Max Reward: 47.01
Iteration: 448 | Episodes: 18200 | Median Reward: 43.66 | Avg Reward: 42.87 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 450          |
|    time_elapsed         | 1963         |
|    total_timesteps      | 1843200      |
| train/                  |              |
|    approx_kl            | 0.0048767366 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -232         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -19.9        |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.0227      |
|    std                  | 1.12         |
|    value_loss           | 6.95         |
------------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 44.43 | Avg Reward: 43.03 | Max Reward: 47.01
Iteration: 453 | Episodes: 18400 | Median Reward: 41.96 | Avg Reward: 42.25 | Max Reward: 47.01
Iteration: 456 | Episodes: 18500 | Median Reward: 44.61 | Avg Reward: 43.55 | Max Reward: 47.01
Iteration: 458 | Episodes: 18600 | Median Reward: 45.02 | Avg Reward: 44.64 | Max Reward: 47.01
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.4         |
| time/                   |               |
|    fps                  | 938           |
|    iterations           | 460           |
|    time_elapsed         | 2007          |
|    total_timesteps      | 1884160       |
| train/                  |               |
|    approx_kl            | 0.00017271422 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -233          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -19.9         |
|    n_updates            | 4590          |
|    policy_gradient_loss | -0.000649     |
|    std                  | 1.12          |
|    value_loss           | 6.18          |
-------------------------------------------
Iteration: 461 | Episodes: 18700 | Median Reward: 44.91 | Avg Reward: 44.08 | Max Reward: 47.01
Iteration: 463 | Episodes: 18800 | Median Reward: 45.37 | Avg Reward: 43.97 | Max Reward: 47.01
Iteration: 466 | Episodes: 18900 | Median Reward: 42.76 | Avg Reward: 42.86 | Max Reward: 47.01
Iteration: 468 | Episodes: 19000 | Median Reward: 42.43 | Avg Reward: 42.75 | Max Reward: 47.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.3       |
| time/                   |             |
|    fps                  | 938         |
|    iterations           | 470         |
|    time_elapsed         | 2050        |
|    total_timesteps      | 1925120     |
| train/                  |             |
|    approx_kl            | 0.010492573 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -234        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.6       |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.0113     |
|    std                  | 1.13        |
|    value_loss           | 1.97        |
-----------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 42.38 | Avg Reward: 41.94 | Max Reward: 47.01
Iteration: 473 | Episodes: 19200 | Median Reward: 44.34 | Avg Reward: 43.74 | Max Reward: 47.01
Iteration: 475 | Episodes: 19300 | Median Reward: 45.09 | Avg Reward: 44.18 | Max Reward: 47.01
Iteration: 478 | Episodes: 19400 | Median Reward: 45.17 | Avg Reward: 44.73 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.5        |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 480          |
|    time_elapsed         | 2095         |
|    total_timesteps      | 1966080      |
| train/                  |              |
|    approx_kl            | 0.0021911543 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -235         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.4        |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.000932    |
|    std                  | 1.13         |
|    value_loss           | 0.566        |
------------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 45.41 | Avg Reward: 44.20 | Max Reward: 47.01
Iteration: 483 | Episodes: 19600 | Median Reward: 42.20 | Avg Reward: 42.44 | Max Reward: 47.01
Iteration: 485 | Episodes: 19700 | Median Reward: 42.58 | Avg Reward: 43.40 | Max Reward: 47.01
Iteration: 488 | Episodes: 19800 | Median Reward: 43.02 | Avg Reward: 43.41 | Max Reward: 47.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.3       |
| time/                   |             |
|    fps                  | 938         |
|    iterations           | 490         |
|    time_elapsed         | 2138        |
|    total_timesteps      | 2007040     |
| train/                  |             |
|    approx_kl            | 0.001575039 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -236        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.2       |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.00266    |
|    std                  | 1.14        |
|    value_loss           | 2.24        |
-----------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 42.43 | Avg Reward: 41.94 | Max Reward: 47.01
Iteration: 493 | Episodes: 20000 | Median Reward: 42.71 | Avg Reward: 43.42 | Max Reward: 47.01
Iteration: 495 | Episodes: 20100 | Median Reward: 45.09 | Avg Reward: 43.31 | Max Reward: 47.01
Iteration: 498 | Episodes: 20200 | Median Reward: 42.57 | Avg Reward: 42.81 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 500          |
|    time_elapsed         | 2182         |
|    total_timesteps      | 2048000      |
| train/                  |              |
|    approx_kl            | 0.0008629118 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -237         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.3        |
|    n_updates            | 4990         |
|    policy_gradient_loss | 0.00314      |
|    std                  | 1.14         |
|    value_loss           | 4.62         |
------------------------------------------
Iteration: 500 | Episodes: 20300 | Median Reward: 42.14 | Avg Reward: 42.41 | Max Reward: 47.01
Iteration: 503 | Episodes: 20400 | Median Reward: 44.55 | Avg Reward: 43.56 | Max Reward: 47.01
Iteration: 505 | Episodes: 20500 | Median Reward: 42.79 | Avg Reward: 42.39 | Max Reward: 47.01
Iteration: 507 | Episodes: 20600 | Median Reward: 44.90 | Avg Reward: 44.09 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.7        |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 510          |
|    time_elapsed         | 2226         |
|    total_timesteps      | 2088960      |
| train/                  |              |
|    approx_kl            | 0.0046959855 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -238         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.3        |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.00284     |
|    std                  | 1.14         |
|    value_loss           | 1.56         |
------------------------------------------
Iteration: 510 | Episodes: 20700 | Median Reward: 45.51 | Avg Reward: 44.25 | Max Reward: 47.01
Iteration: 512 | Episodes: 20800 | Median Reward: 44.84 | Avg Reward: 44.20 | Max Reward: 47.01
Iteration: 515 | Episodes: 20900 | Median Reward: 44.12 | Avg Reward: 43.75 | Max Reward: 47.01
Iteration: 517 | Episodes: 21000 | Median Reward: 43.02 | Avg Reward: 42.36 | Max Reward: 47.01
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 520          |
|    time_elapsed         | 2269         |
|    total_timesteps      | 2129920      |
| train/                  |              |
|    approx_kl            | 0.0048305714 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -239         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.5        |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00505     |
|    std                  | 1.15         |
|    value_loss           | 1.3          |
------------------------------------------
Iteration: 520 | Episodes: 21100 | Median Reward: 44.55 | Avg Reward: 43.24 | Max Reward: 47.61
Iteration: 522 | Episodes: 21200 | Median Reward: 44.48 | Avg Reward: 43.61 | Max Reward: 47.61
Iteration: 525 | Episodes: 21300 | Median Reward: 44.53 | Avg Reward: 42.93 | Max Reward: 47.61
Iteration: 527 | Episodes: 21400 | Median Reward: 44.49 | Avg Reward: 43.95 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 938          |
|    iterations           | 530          |
|    time_elapsed         | 2314         |
|    total_timesteps      | 2170880      |
| train/                  |              |
|    approx_kl            | 0.0007954833 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -240         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -22.7        |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.00119     |
|    std                  | 1.15         |
|    value_loss           | 0.842        |
------------------------------------------
Iteration: 530 | Episodes: 21500 | Median Reward: 44.54 | Avg Reward: 44.04 | Max Reward: 47.61
Iteration: 532 | Episodes: 21600 | Median Reward: 45.12 | Avg Reward: 44.39 | Max Reward: 47.61
Iteration: 535 | Episodes: 21700 | Median Reward: 44.29 | Avg Reward: 43.58 | Max Reward: 47.61
Iteration: 537 | Episodes: 21800 | Median Reward: 44.83 | Avg Reward: 44.40 | Max Reward: 47.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.3       |
| time/                   |             |
|    fps                  | 938         |
|    iterations           | 540         |
|    time_elapsed         | 2357        |
|    total_timesteps      | 2211840     |
| train/                  |             |
|    approx_kl            | 0.007599869 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -240        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.9       |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.0181     |
|    std                  | 1.16        |
|    value_loss           | 4           |
-----------------------------------------
Iteration: 540 | Episodes: 21900 | Median Reward: 42.50 | Avg Reward: 42.66 | Max Reward: 47.61
Iteration: 542 | Episodes: 22000 | Median Reward: 44.79 | Avg Reward: 43.85 | Max Reward: 47.61
Iteration: 544 | Episodes: 22100 | Median Reward: 44.98 | Avg Reward: 43.58 | Max Reward: 47.61
Iteration: 547 | Episodes: 22200 | Median Reward: 43.32 | Avg Reward: 43.11 | Max Reward: 47.61
Iteration: 549 | Episodes: 22300 | Median Reward: 44.79 | Avg Reward: 44.23 | Max Reward: 47.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.7         |
| time/                   |               |
|    fps                  | 937           |
|    iterations           | 550           |
|    time_elapsed         | 2402          |
|    total_timesteps      | 2252800       |
| train/                  |               |
|    approx_kl            | 0.00028827487 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -241          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -22.2         |
|    n_updates            | 5490          |
|    policy_gradient_loss | -0.000457     |
|    std                  | 1.16          |
|    value_loss           | 2.04          |
-------------------------------------------
Iteration: 552 | Episodes: 22400 | Median Reward: 42.89 | Avg Reward: 43.12 | Max Reward: 47.61
Iteration: 554 | Episodes: 22500 | Median Reward: 45.50 | Avg Reward: 44.26 | Max Reward: 47.61
Iteration: 557 | Episodes: 22600 | Median Reward: 44.73 | Avg Reward: 43.42 | Max Reward: 47.61
Iteration: 559 | Episodes: 22700 | Median Reward: 45.73 | Avg Reward: 44.97 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.8        |
| time/                   |              |
|    fps                  | 937          |
|    iterations           | 560          |
|    time_elapsed         | 2447         |
|    total_timesteps      | 2293760      |
| train/                  |              |
|    approx_kl            | 0.0038534077 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -242         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.7        |
|    n_updates            | 5590         |
|    policy_gradient_loss | -0.00315     |
|    std                  | 1.16         |
|    value_loss           | 0.497        |
------------------------------------------
Iteration: 562 | Episodes: 22800 | Median Reward: 44.98 | Avg Reward: 44.32 | Max Reward: 47.61
Iteration: 564 | Episodes: 22900 | Median Reward: 45.39 | Avg Reward: 44.33 | Max Reward: 47.61
Iteration: 567 | Episodes: 23000 | Median Reward: 42.41 | Avg Reward: 42.98 | Max Reward: 47.61
Iteration: 569 | Episodes: 23100 | Median Reward: 44.96 | Avg Reward: 44.26 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 570          |
|    time_elapsed         | 2491         |
|    total_timesteps      | 2334720      |
| train/                  |              |
|    approx_kl            | 0.0015168395 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -243         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24          |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 1.17         |
|    value_loss           | 0.575        |
------------------------------------------
Iteration: 572 | Episodes: 23200 | Median Reward: 44.98 | Avg Reward: 43.71 | Max Reward: 47.61
Iteration: 574 | Episodes: 23300 | Median Reward: 45.45 | Avg Reward: 44.20 | Max Reward: 47.61
Iteration: 577 | Episodes: 23400 | Median Reward: 45.12 | Avg Reward: 43.58 | Max Reward: 47.61
Iteration: 579 | Episodes: 23500 | Median Reward: 44.37 | Avg Reward: 43.98 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 580          |
|    time_elapsed         | 2536         |
|    total_timesteps      | 2375680      |
| train/                  |              |
|    approx_kl            | 0.0065752324 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -244         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -23.7        |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.00246     |
|    std                  | 1.18         |
|    value_loss           | 0.594        |
------------------------------------------
Iteration: 581 | Episodes: 23600 | Median Reward: 44.76 | Avg Reward: 44.31 | Max Reward: 47.61
Iteration: 584 | Episodes: 23700 | Median Reward: 45.13 | Avg Reward: 44.39 | Max Reward: 47.61
Iteration: 586 | Episodes: 23800 | Median Reward: 45.14 | Avg Reward: 44.20 | Max Reward: 47.61
Iteration: 589 | Episodes: 23900 | Median Reward: 45.04 | Avg Reward: 44.07 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.6        |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 590          |
|    time_elapsed         | 2581         |
|    total_timesteps      | 2416640      |
| train/                  |              |
|    approx_kl            | 0.0056545446 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -246         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.9        |
|    n_updates            | 5890         |
|    policy_gradient_loss | 0.00321      |
|    std                  | 1.18         |
|    value_loss           | 0.841        |
------------------------------------------
Iteration: 591 | Episodes: 24000 | Median Reward: 45.44 | Avg Reward: 45.03 | Max Reward: 47.61
Iteration: 594 | Episodes: 24100 | Median Reward: 43.63 | Avg Reward: 43.00 | Max Reward: 47.61
Iteration: 596 | Episodes: 24200 | Median Reward: 43.43 | Avg Reward: 43.66 | Max Reward: 47.61
Iteration: 599 | Episodes: 24300 | Median Reward: 44.82 | Avg Reward: 44.16 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 600          |
|    time_elapsed         | 2625         |
|    total_timesteps      | 2457600      |
| train/                  |              |
|    approx_kl            | 0.0009386927 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -247         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.4        |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 1.19         |
|    value_loss           | 1.87         |
------------------------------------------
Iteration: 601 | Episodes: 24400 | Median Reward: 44.61 | Avg Reward: 43.77 | Max Reward: 47.61
Iteration: 604 | Episodes: 24500 | Median Reward: 45.18 | Avg Reward: 45.14 | Max Reward: 47.61
Iteration: 606 | Episodes: 24600 | Median Reward: 44.92 | Avg Reward: 44.11 | Max Reward: 47.61
Iteration: 609 | Episodes: 24700 | Median Reward: 45.78 | Avg Reward: 45.12 | Max Reward: 47.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.9         |
| time/                   |               |
|    fps                  | 936           |
|    iterations           | 610           |
|    time_elapsed         | 2668          |
|    total_timesteps      | 2498560       |
| train/                  |               |
|    approx_kl            | 0.00084176875 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -247          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -17.3         |
|    n_updates            | 6090          |
|    policy_gradient_loss | -0.00139      |
|    std                  | 1.19          |
|    value_loss           | 8.52          |
-------------------------------------------
Iteration: 611 | Episodes: 24800 | Median Reward: 44.57 | Avg Reward: 43.73 | Max Reward: 47.61
Iteration: 613 | Episodes: 24900 | Median Reward: 44.78 | Avg Reward: 44.56 | Max Reward: 47.61
Iteration: 616 | Episodes: 25000 | Median Reward: 42.15 | Avg Reward: 42.92 | Max Reward: 47.61
Iteration: 618 | Episodes: 25100 | Median Reward: 44.65 | Avg Reward: 44.38 | Max Reward: 47.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.7       |
| time/                   |             |
|    fps                  | 936         |
|    iterations           | 620         |
|    time_elapsed         | 2713        |
|    total_timesteps      | 2539520     |
| train/                  |             |
|    approx_kl            | 0.001591528 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -248        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.4       |
|    n_updates            | 6190        |
|    policy_gradient_loss | -0.00147    |
|    std                  | 1.2         |
|    value_loss           | 1.59        |
-----------------------------------------
Iteration: 621 | Episodes: 25200 | Median Reward: 45.04 | Avg Reward: 44.27 | Max Reward: 47.61
Iteration: 623 | Episodes: 25300 | Median Reward: 45.44 | Avg Reward: 44.21 | Max Reward: 47.61
Iteration: 626 | Episodes: 25400 | Median Reward: 45.46 | Avg Reward: 44.46 | Max Reward: 47.61
Iteration: 628 | Episodes: 25500 | Median Reward: 42.66 | Avg Reward: 43.01 | Max Reward: 47.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.9       |
| time/                   |             |
|    fps                  | 936         |
|    iterations           | 630         |
|    time_elapsed         | 2756        |
|    total_timesteps      | 2580480     |
| train/                  |             |
|    approx_kl            | 0.012010321 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -249        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -24.6       |
|    n_updates            | 6290        |
|    policy_gradient_loss | -0.0148     |
|    std                  | 1.21        |
|    value_loss           | 0.523       |
-----------------------------------------
Iteration: 631 | Episodes: 25600 | Median Reward: 45.45 | Avg Reward: 44.21 | Max Reward: 47.61
Iteration: 633 | Episodes: 25700 | Median Reward: 45.43 | Avg Reward: 44.79 | Max Reward: 47.61
Iteration: 636 | Episodes: 25800 | Median Reward: 45.18 | Avg Reward: 44.80 | Max Reward: 47.61
Iteration: 638 | Episodes: 25900 | Median Reward: 44.96 | Avg Reward: 43.89 | Max Reward: 47.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.1       |
| time/                   |             |
|    fps                  | 936         |
|    iterations           | 640         |
|    time_elapsed         | 2800        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.000517312 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -250        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -22.4       |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.00169    |
|    std                  | 1.21        |
|    value_loss           | 5.41        |
-----------------------------------------
Iteration: 641 | Episodes: 26000 | Median Reward: 44.39 | Avg Reward: 43.87 | Max Reward: 47.61
Iteration: 643 | Episodes: 26100 | Median Reward: 45.47 | Avg Reward: 45.41 | Max Reward: 47.61
Iteration: 646 | Episodes: 26200 | Median Reward: 44.64 | Avg Reward: 44.45 | Max Reward: 47.61
Iteration: 648 | Episodes: 26300 | Median Reward: 45.44 | Avg Reward: 44.43 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.5        |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 650          |
|    time_elapsed         | 2843         |
|    total_timesteps      | 2662400      |
| train/                  |              |
|    approx_kl            | 0.0023592184 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.8        |
|    n_updates            | 6490         |
|    policy_gradient_loss | 0.00122      |
|    std                  | 1.21         |
|    value_loss           | 1.48         |
------------------------------------------
Iteration: 650 | Episodes: 26400 | Median Reward: 45.49 | Avg Reward: 45.20 | Max Reward: 47.61
Iteration: 653 | Episodes: 26500 | Median Reward: 45.15 | Avg Reward: 44.30 | Max Reward: 47.61
Iteration: 655 | Episodes: 26600 | Median Reward: 45.46 | Avg Reward: 45.00 | Max Reward: 47.61
Iteration: 658 | Episodes: 26700 | Median Reward: 43.52 | Avg Reward: 43.90 | Max Reward: 47.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.4       |
| time/                   |             |
|    fps                  | 936         |
|    iterations           | 660         |
|    time_elapsed         | 2887        |
|    total_timesteps      | 2703360     |
| train/                  |             |
|    approx_kl            | 0.000558597 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -251        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.3       |
|    n_updates            | 6590        |
|    policy_gradient_loss | -0.00106    |
|    std                  | 1.22        |
|    value_loss           | 2.19        |
-----------------------------------------
Iteration: 660 | Episodes: 26800 | Median Reward: 45.53 | Avg Reward: 45.08 | Max Reward: 47.61
Iteration: 663 | Episodes: 26900 | Median Reward: 45.46 | Avg Reward: 44.65 | Max Reward: 47.61
Iteration: 665 | Episodes: 27000 | Median Reward: 45.09 | Avg Reward: 44.24 | Max Reward: 47.61
Iteration: 668 | Episodes: 27100 | Median Reward: 44.91 | Avg Reward: 43.60 | Max Reward: 47.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.9         |
| time/                   |               |
|    fps                  | 936           |
|    iterations           | 670           |
|    time_elapsed         | 2930          |
|    total_timesteps      | 2744320       |
| train/                  |               |
|    approx_kl            | 0.00016943071 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -252          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -24.4         |
|    n_updates            | 6690          |
|    policy_gradient_loss | -2.67e-05     |
|    std                  | 1.22          |
|    value_loss           | 4.75          |
-------------------------------------------
Iteration: 670 | Episodes: 27200 | Median Reward: 45.47 | Avg Reward: 45.18 | Max Reward: 47.61
Iteration: 673 | Episodes: 27300 | Median Reward: 44.55 | Avg Reward: 43.91 | Max Reward: 47.61
Iteration: 675 | Episodes: 27400 | Median Reward: 45.35 | Avg Reward: 44.82 | Max Reward: 47.61
Iteration: 678 | Episodes: 27500 | Median Reward: 44.83 | Avg Reward: 44.73 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.2        |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 680          |
|    time_elapsed         | 2974         |
|    total_timesteps      | 2785280      |
| train/                  |              |
|    approx_kl            | 0.0041078697 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -251         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.9        |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.00391     |
|    std                  | 1.23         |
|    value_loss           | 0.956        |
------------------------------------------
Iteration: 680 | Episodes: 27600 | Median Reward: 45.47 | Avg Reward: 44.47 | Max Reward: 47.61
Iteration: 683 | Episodes: 27700 | Median Reward: 45.17 | Avg Reward: 44.69 | Max Reward: 47.61
Iteration: 685 | Episodes: 27800 | Median Reward: 44.03 | Avg Reward: 43.60 | Max Reward: 47.61
Iteration: 687 | Episodes: 27900 | Median Reward: 45.00 | Avg Reward: 44.53 | Max Reward: 47.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.4         |
| time/                   |               |
|    fps                  | 936           |
|    iterations           | 690           |
|    time_elapsed         | 3017          |
|    total_timesteps      | 2826240       |
| train/                  |               |
|    approx_kl            | 0.00066132477 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -252          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -23.7         |
|    n_updates            | 6890          |
|    policy_gradient_loss | -0.000738     |
|    std                  | 1.24          |
|    value_loss           | 1.63          |
-------------------------------------------
Iteration: 690 | Episodes: 28000 | Median Reward: 45.01 | Avg Reward: 44.36 | Max Reward: 47.61
Iteration: 692 | Episodes: 28100 | Median Reward: 44.61 | Avg Reward: 44.36 | Max Reward: 47.61
Iteration: 695 | Episodes: 28200 | Median Reward: 44.80 | Avg Reward: 43.75 | Max Reward: 47.61
Iteration: 697 | Episodes: 28300 | Median Reward: 45.40 | Avg Reward: 43.70 | Max Reward: 47.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.2       |
| time/                   |             |
|    fps                  | 936         |
|    iterations           | 700         |
|    time_elapsed         | 3061        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.020058483 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -252        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.8       |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.0102     |
|    std                  | 1.24        |
|    value_loss           | 1.7         |
-----------------------------------------
Iteration: 700 | Episodes: 28400 | Median Reward: 44.96 | Avg Reward: 44.06 | Max Reward: 47.61
Iteration: 702 | Episodes: 28500 | Median Reward: 44.63 | Avg Reward: 44.40 | Max Reward: 47.61
Iteration: 705 | Episodes: 28600 | Median Reward: 45.41 | Avg Reward: 44.35 | Max Reward: 47.61
Iteration: 707 | Episodes: 28700 | Median Reward: 45.41 | Avg Reward: 44.77 | Max Reward: 47.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.5       |
| time/                   |             |
|    fps                  | 936         |
|    iterations           | 710         |
|    time_elapsed         | 3105        |
|    total_timesteps      | 2908160     |
| train/                  |             |
|    approx_kl            | 0.004401153 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -253        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -25.2       |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.00499    |
|    std                  | 1.25        |
|    value_loss           | 0.864       |
-----------------------------------------
Iteration: 710 | Episodes: 28800 | Median Reward: 45.40 | Avg Reward: 44.35 | Max Reward: 47.61
Iteration: 712 | Episodes: 28900 | Median Reward: 45.18 | Avg Reward: 43.96 | Max Reward: 47.61
Iteration: 715 | Episodes: 29000 | Median Reward: 44.78 | Avg Reward: 43.34 | Max Reward: 47.61
Iteration: 717 | Episodes: 29100 | Median Reward: 44.61 | Avg Reward: 43.92 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.1        |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 720          |
|    time_elapsed         | 3148         |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0028237896 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -253         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.1        |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.000786    |
|    std                  | 1.25         |
|    value_loss           | 0.283        |
------------------------------------------
Iteration: 720 | Episodes: 29200 | Median Reward: 45.45 | Avg Reward: 44.70 | Max Reward: 47.61
Iteration: 722 | Episodes: 29300 | Median Reward: 44.61 | Avg Reward: 43.20 | Max Reward: 47.61
Iteration: 724 | Episodes: 29400 | Median Reward: 45.47 | Avg Reward: 44.44 | Max Reward: 47.61
Iteration: 727 | Episodes: 29500 | Median Reward: 45.46 | Avg Reward: 44.95 | Max Reward: 47.61
Iteration: 729 | Episodes: 29600 | Median Reward: 45.15 | Avg Reward: 44.70 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 730          |
|    time_elapsed         | 3192         |
|    total_timesteps      | 2990080      |
| train/                  |              |
|    approx_kl            | 0.0005649293 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -255         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.2        |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.00027     |
|    std                  | 1.26         |
|    value_loss           | 0.989        |
------------------------------------------
Iteration: 732 | Episodes: 29700 | Median Reward: 45.52 | Avg Reward: 44.97 | Max Reward: 47.61
Iteration: 734 | Episodes: 29800 | Median Reward: 45.19 | Avg Reward: 44.62 | Max Reward: 47.61
Iteration: 737 | Episodes: 29900 | Median Reward: 45.50 | Avg Reward: 44.99 | Max Reward: 47.61
Iteration: 739 | Episodes: 30000 | Median Reward: 45.69 | Avg Reward: 44.59 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 740          |
|    time_elapsed         | 3237         |
|    total_timesteps      | 3031040      |
| train/                  |              |
|    approx_kl            | 0.0047471575 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -256         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.6        |
|    n_updates            | 7390         |
|    policy_gradient_loss | -0.00827     |
|    std                  | 1.27         |
|    value_loss           | 0.748        |
------------------------------------------
Iteration: 742 | Episodes: 30100 | Median Reward: 45.19 | Avg Reward: 44.46 | Max Reward: 47.61
Iteration: 744 | Episodes: 30200 | Median Reward: 44.92 | Avg Reward: 44.52 | Max Reward: 47.61
Iteration: 747 | Episodes: 30300 | Median Reward: 45.40 | Avg Reward: 44.57 | Max Reward: 47.61
Iteration: 749 | Episodes: 30400 | Median Reward: 45.22 | Avg Reward: 45.03 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.9        |
| time/                   |              |
|    fps                  | 936          |
|    iterations           | 750          |
|    time_elapsed         | 3281         |
|    total_timesteps      | 3072000      |
| train/                  |              |
|    approx_kl            | 3.326041e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -256         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.3        |
|    n_updates            | 7490         |
|    policy_gradient_loss | -4.01e-05    |
|    std                  | 1.27         |
|    value_loss           | 1.29         |
------------------------------------------
Iteration: 752 | Episodes: 30500 | Median Reward: 45.86 | Avg Reward: 45.24 | Max Reward: 47.61
Iteration: 754 | Episodes: 30600 | Median Reward: 45.06 | Avg Reward: 44.43 | Max Reward: 47.61
Iteration: 757 | Episodes: 30700 | Median Reward: 45.70 | Avg Reward: 45.54 | Max Reward: 47.61
Iteration: 759 | Episodes: 30800 | Median Reward: 44.82 | Avg Reward: 44.11 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 935          |
|    iterations           | 760          |
|    time_elapsed         | 3325         |
|    total_timesteps      | 3112960      |
| train/                  |              |
|    approx_kl            | 0.0007051948 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -257         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.8        |
|    n_updates            | 7590         |
|    policy_gradient_loss | -0.000362    |
|    std                  | 1.28         |
|    value_loss           | 2.44         |
------------------------------------------
Iteration: 761 | Episodes: 30900 | Median Reward: 45.45 | Avg Reward: 44.74 | Max Reward: 47.61
Iteration: 764 | Episodes: 31000 | Median Reward: 44.81 | Avg Reward: 43.64 | Max Reward: 47.61
Iteration: 766 | Episodes: 31100 | Median Reward: 45.59 | Avg Reward: 45.31 | Max Reward: 47.61
Iteration: 769 | Episodes: 31200 | Median Reward: 45.51 | Avg Reward: 45.41 | Max Reward: 47.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.5         |
| time/                   |               |
|    fps                  | 935           |
|    iterations           | 770           |
|    time_elapsed         | 3369          |
|    total_timesteps      | 3153920       |
| train/                  |               |
|    approx_kl            | 3.7303485e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -258          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -23.7         |
|    n_updates            | 7690          |
|    policy_gradient_loss | -0.000104     |
|    std                  | 1.28          |
|    value_loss           | 4.08          |
-------------------------------------------
Iteration: 771 | Episodes: 31300 | Median Reward: 45.47 | Avg Reward: 45.02 | Max Reward: 47.61
Iteration: 774 | Episodes: 31400 | Median Reward: 45.53 | Avg Reward: 43.69 | Max Reward: 47.61
Iteration: 776 | Episodes: 31500 | Median Reward: 45.81 | Avg Reward: 45.07 | Max Reward: 47.61
Iteration: 779 | Episodes: 31600 | Median Reward: 45.47 | Avg Reward: 43.97 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.8        |
| time/                   |              |
|    fps                  | 935          |
|    iterations           | 780          |
|    time_elapsed         | 3413         |
|    total_timesteps      | 3194880      |
| train/                  |              |
|    approx_kl            | 0.0008325699 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -258         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.4        |
|    n_updates            | 7790         |
|    policy_gradient_loss | -0.000763    |
|    std                  | 1.28         |
|    value_loss           | 2.85         |
------------------------------------------
Iteration: 781 | Episodes: 31700 | Median Reward: 45.53 | Avg Reward: 44.66 | Max Reward: 47.61
Iteration: 784 | Episodes: 31800 | Median Reward: 45.47 | Avg Reward: 44.53 | Max Reward: 47.61
Iteration: 786 | Episodes: 31900 | Median Reward: 45.62 | Avg Reward: 44.84 | Max Reward: 47.61
Iteration: 789 | Episodes: 32000 | Median Reward: 45.44 | Avg Reward: 44.71 | Max Reward: 47.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.9       |
| time/                   |             |
|    fps                  | 935         |
|    iterations           | 790         |
|    time_elapsed         | 3458        |
|    total_timesteps      | 3235840     |
| train/                  |             |
|    approx_kl            | 0.030501254 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.3         |
|    entropy_loss         | -259        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -25.9       |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.0117     |
|    std                  | 1.29        |
|    value_loss           | 0.7         |
-----------------------------------------
Iteration: 791 | Episodes: 32100 | Median Reward: 45.86 | Avg Reward: 44.26 | Max Reward: 47.61
Iteration: 793 | Episodes: 32200 | Median Reward: 44.85 | Avg Reward: 44.17 | Max Reward: 47.61
Iteration: 796 | Episodes: 32300 | Median Reward: 44.94 | Avg Reward: 44.56 | Max Reward: 47.61
Iteration: 798 | Episodes: 32400 | Median Reward: 43.88 | Avg Reward: 43.57 | Max Reward: 47.61
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -56.5      |
| time/                   |            |
|    fps                  | 935        |
|    iterations           | 800        |
|    time_elapsed         | 3502       |
|    total_timesteps      | 3276800    |
| train/                  |            |
|    approx_kl            | 0.04491988 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.3        |
|    entropy_loss         | -261       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -25.2      |
|    n_updates            | 7990       |
|    policy_gradient_loss | -0.0354    |
|    std                  | 1.29       |
|    value_loss           | 0.706      |
----------------------------------------
Iteration: 801 | Episodes: 32500 | Median Reward: 45.50 | Avg Reward: 43.65 | Max Reward: 47.61
Iteration: 803 | Episodes: 32600 | Median Reward: 44.85 | Avg Reward: 44.25 | Max Reward: 47.61
Iteration: 806 | Episodes: 32700 | Median Reward: 45.03 | Avg Reward: 44.96 | Max Reward: 47.61
Iteration: 808 | Episodes: 32800 | Median Reward: 45.44 | Avg Reward: 44.58 | Max Reward: 47.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.2        |
| time/                   |              |
|    fps                  | 935          |
|    iterations           | 810          |
|    time_elapsed         | 3546         |
|    total_timesteps      | 3317760      |
| train/                  |              |
|    approx_kl            | 0.0013155803 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -261         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -24.2        |
|    n_updates            | 8090         |
|    policy_gradient_loss | -0.00238     |
|    std                  | 1.3          |
|    value_loss           | 2.91         |
------------------------------------------
Iteration: 811 | Episodes: 32900 | Median Reward: 45.50 | Avg Reward: 44.53 | Max Reward: 47.61
Iteration: 813 | Episodes: 33000 | Median Reward: 45.49 | Avg Reward: 45.33 | Max Reward: 47.61
Iteration: 816 | Episodes: 33100 | Median Reward: 45.44 | Avg Reward: 44.96 | Max Reward: 47.61
Iteration: 818 | Episodes: 33200 | Median Reward: 45.70 | Avg Reward: 45.53 | Max Reward: 47.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.4         |
| time/                   |               |
|    fps                  | 935           |
|    iterations           | 820           |
|    time_elapsed         | 3588          |
|    total_timesteps      | 3358720       |
| train/                  |               |
|    approx_kl            | 8.9280686e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -262          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -23.6         |
|    n_updates            | 8190          |
|    policy_gradient_loss | -0.000138     |
|    std                  | 1.3           |
|    value_loss           | 3.63          |
-------------------------------------------
Iteration: 821 | Episodes: 33300 | Median Reward: 45.18 | Avg Reward: 44.35 | Max Reward: 47.61
Iteration: 823 | Episodes: 33400 | Median Reward: 45.45 | Avg Reward: 45.20 | Max Reward: 47.89
Iteration: 826 | Episodes: 33500 | Median Reward: 45.40 | Avg Reward: 43.52 | Max Reward: 47.89
Iteration: 828 | Episodes: 33600 | Median Reward: 45.46 | Avg Reward: 44.98 | Max Reward: 47.89
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.1         |
| time/                   |               |
|    fps                  | 935           |
|    iterations           | 830           |
|    time_elapsed         | 3632          |
|    total_timesteps      | 3399680       |
| train/                  |               |
|    approx_kl            | 0.00034311847 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -263          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -24.9         |
|    n_updates            | 8290          |
|    policy_gradient_loss | 0.000287      |
|    std                  | 1.3           |
|    value_loss           | 1.96          |
-------------------------------------------
Iteration: 830 | Episodes: 33700 | Median Reward: 44.86 | Avg Reward: 44.91 | Max Reward: 47.89
Iteration: 833 | Episodes: 33800 | Median Reward: 45.16 | Avg Reward: 43.96 | Max Reward: 47.89
Iteration: 835 | Episodes: 33900 | Median Reward: 44.79 | Avg Reward: 44.37 | Max Reward: 47.89
Iteration: 838 | Episodes: 34000 | Median Reward: 45.02 | Avg Reward: 44.51 | Max Reward: 47.89
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.4         |
| time/                   |               |
|    fps                  | 935           |
|    iterations           | 840           |
|    time_elapsed         | 3676          |
|    total_timesteps      | 3440640       |
| train/                  |               |
|    approx_kl            | 0.00019857734 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -263          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -26           |
|    n_updates            | 8390          |
|    policy_gradient_loss | -0.000129     |
|    std                  | 1.31          |
|    value_loss           | 2.18          |
-------------------------------------------
Iteration: 840 | Episodes: 34100 | Median Reward: 45.03 | Avg Reward: 44.78 | Max Reward: 47.89
Iteration: 843 | Episodes: 34200 | Median Reward: 45.42 | Avg Reward: 43.70 | Max Reward: 47.89
Iteration: 845 | Episodes: 34300 | Median Reward: 45.49 | Avg Reward: 44.70 | Max Reward: 47.89
Iteration: 848 | Episodes: 34400 | Median Reward: 45.74 | Avg Reward: 44.97 | Max Reward: 47.89
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.9         |
| time/                   |               |
|    fps                  | 935           |
|    iterations           | 850           |
|    time_elapsed         | 3720          |
|    total_timesteps      | 3481600       |
| train/                  |               |
|    approx_kl            | 0.00043876242 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -264          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -23.7         |
|    n_updates            | 8490          |
|    policy_gradient_loss | -0.00228      |
|    std                  | 1.31          |
|    value_loss           | 1.99          |
-------------------------------------------
Iteration: 850 | Episodes: 34500 | Median Reward: 45.83 | Avg Reward: 44.62 | Max Reward: 47.89
Iteration: 853 | Episodes: 34600 | Median Reward: 45.50 | Avg Reward: 44.99 | Max Reward: 47.89
Iteration: 855 | Episodes: 34700 | Median Reward: 44.98 | Avg Reward: 44.17 | Max Reward: 47.89
Iteration: 858 | Episodes: 34800 | Median Reward: 45.45 | Avg Reward: 44.91 | Max Reward: 47.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55          |
| time/                   |              |
|    fps                  | 935          |
|    iterations           | 860          |
|    time_elapsed         | 3764         |
|    total_timesteps      | 3522560      |
| train/                  |              |
|    approx_kl            | 0.0011236214 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -265         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.3        |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.00162     |
|    std                  | 1.32         |
|    value_loss           | 0.871        |
------------------------------------------
Iteration: 860 | Episodes: 34900 | Median Reward: 45.22 | Avg Reward: 44.98 | Max Reward: 47.89
Iteration: 863 | Episodes: 35000 | Median Reward: 45.50 | Avg Reward: 45.16 | Max Reward: 47.89
Iteration: 865 | Episodes: 35100 | Median Reward: 44.75 | Avg Reward: 44.25 | Max Reward: 47.89
Iteration: 867 | Episodes: 35200 | Median Reward: 45.50 | Avg Reward: 45.05 | Max Reward: 47.89
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55           |
| time/                   |               |
|    fps                  | 935           |
|    iterations           | 870           |
|    time_elapsed         | 3809          |
|    total_timesteps      | 3563520       |
| train/                  |               |
|    approx_kl            | 0.00029934128 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -265          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -26           |
|    n_updates            | 8690          |
|    policy_gradient_loss | 0.000394      |
|    std                  | 1.32          |
|    value_loss           | 0.938         |
-------------------------------------------
Iteration: 870 | Episodes: 35300 | Median Reward: 45.50 | Avg Reward: 44.95 | Max Reward: 47.89
Iteration: 872 | Episodes: 35400 | Median Reward: 44.92 | Avg Reward: 44.26 | Max Reward: 47.89
Iteration: 875 | Episodes: 35500 | Median Reward: 45.51 | Avg Reward: 45.12 | Max Reward: 47.89
Iteration: 877 | Episodes: 35600 | Median Reward: 45.89 | Avg Reward: 45.55 | Max Reward: 47.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.6       |
| time/                   |             |
|    fps                  | 935         |
|    iterations           | 880         |
|    time_elapsed         | 3854        |
|    total_timesteps      | 3604480     |
| train/                  |             |
|    approx_kl            | 0.012257991 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -266        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -26.6       |
|    n_updates            | 8790        |
|    policy_gradient_loss | -0.0176     |
|    std                  | 1.33        |
|    value_loss           | 0.346       |
-----------------------------------------
Iteration: 880 | Episodes: 35700 | Median Reward: 45.85 | Avg Reward: 45.37 | Max Reward: 47.89
Iteration: 882 | Episodes: 35800 | Median Reward: 45.49 | Avg Reward: 45.40 | Max Reward: 47.89
Iteration: 885 | Episodes: 35900 | Median Reward: 45.49 | Avg Reward: 44.81 | Max Reward: 47.89
Iteration: 887 | Episodes: 36000 | Median Reward: 45.80 | Avg Reward: 45.44 | Max Reward: 47.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55          |
| time/                   |              |
|    fps                  | 935          |
|    iterations           | 890          |
|    time_elapsed         | 3898         |
|    total_timesteps      | 3645440      |
| train/                  |              |
|    approx_kl            | 0.0008334447 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -266         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.8        |
|    n_updates            | 8890         |
|    policy_gradient_loss | -0.00128     |
|    std                  | 1.33         |
|    value_loss           | 0.72         |
------------------------------------------
Iteration: 890 | Episodes: 36100 | Median Reward: 45.63 | Avg Reward: 44.98 | Max Reward: 47.89
Iteration: 892 | Episodes: 36200 | Median Reward: 44.22 | Avg Reward: 44.24 | Max Reward: 47.89
Iteration: 895 | Episodes: 36300 | Median Reward: 45.45 | Avg Reward: 45.00 | Max Reward: 47.89
Iteration: 897 | Episodes: 36400 | Median Reward: 45.62 | Avg Reward: 45.51 | Max Reward: 47.89
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.4         |
| time/                   |               |
|    fps                  | 934           |
|    iterations           | 900           |
|    time_elapsed         | 3943          |
|    total_timesteps      | 3686400       |
| train/                  |               |
|    approx_kl            | 0.00019771971 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -267          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -25.8         |
|    n_updates            | 8990          |
|    policy_gradient_loss | -0.000959     |
|    std                  | 1.34          |
|    value_loss           | 1.11          |
-------------------------------------------
Iteration: 900 | Episodes: 36500 | Median Reward: 45.15 | Avg Reward: 44.48 | Max Reward: 47.89
Iteration: 902 | Episodes: 36600 | Median Reward: 44.86 | Avg Reward: 44.49 | Max Reward: 47.89
Iteration: 904 | Episodes: 36700 | Median Reward: 45.87 | Avg Reward: 44.71 | Max Reward: 47.89
Iteration: 907 | Episodes: 36800 | Median Reward: 45.86 | Avg Reward: 45.06 | Max Reward: 47.89
Iteration: 909 | Episodes: 36900 | Median Reward: 45.50 | Avg Reward: 44.60 | Max Reward: 47.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.4       |
| time/                   |             |
|    fps                  | 934         |
|    iterations           | 910         |
|    time_elapsed         | 3986        |
|    total_timesteps      | 3727360     |
| train/                  |             |
|    approx_kl            | 0.017671824 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -267        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -26.6       |
|    n_updates            | 9090        |
|    policy_gradient_loss | -0.0256     |
|    std                  | 1.34        |
|    value_loss           | 0.34        |
-----------------------------------------
Iteration: 912 | Episodes: 37000 | Median Reward: 45.80 | Avg Reward: 45.21 | Max Reward: 47.89
Iteration: 914 | Episodes: 37100 | Median Reward: 44.78 | Avg Reward: 44.44 | Max Reward: 47.89
Iteration: 917 | Episodes: 37200 | Median Reward: 45.45 | Avg Reward: 44.34 | Max Reward: 47.89
Iteration: 919 | Episodes: 37300 | Median Reward: 45.46 | Avg Reward: 44.42 | Max Reward: 47.89
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -55.5      |
| time/                   |            |
|    fps                  | 934        |
|    iterations           | 920        |
|    time_elapsed         | 4031       |
|    total_timesteps      | 3768320    |
| train/                  |            |
|    approx_kl            | 0.00401627 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -268       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -26.6      |
|    n_updates            | 9190       |
|    policy_gradient_loss | -0.00262   |
|    std                  | 1.35       |
|    value_loss           | 0.264      |
----------------------------------------
Iteration: 922 | Episodes: 37400 | Median Reward: 45.49 | Avg Reward: 44.38 | Max Reward: 47.89
Iteration: 924 | Episodes: 37500 | Median Reward: 45.67 | Avg Reward: 44.81 | Max Reward: 47.89
Iteration: 927 | Episodes: 37600 | Median Reward: 44.85 | Avg Reward: 44.38 | Max Reward: 47.89
Iteration: 929 | Episodes: 37700 | Median Reward: 45.40 | Avg Reward: 45.02 | Max Reward: 47.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.1        |
| time/                   |              |
|    fps                  | 934          |
|    iterations           | 930          |
|    time_elapsed         | 4076         |
|    total_timesteps      | 3809280      |
| train/                  |              |
|    approx_kl            | 7.618734e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -268         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26.7        |
|    n_updates            | 9290         |
|    policy_gradient_loss | -0.000199    |
|    std                  | 1.35         |
|    value_loss           | 0.45         |
------------------------------------------
Iteration: 932 | Episodes: 37800 | Median Reward: 45.40 | Avg Reward: 44.23 | Max Reward: 47.89
Iteration: 934 | Episodes: 37900 | Median Reward: 44.96 | Avg Reward: 44.24 | Max Reward: 47.89
Iteration: 937 | Episodes: 38000 | Median Reward: 45.53 | Avg Reward: 44.58 | Max Reward: 47.89
Iteration: 939 | Episodes: 38100 | Median Reward: 45.20 | Avg Reward: 44.15 | Max Reward: 47.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 934         |
|    iterations           | 940         |
|    time_elapsed         | 4121        |
|    total_timesteps      | 3850240     |
| train/                  |             |
|    approx_kl            | 0.005790691 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -269        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -25.5       |
|    n_updates            | 9390        |
|    policy_gradient_loss | -0.0256     |
|    std                  | 1.35        |
|    value_loss           | 5.22        |
-----------------------------------------
Iteration: 941 | Episodes: 38200 | Median Reward: 45.43 | Avg Reward: 44.91 | Max Reward: 47.89
Iteration: 944 | Episodes: 38300 | Median Reward: 45.48 | Avg Reward: 44.55 | Max Reward: 47.89
Iteration: 946 | Episodes: 38400 | Median Reward: 45.51 | Avg Reward: 44.48 | Max Reward: 47.89
Iteration: 949 | Episodes: 38500 | Median Reward: 45.45 | Avg Reward: 44.51 | Max Reward: 47.89
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.5         |
| time/                   |               |
|    fps                  | 934           |
|    iterations           | 950           |
|    time_elapsed         | 4165          |
|    total_timesteps      | 3891200       |
| train/                  |               |
|    approx_kl            | 0.00012576084 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -270          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -26.6         |
|    n_updates            | 9490          |
|    policy_gradient_loss | -0.000106     |
|    std                  | 1.36          |
|    value_loss           | 0.705         |
-------------------------------------------
Iteration: 951 | Episodes: 38600 | Median Reward: 45.15 | Avg Reward: 44.43 | Max Reward: 47.89
Iteration: 954 | Episodes: 38700 | Median Reward: 45.47 | Avg Reward: 44.85 | Max Reward: 47.89
Iteration: 956 | Episodes: 38800 | Median Reward: 45.46 | Avg Reward: 44.99 | Max Reward: 47.89
Iteration: 959 | Episodes: 38900 | Median Reward: 45.06 | Avg Reward: 43.55 | Max Reward: 47.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.3        |
| time/                   |              |
|    fps                  | 934          |
|    iterations           | 960          |
|    time_elapsed         | 4209         |
|    total_timesteps      | 3932160      |
| train/                  |              |
|    approx_kl            | 0.0025074822 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -270         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.8        |
|    n_updates            | 9590         |
|    policy_gradient_loss | 0.000543     |
|    std                  | 1.36         |
|    value_loss           | 0.841        |
------------------------------------------
Iteration: 961 | Episodes: 39000 | Median Reward: 45.45 | Avg Reward: 44.74 | Max Reward: 47.89
Iteration: 964 | Episodes: 39100 | Median Reward: 44.54 | Avg Reward: 44.51 | Max Reward: 47.89
Iteration: 966 | Episodes: 39200 | Median Reward: 45.45 | Avg Reward: 45.12 | Max Reward: 47.89
Iteration: 969 | Episodes: 39300 | Median Reward: 45.51 | Avg Reward: 45.56 | Max Reward: 47.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.4        |
| time/                   |              |
|    fps                  | 934          |
|    iterations           | 970          |
|    time_elapsed         | 4253         |
|    total_timesteps      | 3973120      |
| train/                  |              |
|    approx_kl            | 5.177458e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -271         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27          |
|    n_updates            | 9690         |
|    policy_gradient_loss | -0.000355    |
|    std                  | 1.37         |
|    value_loss           | 0.643        |
------------------------------------------
Iteration: 971 | Episodes: 39400 | Median Reward: 45.51 | Avg Reward: 45.27 | Max Reward: 47.89
Iteration: 973 | Episodes: 39500 | Median Reward: 45.09 | Avg Reward: 44.16 | Max Reward: 47.89
Iteration: 976 | Episodes: 39600 | Median Reward: 45.87 | Avg Reward: 45.75 | Max Reward: 47.89
Iteration: 978 | Episodes: 39700 | Median Reward: 45.80 | Avg Reward: 44.98 | Max Reward: 47.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.9       |
| time/                   |             |
|    fps                  | 934         |
|    iterations           | 980         |
|    time_elapsed         | 4297        |
|    total_timesteps      | 4014080     |
| train/                  |             |
|    approx_kl            | 0.000907957 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -271        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -26.6       |
|    n_updates            | 9790        |
|    policy_gradient_loss | -0.000281   |
|    std                  | 1.38        |
|    value_loss           | 0.905       |
-----------------------------------------
Iteration: 981 | Episodes: 39800 | Median Reward: 45.43 | Avg Reward: 44.94 | Max Reward: 47.89
Iteration: 983 | Episodes: 39900 | Median Reward: 45.43 | Avg Reward: 44.63 | Max Reward: 47.89
Iteration: 986 | Episodes: 40000 | Median Reward: 45.46 | Avg Reward: 45.32 | Max Reward: 47.89
Iteration: 988 | Episodes: 40100 | Median Reward: 45.69 | Avg Reward: 44.86 | Max Reward: 47.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.2        |
| time/                   |              |
|    fps                  | 934          |
|    iterations           | 990          |
|    time_elapsed         | 4341         |
|    total_timesteps      | 4055040      |
| train/                  |              |
|    approx_kl            | 0.0006512403 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -272         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.1        |
|    n_updates            | 9890         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 1.38         |
|    value_loss           | 0.971        |
------------------------------------------
Iteration: 991 | Episodes: 40200 | Median Reward: 45.65 | Avg Reward: 44.89 | Max Reward: 47.89
Iteration: 993 | Episodes: 40300 | Median Reward: 45.51 | Avg Reward: 44.58 | Max Reward: 47.89
Iteration: 996 | Episodes: 40400 | Median Reward: 45.49 | Avg Reward: 44.82 | Max Reward: 47.89
Iteration: 998 | Episodes: 40500 | Median Reward: 44.41 | Avg Reward: 43.95 | Max Reward: 47.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.7        |
| time/                   |              |
|    fps                  | 933          |
|    iterations           | 1000         |
|    time_elapsed         | 4385         |
|    total_timesteps      | 4096000      |
| train/                  |              |
|    approx_kl            | 0.0059255166 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -273         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26.5        |
|    n_updates            | 9990         |
|    policy_gradient_loss | -0.00788     |
|    std                  | 1.39         |
|    value_loss           | 0.483        |
------------------------------------------
Training End | Episodes: 40552 | Median Reward: 44.85 | Avg Reward: 44.50 | Max Reward: 47.89
Plot saved as fig_code1_1_residual_d.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> exit
exit

Script done on 2024-10-24 20:58:34-04:00 [COMMAND_EXIT_CODE="0"]
