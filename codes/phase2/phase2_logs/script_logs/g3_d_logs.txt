Script started on 2024-10-16 02:50:00-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/21" COLUMNS="120" LINES="30"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> pytho[K[K[K[K[Kconda activate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python cg[K[Kenv_setup_g3_d.py
GPU 6: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
Iteration: 1 | Episodes: 100 | Median Reward: 28.04 | Max Reward: 40.29
Iteration: 3 | Episodes: 200 | Median Reward: 23.52 | Max Reward: 40.29
Iteration: 4 | Episodes: 300 | Median Reward: 22.90 | Max Reward: 40.29
Iteration: 6 | Episodes: 400 | Median Reward: 21.58 | Max Reward: 40.29
Iteration: 8 | Episodes: 500 | Median Reward: 20.48 | Max Reward: 40.29
Iteration: 9 | Episodes: 600 | Median Reward: 24.46 | Max Reward: 40.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -75.2     |
| time/                   |           |
|    fps                  | 371       |
|    iterations           | 10        |
|    time_elapsed         | 165       |
|    total_timesteps      | 61440     |
| train/                  |           |
|    approx_kl            | 0.0642861 |
|    clip_fraction        | 0.0625    |
|    clip_range           | 0.4       |
|    entropy_loss         | -42.4     |
|    explained_variance   | -0.0384   |
|    learning_rate        | 0.0001    |
|    loss                 | 95.4      |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.0168   |
|    std                  | 1         |
|    value_loss           | 193       |
---------------------------------------
Iteration: 11 | Episodes: 700 | Median Reward: 20.69 | Max Reward: 40.29
Iteration: 13 | Episodes: 800 | Median Reward: 21.27 | Max Reward: 40.29
Iteration: 14 | Episodes: 900 | Median Reward: 22.96 | Max Reward: 42.71
Iteration: 16 | Episodes: 1000 | Median Reward: 19.29 | Max Reward: 42.71
Iteration: 18 | Episodes: 1100 | Median Reward: 17.22 | Max Reward: 42.71
Iteration: 19 | Episodes: 1200 | Median Reward: 16.63 | Max Reward: 42.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.4       |
| time/                   |             |
|    fps                  | 357         |
|    iterations           | 20          |
|    time_elapsed         | 343         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.044920105 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.4         |
|    entropy_loss         | -52.3       |
|    explained_variance   | -0.0219     |
|    learning_rate        | 0.0001      |
|    loss                 | 104         |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00046    |
|    std                  | 1.01        |
|    value_loss           | 189         |
-----------------------------------------
Iteration: 21 | Episodes: 1300 | Median Reward: 15.69 | Max Reward: 42.71
Iteration: 23 | Episodes: 1400 | Median Reward: 16.40 | Max Reward: 42.71
Iteration: 24 | Episodes: 1500 | Median Reward: 17.28 | Max Reward: 42.71
Iteration: 26 | Episodes: 1600 | Median Reward: 16.58 | Max Reward: 42.71
Iteration: 28 | Episodes: 1700 | Median Reward: 16.32 | Max Reward: 42.71
Iteration: 29 | Episodes: 1800 | Median Reward: 25.90 | Max Reward: 42.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76.7       |
| time/                   |             |
|    fps                  | 353         |
|    iterations           | 30          |
|    time_elapsed         | 522         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.032904096 |
|    clip_fraction        | 0.0408      |
|    clip_range           | 0.4         |
|    entropy_loss         | -62.1       |
|    explained_variance   | -0.0116     |
|    learning_rate        | 0.0001      |
|    loss                 | 96.1        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00569    |
|    std                  | 1.01        |
|    value_loss           | 216         |
-----------------------------------------
Iteration: 31 | Episodes: 1900 | Median Reward: 17.07 | Max Reward: 42.89
Iteration: 32 | Episodes: 2000 | Median Reward: 21.67 | Max Reward: 42.89
Iteration: 34 | Episodes: 2100 | Median Reward: 19.42 | Max Reward: 42.89
Iteration: 36 | Episodes: 2200 | Median Reward: 19.10 | Max Reward: 42.89
Iteration: 37 | Episodes: 2300 | Median Reward: 14.88 | Max Reward: 42.89
Iteration: 39 | Episodes: 2400 | Median Reward: 11.94 | Max Reward: 42.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -85.3       |
| time/                   |             |
|    fps                  | 350         |
|    iterations           | 40          |
|    time_elapsed         | 701         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.006504873 |
|    clip_fraction        | 0.00305     |
|    clip_range           | 0.4         |
|    entropy_loss         | -62.5       |
|    explained_variance   | -0.0132     |
|    learning_rate        | 0.0001      |
|    loss                 | 80.1        |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.00224     |
|    std                  | 1.01        |
|    value_loss           | 158         |
-----------------------------------------
Iteration: 41 | Episodes: 2500 | Median Reward: 11.84 | Max Reward: 42.89
Iteration: 42 | Episodes: 2600 | Median Reward: 22.15 | Max Reward: 42.89
Iteration: 44 | Episodes: 2700 | Median Reward: 14.46 | Max Reward: 42.89
Iteration: 46 | Episodes: 2800 | Median Reward: 24.55 | Max Reward: 42.89
Iteration: 47 | Episodes: 2900 | Median Reward: 24.38 | Max Reward: 42.89
Iteration: 49 | Episodes: 3000 | Median Reward: 15.03 | Max Reward: 42.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -82.1       |
| time/                   |             |
|    fps                  | 346         |
|    iterations           | 50          |
|    time_elapsed         | 887         |
|    total_timesteps      | 307200      |
| train/                  |             |
|    approx_kl            | 0.024273004 |
|    clip_fraction        | 0.0463      |
|    clip_range           | 0.4         |
|    entropy_loss         | -71         |
|    explained_variance   | -0.00709    |
|    learning_rate        | 0.0001      |
|    loss                 | 72.2        |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.0105     |
|    std                  | 1.02        |
|    value_loss           | 166         |
-----------------------------------------
Iteration: 50 | Episodes: 3100 | Median Reward: 15.71 | Max Reward: 42.89
Iteration: 52 | Episodes: 3200 | Median Reward: 13.13 | Max Reward: 42.89
Iteration: 54 | Episodes: 3300 | Median Reward: 19.15 | Max Reward: 42.89
Iteration: 55 | Episodes: 3400 | Median Reward: 19.15 | Max Reward: 42.89
Iteration: 57 | Episodes: 3500 | Median Reward: 23.96 | Max Reward: 42.89
Iteration: 59 | Episodes: 3600 | Median Reward: 21.55 | Max Reward: 42.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -81.4        |
| time/                   |              |
|    fps                  | 345          |
|    iterations           | 60           |
|    time_elapsed         | 1068         |
|    total_timesteps      | 368640       |
| train/                  |              |
|    approx_kl            | 0.0056876186 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -74.8        |
|    explained_variance   | -0.00382     |
|    learning_rate        | 0.0001       |
|    loss                 | 86           |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.0056      |
|    std                  | 1.02         |
|    value_loss           | 175          |
------------------------------------------
Iteration: 60 | Episodes: 3700 | Median Reward: 22.34 | Max Reward: 42.89
Iteration: 62 | Episodes: 3800 | Median Reward: 11.32 | Max Reward: 42.89
Iteration: 64 | Episodes: 3900 | Median Reward: 20.63 | Max Reward: 42.89
Iteration: 65 | Episodes: 4000 | Median Reward: 22.55 | Max Reward: 42.89
Iteration: 67 | Episodes: 4100 | Median Reward: 19.70 | Max Reward: 42.89
Iteration: 69 | Episodes: 4200 | Median Reward: 22.45 | Max Reward: 42.89
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -75.1       |
| time/                   |             |
|    fps                  | 345         |
|    iterations           | 70          |
|    time_elapsed         | 1245        |
|    total_timesteps      | 430080      |
| train/                  |             |
|    approx_kl            | 0.019413212 |
|    clip_fraction        | 0.00833     |
|    clip_range           | 0.4         |
|    entropy_loss         | -77.3       |
|    explained_variance   | -0.00221    |
|    learning_rate        | 0.0001      |
|    loss                 | 84.2        |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00389    |
|    std                  | 1.03        |
|    value_loss           | 203         |
-----------------------------------------
Iteration: 70 | Episodes: 4300 | Median Reward: 25.04 | Max Reward: 42.89
Iteration: 72 | Episodes: 4400 | Median Reward: 18.43 | Max Reward: 42.89
Iteration: 73 | Episodes: 4500 | Median Reward: 17.72 | Max Reward: 42.89
Iteration: 75 | Episodes: 4600 | Median Reward: 24.73 | Max Reward: 42.89
Iteration: 77 | Episodes: 4700 | Median Reward: 24.31 | Max Reward: 42.89
Iteration: 78 | Episodes: 4800 | Median Reward: 18.67 | Max Reward: 42.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.4        |
| time/                   |              |
|    fps                  | 342          |
|    iterations           | 80           |
|    time_elapsed         | 1435         |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0041911975 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -79.2        |
|    explained_variance   | -0.00159     |
|    learning_rate        | 0.0001       |
|    loss                 | 71.7         |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00347     |
|    std                  | 1.03         |
|    value_loss           | 175          |
------------------------------------------
Iteration: 80 | Episodes: 4900 | Median Reward: 21.89 | Max Reward: 42.89
Iteration: 82 | Episodes: 5000 | Median Reward: 17.67 | Max Reward: 42.89
Iteration: 83 | Episodes: 5100 | Median Reward: 19.71 | Max Reward: 42.89
Iteration: 85 | Episodes: 5200 | Median Reward: 22.01 | Max Reward: 42.89
Iteration: 87 | Episodes: 5300 | Median Reward: 19.77 | Max Reward: 42.89
Iteration: 88 | Episodes: 5400 | Median Reward: 22.17 | Max Reward: 42.89
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.6        |
| time/                   |              |
|    fps                  | 342          |
|    iterations           | 90           |
|    time_elapsed         | 1615         |
|    total_timesteps      | 552960       |
| train/                  |              |
|    approx_kl            | 0.0025791689 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -81.4        |
|    explained_variance   | -0.00106     |
|    learning_rate        | 0.0001       |
|    loss                 | 87           |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.00264     |
|    std                  | 1.04         |
|    value_loss           | 177          |
------------------------------------------
Iteration: 90 | Episodes: 5500 | Median Reward: 23.76 | Max Reward: 42.89
Iteration: 92 | Episodes: 5600 | Median Reward: 25.68 | Max Reward: 42.89
Iteration: 93 | Episodes: 5700 | Median Reward: 19.79 | Max Reward: 42.89
Iteration: 95 | Episodes: 5800 | Median Reward: 20.33 | Max Reward: 44.31
Iteration: 97 | Episodes: 5900 | Median Reward: 23.23 | Max Reward: 44.31
Iteration: 98 | Episodes: 6000 | Median Reward: 24.05 | Max Reward: 44.31
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.9        |
| time/                   |              |
|    fps                  | 342          |
|    iterations           | 100          |
|    time_elapsed         | 1794         |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0008150837 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -82.3        |
|    explained_variance   | -0.000724    |
|    learning_rate        | 0.0001       |
|    loss                 | 77.6         |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00165     |
|    std                  | 1.05         |
|    value_loss           | 181          |
------------------------------------------
Iteration: 100 | Episodes: 6100 | Median Reward: 20.23 | Max Reward: 44.31
Iteration: 101 | Episodes: 6200 | Median Reward: 14.90 | Max Reward: 44.31
Iteration: 103 | Episodes: 6300 | Median Reward: 22.58 | Max Reward: 44.31
Iteration: 105 | Episodes: 6400 | Median Reward: 27.79 | Max Reward: 44.31
Iteration: 106 | Episodes: 6500 | Median Reward: 31.54 | Max Reward: 44.31
Iteration: 108 | Episodes: 6600 | Median Reward: 25.20 | Max Reward: 44.31
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -76.8      |
| time/                   |            |
|    fps                  | 340        |
|    iterations           | 110        |
|    time_elapsed         | 1983       |
|    total_timesteps      | 675840     |
| train/                  |            |
|    approx_kl            | 0.00183245 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -82.5      |
|    explained_variance   | -0.000434  |
|    learning_rate        | 0.0001     |
|    loss                 | 106        |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.000984  |
|    std                  | 1.06       |
|    value_loss           | 225        |
----------------------------------------
Iteration: 110 | Episodes: 6700 | Median Reward: 24.56 | Max Reward: 44.31
Iteration: 111 | Episodes: 6800 | Median Reward: 26.42 | Max Reward: 44.31
Iteration: 113 | Episodes: 6900 | Median Reward: 28.57 | Max Reward: 44.31
Iteration: 115 | Episodes: 7000 | Median Reward: 28.14 | Max Reward: 44.31
Iteration: 116 | Episodes: 7100 | Median Reward: 24.72 | Max Reward: 44.31
Iteration: 118 | Episodes: 7200 | Median Reward: 23.92 | Max Reward: 44.31
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.6       |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 120         |
|    time_elapsed         | 2162        |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.016740147 |
|    clip_fraction        | 0.00942     |
|    clip_range           | 0.4         |
|    entropy_loss         | -83         |
|    explained_variance   | -0.000319   |
|    learning_rate        | 0.0001      |
|    loss                 | 97.1        |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00189    |
|    std                  | 1.07        |
|    value_loss           | 194         |
-----------------------------------------
Iteration: 120 | Episodes: 7300 | Median Reward: 19.30 | Max Reward: 44.31
Iteration: 121 | Episodes: 7400 | Median Reward: 31.37 | Max Reward: 44.31
Iteration: 123 | Episodes: 7500 | Median Reward: 27.68 | Max Reward: 44.31
Iteration: 124 | Episodes: 7600 | Median Reward: 23.71 | Max Reward: 44.31
Iteration: 126 | Episodes: 7700 | Median Reward: 27.62 | Max Reward: 44.31
Iteration: 128 | Episodes: 7800 | Median Reward: 25.31 | Max Reward: 44.31
Iteration: 129 | Episodes: 7900 | Median Reward: 23.12 | Max Reward: 44.31
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -73.6        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 130          |
|    time_elapsed         | 2343         |
|    total_timesteps      | 798720       |
| train/                  |              |
|    approx_kl            | 0.0018290244 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -83.8        |
|    explained_variance   | -0.000203    |
|    learning_rate        | 0.0001       |
|    loss                 | 100          |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 1.08         |
|    value_loss           | 216          |
------------------------------------------
Iteration: 131 | Episodes: 8000 | Median Reward: 27.31 | Max Reward: 44.31
Iteration: 133 | Episodes: 8100 | Median Reward: 32.60 | Max Reward: 44.31
Iteration: 134 | Episodes: 8200 | Median Reward: 32.66 | Max Reward: 44.31
Iteration: 136 | Episodes: 8300 | Median Reward: 27.03 | Max Reward: 44.31
Iteration: 138 | Episodes: 8400 | Median Reward: 31.37 | Max Reward: 44.31
Iteration: 139 | Episodes: 8500 | Median Reward: 25.94 | Max Reward: 44.31
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -73.6      |
| time/                   |            |
|    fps                  | 339        |
|    iterations           | 140        |
|    time_elapsed         | 2533       |
|    total_timesteps      | 860160     |
| train/                  |            |
|    approx_kl            | 0.38629907 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.4        |
|    entropy_loss         | -79.1      |
|    explained_variance   | -0.000111  |
|    learning_rate        | 0.0001     |
|    loss                 | 117        |
|    n_updates            | 1390       |
|    policy_gradient_loss | 0.00247    |
|    std                  | 1.09       |
|    value_loss           | 235        |
----------------------------------------
Iteration: 141 | Episodes: 8600 | Median Reward: 24.68 | Max Reward: 44.31
Iteration: 143 | Episodes: 8700 | Median Reward: 35.06 | Max Reward: 44.31
Iteration: 144 | Episodes: 8800 | Median Reward: 23.73 | Max Reward: 44.31
Iteration: 146 | Episodes: 8900 | Median Reward: 25.38 | Max Reward: 44.31
Iteration: 147 | Episodes: 9000 | Median Reward: 31.91 | Max Reward: 44.31
Iteration: 149 | Episodes: 9100 | Median Reward: 27.90 | Max Reward: 44.31
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.3     |
| time/                   |           |
|    fps                  | 339       |
|    iterations           | 150       |
|    time_elapsed         | 2718      |
|    total_timesteps      | 921600    |
| train/                  |           |
|    approx_kl            | 1.4848368 |
|    clip_fraction        | 0.173     |
|    clip_range           | 0.4       |
|    entropy_loss         | -75.3     |
|    explained_variance   | -7.26e-05 |
|    learning_rate        | 0.0001    |
|    loss                 | 127       |
|    n_updates            | 1490      |
|    policy_gradient_loss | -0.000974 |
|    std                  | 1.1       |
|    value_loss           | 251       |
---------------------------------------
Iteration: 151 | Episodes: 9200 | Median Reward: 30.49 | Max Reward: 44.31
Iteration: 152 | Episodes: 9300 | Median Reward: 29.40 | Max Reward: 44.31
Iteration: 154 | Episodes: 9400 | Median Reward: 29.82 | Max Reward: 46.63
Iteration: 156 | Episodes: 9500 | Median Reward: 29.22 | Max Reward: 46.63
Iteration: 157 | Episodes: 9600 | Median Reward: 26.49 | Max Reward: 46.63
Iteration: 159 | Episodes: 9700 | Median Reward: 35.10 | Max Reward: 46.63
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -69.1      |
| time/                   |            |
|    fps                  | 339        |
|    iterations           | 160        |
|    time_elapsed         | 2899       |
|    total_timesteps      | 983040     |
| train/                  |            |
|    approx_kl            | 0.61554945 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.4        |
|    entropy_loss         | -73.6      |
|    explained_variance   | -3.64e-05  |
|    learning_rate        | 0.0001     |
|    loss                 | 191        |
|    n_updates            | 1590       |
|    policy_gradient_loss | 0.0179     |
|    std                  | 1.11       |
|    value_loss           | 344        |
----------------------------------------
Iteration: 161 | Episodes: 9800 | Median Reward: 34.38 | Max Reward: 46.63
Iteration: 162 | Episodes: 9900 | Median Reward: 33.88 | Max Reward: 46.63
Iteration: 164 | Episodes: 10000 | Median Reward: 25.38 | Max Reward: 46.63
Iteration: 166 | Episodes: 10100 | Median Reward: 27.92 | Max Reward: 46.63
Iteration: 167 | Episodes: 10200 | Median Reward: 33.85 | Max Reward: 46.63
Iteration: 169 | Episodes: 10300 | Median Reward: 32.27 | Max Reward: 46.63
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -73.7      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 170        |
|    time_elapsed         | 3088       |
|    total_timesteps      | 1044480    |
| train/                  |            |
|    approx_kl            | 0.57256687 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.4        |
|    entropy_loss         | -65.4      |
|    explained_variance   | -2.29e-05  |
|    learning_rate        | 0.0001     |
|    loss                 | 151        |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0492    |
|    std                  | 1.13       |
|    value_loss           | 253        |
----------------------------------------
Iteration: 171 | Episodes: 10400 | Median Reward: 31.04 | Max Reward: 46.63
Iteration: 172 | Episodes: 10500 | Median Reward: 26.90 | Max Reward: 46.63
Iteration: 174 | Episodes: 10600 | Median Reward: 26.18 | Max Reward: 47.40
Iteration: 175 | Episodes: 10700 | Median Reward: 1.80 | Max Reward: 47.40
Iteration: 177 | Episodes: 10800 | Median Reward: -3.73 | Max Reward: 47.40
Iteration: 179 | Episodes: 10900 | Median Reward: -3.66 | Max Reward: 47.40
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -103       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 180        |
|    time_elapsed         | 3265       |
|    total_timesteps      | 1105920    |
| train/                  |            |
|    approx_kl            | 0.04119069 |
|    clip_fraction        | 0.0667     |
|    clip_range           | 0.4        |
|    entropy_loss         | -71.3      |
|    explained_variance   | -3.34e-05  |
|    learning_rate        | 0.0001     |
|    loss                 | 53.4       |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.00911   |
|    std                  | 1.13       |
|    value_loss           | 123        |
----------------------------------------
Iteration: 180 | Episodes: 11000 | Median Reward: -3.46 | Max Reward: 47.40
Iteration: 182 | Episodes: 11100 | Median Reward: -0.48 | Max Reward: 47.40
Iteration: 184 | Episodes: 11200 | Median Reward: 1.53 | Max Reward: 47.40
Iteration: 185 | Episodes: 11300 | Median Reward: -7.45 | Max Reward: 47.40
Iteration: 187 | Episodes: 11400 | Median Reward: 1.09 | Max Reward: 47.40
Iteration: 189 | Episodes: 11500 | Median Reward: 1.86 | Max Reward: 47.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -95.7       |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 190         |
|    time_elapsed         | 3444        |
|    total_timesteps      | 1167360     |
| train/                  |             |
|    approx_kl            | 0.033228837 |
|    clip_fraction        | 0.0555      |
|    clip_range           | 0.4         |
|    entropy_loss         | -71         |
|    explained_variance   | -3.04e-05   |
|    learning_rate        | 0.0001      |
|    loss                 | 67.1        |
|    n_updates            | 1890        |
|    policy_gradient_loss | 0.00233     |
|    std                  | 1.13        |
|    value_loss           | 127         |
-----------------------------------------
Iteration: 190 | Episodes: 11600 | Median Reward: 5.91 | Max Reward: 47.40
Iteration: 192 | Episodes: 11700 | Median Reward: 7.85 | Max Reward: 47.40
Iteration: 194 | Episodes: 11800 | Median Reward: 0.10 | Max Reward: 47.40
Iteration: 195 | Episodes: 11900 | Median Reward: 4.76 | Max Reward: 47.40
Iteration: 197 | Episodes: 12000 | Median Reward: 21.77 | Max Reward: 47.40
Iteration: 198 | Episodes: 12100 | Median Reward: 30.95 | Max Reward: 47.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.8     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 200       |
|    time_elapsed         | 3631      |
|    total_timesteps      | 1228800   |
| train/                  |           |
|    approx_kl            | 0.6044875 |
|    clip_fraction        | 0.291     |
|    clip_range           | 0.4       |
|    entropy_loss         | -76.1     |
|    explained_variance   | -1.65e-05 |
|    learning_rate        | 0.0001    |
|    loss                 | 198       |
|    n_updates            | 1990      |
|    policy_gradient_loss | 0.063     |
|    std                  | 1.13      |
|    value_loss           | 356       |
---------------------------------------
Iteration: 200 | Episodes: 12200 | Median Reward: 31.16 | Max Reward: 47.40
Iteration: 202 | Episodes: 12300 | Median Reward: 30.52 | Max Reward: 47.40
Iteration: 203 | Episodes: 12400 | Median Reward: 35.68 | Max Reward: 47.40
Iteration: 205 | Episodes: 12500 | Median Reward: 25.97 | Max Reward: 47.40
Iteration: 207 | Episodes: 12600 | Median Reward: 22.78 | Max Reward: 47.40
Iteration: 208 | Episodes: 12700 | Median Reward: 22.17 | Max Reward: 47.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82       |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 210       |
|    time_elapsed         | 3809      |
|    total_timesteps      | 1290240   |
| train/                  |           |
|    approx_kl            | 1.7519848 |
|    clip_fraction        | 0.322     |
|    clip_range           | 0.4       |
|    entropy_loss         | -77.9     |
|    explained_variance   | -1.41e-05 |
|    learning_rate        | 0.0001    |
|    loss                 | 137       |
|    n_updates            | 2090      |
|    policy_gradient_loss | 0.0237    |
|    std                  | 1.14      |
|    value_loss           | 273       |
---------------------------------------
Iteration: 210 | Episodes: 12800 | Median Reward: 28.21 | Max Reward: 47.40
Iteration: 212 | Episodes: 12900 | Median Reward: 27.20 | Max Reward: 47.40
Iteration: 213 | Episodes: 13000 | Median Reward: 3.61 | Max Reward: 47.40
Iteration: 215 | Episodes: 13100 | Median Reward: 23.52 | Max Reward: 47.40
Iteration: 216 | Episodes: 13200 | Median Reward: 26.07 | Max Reward: 47.40
Iteration: 218 | Episodes: 13300 | Median Reward: 27.61 | Max Reward: 47.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.3     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 220       |
|    time_elapsed         | 3987      |
|    total_timesteps      | 1351680   |
| train/                  |           |
|    approx_kl            | 0.6987011 |
|    clip_fraction        | 0.207     |
|    clip_range           | 0.4       |
|    entropy_loss         | -76.2     |
|    explained_variance   | -1.16e-05 |
|    learning_rate        | 0.0001    |
|    loss                 | 151       |
|    n_updates            | 2190      |
|    policy_gradient_loss | -8.89e-05 |
|    std                  | 1.14      |
|    value_loss           | 301       |
---------------------------------------
Iteration: 220 | Episodes: 13400 | Median Reward: 20.81 | Max Reward: 47.40
Iteration: 221 | Episodes: 13500 | Median Reward: 9.91 | Max Reward: 47.40
Iteration: 223 | Episodes: 13600 | Median Reward: 30.07 | Max Reward: 47.40
Iteration: 225 | Episodes: 13700 | Median Reward: 13.27 | Max Reward: 47.40
Iteration: 226 | Episodes: 13800 | Median Reward: 17.31 | Max Reward: 47.40
Iteration: 228 | Episodes: 13900 | Median Reward: 26.95 | Max Reward: 47.40
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -84.9    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 230      |
|    time_elapsed         | 4175     |
|    total_timesteps      | 1413120  |
| train/                  |          |
|    approx_kl            | 2.203875 |
|    clip_fraction        | 0.41     |
|    clip_range           | 0.4      |
|    entropy_loss         | -70.9    |
|    explained_variance   | -1.1e-05 |
|    learning_rate        | 0.0001   |
|    loss                 | 69       |
|    n_updates            | 2290     |
|    policy_gradient_loss | 0.113    |
|    std                  | 1.15     |
|    value_loss           | 231      |
--------------------------------------
Iteration: 230 | Episodes: 14000 | Median Reward: 22.99 | Max Reward: 47.40
Iteration: 231 | Episodes: 14100 | Median Reward: 32.01 | Max Reward: 47.40
Iteration: 233 | Episodes: 14200 | Median Reward: 33.31 | Max Reward: 47.40
Iteration: 235 | Episodes: 14300 | Median Reward: 33.58 | Max Reward: 47.40
Iteration: 236 | Episodes: 14400 | Median Reward: 31.55 | Max Reward: 47.40
Iteration: 238 | Episodes: 14500 | Median Reward: 31.48 | Max Reward: 47.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 240       |
|    time_elapsed         | 4353      |
|    total_timesteps      | 1474560   |
| train/                  |           |
|    approx_kl            | 10.103783 |
|    clip_fraction        | 0.253     |
|    clip_range           | 0.4       |
|    entropy_loss         | -80.5     |
|    explained_variance   | -8.46e-06 |
|    learning_rate        | 0.0001    |
|    loss                 | 162       |
|    n_updates            | 2390      |
|    policy_gradient_loss | 0.0821    |
|    std                  | 1.15      |
|    value_loss           | 333       |
---------------------------------------
Iteration: 240 | Episodes: 14600 | Median Reward: 31.48 | Max Reward: 47.40
Iteration: 241 | Episodes: 14700 | Median Reward: 31.78 | Max Reward: 47.40
Iteration: 243 | Episodes: 14800 | Median Reward: 37.84 | Max Reward: 47.40
Iteration: 245 | Episodes: 14900 | Median Reward: 36.29 | Max Reward: 47.40
Iteration: 246 | Episodes: 15000 | Median Reward: 31.29 | Max Reward: 47.40
Iteration: 248 | Episodes: 15100 | Median Reward: 32.89 | Max Reward: 47.40
Iteration: 249 | Episodes: 15200 | Median Reward: 35.50 | Max Reward: 47.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -72       |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 250       |
|    time_elapsed         | 4543      |
|    total_timesteps      | 1536000   |
| train/                  |           |
|    approx_kl            | 3.5645864 |
|    clip_fraction        | 0.263     |
|    clip_range           | 0.4       |
|    entropy_loss         | -80.8     |
|    explained_variance   | -6.32e-06 |
|    learning_rate        | 0.0001    |
|    loss                 | 141       |
|    n_updates            | 2490      |
|    policy_gradient_loss | 0.0199    |
|    std                  | 1.16      |
|    value_loss           | 302       |
---------------------------------------
Iteration: 251 | Episodes: 15300 | Median Reward: 32.30 | Max Reward: 47.40
Iteration: 253 | Episodes: 15400 | Median Reward: 32.49 | Max Reward: 47.40
Iteration: 254 | Episodes: 15500 | Median Reward: 28.71 | Max Reward: 47.40
Iteration: 256 | Episodes: 15600 | Median Reward: 14.97 | Max Reward: 47.40
Iteration: 258 | Episodes: 15700 | Median Reward: 8.75 | Max Reward: 47.40
Iteration: 259 | Episodes: 15800 | Median Reward: -0.34 | Max Reward: 47.40
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -103       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 260        |
|    time_elapsed         | 4720       |
|    total_timesteps      | 1597440    |
| train/                  |            |
|    approx_kl            | 0.03974314 |
|    clip_fraction        | 0.0469     |
|    clip_range           | 0.4        |
|    entropy_loss         | -67.3      |
|    explained_variance   | -8.7e-06   |
|    learning_rate        | 0.0001     |
|    loss                 | 66.8       |
|    n_updates            | 2590       |
|    policy_gradient_loss | 0.00463    |
|    std                  | 1.16       |
|    value_loss           | 132        |
----------------------------------------
Iteration: 261 | Episodes: 15900 | Median Reward: 0.38 | Max Reward: 47.40
Iteration: 263 | Episodes: 16000 | Median Reward: 4.69 | Max Reward: 47.40
Iteration: 264 | Episodes: 16100 | Median Reward: 2.91 | Max Reward: 47.40
Iteration: 266 | Episodes: 16200 | Median Reward: 4.13 | Max Reward: 47.40
Iteration: 267 | Episodes: 16300 | Median Reward: 7.69 | Max Reward: 47.40
Iteration: 269 | Episodes: 16400 | Median Reward: 15.64 | Max Reward: 47.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -85.7     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 270       |
|    time_elapsed         | 4900      |
|    total_timesteps      | 1658880   |
| train/                  |           |
|    approx_kl            | 5.7970533 |
|    clip_fraction        | 0.338     |
|    clip_range           | 0.4       |
|    entropy_loss         | -68.4     |
|    explained_variance   | -5.36e-06 |
|    learning_rate        | 0.0001    |
|    loss                 | 150       |
|    n_updates            | 2690      |
|    policy_gradient_loss | 0.0926    |
|    std                  | 1.17      |
|    value_loss           | 261       |
---------------------------------------
Iteration: 271 | Episodes: 16500 | Median Reward: 3.59 | Max Reward: 47.40
Iteration: 272 | Episodes: 16600 | Median Reward: 2.86 | Max Reward: 47.40
Iteration: 274 | Episodes: 16700 | Median Reward: 3.15 | Max Reward: 47.40
Iteration: 276 | Episodes: 16800 | Median Reward: 9.23 | Max Reward: 47.40
Iteration: 277 | Episodes: 16900 | Median Reward: 5.69 | Max Reward: 47.40
Iteration: 279 | Episodes: 17000 | Median Reward: 4.39 | Max Reward: 47.40
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 280       |
|    time_elapsed         | 5086      |
|    total_timesteps      | 1720320   |
| train/                  |           |
|    approx_kl            | 0.4371786 |
|    clip_fraction        | 0.104     |
|    clip_range           | 0.4       |
|    entropy_loss         | -66.6     |
|    explained_variance   | -7.39e-06 |
|    learning_rate        | 0.0001    |
|    loss                 | 53        |
|    n_updates            | 2790      |
|    policy_gradient_loss | 0.0643    |
|    std                  | 1.17      |
|    value_loss           | 149       |
---------------------------------------
Iteration: 281 | Episodes: 17100 | Median Reward: -2.71 | Max Reward: 47.40
Iteration: 282 | Episodes: 17200 | Median Reward: 12.14 | Max Reward: 47.40
Iteration: 284 | Episodes: 17300 | Median Reward: 8.58 | Max Reward: 47.40
Iteration: 286 | Episodes: 17400 | Median Reward: -0.32 | Max Reward: 47.40
Iteration: 287 | Episodes: 17500 | Median Reward: 4.17 | Max Reward: 47.40
Iteration: 289 | Episodes: 17600 | Median Reward: 0.73 | Max Reward: 47.40
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -96.6      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 290        |
|    time_elapsed         | 5263       |
|    total_timesteps      | 1781760    |
| train/                  |            |
|    approx_kl            | 0.29557115 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.4        |
|    entropy_loss         | -65.8      |
|    explained_variance   | -5.72e-06  |
|    learning_rate        | 0.0001     |
|    loss                 | 84.7       |
|    n_updates            | 2890       |
|    policy_gradient_loss | 0.0137     |
|    std                  | 1.17       |
|    value_loss           | 212        |
----------------------------------------
Iteration: 290 | Episodes: 17700 | Median Reward: 4.24 | Max Reward: 47.40
Iteration: 292 | Episodes: 17800 | Median Reward: 14.74 | Max Reward: 47.40
Iteration: 294 | Episodes: 17900 | Median Reward: 6.94 | Max Reward: 47.40
Iteration: 295 | Episodes: 18000 | Median Reward: 7.27 | Max Reward: 47.40
Iteration: 297 | Episodes: 18100 | Median Reward: 9.31 | Max Reward: 47.40
Iteration: 299 | Episodes: 18200 | Median Reward: 2.73 | Max Reward: 47.40
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -87.5      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 300        |
|    time_elapsed         | 5438       |
|    total_timesteps      | 1843200    |
| train/                  |            |
|    approx_kl            | 0.10986099 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.4        |
|    entropy_loss         | -66.2      |
|    explained_variance   | -5.01e-06  |
|    learning_rate        | 0.0001     |
|    loss                 | 68.4       |
|    n_updates            | 2990       |
|    policy_gradient_loss | 0.00942    |
|    std                  | 1.18       |
|    value_loss           | 205        |
----------------------------------------
Iteration: 300 | Episodes: 18300 | Median Reward: 17.51 | Max Reward: 47.40
Iteration: 302 | Episodes: 18400 | Median Reward: 16.41 | Max Reward: 47.40
Iteration: 304 | Episodes: 18500 | Median Reward: 20.62 | Max Reward: 47.40
Iteration: 305 | Episodes: 18600 | Median Reward: -0.05 | Max Reward: 47.40
Iteration: 307 | Episodes: 18700 | Median Reward: -1.02 | Max Reward: 47.40
Iteration: 308 | Episodes: 18800 | Median Reward: -0.38 | Max Reward: 49.11
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 96.3       |
|    ep_rew_mean          | -89.3      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 310        |
|    time_elapsed         | 5626       |
|    total_timesteps      | 1904640    |
| train/                  |            |
|    approx_kl            | 0.65345967 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -70.6      |
|    explained_variance   | -4.17e-06  |
|    learning_rate        | 0.0001     |
|    loss                 | 286        |
|    n_updates            | 3090       |
|    policy_gradient_loss | 0.019      |
|    std                  | 1.18       |
|    value_loss           | 298        |
----------------------------------------
Iteration: 310 | Episodes: 18900 | Median Reward: 1.31 | Max Reward: 49.11
Iteration: 312 | Episodes: 19000 | Median Reward: -1.66 | Max Reward: 49.11
Iteration: 313 | Episodes: 19100 | Median Reward: -3.32 | Max Reward: 49.11
Iteration: 315 | Episodes: 19200 | Median Reward: 7.14 | Max Reward: 49.11
Iteration: 317 | Episodes: 19300 | Median Reward: -0.88 | Max Reward: 49.11
Iteration: 318 | Episodes: 19400 | Median Reward: 4.00 | Max Reward: 49.11
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -99       |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 320       |
|    time_elapsed         | 5803      |
|    total_timesteps      | 1966080   |
| train/                  |           |
|    approx_kl            | 1.9981477 |
|    clip_fraction        | 0.139     |
|    clip_range           | 0.4       |
|    entropy_loss         | -71.5     |
|    explained_variance   | -4.65e-06 |
|    learning_rate        | 0.0001    |
|    loss                 | 45        |
|    n_updates            | 3190      |
|    policy_gradient_loss | 0.0253    |
|    std                  | 1.18      |
|    value_loss           | 168       |
---------------------------------------
Iteration: 320 | Episodes: 19500 | Median Reward: -2.15 | Max Reward: 49.11
Iteration: 322 | Episodes: 19600 | Median Reward: 2.90 | Max Reward: 49.11
Iteration: 323 | Episodes: 19700 | Median Reward: 5.03 | Max Reward: 49.11
Iteration: 325 | Episodes: 19800 | Median Reward: 3.32 | Max Reward: 49.11
Iteration: 327 | Episodes: 19900 | Median Reward: -1.66 | Max Reward: 49.11
Iteration: 328 | Episodes: 20000 | Median Reward: 4.09 | Max Reward: 49.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -100        |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 330         |
|    time_elapsed         | 5981        |
|    total_timesteps      | 2027520     |
| train/                  |             |
|    approx_kl            | 0.027254255 |
|    clip_fraction        | 0.0605      |
|    clip_range           | 0.4         |
|    entropy_loss         | -65.1       |
|    explained_variance   | -5.13e-06   |
|    learning_rate        | 0.0001      |
|    loss                 | 73.2        |
|    n_updates            | 3290        |
|    policy_gradient_loss | 0.00387     |
|    std                  | 1.19        |
|    value_loss           | 144         |
-----------------------------------------
Iteration: 330 | Episodes: 20100 | Median Reward: 3.70 | Max Reward: 49.11
Iteration: 331 | Episodes: 20200 | Median Reward: 11.05 | Max Reward: 49.11
Iteration: 333 | Episodes: 20300 | Median Reward: 4.52 | Max Reward: 49.11
Iteration: 335 | Episodes: 20400 | Median Reward: 0.81 | Max Reward: 49.11
Iteration: 336 | Episodes: 20500 | Median Reward: 15.08 | Max Reward: 49.11
Iteration: 338 | Episodes: 20600 | Median Reward: 12.38 | Max Reward: 49.11
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -90.4      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 340        |
|    time_elapsed         | 6168       |
|    total_timesteps      | 2088960    |
| train/                  |            |
|    approx_kl            | 0.07717085 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -65.1      |
|    explained_variance   | -3.46e-06  |
|    learning_rate        | 0.0001     |
|    loss                 | 86.6       |
|    n_updates            | 3390       |
|    policy_gradient_loss | -0.0214    |
|    std                  | 1.2        |
|    value_loss           | 224        |
----------------------------------------
Iteration: 340 | Episodes: 20700 | Median Reward: 11.52 | Max Reward: 49.11
Iteration: 341 | Episodes: 20800 | Median Reward: 11.47 | Max Reward: 49.11
Iteration: 343 | Episodes: 20900 | Median Reward: 14.08 | Max Reward: 49.11
Iteration: 345 | Episodes: 21000 | Median Reward: 8.03 | Max Reward: 49.11
Iteration: 346 | Episodes: 21100 | Median Reward: 14.33 | Max Reward: 49.11
Iteration: 348 | Episodes: 21200 | Median Reward: 21.24 | Max Reward: 49.11
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 350       |
|    time_elapsed         | 6346      |
|    total_timesteps      | 2150400   |
| train/                  |           |
|    approx_kl            | 1.4336152 |
|    clip_fraction        | 0.146     |
|    clip_range           | 0.4       |
|    entropy_loss         | -72.9     |
|    explained_variance   | -2.38e-06 |
|    learning_rate        | 0.0001    |
|    loss                 | 116       |
|    n_updates            | 3490      |
|    policy_gradient_loss | 0.016     |
|    std                  | 1.2       |
|    value_loss           | 350       |
---------------------------------------
Iteration: 350 | Episodes: 21300 | Median Reward: 19.34 | Max Reward: 49.11
Iteration: 351 | Episodes: 21400 | Median Reward: 22.34 | Max Reward: 49.11
Iteration: 353 | Episodes: 21500 | Median Reward: 27.80 | Max Reward: 49.11
Iteration: 354 | Episodes: 21600 | Median Reward: 27.07 | Max Reward: 49.11
Iteration: 356 | Episodes: 21700 | Median Reward: 34.47 | Max Reward: 49.11
Iteration: 358 | Episodes: 21800 | Median Reward: 23.36 | Max Reward: 49.11
Iteration: 359 | Episodes: 21900 | Median Reward: 34.49 | Max Reward: 49.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.1       |
| time/                   |             |
|    fps                  | 339         |
|    iterations           | 360         |
|    time_elapsed         | 6520        |
|    total_timesteps      | 2211840     |
| train/                  |             |
|    approx_kl            | 0.002178303 |
|    clip_fraction        | 0.00807     |
|    clip_range           | 0.4         |
|    entropy_loss         | -87         |
|    explained_variance   | -2.38e-06   |
|    learning_rate        | 0.0001      |
|    loss                 | 203         |
|    n_updates            | 3590        |
|    policy_gradient_loss | 0.00538     |
|    std                  | 1.21        |
|    value_loss           | 362         |
-----------------------------------------
Iteration: 361 | Episodes: 22000 | Median Reward: 32.25 | Max Reward: 49.11
Iteration: 363 | Episodes: 22100 | Median Reward: 32.25 | Max Reward: 49.11
Iteration: 364 | Episodes: 22200 | Median Reward: 31.82 | Max Reward: 49.11
Iteration: 366 | Episodes: 22300 | Median Reward: 33.59 | Max Reward: 49.11
Iteration: 368 | Episodes: 22400 | Median Reward: 36.55 | Max Reward: 49.11
Iteration: 369 | Episodes: 22500 | Median Reward: 35.47 | Max Reward: 49.11
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.8         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 370           |
|    time_elapsed         | 6710          |
|    total_timesteps      | 2273280       |
| train/                  |               |
|    approx_kl            | 1.7450344e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -87.3         |
|    explained_variance   | -2.03e-06     |
|    learning_rate        | 0.0001        |
|    loss                 | 174           |
|    n_updates            | 3690          |
|    policy_gradient_loss | -6.81e-05     |
|    std                  | 1.21          |
|    value_loss           | 382           |
-------------------------------------------
Iteration: 371 | Episodes: 22600 | Median Reward: 33.71 | Max Reward: 49.11
Iteration: 373 | Episodes: 22700 | Median Reward: 34.59 | Max Reward: 49.11
Iteration: 374 | Episodes: 22800 | Median Reward: 28.04 | Max Reward: 49.11
Iteration: 376 | Episodes: 22900 | Median Reward: 30.60 | Max Reward: 49.11
Iteration: 378 | Episodes: 23000 | Median Reward: 33.90 | Max Reward: 49.11
Iteration: 379 | Episodes: 23100 | Median Reward: 31.94 | Max Reward: 49.11
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.8         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 380           |
|    time_elapsed         | 6886          |
|    total_timesteps      | 2334720       |
| train/                  |               |
|    approx_kl            | 0.00025855005 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -87.6         |
|    explained_variance   | -2.26e-06     |
|    learning_rate        | 0.0001        |
|    loss                 | 124           |
|    n_updates            | 3790          |
|    policy_gradient_loss | -0.00109      |
|    std                  | 1.21          |
|    value_loss           | 324           |
-------------------------------------------
Iteration: 381 | Episodes: 23200 | Median Reward: 30.87 | Max Reward: 49.11
Iteration: 382 | Episodes: 23300 | Median Reward: 33.84 | Max Reward: 49.11
Iteration: 384 | Episodes: 23400 | Median Reward: 30.37 | Max Reward: 49.11
Iteration: 386 | Episodes: 23500 | Median Reward: 33.62 | Max Reward: 49.11
Iteration: 387 | Episodes: 23600 | Median Reward: 34.39 | Max Reward: 49.11
Iteration: 389 | Episodes: 23700 | Median Reward: 35.68 | Max Reward: 49.11
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.1         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 390           |
|    time_elapsed         | 7075          |
|    total_timesteps      | 2396160       |
| train/                  |               |
|    approx_kl            | 4.8962494e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -87.8         |
|    explained_variance   | -1.91e-06     |
|    learning_rate        | 0.0001        |
|    loss                 | 201           |
|    n_updates            | 3890          |
|    policy_gradient_loss | -0.000224     |
|    std                  | 1.22          |
|    value_loss           | 397           |
-------------------------------------------
Iteration: 391 | Episodes: 23800 | Median Reward: 32.54 | Max Reward: 49.11
Iteration: 392 | Episodes: 23900 | Median Reward: 31.93 | Max Reward: 49.11
Iteration: 394 | Episodes: 24000 | Median Reward: 33.16 | Max Reward: 49.11
Iteration: 396 | Episodes: 24100 | Median Reward: 28.80 | Max Reward: 49.11
Iteration: 397 | Episodes: 24200 | Median Reward: 36.21 | Max Reward: 49.11
Iteration: 399 | Episodes: 24300 | Median Reward: 34.09 | Max Reward: 49.11
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.9         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 400           |
|    time_elapsed         | 7254          |
|    total_timesteps      | 2457600       |
| train/                  |               |
|    approx_kl            | 0.00024085779 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -88           |
|    explained_variance   | -1.91e-06     |
|    learning_rate        | 0.0001        |
|    loss                 | 167           |
|    n_updates            | 3990          |
|    policy_gradient_loss | -0.00103      |
|    std                  | 1.22          |
|    value_loss           | 321           |
-------------------------------------------
Iteration: 401 | Episodes: 24400 | Median Reward: 33.61 | Max Reward: 49.11
Iteration: 402 | Episodes: 24500 | Median Reward: 35.05 | Max Reward: 49.11
Iteration: 404 | Episodes: 24600 | Median Reward: 34.49 | Max Reward: 49.11
Iteration: 405 | Episodes: 24700 | Median Reward: 27.83 | Max Reward: 49.11
Iteration: 407 | Episodes: 24800 | Median Reward: 34.62 | Max Reward: 49.11
Iteration: 409 | Episodes: 24900 | Median Reward: 32.85 | Max Reward: 49.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.2       |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 410         |
|    time_elapsed         | 7433        |
|    total_timesteps      | 2519040     |
| train/                  |             |
|    approx_kl            | 8.49902e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -88.3       |
|    explained_variance   | -1.67e-06   |
|    learning_rate        | 0.0001      |
|    loss                 | 172         |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.000109   |
|    std                  | 1.23        |
|    value_loss           | 331         |
-----------------------------------------
Iteration: 410 | Episodes: 25000 | Median Reward: 35.14 | Max Reward: 49.11
Iteration: 412 | Episodes: 25100 | Median Reward: 32.67 | Max Reward: 49.11
Iteration: 414 | Episodes: 25200 | Median Reward: 29.43 | Max Reward: 49.11
Iteration: 415 | Episodes: 25300 | Median Reward: 29.47 | Max Reward: 49.11
Iteration: 417 | Episodes: 25400 | Median Reward: 33.24 | Max Reward: 49.11
Iteration: 419 | Episodes: 25500 | Median Reward: 34.98 | Max Reward: 49.11
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.8         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 420           |
|    time_elapsed         | 7624          |
|    total_timesteps      | 2580480       |
| train/                  |               |
|    approx_kl            | 1.9662424e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -88.7         |
|    explained_variance   | -1.43e-06     |
|    learning_rate        | 0.0001        |
|    loss                 | 185           |
|    n_updates            | 4190          |
|    policy_gradient_loss | -2.73e-05     |
|    std                  | 1.24          |
|    value_loss           | 371           |
-------------------------------------------
Iteration: 420 | Episodes: 25600 | Median Reward: 30.49 | Max Reward: 49.11
Iteration: 422 | Episodes: 25700 | Median Reward: 32.88 | Max Reward: 49.11
Iteration: 424 | Episodes: 25800 | Median Reward: 33.91 | Max Reward: 49.11
Iteration: 425 | Episodes: 25900 | Median Reward: 31.17 | Max Reward: 49.11
Iteration: 427 | Episodes: 26000 | Median Reward: 32.17 | Max Reward: 49.11
Iteration: 428 | Episodes: 26100 | Median Reward: 34.36 | Max Reward: 49.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.9        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 430          |
|    time_elapsed         | 7802         |
|    total_timesteps      | 2641920      |
| train/                  |              |
|    approx_kl            | 0.0009005445 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -89.1        |
|    explained_variance   | -1.19e-06    |
|    learning_rate        | 0.0001       |
|    loss                 | 166          |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.000779    |
|    std                  | 1.26         |
|    value_loss           | 312          |
------------------------------------------
Iteration: 430 | Episodes: 26200 | Median Reward: 36.92 | Max Reward: 49.11
Iteration: 432 | Episodes: 26300 | Median Reward: 28.77 | Max Reward: 49.11
Iteration: 433 | Episodes: 26400 | Median Reward: 35.20 | Max Reward: 49.11
Iteration: 435 | Episodes: 26500 | Median Reward: 35.29 | Max Reward: 49.11
Iteration: 437 | Episodes: 26600 | Median Reward: 32.84 | Max Reward: 49.11
Iteration: 438 | Episodes: 26700 | Median Reward: 35.77 | Max Reward: 49.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.1        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 440          |
|    time_elapsed         | 7981         |
|    total_timesteps      | 2703360      |
| train/                  |              |
|    approx_kl            | 0.0023848345 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -89.4        |
|    explained_variance   | -8.34e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 171          |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 1.27         |
|    value_loss           | 327          |
------------------------------------------
Iteration: 440 | Episodes: 26800 | Median Reward: 35.29 | Max Reward: 49.11
Iteration: 442 | Episodes: 26900 | Median Reward: 33.01 | Max Reward: 49.11
Iteration: 443 | Episodes: 27000 | Median Reward: 34.05 | Max Reward: 49.11
Iteration: 445 | Episodes: 27100 | Median Reward: 35.12 | Max Reward: 49.11
Iteration: 447 | Episodes: 27200 | Median Reward: 36.14 | Max Reward: 49.11
Iteration: 448 | Episodes: 27300 | Median Reward: 34.91 | Max Reward: 49.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.6        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 450          |
|    time_elapsed         | 8168         |
|    total_timesteps      | 2764800      |
| train/                  |              |
|    approx_kl            | 0.0008791108 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -89.7        |
|    explained_variance   | -8.34e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 158          |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.00112     |
|    std                  | 1.29         |
|    value_loss           | 307          |
------------------------------------------
Iteration: 450 | Episodes: 27400 | Median Reward: 34.91 | Max Reward: 49.11
Iteration: 452 | Episodes: 27500 | Median Reward: 32.31 | Max Reward: 49.11
Iteration: 453 | Episodes: 27600 | Median Reward: 37.39 | Max Reward: 49.11
Iteration: 455 | Episodes: 27700 | Median Reward: 37.15 | Max Reward: 49.11
Iteration: 456 | Episodes: 27800 | Median Reward: 31.61 | Max Reward: 49.11
Iteration: 458 | Episodes: 27900 | Median Reward: 36.55 | Max Reward: 49.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.2        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 460          |
|    time_elapsed         | 8347         |
|    total_timesteps      | 2826240      |
| train/                  |              |
|    approx_kl            | 0.0035914653 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -90.1        |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 148          |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.00101     |
|    std                  | 1.31         |
|    value_loss           | 350          |
------------------------------------------
Iteration: 460 | Episodes: 28000 | Median Reward: 35.48 | Max Reward: 49.11
Iteration: 461 | Episodes: 28100 | Median Reward: 36.05 | Max Reward: 49.11
Iteration: 463 | Episodes: 28200 | Median Reward: 34.54 | Max Reward: 49.11
Iteration: 465 | Episodes: 28300 | Median Reward: 37.06 | Max Reward: 49.11
Iteration: 466 | Episodes: 28400 | Median Reward: 36.45 | Max Reward: 49.11
Iteration: 468 | Episodes: 28500 | Median Reward: 33.44 | Max Reward: 49.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.3        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 470          |
|    time_elapsed         | 8522         |
|    total_timesteps      | 2887680      |
| train/                  |              |
|    approx_kl            | 0.0010492338 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -90.4        |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 149          |
|    n_updates            | 4690         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 1.32         |
|    value_loss           | 342          |
------------------------------------------
Iteration: 470 | Episodes: 28600 | Median Reward: 35.04 | Max Reward: 49.11
Iteration: 471 | Episodes: 28700 | Median Reward: 32.95 | Max Reward: 49.11
Iteration: 473 | Episodes: 28800 | Median Reward: 36.02 | Max Reward: 49.11
Iteration: 475 | Episodes: 28900 | Median Reward: 36.02 | Max Reward: 49.11
Iteration: 476 | Episodes: 29000 | Median Reward: 36.95 | Max Reward: 49.11
Iteration: 478 | Episodes: 29100 | Median Reward: 35.74 | Max Reward: 49.11
Iteration: 479 | Episodes: 29200 | Median Reward: 35.74 | Max Reward: 49.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.2        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 480          |
|    time_elapsed         | 8716         |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0005490276 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -90.7        |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 155          |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.000497    |
|    std                  | 1.34         |
|    value_loss           | 321          |
------------------------------------------
Iteration: 481 | Episodes: 29300 | Median Reward: 32.29 | Max Reward: 49.11
Iteration: 483 | Episodes: 29400 | Median Reward: 36.57 | Max Reward: 49.11
Iteration: 484 | Episodes: 29500 | Median Reward: 32.04 | Max Reward: 49.11
Iteration: 486 | Episodes: 29600 | Median Reward: 34.89 | Max Reward: 49.11
Iteration: 488 | Episodes: 29700 | Median Reward: 35.28 | Max Reward: 49.11
Iteration: 489 | Episodes: 29800 | Median Reward: 37.47 | Max Reward: 49.11
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.8         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 490           |
|    time_elapsed         | 8891          |
|    total_timesteps      | 3010560       |
| train/                  |               |
|    approx_kl            | 0.00037131118 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -91           |
|    explained_variance   | -4.77e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 178           |
|    n_updates            | 4890          |
|    policy_gradient_loss | -0.00045      |
|    std                  | 1.36          |
|    value_loss           | 361           |
-------------------------------------------
Iteration: 491 | Episodes: 29900 | Median Reward: 37.47 | Max Reward: 49.11
Iteration: 493 | Episodes: 30000 | Median Reward: 35.88 | Max Reward: 49.11
Iteration: 494 | Episodes: 30100 | Median Reward: 35.39 | Max Reward: 49.11
Iteration: 496 | Episodes: 30200 | Median Reward: 34.76 | Max Reward: 49.11
Iteration: 497 | Episodes: 30300 | Median Reward: 34.75 | Max Reward: 49.11
Iteration: 499 | Episodes: 30400 | Median Reward: 37.35 | Max Reward: 49.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64         |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 500         |
|    time_elapsed         | 9082        |
|    total_timesteps      | 3072000     |
| train/                  |             |
|    approx_kl            | 0.000641337 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -91.3       |
|    explained_variance   | -3.58e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 150         |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.000251   |
|    std                  | 1.37        |
|    value_loss           | 337         |
-----------------------------------------
Iteration: 501 | Episodes: 30500 | Median Reward: 34.95 | Max Reward: 49.11
Iteration: 502 | Episodes: 30600 | Median Reward: 40.71 | Max Reward: 49.11
Iteration: 504 | Episodes: 30700 | Median Reward: 35.03 | Max Reward: 49.11
Iteration: 506 | Episodes: 30800 | Median Reward: 37.34 | Max Reward: 49.11
Iteration: 507 | Episodes: 30900 | Median Reward: 37.92 | Max Reward: 49.11
Iteration: 509 | Episodes: 31000 | Median Reward: 34.61 | Max Reward: 49.11
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68           |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 510           |
|    time_elapsed         | 9256          |
|    total_timesteps      | 3133440       |
| train/                  |               |
|    approx_kl            | 0.00056827057 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -91.5         |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 154           |
|    n_updates            | 5090          |
|    policy_gradient_loss | -0.000209     |
|    std                  | 1.39          |
|    value_loss           | 317           |
-------------------------------------------
Iteration: 511 | Episodes: 31100 | Median Reward: 35.54 | Max Reward: 49.11
Iteration: 512 | Episodes: 31200 | Median Reward: 35.33 | Max Reward: 49.11
Iteration: 514 | Episodes: 31300 | Median Reward: 39.69 | Max Reward: 49.11
Iteration: 516 | Episodes: 31400 | Median Reward: 35.28 | Max Reward: 49.11
Iteration: 517 | Episodes: 31500 | Median Reward: 28.69 | Max Reward: 49.11
Iteration: 519 | Episodes: 31600 | Median Reward: 39.48 | Max Reward: 49.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.4        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 520          |
|    time_elapsed         | 9437         |
|    total_timesteps      | 3194880      |
| train/                  |              |
|    approx_kl            | 0.0022266486 |
|    clip_fraction        | 0.00121      |
|    clip_range           | 0.4          |
|    entropy_loss         | -91.7        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 153          |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00179     |
|    std                  | 1.4          |
|    value_loss           | 330          |
------------------------------------------
Iteration: 521 | Episodes: 31700 | Median Reward: 33.63 | Max Reward: 49.11
Iteration: 522 | Episodes: 31800 | Median Reward: 36.49 | Max Reward: 49.11
Iteration: 524 | Episodes: 31900 | Median Reward: 38.24 | Max Reward: 49.11
Iteration: 525 | Episodes: 32000 | Median Reward: 35.21 | Max Reward: 49.11
Iteration: 527 | Episodes: 32100 | Median Reward: 29.77 | Max Reward: 49.11
Iteration: 529 | Episodes: 32200 | Median Reward: 39.12 | Max Reward: 49.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.9        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 530          |
|    time_elapsed         | 9623         |
|    total_timesteps      | 3256320      |
| train/                  |              |
|    approx_kl            | 0.0005410087 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -91.8        |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 158          |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.000667    |
|    std                  | 1.41         |
|    value_loss           | 353          |
------------------------------------------
Iteration: 530 | Episodes: 32300 | Median Reward: 35.06 | Max Reward: 49.11
Iteration: 532 | Episodes: 32400 | Median Reward: 39.08 | Max Reward: 49.11
Iteration: 534 | Episodes: 32500 | Median Reward: 37.73 | Max Reward: 49.11
Iteration: 535 | Episodes: 32600 | Median Reward: 34.29 | Max Reward: 49.11
Iteration: 537 | Episodes: 32700 | Median Reward: 35.68 | Max Reward: 49.11
Iteration: 539 | Episodes: 32800 | Median Reward: 33.89 | Max Reward: 49.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.6       |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 540         |
|    time_elapsed         | 9802        |
|    total_timesteps      | 3317760     |
| train/                  |             |
|    approx_kl            | 0.008153783 |
|    clip_fraction        | 0.00269     |
|    clip_range           | 0.4         |
|    entropy_loss         | -91.8       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 170         |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.00443    |
|    std                  | 1.43        |
|    value_loss           | 335         |
-----------------------------------------
Iteration: 540 | Episodes: 32900 | Median Reward: 33.34 | Max Reward: 49.11
Iteration: 542 | Episodes: 33000 | Median Reward: 38.84 | Max Reward: 49.11
Iteration: 544 | Episodes: 33100 | Median Reward: 36.67 | Max Reward: 49.11
Iteration: 545 | Episodes: 33200 | Median Reward: 37.87 | Max Reward: 49.11
Iteration: 547 | Episodes: 33300 | Median Reward: 35.42 | Max Reward: 49.11
Iteration: 548 | Episodes: 33400 | Median Reward: 37.92 | Max Reward: 49.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.7       |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 550         |
|    time_elapsed         | 9982        |
|    total_timesteps      | 3379200     |
| train/                  |             |
|    approx_kl            | 0.026755653 |
|    clip_fraction        | 0.0138      |
|    clip_range           | 0.4         |
|    entropy_loss         | -92         |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 137         |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00754    |
|    std                  | 1.44        |
|    value_loss           | 332         |
-----------------------------------------
Iteration: 550 | Episodes: 33500 | Median Reward: 38.82 | Max Reward: 49.11
Iteration: 552 | Episodes: 33600 | Median Reward: 37.97 | Max Reward: 49.11
Iteration: 553 | Episodes: 33700 | Median Reward: 34.09 | Max Reward: 49.11
Iteration: 555 | Episodes: 33800 | Median Reward: 38.04 | Max Reward: 49.11
Iteration: 557 | Episodes: 33900 | Median Reward: 37.55 | Max Reward: 49.11
Iteration: 558 | Episodes: 34000 | Median Reward: 39.43 | Max Reward: 49.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.1        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 560          |
|    time_elapsed         | 10170        |
|    total_timesteps      | 3440640      |
| train/                  |              |
|    approx_kl            | 0.0014897329 |
|    clip_fraction        | 0.00193      |
|    clip_range           | 0.4          |
|    entropy_loss         | -92.3        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 180          |
|    n_updates            | 5590         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 1.46         |
|    value_loss           | 349          |
------------------------------------------
Iteration: 560 | Episodes: 34100 | Median Reward: 38.00 | Max Reward: 49.11
Iteration: 562 | Episodes: 34200 | Median Reward: 34.89 | Max Reward: 49.11
Iteration: 563 | Episodes: 34300 | Median Reward: 36.33 | Max Reward: 49.11
Iteration: 565 | Episodes: 34400 | Median Reward: 38.34 | Max Reward: 49.11
Iteration: 567 | Episodes: 34500 | Median Reward: 36.11 | Max Reward: 49.11
Iteration: 568 | Episodes: 34600 | Median Reward: 39.33 | Max Reward: 49.11
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.9       |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 570         |
|    time_elapsed         | 10350       |
|    total_timesteps      | 3502080     |
| train/                  |             |
|    approx_kl            | 0.015736168 |
|    clip_fraction        | 0.0106      |
|    clip_range           | 0.4         |
|    entropy_loss         | -92.6       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 147         |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.0069     |
|    std                  | 1.47        |
|    value_loss           | 322         |
-----------------------------------------
Iteration: 570 | Episodes: 34700 | Median Reward: 38.33 | Max Reward: 49.11
Iteration: 571 | Episodes: 34800 | Median Reward: 38.87 | Max Reward: 49.11
Iteration: 573 | Episodes: 34900 | Median Reward: 24.73 | Max Reward: 49.11
Iteration: 575 | Episodes: 35000 | Median Reward: 19.18 | Max Reward: 49.11
Iteration: 576 | Episodes: 35100 | Median Reward: 26.23 | Max Reward: 49.11
Iteration: 578 | Episodes: 35200 | Median Reward: 23.05 | Max Reward: 49.11
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -76.4     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 580       |
|    time_elapsed         | 10529     |
|    total_timesteps      | 3563520   |
| train/                  |           |
|    approx_kl            | 0.7678418 |
|    clip_fraction        | 0.272     |
|    clip_range           | 0.4       |
|    entropy_loss         | -74.8     |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 111       |
|    n_updates            | 5790      |
|    policy_gradient_loss | 0.0152    |
|    std                  | 1.48      |
|    value_loss           | 242       |
---------------------------------------
Iteration: 580 | Episodes: 35300 | Median Reward: 27.79 | Max Reward: 49.11
Iteration: 581 | Episodes: 35400 | Median Reward: 27.07 | Max Reward: 49.11
Iteration: 583 | Episodes: 35500 | Median Reward: 28.14 | Max Reward: 49.11
Iteration: 585 | Episodes: 35600 | Median Reward: 28.50 | Max Reward: 49.11
Iteration: 586 | Episodes: 35700 | Median Reward: 25.51 | Max Reward: 49.11
Iteration: 588 | Episodes: 35800 | Median Reward: 35.45 | Max Reward: 49.11
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -79.1      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 590        |
|    time_elapsed         | 10720      |
|    total_timesteps      | 3624960    |
| train/                  |            |
|    approx_kl            | 0.39714873 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -78.8      |
|    explained_variance   | -2.38e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 134        |
|    n_updates            | 5890       |
|    policy_gradient_loss | 0.0152     |
|    std                  | 1.48       |
|    value_loss           | 269        |
----------------------------------------
Iteration: 590 | Episodes: 35900 | Median Reward: 25.78 | Max Reward: 49.11
Iteration: 591 | Episodes: 36000 | Median Reward: 25.22 | Max Reward: 49.11
Iteration: 593 | Episodes: 36100 | Median Reward: 34.18 | Max Reward: 49.11
Iteration: 595 | Episodes: 36200 | Median Reward: 1.75 | Max Reward: 49.11
Iteration: 596 | Episodes: 36300 | Median Reward: 29.01 | Max Reward: 49.11
Iteration: 598 | Episodes: 36400 | Median Reward: -1.98 | Max Reward: 49.11
Iteration: 599 | Episodes: 36500 | Median Reward: 28.66 | Max Reward: 49.11
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -80.7      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 600        |
|    time_elapsed         | 10896      |
|    total_timesteps      | 3686400    |
| train/                  |            |
|    approx_kl            | 0.73259056 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.4        |
|    entropy_loss         | -79.9      |
|    explained_variance   | -2.38e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 98.9       |
|    n_updates            | 5990       |
|    policy_gradient_loss | 0.00534    |
|    std                  | 1.49       |
|    value_loss           | 217        |
----------------------------------------
Iteration: 601 | Episodes: 36600 | Median Reward: 1.38 | Max Reward: 49.11
Iteration: 603 | Episodes: 36700 | Median Reward: -0.20 | Max Reward: 49.11
Iteration: 604 | Episodes: 36800 | Median Reward: -11.43 | Max Reward: 49.11
Iteration: 606 | Episodes: 36900 | Median Reward: -2.06 | Max Reward: 49.11
Iteration: 608 | Episodes: 37000 | Median Reward: 18.56 | Max Reward: 49.11
Iteration: 609 | Episodes: 37100 | Median Reward: 37.68 | Max Reward: 49.11
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 610       |
|    time_elapsed         | 11087     |
|    total_timesteps      | 3747840   |
| train/                  |           |
|    approx_kl            | 8.636809  |
|    clip_fraction        | 0.394     |
|    clip_range           | 0.4       |
|    entropy_loss         | -82.6     |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 209       |
|    n_updates            | 6090      |
|    policy_gradient_loss | -0.0198   |
|    std                  | 1.49      |
|    value_loss           | 347       |
---------------------------------------
Iteration: 611 | Episodes: 37200 | Median Reward: 37.73 | Max Reward: 49.11
Iteration: 613 | Episodes: 37300 | Median Reward: 38.68 | Max Reward: 49.11
Iteration: 614 | Episodes: 37400 | Median Reward: 39.04 | Max Reward: 49.11
Iteration: 616 | Episodes: 37500 | Median Reward: 39.00 | Max Reward: 49.11
Iteration: 618 | Episodes: 37600 | Median Reward: 33.42 | Max Reward: 49.11
Iteration: 619 | Episodes: 37700 | Median Reward: 36.21 | Max Reward: 49.11
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.7        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 620          |
|    time_elapsed         | 11263        |
|    total_timesteps      | 3809280      |
| train/                  |              |
|    approx_kl            | 0.0032468354 |
|    clip_fraction        | 0.000513     |
|    clip_range           | 0.4          |
|    entropy_loss         | -93          |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 132          |
|    n_updates            | 6190         |
|    policy_gradient_loss | -0.00314     |
|    std                  | 1.5          |
|    value_loss           | 328          |
------------------------------------------
Iteration: 621 | Episodes: 37800 | Median Reward: 38.04 | Max Reward: 49.11
Iteration: 622 | Episodes: 37900 | Median Reward: 37.60 | Max Reward: 49.11
Iteration: 624 | Episodes: 38000 | Median Reward: 39.24 | Max Reward: 49.11
Iteration: 626 | Episodes: 38100 | Median Reward: 40.29 | Max Reward: 49.11
Iteration: 627 | Episodes: 38200 | Median Reward: 41.63 | Max Reward: 49.27
Iteration: 629 | Episodes: 38300 | Median Reward: 38.95 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.2         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 630           |
|    time_elapsed         | 11443         |
|    total_timesteps      | 3870720       |
| train/                  |               |
|    approx_kl            | 0.00043024123 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.3         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 181           |
|    n_updates            | 6290          |
|    policy_gradient_loss | 5.54e-05      |
|    std                  | 1.5           |
|    value_loss           | 348           |
-------------------------------------------
Iteration: 631 | Episodes: 38400 | Median Reward: 36.78 | Max Reward: 49.27
Iteration: 632 | Episodes: 38500 | Median Reward: 38.89 | Max Reward: 49.27
Iteration: 634 | Episodes: 38600 | Median Reward: 38.95 | Max Reward: 49.27
Iteration: 636 | Episodes: 38700 | Median Reward: 39.14 | Max Reward: 49.27
Iteration: 637 | Episodes: 38800 | Median Reward: 40.06 | Max Reward: 49.27
Iteration: 639 | Episodes: 38900 | Median Reward: 34.68 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.5         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 640           |
|    time_elapsed         | 11632         |
|    total_timesteps      | 3932160       |
| train/                  |               |
|    approx_kl            | 0.00032294993 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.4         |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 182           |
|    n_updates            | 6390          |
|    policy_gradient_loss | -0.000322     |
|    std                  | 1.52          |
|    value_loss           | 332           |
-------------------------------------------
Iteration: 641 | Episodes: 39000 | Median Reward: 34.54 | Max Reward: 49.27
Iteration: 642 | Episodes: 39100 | Median Reward: 36.31 | Max Reward: 49.27
Iteration: 644 | Episodes: 39200 | Median Reward: 38.65 | Max Reward: 49.27
Iteration: 645 | Episodes: 39300 | Median Reward: 35.98 | Max Reward: 49.27
Iteration: 647 | Episodes: 39400 | Median Reward: 36.52 | Max Reward: 49.27
Iteration: 649 | Episodes: 39500 | Median Reward: 35.66 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.8         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 650           |
|    time_elapsed         | 11813         |
|    total_timesteps      | 3993600       |
| train/                  |               |
|    approx_kl            | 0.00075064134 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.8         |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 153           |
|    n_updates            | 6490          |
|    policy_gradient_loss | -0.000646     |
|    std                  | 1.54          |
|    value_loss           | 314           |
-------------------------------------------
Iteration: 650 | Episodes: 39600 | Median Reward: 39.23 | Max Reward: 49.27
Iteration: 652 | Episodes: 39700 | Median Reward: 34.13 | Max Reward: 49.27
Iteration: 654 | Episodes: 39800 | Median Reward: 42.15 | Max Reward: 49.27
Iteration: 655 | Episodes: 39900 | Median Reward: 36.15 | Max Reward: 49.27
Iteration: 657 | Episodes: 40000 | Median Reward: 37.71 | Max Reward: 49.27
Iteration: 659 | Episodes: 40100 | Median Reward: 37.71 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.6         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 660           |
|    time_elapsed         | 11993         |
|    total_timesteps      | 4055040       |
| train/                  |               |
|    approx_kl            | 0.00024526566 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94.1         |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 155           |
|    n_updates            | 6590          |
|    policy_gradient_loss | -0.000214     |
|    std                  | 1.55          |
|    value_loss           | 333           |
-------------------------------------------
Iteration: 660 | Episodes: 40200 | Median Reward: 36.81 | Max Reward: 49.27
Iteration: 662 | Episodes: 40300 | Median Reward: 35.61 | Max Reward: 49.27
Iteration: 664 | Episodes: 40400 | Median Reward: 36.24 | Max Reward: 49.27
Iteration: 665 | Episodes: 40500 | Median Reward: 37.83 | Max Reward: 49.27
Iteration: 667 | Episodes: 40600 | Median Reward: 38.19 | Max Reward: 49.27
Iteration: 668 | Episodes: 40700 | Median Reward: 35.59 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.5        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 670          |
|    time_elapsed         | 12181        |
|    total_timesteps      | 4116480      |
| train/                  |              |
|    approx_kl            | 0.0012753778 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.4          |
|    entropy_loss         | -94.5        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 160          |
|    n_updates            | 6690         |
|    policy_gradient_loss | -0.00173     |
|    std                  | 1.57         |
|    value_loss           | 342          |
------------------------------------------
Iteration: 670 | Episodes: 40800 | Median Reward: 38.81 | Max Reward: 49.27
Iteration: 672 | Episodes: 40900 | Median Reward: 40.99 | Max Reward: 49.27
Iteration: 673 | Episodes: 41000 | Median Reward: 37.65 | Max Reward: 49.27
Iteration: 675 | Episodes: 41100 | Median Reward: 37.94 | Max Reward: 49.27
Iteration: 677 | Episodes: 41200 | Median Reward: 38.62 | Max Reward: 49.27
Iteration: 678 | Episodes: 41300 | Median Reward: 38.53 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.4         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 680           |
|    time_elapsed         | 12359         |
|    total_timesteps      | 4177920       |
| train/                  |               |
|    approx_kl            | 5.7877056e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94.7         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 183           |
|    n_updates            | 6790          |
|    policy_gradient_loss | -0.000331     |
|    std                  | 1.59          |
|    value_loss           | 373           |
-------------------------------------------
Iteration: 680 | Episodes: 41400 | Median Reward: 39.25 | Max Reward: 49.27
Iteration: 682 | Episodes: 41500 | Median Reward: 42.89 | Max Reward: 49.27
Iteration: 683 | Episodes: 41600 | Median Reward: 37.32 | Max Reward: 49.27
Iteration: 685 | Episodes: 41700 | Median Reward: 35.78 | Max Reward: 49.27
Iteration: 687 | Episodes: 41800 | Median Reward: 35.65 | Max Reward: 49.27
Iteration: 688 | Episodes: 41900 | Median Reward: 37.65 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.6         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 690           |
|    time_elapsed         | 12538         |
|    total_timesteps      | 4239360       |
| train/                  |               |
|    approx_kl            | 0.00092684344 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95           |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 187           |
|    n_updates            | 6890          |
|    policy_gradient_loss | -0.000274     |
|    std                  | 1.61          |
|    value_loss           | 358           |
-------------------------------------------
Iteration: 690 | Episodes: 42000 | Median Reward: 34.95 | Max Reward: 49.27
Iteration: 692 | Episodes: 42100 | Median Reward: 40.13 | Max Reward: 49.27
Iteration: 693 | Episodes: 42200 | Median Reward: 38.04 | Max Reward: 49.27
Iteration: 695 | Episodes: 42300 | Median Reward: 41.15 | Max Reward: 49.27
Iteration: 696 | Episodes: 42400 | Median Reward: 42.94 | Max Reward: 49.27
Iteration: 698 | Episodes: 42500 | Median Reward: 39.64 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.9        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 700          |
|    time_elapsed         | 12730        |
|    total_timesteps      | 4300800      |
| train/                  |              |
|    approx_kl            | 0.0012900915 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -95.2        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 160          |
|    n_updates            | 6990         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.63         |
|    value_loss           | 337          |
------------------------------------------
Iteration: 700 | Episodes: 42600 | Median Reward: 40.99 | Max Reward: 49.27
Iteration: 701 | Episodes: 42700 | Median Reward: 39.16 | Max Reward: 49.27
Iteration: 703 | Episodes: 42800 | Median Reward: 36.25 | Max Reward: 49.27
Iteration: 705 | Episodes: 42900 | Median Reward: 36.78 | Max Reward: 49.27
Iteration: 706 | Episodes: 43000 | Median Reward: 37.31 | Max Reward: 49.27
Iteration: 708 | Episodes: 43100 | Median Reward: 35.76 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.6        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 710          |
|    time_elapsed         | 12909        |
|    total_timesteps      | 4362240      |
| train/                  |              |
|    approx_kl            | 0.0008895105 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -95.6        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 189          |
|    n_updates            | 7090         |
|    policy_gradient_loss | -0.000695    |
|    std                  | 1.64         |
|    value_loss           | 347          |
------------------------------------------
Iteration: 710 | Episodes: 43200 | Median Reward: 36.76 | Max Reward: 49.27
Iteration: 711 | Episodes: 43300 | Median Reward: 37.05 | Max Reward: 49.27
Iteration: 713 | Episodes: 43400 | Median Reward: 36.00 | Max Reward: 49.27
Iteration: 714 | Episodes: 43500 | Median Reward: 39.68 | Max Reward: 49.27
Iteration: 716 | Episodes: 43600 | Median Reward: 36.98 | Max Reward: 49.27
Iteration: 718 | Episodes: 43700 | Median Reward: 36.18 | Max Reward: 49.27
Iteration: 719 | Episodes: 43800 | Median Reward: 39.88 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.9         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 720           |
|    time_elapsed         | 13100         |
|    total_timesteps      | 4423680       |
| train/                  |               |
|    approx_kl            | 1.3491903e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95.9         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 176           |
|    n_updates            | 7190          |
|    policy_gradient_loss | 2.85e-06      |
|    std                  | 1.66          |
|    value_loss           | 360           |
-------------------------------------------
Iteration: 721 | Episodes: 43900 | Median Reward: 33.15 | Max Reward: 49.27
Iteration: 723 | Episodes: 44000 | Median Reward: 36.84 | Max Reward: 49.27
Iteration: 724 | Episodes: 44100 | Median Reward: 34.44 | Max Reward: 49.27
Iteration: 726 | Episodes: 44200 | Median Reward: 41.39 | Max Reward: 49.27
Iteration: 728 | Episodes: 44300 | Median Reward: 35.43 | Max Reward: 49.27
Iteration: 729 | Episodes: 44400 | Median Reward: 38.29 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.2        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 730          |
|    time_elapsed         | 13280        |
|    total_timesteps      | 4485120      |
| train/                  |              |
|    approx_kl            | 0.0005113718 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -96.1        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 187          |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.000652    |
|    std                  | 1.67         |
|    value_loss           | 346          |
------------------------------------------
Iteration: 731 | Episodes: 44500 | Median Reward: 39.36 | Max Reward: 49.27
Iteration: 733 | Episodes: 44600 | Median Reward: 39.44 | Max Reward: 49.27
Iteration: 734 | Episodes: 44700 | Median Reward: 39.60 | Max Reward: 49.27
Iteration: 736 | Episodes: 44800 | Median Reward: 37.36 | Max Reward: 49.27
Iteration: 738 | Episodes: 44900 | Median Reward: 39.17 | Max Reward: 49.27
Iteration: 739 | Episodes: 45000 | Median Reward: 38.22 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.4         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 740           |
|    time_elapsed         | 13458         |
|    total_timesteps      | 4546560       |
| train/                  |               |
|    approx_kl            | 1.7021826e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -96.4         |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 162           |
|    n_updates            | 7390          |
|    policy_gradient_loss | -2.68e-05     |
|    std                  | 1.69          |
|    value_loss           | 375           |
-------------------------------------------
Iteration: 741 | Episodes: 45100 | Median Reward: 35.40 | Max Reward: 49.27
Iteration: 742 | Episodes: 45200 | Median Reward: 38.07 | Max Reward: 49.27
Iteration: 744 | Episodes: 45300 | Median Reward: 37.90 | Max Reward: 49.27
Iteration: 746 | Episodes: 45400 | Median Reward: 37.53 | Max Reward: 49.27
Iteration: 747 | Episodes: 45500 | Median Reward: 37.28 | Max Reward: 49.27
Iteration: 749 | Episodes: 45600 | Median Reward: 41.01 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.8        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 750          |
|    time_elapsed         | 13651        |
|    total_timesteps      | 4608000      |
| train/                  |              |
|    approx_kl            | 0.0011496737 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -96.7        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 171          |
|    n_updates            | 7490         |
|    policy_gradient_loss | -0.000688    |
|    std                  | 1.72         |
|    value_loss           | 362          |
------------------------------------------
Iteration: 751 | Episodes: 45700 | Median Reward: 38.32 | Max Reward: 49.27
Iteration: 752 | Episodes: 45800 | Median Reward: 36.19 | Max Reward: 49.27
Iteration: 754 | Episodes: 45900 | Median Reward: 38.30 | Max Reward: 49.27
Iteration: 756 | Episodes: 46000 | Median Reward: 40.34 | Max Reward: 49.27
Iteration: 757 | Episodes: 46100 | Median Reward: 36.10 | Max Reward: 49.27
Iteration: 759 | Episodes: 46200 | Median Reward: 37.99 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.7         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 760           |
|    time_elapsed         | 13832         |
|    total_timesteps      | 4669440       |
| train/                  |               |
|    approx_kl            | 0.00048221316 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -97           |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 174           |
|    n_updates            | 7590          |
|    policy_gradient_loss | -0.0004       |
|    std                  | 1.73          |
|    value_loss           | 330           |
-------------------------------------------
Iteration: 761 | Episodes: 46300 | Median Reward: 41.31 | Max Reward: 49.27
Iteration: 762 | Episodes: 46400 | Median Reward: 36.13 | Max Reward: 49.27
Iteration: 764 | Episodes: 46500 | Median Reward: 39.38 | Max Reward: 49.27
Iteration: 765 | Episodes: 46600 | Median Reward: 34.77 | Max Reward: 49.27
Iteration: 767 | Episodes: 46700 | Median Reward: 39.87 | Max Reward: 49.27
Iteration: 769 | Episodes: 46800 | Median Reward: 38.91 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.6         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 770           |
|    time_elapsed         | 14013         |
|    total_timesteps      | 4730880       |
| train/                  |               |
|    approx_kl            | 0.00017246392 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -97.2         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 181           |
|    n_updates            | 7690          |
|    policy_gradient_loss | -6.41e-06     |
|    std                  | 1.75          |
|    value_loss           | 331           |
-------------------------------------------
Iteration: 770 | Episodes: 46900 | Median Reward: 38.90 | Max Reward: 49.27
Iteration: 772 | Episodes: 47000 | Median Reward: 35.71 | Max Reward: 49.27
Iteration: 774 | Episodes: 47100 | Median Reward: 35.61 | Max Reward: 49.27
Iteration: 775 | Episodes: 47200 | Median Reward: 35.28 | Max Reward: 49.27
Iteration: 777 | Episodes: 47300 | Median Reward: 41.55 | Max Reward: 49.27
Iteration: 779 | Episodes: 47400 | Median Reward: 37.72 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.7         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 780           |
|    time_elapsed         | 14203         |
|    total_timesteps      | 4792320       |
| train/                  |               |
|    approx_kl            | 0.00018752534 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -97.5         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 173           |
|    n_updates            | 7790          |
|    policy_gradient_loss | -0.000225     |
|    std                  | 1.77          |
|    value_loss           | 333           |
-------------------------------------------
Iteration: 780 | Episodes: 47500 | Median Reward: 42.35 | Max Reward: 49.27
Iteration: 782 | Episodes: 47600 | Median Reward: 39.46 | Max Reward: 49.27
Iteration: 784 | Episodes: 47700 | Median Reward: 38.56 | Max Reward: 49.27
Iteration: 785 | Episodes: 47800 | Median Reward: 40.13 | Max Reward: 49.27
Iteration: 787 | Episodes: 47900 | Median Reward: 41.48 | Max Reward: 49.27
Iteration: 788 | Episodes: 48000 | Median Reward: 38.75 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.7        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 790          |
|    time_elapsed         | 14383        |
|    total_timesteps      | 4853760      |
| train/                  |              |
|    approx_kl            | 0.0013901272 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.7        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 166          |
|    n_updates            | 7890         |
|    policy_gradient_loss | -0.000815    |
|    std                  | 1.78         |
|    value_loss           | 333          |
------------------------------------------
Iteration: 790 | Episodes: 48100 | Median Reward: 38.78 | Max Reward: 49.27
Iteration: 792 | Episodes: 48200 | Median Reward: 38.28 | Max Reward: 49.27
Iteration: 793 | Episodes: 48300 | Median Reward: 39.41 | Max Reward: 49.27
Iteration: 795 | Episodes: 48400 | Median Reward: 36.26 | Max Reward: 49.27
Iteration: 797 | Episodes: 48500 | Median Reward: 38.63 | Max Reward: 49.27
Iteration: 798 | Episodes: 48600 | Median Reward: 39.14 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.9         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 800           |
|    time_elapsed         | 14571         |
|    total_timesteps      | 4915200       |
| train/                  |               |
|    approx_kl            | 0.00019916851 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -98           |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 186           |
|    n_updates            | 7990          |
|    policy_gradient_loss | -0.000238     |
|    std                  | 1.81          |
|    value_loss           | 366           |
-------------------------------------------
Iteration: 800 | Episodes: 48700 | Median Reward: 40.53 | Max Reward: 49.27
Iteration: 802 | Episodes: 48800 | Median Reward: 41.20 | Max Reward: 49.27
Iteration: 803 | Episodes: 48900 | Median Reward: 41.75 | Max Reward: 49.27
Iteration: 805 | Episodes: 49000 | Median Reward: 38.67 | Max Reward: 49.27
Iteration: 807 | Episodes: 49100 | Median Reward: 39.55 | Max Reward: 49.27
Iteration: 808 | Episodes: 49200 | Median Reward: 40.09 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.9         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 810           |
|    time_elapsed         | 14752         |
|    total_timesteps      | 4976640       |
| train/                  |               |
|    approx_kl            | 0.00013593781 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -98.1         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 187           |
|    n_updates            | 8090          |
|    policy_gradient_loss | -0.00025      |
|    std                  | 1.82          |
|    value_loss           | 383           |
-------------------------------------------
Iteration: 810 | Episodes: 49300 | Median Reward: 39.53 | Max Reward: 49.27
Iteration: 811 | Episodes: 49400 | Median Reward: 39.45 | Max Reward: 49.27
Iteration: 813 | Episodes: 49500 | Median Reward: 38.82 | Max Reward: 49.27
Iteration: 815 | Episodes: 49600 | Median Reward: 39.72 | Max Reward: 49.27
Iteration: 816 | Episodes: 49700 | Median Reward: 40.84 | Max Reward: 49.27
Iteration: 818 | Episodes: 49800 | Median Reward: 37.52 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64          |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 820          |
|    time_elapsed         | 14932        |
|    total_timesteps      | 5038080      |
| train/                  |              |
|    approx_kl            | 0.0030217846 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -98.4        |
|    explained_variance   | 0            |
|    learning_rate        | 0.0001       |
|    loss                 | 150          |
|    n_updates            | 8190         |
|    policy_gradient_loss | -0.00215     |
|    std                  | 1.85         |
|    value_loss           | 323          |
------------------------------------------
Iteration: 820 | Episodes: 49900 | Median Reward: 37.94 | Max Reward: 49.27
Iteration: 821 | Episodes: 50000 | Median Reward: 39.44 | Max Reward: 49.27
Iteration: 823 | Episodes: 50100 | Median Reward: 39.67 | Max Reward: 49.27
Iteration: 825 | Episodes: 50200 | Median Reward: 38.30 | Max Reward: 49.27
Iteration: 826 | Episodes: 50300 | Median Reward: 38.81 | Max Reward: 49.27
Iteration: 828 | Episodes: 50400 | Median Reward: 37.64 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.4         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 830           |
|    time_elapsed         | 15121         |
|    total_timesteps      | 5099520       |
| train/                  |               |
|    approx_kl            | 0.00010775767 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -98.6         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 152           |
|    n_updates            | 8290          |
|    policy_gradient_loss | 9.5e-06       |
|    std                  | 1.87          |
|    value_loss           | 310           |
-------------------------------------------
Iteration: 830 | Episodes: 50500 | Median Reward: 38.47 | Max Reward: 49.27
Iteration: 831 | Episodes: 50600 | Median Reward: 39.70 | Max Reward: 49.27
Iteration: 833 | Episodes: 50700 | Median Reward: 36.60 | Max Reward: 49.27
Iteration: 834 | Episodes: 50800 | Median Reward: 38.73 | Max Reward: 49.27
Iteration: 836 | Episodes: 50900 | Median Reward: 39.40 | Max Reward: 49.27
Iteration: 838 | Episodes: 51000 | Median Reward: 35.34 | Max Reward: 49.27
Iteration: 839 | Episodes: 51100 | Median Reward: 36.87 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.2         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 840           |
|    time_elapsed         | 15300         |
|    total_timesteps      | 5160960       |
| train/                  |               |
|    approx_kl            | 0.00018192339 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -98.9         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 143           |
|    n_updates            | 8390          |
|    policy_gradient_loss | 1.05e-05      |
|    std                  | 1.89          |
|    value_loss           | 311           |
-------------------------------------------
Iteration: 841 | Episodes: 51200 | Median Reward: 40.01 | Max Reward: 49.27
Iteration: 843 | Episodes: 51300 | Median Reward: 37.96 | Max Reward: 49.27
Iteration: 844 | Episodes: 51400 | Median Reward: 40.04 | Max Reward: 49.27
Iteration: 846 | Episodes: 51500 | Median Reward: 36.89 | Max Reward: 49.27
Iteration: 848 | Episodes: 51600 | Median Reward: 38.39 | Max Reward: 49.27
Iteration: 849 | Episodes: 51700 | Median Reward: 38.53 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.5        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 850          |
|    time_elapsed         | 15481        |
|    total_timesteps      | 5222400      |
| train/                  |              |
|    approx_kl            | 0.0019217329 |
|    clip_fraction        | 0.0012       |
|    clip_range           | 0.4          |
|    entropy_loss         | -99.2        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 129          |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 1.92         |
|    value_loss           | 320          |
------------------------------------------
Iteration: 851 | Episodes: 51800 | Median Reward: 39.35 | Max Reward: 49.27
Iteration: 853 | Episodes: 51900 | Median Reward: 38.08 | Max Reward: 49.27
Iteration: 854 | Episodes: 52000 | Median Reward: 38.83 | Max Reward: 49.27
Iteration: 856 | Episodes: 52100 | Median Reward: 40.62 | Max Reward: 49.27
Iteration: 858 | Episodes: 52200 | Median Reward: 40.62 | Max Reward: 49.27
Iteration: 859 | Episodes: 52300 | Median Reward: 41.63 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.8         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 860           |
|    time_elapsed         | 15671         |
|    total_timesteps      | 5283840       |
| train/                  |               |
|    approx_kl            | 5.3614407e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -99.6         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 159           |
|    n_updates            | 8590          |
|    policy_gradient_loss | 3.21e-05      |
|    std                  | 1.94          |
|    value_loss           | 373           |
-------------------------------------------
Iteration: 861 | Episodes: 52400 | Median Reward: 38.89 | Max Reward: 49.27
Iteration: 862 | Episodes: 52500 | Median Reward: 42.14 | Max Reward: 49.27
Iteration: 864 | Episodes: 52600 | Median Reward: 39.22 | Max Reward: 49.27
Iteration: 866 | Episodes: 52700 | Median Reward: 39.11 | Max Reward: 49.27
Iteration: 867 | Episodes: 52800 | Median Reward: 40.56 | Max Reward: 49.27
Iteration: 869 | Episodes: 52900 | Median Reward: 37.65 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.2         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 870           |
|    time_elapsed         | 15849         |
|    total_timesteps      | 5345280       |
| train/                  |               |
|    approx_kl            | 4.6640802e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -99.8         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 135           |
|    n_updates            | 8690          |
|    policy_gradient_loss | -0.000115     |
|    std                  | 1.96          |
|    value_loss           | 281           |
-------------------------------------------
Iteration: 871 | Episodes: 53000 | Median Reward: 39.02 | Max Reward: 49.27
Iteration: 872 | Episodes: 53100 | Median Reward: 41.32 | Max Reward: 49.27
Iteration: 874 | Episodes: 53200 | Median Reward: 40.07 | Max Reward: 49.27
Iteration: 876 | Episodes: 53300 | Median Reward: 38.01 | Max Reward: 49.27
Iteration: 877 | Episodes: 53400 | Median Reward: 39.94 | Max Reward: 49.27
Iteration: 879 | Episodes: 53500 | Median Reward: 37.45 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -79.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 880       |
|    time_elapsed         | 16040     |
|    total_timesteps      | 5406720   |
| train/                  |           |
|    approx_kl            | 25.265125 |
|    clip_fraction        | 0.278     |
|    clip_range           | 0.4       |
|    entropy_loss         | -97.2     |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 142       |
|    n_updates            | 8790      |
|    policy_gradient_loss | 0.0206    |
|    std                  | 1.98      |
|    value_loss           | 322       |
---------------------------------------
Iteration: 881 | Episodes: 53600 | Median Reward: -3.73 | Max Reward: 49.27
Iteration: 882 | Episodes: 53700 | Median Reward: -5.75 | Max Reward: 49.27
Iteration: 884 | Episodes: 53800 | Median Reward: 0.93 | Max Reward: 49.27
Iteration: 885 | Episodes: 53900 | Median Reward: 30.43 | Max Reward: 49.27
Iteration: 887 | Episodes: 54000 | Median Reward: 27.71 | Max Reward: 49.27
Iteration: 889 | Episodes: 54100 | Median Reward: -4.13 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -105      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 890       |
|    time_elapsed         | 16214     |
|    total_timesteps      | 5468160   |
| train/                  |           |
|    approx_kl            | 11.169455 |
|    clip_fraction        | 0.208     |
|    clip_range           | 0.4       |
|    entropy_loss         | -92.2     |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 124       |
|    n_updates            | 8890      |
|    policy_gradient_loss | 0.0287    |
|    std                  | 1.99      |
|    value_loss           | 251       |
---------------------------------------
Iteration: 890 | Episodes: 54200 | Median Reward: -24.13 | Max Reward: 49.27
Iteration: 892 | Episodes: 54300 | Median Reward: -17.52 | Max Reward: 49.27
Iteration: 894 | Episodes: 54400 | Median Reward: -16.00 | Max Reward: 49.27
Iteration: 895 | Episodes: 54500 | Median Reward: -14.58 | Max Reward: 49.27
Iteration: 897 | Episodes: 54600 | Median Reward: -14.54 | Max Reward: 49.27
Iteration: 899 | Episodes: 54700 | Median Reward: -10.93 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -113        |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 900         |
|    time_elapsed         | 16393       |
|    total_timesteps      | 5529600     |
| train/                  |             |
|    approx_kl            | 0.021588715 |
|    clip_fraction        | 0.0652      |
|    clip_range           | 0.4         |
|    entropy_loss         | -89         |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 75.7        |
|    n_updates            | 8990        |
|    policy_gradient_loss | 0.00162     |
|    std                  | 1.99        |
|    value_loss           | 124         |
-----------------------------------------
Iteration: 900 | Episodes: 54800 | Median Reward: -19.89 | Max Reward: 49.27
Iteration: 902 | Episodes: 54900 | Median Reward: -13.54 | Max Reward: 49.27
Iteration: 904 | Episodes: 55000 | Median Reward: -31.54 | Max Reward: 49.27
Iteration: 905 | Episodes: 55100 | Median Reward: -18.56 | Max Reward: 49.27
Iteration: 907 | Episodes: 55200 | Median Reward: -21.14 | Max Reward: 49.27
Iteration: 908 | Episodes: 55300 | Median Reward: -15.64 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -121       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 910        |
|    time_elapsed         | 16582      |
|    total_timesteps      | 5591040    |
| train/                  |            |
|    approx_kl            | 0.06811643 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.4        |
|    entropy_loss         | -91        |
|    explained_variance   | -2.38e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 43.8       |
|    n_updates            | 9090       |
|    policy_gradient_loss | -0.0282    |
|    std                  | 1.99       |
|    value_loss           | 107        |
----------------------------------------
Iteration: 910 | Episodes: 55400 | Median Reward: -20.33 | Max Reward: 49.27
Iteration: 912 | Episodes: 55500 | Median Reward: -18.85 | Max Reward: 49.27
Iteration: 913 | Episodes: 55600 | Median Reward: -16.26 | Max Reward: 49.27
Iteration: 915 | Episodes: 55700 | Median Reward: -19.91 | Max Reward: 49.27
Iteration: 917 | Episodes: 55800 | Median Reward: -15.49 | Max Reward: 49.27
Iteration: 918 | Episodes: 55900 | Median Reward: -19.46 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -116         |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 920          |
|    time_elapsed         | 16761        |
|    total_timesteps      | 5652480      |
| train/                  |              |
|    approx_kl            | 0.0071971323 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.4          |
|    entropy_loss         | -85.8        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 41.1         |
|    n_updates            | 9190         |
|    policy_gradient_loss | 0.000178     |
|    std                  | 1.99         |
|    value_loss           | 92.4         |
------------------------------------------
Iteration: 920 | Episodes: 56000 | Median Reward: -15.73 | Max Reward: 49.27
Iteration: 922 | Episodes: 56100 | Median Reward: -8.00 | Max Reward: 49.27
Iteration: 923 | Episodes: 56200 | Median Reward: -4.95 | Max Reward: 49.27
Iteration: 925 | Episodes: 56300 | Median Reward: -3.07 | Max Reward: 49.27
Iteration: 927 | Episodes: 56400 | Median Reward: 3.73 | Max Reward: 49.27
Iteration: 928 | Episodes: 56500 | Median Reward: -0.58 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -93.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 930       |
|    time_elapsed         | 16941     |
|    total_timesteps      | 5713920   |
| train/                  |           |
|    approx_kl            | 0.0639154 |
|    clip_fraction        | 0.0261    |
|    clip_range           | 0.4       |
|    entropy_loss         | -84.1     |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 69.6      |
|    n_updates            | 9290      |
|    policy_gradient_loss | -0.0109   |
|    std                  | 1.99      |
|    value_loss           | 147       |
---------------------------------------
Iteration: 930 | Episodes: 56600 | Median Reward: 5.62 | Max Reward: 49.27
Iteration: 931 | Episodes: 56700 | Median Reward: -3.01 | Max Reward: 49.27
Iteration: 933 | Episodes: 56800 | Median Reward: 1.67 | Max Reward: 49.27
Iteration: 935 | Episodes: 56900 | Median Reward: 17.81 | Max Reward: 49.27
Iteration: 936 | Episodes: 57000 | Median Reward: -0.67 | Max Reward: 49.27
Iteration: 938 | Episodes: 57100 | Median Reward: 22.49 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63         |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 940         |
|    time_elapsed         | 17132       |
|    total_timesteps      | 5775360     |
| train/                  |             |
|    approx_kl            | 0.010744501 |
|    clip_fraction        | 0.00842     |
|    clip_range           | 0.4         |
|    entropy_loss         | -97.7       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 214         |
|    n_updates            | 9390        |
|    policy_gradient_loss | -0.00339    |
|    std                  | 2           |
|    value_loss           | 381         |
-----------------------------------------
Iteration: 940 | Episodes: 57200 | Median Reward: 36.88 | Max Reward: 49.27
Iteration: 941 | Episodes: 57300 | Median Reward: 38.88 | Max Reward: 49.27
Iteration: 943 | Episodes: 57400 | Median Reward: 38.87 | Max Reward: 49.27
Iteration: 945 | Episodes: 57500 | Median Reward: 39.52 | Max Reward: 49.27
Iteration: 946 | Episodes: 57600 | Median Reward: 39.04 | Max Reward: 49.27
Iteration: 948 | Episodes: 57700 | Median Reward: 36.97 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.9         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 950           |
|    time_elapsed         | 17308         |
|    total_timesteps      | 5836800       |
| train/                  |               |
|    approx_kl            | 0.00081193267 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -99.3         |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 181           |
|    n_updates            | 9490          |
|    policy_gradient_loss | -0.00262      |
|    std                  | 2             |
|    value_loss           | 376           |
-------------------------------------------
Iteration: 950 | Episodes: 57800 | Median Reward: 38.76 | Max Reward: 49.27
Iteration: 951 | Episodes: 57900 | Median Reward: 39.12 | Max Reward: 49.27
Iteration: 953 | Episodes: 58000 | Median Reward: 36.63 | Max Reward: 49.27
Iteration: 954 | Episodes: 58100 | Median Reward: 39.59 | Max Reward: 49.27
Iteration: 956 | Episodes: 58200 | Median Reward: 37.18 | Max Reward: 49.27
Iteration: 958 | Episodes: 58300 | Median Reward: 35.80 | Max Reward: 49.27
Iteration: 959 | Episodes: 58400 | Median Reward: 39.27 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.1       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 960         |
|    time_elapsed         | 17490       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 8.99506e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -99.5       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 187         |
|    n_updates            | 9590        |
|    policy_gradient_loss | -0.000893   |
|    std                  | 2.01        |
|    value_loss           | 359         |
-----------------------------------------
Iteration: 961 | Episodes: 58500 | Median Reward: 42.79 | Max Reward: 49.27
Iteration: 963 | Episodes: 58600 | Median Reward: 38.33 | Max Reward: 49.27
Iteration: 964 | Episodes: 58700 | Median Reward: 39.88 | Max Reward: 49.27
Iteration: 966 | Episodes: 58800 | Median Reward: 43.59 | Max Reward: 49.27
Iteration: 968 | Episodes: 58900 | Median Reward: 42.81 | Max Reward: 49.27
Iteration: 969 | Episodes: 59000 | Median Reward: 38.32 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.8       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 970         |
|    time_elapsed         | 17681       |
|    total_timesteps      | 5959680     |
| train/                  |             |
|    approx_kl            | 0.012688318 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.4         |
|    entropy_loss         | -99.8       |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 176         |
|    n_updates            | 9690        |
|    policy_gradient_loss | 5.78e-05    |
|    std                  | 2.02        |
|    value_loss           | 357         |
-----------------------------------------
Iteration: 971 | Episodes: 59100 | Median Reward: 38.64 | Max Reward: 49.27
Iteration: 973 | Episodes: 59200 | Median Reward: 42.56 | Max Reward: 49.27
Iteration: 974 | Episodes: 59300 | Median Reward: 42.40 | Max Reward: 49.27
Iteration: 976 | Episodes: 59400 | Median Reward: 36.96 | Max Reward: 49.27
Iteration: 978 | Episodes: 59500 | Median Reward: 35.99 | Max Reward: 49.27
Iteration: 979 | Episodes: 59600 | Median Reward: 39.45 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.4        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 980          |
|    time_elapsed         | 17860        |
|    total_timesteps      | 6021120      |
| train/                  |              |
|    approx_kl            | 0.0016577367 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -100         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0001       |
|    loss                 | 174          |
|    n_updates            | 9790         |
|    policy_gradient_loss | -0.000848    |
|    std                  | 2.04         |
|    value_loss           | 368          |
------------------------------------------
Iteration: 981 | Episodes: 59700 | Median Reward: 36.61 | Max Reward: 49.27
Iteration: 982 | Episodes: 59800 | Median Reward: 41.49 | Max Reward: 49.27
Iteration: 984 | Episodes: 59900 | Median Reward: 36.30 | Max Reward: 49.27
Iteration: 986 | Episodes: 60000 | Median Reward: 39.33 | Max Reward: 49.27
Iteration: 987 | Episodes: 60100 | Median Reward: 37.89 | Max Reward: 49.27
Iteration: 989 | Episodes: 60200 | Median Reward: 40.49 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.6         |
| time/                   |               |
|    fps                  | 336           |
|    iterations           | 990           |
|    time_elapsed         | 18050         |
|    total_timesteps      | 6082560       |
| train/                  |               |
|    approx_kl            | 0.00059090613 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -101          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 180           |
|    n_updates            | 9890          |
|    policy_gradient_loss | -0.000868     |
|    std                  | 2.06          |
|    value_loss           | 363           |
-------------------------------------------
Iteration: 991 | Episodes: 60300 | Median Reward: 37.59 | Max Reward: 49.27
Iteration: 992 | Episodes: 60400 | Median Reward: 40.84 | Max Reward: 49.27
Iteration: 994 | Episodes: 60500 | Median Reward: 35.99 | Max Reward: 49.27
Iteration: 996 | Episodes: 60600 | Median Reward: 37.60 | Max Reward: 49.27
Iteration: 997 | Episodes: 60700 | Median Reward: 38.62 | Max Reward: 49.27
Iteration: 999 | Episodes: 60800 | Median Reward: 37.03 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.4       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 1000        |
|    time_elapsed         | 18231       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.000734809 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -101        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 154         |
|    n_updates            | 9990        |
|    policy_gradient_loss | -0.000369   |
|    std                  | 2.09        |
|    value_loss           | 339         |
-----------------------------------------
Iteration: 1001 | Episodes: 60900 | Median Reward: 39.76 | Max Reward: 49.27
Iteration: 1002 | Episodes: 61000 | Median Reward: 38.36 | Max Reward: 49.27
Iteration: 1004 | Episodes: 61100 | Median Reward: 38.93 | Max Reward: 49.27
Iteration: 1005 | Episodes: 61200 | Median Reward: 39.77 | Max Reward: 49.27
Iteration: 1007 | Episodes: 61300 | Median Reward: 37.63 | Max Reward: 49.27
Iteration: 1009 | Episodes: 61400 | Median Reward: 39.67 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60           |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1010          |
|    time_elapsed         | 18409         |
|    total_timesteps      | 6205440       |
| train/                  |               |
|    approx_kl            | 0.00057900057 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 178           |
|    n_updates            | 10090         |
|    policy_gradient_loss | -0.000289     |
|    std                  | 2.13          |
|    value_loss           | 356           |
-------------------------------------------
Iteration: 1010 | Episodes: 61500 | Median Reward: 35.96 | Max Reward: 49.27
Iteration: 1012 | Episodes: 61600 | Median Reward: 37.31 | Max Reward: 49.27
Iteration: 1014 | Episodes: 61700 | Median Reward: 42.66 | Max Reward: 49.27
Iteration: 1015 | Episodes: 61800 | Median Reward: 38.16 | Max Reward: 49.27
Iteration: 1017 | Episodes: 61900 | Median Reward: 38.50 | Max Reward: 49.27
Iteration: 1019 | Episodes: 62000 | Median Reward: 38.60 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61           |
| time/                   |               |
|    fps                  | 336           |
|    iterations           | 1020          |
|    time_elapsed         | 18600         |
|    total_timesteps      | 6266880       |
| train/                  |               |
|    approx_kl            | 0.00014987051 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 150           |
|    n_updates            | 10190         |
|    policy_gradient_loss | -0.000339     |
|    std                  | 2.15          |
|    value_loss           | 345           |
-------------------------------------------
Iteration: 1020 | Episodes: 62100 | Median Reward: 39.10 | Max Reward: 49.27
Iteration: 1022 | Episodes: 62200 | Median Reward: 37.52 | Max Reward: 49.27
Iteration: 1024 | Episodes: 62300 | Median Reward: 38.14 | Max Reward: 49.27
Iteration: 1025 | Episodes: 62400 | Median Reward: 35.16 | Max Reward: 49.27
Iteration: 1027 | Episodes: 62500 | Median Reward: 42.80 | Max Reward: 49.27
Iteration: 1028 | Episodes: 62600 | Median Reward: 42.57 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.7        |
| time/                   |              |
|    fps                  | 336          |
|    iterations           | 1030         |
|    time_elapsed         | 18783        |
|    total_timesteps      | 6328320      |
| train/                  |              |
|    approx_kl            | 0.0005629155 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0001       |
|    loss                 | 204          |
|    n_updates            | 10290        |
|    policy_gradient_loss | 5.45e-05     |
|    std                  | 2.18         |
|    value_loss           | 367          |
------------------------------------------
Iteration: 1030 | Episodes: 62700 | Median Reward: 37.06 | Max Reward: 49.27
Iteration: 1032 | Episodes: 62800 | Median Reward: 38.19 | Max Reward: 49.27
Iteration: 1033 | Episodes: 62900 | Median Reward: 37.57 | Max Reward: 49.27
Iteration: 1035 | Episodes: 63000 | Median Reward: 36.36 | Max Reward: 49.27
Iteration: 1037 | Episodes: 63100 | Median Reward: 35.55 | Max Reward: 49.27
Iteration: 1038 | Episodes: 63200 | Median Reward: 38.28 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.1        |
| time/                   |              |
|    fps                  | 336          |
|    iterations           | 1040         |
|    time_elapsed         | 18962        |
|    total_timesteps      | 6389760      |
| train/                  |              |
|    approx_kl            | 0.0009715866 |
|    clip_fraction        | 0.000256     |
|    clip_range           | 0.4          |
|    entropy_loss         | -103         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0001       |
|    loss                 | 162          |
|    n_updates            | 10390        |
|    policy_gradient_loss | -0.00057     |
|    std                  | 2.21         |
|    value_loss           | 336          |
------------------------------------------
Iteration: 1040 | Episodes: 63300 | Median Reward: 38.89 | Max Reward: 49.27
Iteration: 1042 | Episodes: 63400 | Median Reward: 37.95 | Max Reward: 49.27
Iteration: 1043 | Episodes: 63500 | Median Reward: 40.77 | Max Reward: 49.27
Iteration: 1045 | Episodes: 63600 | Median Reward: 40.06 | Max Reward: 49.27
Iteration: 1047 | Episodes: 63700 | Median Reward: 35.98 | Max Reward: 49.27
Iteration: 1048 | Episodes: 63800 | Median Reward: 42.40 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.1        |
| time/                   |              |
|    fps                  | 336          |
|    iterations           | 1050         |
|    time_elapsed         | 19156        |
|    total_timesteps      | 6451200      |
| train/                  |              |
|    approx_kl            | 0.0017137366 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.4          |
|    entropy_loss         | -103         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 171          |
|    n_updates            | 10490        |
|    policy_gradient_loss | -0.00103     |
|    std                  | 2.24         |
|    value_loss           | 338          |
------------------------------------------
Iteration: 1050 | Episodes: 63900 | Median Reward: 39.09 | Max Reward: 49.27
Iteration: 1051 | Episodes: 64000 | Median Reward: 38.44 | Max Reward: 49.27
Iteration: 1053 | Episodes: 64100 | Median Reward: 38.80 | Max Reward: 49.27
Iteration: 1055 | Episodes: 64200 | Median Reward: 39.95 | Max Reward: 49.27
Iteration: 1056 | Episodes: 64300 | Median Reward: 39.18 | Max Reward: 49.27
Iteration: 1058 | Episodes: 64400 | Median Reward: 38.23 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.1        |
| time/                   |              |
|    fps                  | 336          |
|    iterations           | 1060         |
|    time_elapsed         | 19337        |
|    total_timesteps      | 6512640      |
| train/                  |              |
|    approx_kl            | 0.0001734409 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -103         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0001       |
|    loss                 | 155          |
|    n_updates            | 10590        |
|    policy_gradient_loss | -3.94e-05    |
|    std                  | 2.27         |
|    value_loss           | 332          |
------------------------------------------
Iteration: 1060 | Episodes: 64500 | Median Reward: 38.08 | Max Reward: 49.27
Iteration: 1061 | Episodes: 64600 | Median Reward: 38.23 | Max Reward: 49.27
Iteration: 1063 | Episodes: 64700 | Median Reward: 35.98 | Max Reward: 49.27
Iteration: 1065 | Episodes: 64800 | Median Reward: 39.03 | Max Reward: 49.27
Iteration: 1066 | Episodes: 64900 | Median Reward: 36.06 | Max Reward: 49.27
Iteration: 1068 | Episodes: 65000 | Median Reward: 34.98 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.1         |
| time/                   |               |
|    fps                  | 336           |
|    iterations           | 1070          |
|    time_elapsed         | 19518         |
|    total_timesteps      | 6574080       |
| train/                  |               |
|    approx_kl            | 0.00018100129 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -104          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 158           |
|    n_updates            | 10690         |
|    policy_gradient_loss | -0.000285     |
|    std                  | 2.3           |
|    value_loss           | 338           |
-------------------------------------------
Iteration: 1070 | Episodes: 65100 | Median Reward: 38.51 | Max Reward: 49.27
Iteration: 1071 | Episodes: 65200 | Median Reward: 39.11 | Max Reward: 49.27
Iteration: 1073 | Episodes: 65300 | Median Reward: 39.89 | Max Reward: 49.27
Iteration: 1074 | Episodes: 65400 | Median Reward: 42.16 | Max Reward: 49.27
Iteration: 1076 | Episodes: 65500 | Median Reward: 35.59 | Max Reward: 49.27
Iteration: 1078 | Episodes: 65600 | Median Reward: 38.60 | Max Reward: 49.27
Iteration: 1079 | Episodes: 65700 | Median Reward: 39.14 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.7       |
| time/                   |             |
|    fps                  | 336         |
|    iterations           | 1080        |
|    time_elapsed         | 19709       |
|    total_timesteps      | 6635520     |
| train/                  |             |
|    approx_kl            | 0.008644993 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.4         |
|    entropy_loss         | -104        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 134         |
|    n_updates            | 10790       |
|    policy_gradient_loss | 0.00402     |
|    std                  | 2.33        |
|    value_loss           | 307         |
-----------------------------------------
Iteration: 1081 | Episodes: 65800 | Median Reward: 19.26 | Max Reward: 49.27
Iteration: 1083 | Episodes: 65900 | Median Reward: 22.94 | Max Reward: 49.27
Iteration: 1084 | Episodes: 66000 | Median Reward: 20.57 | Max Reward: 49.27
Iteration: 1086 | Episodes: 66100 | Median Reward: 22.09 | Max Reward: 49.27
Iteration: 1088 | Episodes: 66200 | Median Reward: 24.20 | Max Reward: 49.27
Iteration: 1089 | Episodes: 66300 | Median Reward: 24.35 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -73.7      |
| time/                   |            |
|    fps                  | 336        |
|    iterations           | 1090       |
|    time_elapsed         | 19889      |
|    total_timesteps      | 6696960    |
| train/                  |            |
|    approx_kl            | 0.03130602 |
|    clip_fraction        | 0.0662     |
|    clip_range           | 0.4        |
|    entropy_loss         | -94.3      |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 113        |
|    n_updates            | 10890      |
|    policy_gradient_loss | -0.0105    |
|    std                  | 2.33       |
|    value_loss           | 243        |
----------------------------------------
Iteration: 1091 | Episodes: 66400 | Median Reward: 31.65 | Max Reward: 49.27
Iteration: 1093 | Episodes: 66500 | Median Reward: 35.69 | Max Reward: 49.27
Iteration: 1094 | Episodes: 66600 | Median Reward: 37.85 | Max Reward: 49.27
Iteration: 1096 | Episodes: 66700 | Median Reward: 37.45 | Max Reward: 49.27
Iteration: 1098 | Episodes: 66800 | Median Reward: 42.27 | Max Reward: 49.27
Iteration: 1099 | Episodes: 66900 | Median Reward: 38.76 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.3        |
| time/                   |              |
|    fps                  | 336          |
|    iterations           | 1100         |
|    time_elapsed         | 20078        |
|    total_timesteps      | 6758400      |
| train/                  |              |
|    approx_kl            | 0.0005229013 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -104         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0001       |
|    loss                 | 134          |
|    n_updates            | 10990        |
|    policy_gradient_loss | -0.000668    |
|    std                  | 2.35         |
|    value_loss           | 313          |
------------------------------------------
Iteration: 1101 | Episodes: 67000 | Median Reward: 37.37 | Max Reward: 49.27
Iteration: 1102 | Episodes: 67100 | Median Reward: 34.97 | Max Reward: 49.27
Iteration: 1104 | Episodes: 67200 | Median Reward: 40.33 | Max Reward: 49.27
Iteration: 1106 | Episodes: 67300 | Median Reward: 39.37 | Max Reward: 49.27
Iteration: 1107 | Episodes: 67400 | Median Reward: 37.63 | Max Reward: 49.27
Iteration: 1109 | Episodes: 67500 | Median Reward: 31.61 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.3     |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1110      |
|    time_elapsed         | 20258     |
|    total_timesteps      | 6819840   |
| train/                  |           |
|    approx_kl            | 1.7746483 |
|    clip_fraction        | 0.416     |
|    clip_range           | 0.4       |
|    entropy_loss         | -98.2     |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 115       |
|    n_updates            | 11090     |
|    policy_gradient_loss | -0.00308  |
|    std                  | 2.38      |
|    value_loss           | 272       |
---------------------------------------
Iteration: 1111 | Episodes: 67600 | Median Reward: 35.11 | Max Reward: 49.27
Iteration: 1112 | Episodes: 67700 | Median Reward: 34.43 | Max Reward: 49.27
Iteration: 1112 | Episodes: 67800 | Median Reward: 49.09 | Max Reward: 49.27
Iteration: 1114 | Episodes: 67900 | Median Reward: 40.07 | Max Reward: 49.27
Iteration: 1115 | Episodes: 68000 | Median Reward: 37.45 | Max Reward: 49.27
Iteration: 1117 | Episodes: 68100 | Median Reward: 38.44 | Max Reward: 49.27
Iteration: 1119 | Episodes: 68200 | Median Reward: 35.04 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -63.1     |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1120      |
|    time_elapsed         | 20438     |
|    total_timesteps      | 6881280   |
| train/                  |           |
|    approx_kl            | 2.4946623 |
|    clip_fraction        | 0.0834    |
|    clip_range           | 0.4       |
|    entropy_loss         | -102      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 146       |
|    n_updates            | 11190     |
|    policy_gradient_loss | -0.0137   |
|    std                  | 2.4       |
|    value_loss           | 316       |
---------------------------------------
Iteration: 1120 | Episodes: 68300 | Median Reward: 40.03 | Max Reward: 49.27
Iteration: 1122 | Episodes: 68400 | Median Reward: 39.87 | Max Reward: 49.27
Iteration: 1124 | Episodes: 68500 | Median Reward: 38.89 | Max Reward: 49.27
Iteration: 1125 | Episodes: 68600 | Median Reward: 40.52 | Max Reward: 49.27
Iteration: 1127 | Episodes: 68700 | Median Reward: 40.52 | Max Reward: 49.27
Iteration: 1129 | Episodes: 68800 | Median Reward: 38.09 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -61.4     |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1130      |
|    time_elapsed         | 20630     |
|    total_timesteps      | 6942720   |
| train/                  |           |
|    approx_kl            | 0.3791347 |
|    clip_fraction        | 0.0606    |
|    clip_range           | 0.4       |
|    entropy_loss         | -103      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 164       |
|    n_updates            | 11290     |
|    policy_gradient_loss | -0.0179   |
|    std                  | 2.42      |
|    value_loss           | 338       |
---------------------------------------
Iteration: 1130 | Episodes: 68900 | Median Reward: 38.29 | Max Reward: 49.27
Iteration: 1132 | Episodes: 69000 | Median Reward: 37.35 | Max Reward: 49.27
Iteration: 1134 | Episodes: 69100 | Median Reward: 36.39 | Max Reward: 49.27
Iteration: 1135 | Episodes: 69200 | Median Reward: 38.38 | Max Reward: 49.27
Iteration: 1137 | Episodes: 69300 | Median Reward: 37.27 | Max Reward: 49.27
Iteration: 1138 | Episodes: 69400 | Median Reward: 39.32 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -61       |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1140      |
|    time_elapsed         | 20811     |
|    total_timesteps      | 7004160   |
| train/                  |           |
|    approx_kl            | 1.4193413 |
|    clip_fraction        | 0.0709    |
|    clip_range           | 0.4       |
|    entropy_loss         | -103      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 171       |
|    n_updates            | 11390     |
|    policy_gradient_loss | -0.00694  |
|    std                  | 2.44      |
|    value_loss           | 337       |
---------------------------------------
Iteration: 1140 | Episodes: 69500 | Median Reward: 43.01 | Max Reward: 49.27
Iteration: 1142 | Episodes: 69600 | Median Reward: 41.58 | Max Reward: 49.27
Iteration: 1143 | Episodes: 69700 | Median Reward: 34.80 | Max Reward: 49.27
Iteration: 1145 | Episodes: 69800 | Median Reward: 37.79 | Max Reward: 49.27
Iteration: 1147 | Episodes: 69900 | Median Reward: 39.16 | Max Reward: 49.27
Iteration: 1148 | Episodes: 70000 | Median Reward: 37.97 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.2       |
| time/                   |             |
|    fps                  | 336         |
|    iterations           | 1150        |
|    time_elapsed         | 20992       |
|    total_timesteps      | 7065600     |
| train/                  |             |
|    approx_kl            | 0.033615667 |
|    clip_fraction        | 0.0166      |
|    clip_range           | 0.4         |
|    entropy_loss         | -103        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 153         |
|    n_updates            | 11490       |
|    policy_gradient_loss | -0.00731    |
|    std                  | 2.46        |
|    value_loss           | 322         |
-----------------------------------------
Iteration: 1150 | Episodes: 70100 | Median Reward: 37.96 | Max Reward: 49.27
Iteration: 1152 | Episodes: 70200 | Median Reward: 36.77 | Max Reward: 49.27
Iteration: 1153 | Episodes: 70300 | Median Reward: 41.58 | Max Reward: 49.27
Iteration: 1155 | Episodes: 70400 | Median Reward: 39.03 | Max Reward: 49.27
Iteration: 1157 | Episodes: 70500 | Median Reward: 41.00 | Max Reward: 49.27
Iteration: 1158 | Episodes: 70600 | Median Reward: 38.69 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -62.3    |
| time/                   |          |
|    fps                  | 336      |
|    iterations           | 1160     |
|    time_elapsed         | 21183    |
|    total_timesteps      | 7127040  |
| train/                  |          |
|    approx_kl            | 0.827718 |
|    clip_fraction        | 0.0585   |
|    clip_range           | 0.4      |
|    entropy_loss         | -103     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 174      |
|    n_updates            | 11590    |
|    policy_gradient_loss | -0.00734 |
|    std                  | 2.49     |
|    value_loss           | 349      |
--------------------------------------
Iteration: 1160 | Episodes: 70700 | Median Reward: 35.95 | Max Reward: 49.27
Iteration: 1161 | Episodes: 70800 | Median Reward: 36.78 | Max Reward: 49.27
Iteration: 1163 | Episodes: 70900 | Median Reward: 38.61 | Max Reward: 49.27
Iteration: 1165 | Episodes: 71000 | Median Reward: 39.62 | Max Reward: 49.27
Iteration: 1166 | Episodes: 71100 | Median Reward: 38.67 | Max Reward: 49.27
Iteration: 1168 | Episodes: 71200 | Median Reward: 39.31 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -61.3      |
| time/                   |            |
|    fps                  | 336        |
|    iterations           | 1170       |
|    time_elapsed         | 21366      |
|    total_timesteps      | 7188480    |
| train/                  |            |
|    approx_kl            | 0.09344123 |
|    clip_fraction        | 0.0407     |
|    clip_range           | 0.4        |
|    entropy_loss         | -103       |
|    explained_variance   | -2.38e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 192        |
|    n_updates            | 11690      |
|    policy_gradient_loss | 0.00243    |
|    std                  | 2.51       |
|    value_loss           | 385        |
----------------------------------------
Iteration: 1170 | Episodes: 71300 | Median Reward: 38.65 | Max Reward: 49.27
Iteration: 1171 | Episodes: 71400 | Median Reward: 39.22 | Max Reward: 49.27
Iteration: 1173 | Episodes: 71500 | Median Reward: 38.41 | Max Reward: 49.27
Iteration: 1175 | Episodes: 71600 | Median Reward: 40.29 | Max Reward: 49.27
Iteration: 1176 | Episodes: 71700 | Median Reward: 35.54 | Max Reward: 49.27
Iteration: 1178 | Episodes: 71800 | Median Reward: 39.52 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -60.7     |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1180      |
|    time_elapsed         | 21556     |
|    total_timesteps      | 7249920   |
| train/                  |           |
|    approx_kl            | 0.6567103 |
|    clip_fraction        | 0.0476    |
|    clip_range           | 0.4       |
|    entropy_loss         | -103      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 168       |
|    n_updates            | 11790     |
|    policy_gradient_loss | -0.00883  |
|    std                  | 2.54      |
|    value_loss           | 331       |
---------------------------------------
Iteration: 1180 | Episodes: 71900 | Median Reward: 39.07 | Max Reward: 49.27
Iteration: 1181 | Episodes: 72000 | Median Reward: 38.58 | Max Reward: 49.27
Iteration: 1183 | Episodes: 72100 | Median Reward: 39.60 | Max Reward: 49.27
Iteration: 1185 | Episodes: 72200 | Median Reward: 39.43 | Max Reward: 49.27
Iteration: 1186 | Episodes: 72300 | Median Reward: 41.81 | Max Reward: 49.27
Iteration: 1188 | Episodes: 72400 | Median Reward: 33.73 | Max Reward: 49.27
Iteration: 1189 | Episodes: 72500 | Median Reward: 40.95 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -63       |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1190      |
|    time_elapsed         | 21736     |
|    total_timesteps      | 7311360   |
| train/                  |           |
|    approx_kl            | 20.052313 |
|    clip_fraction        | 0.206     |
|    clip_range           | 0.4       |
|    entropy_loss         | -99       |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 163       |
|    n_updates            | 11890     |
|    policy_gradient_loss | -0.0295   |
|    std                  | 2.55      |
|    value_loss           | 386       |
---------------------------------------
Iteration: 1191 | Episodes: 72600 | Median Reward: 30.85 | Max Reward: 49.27
Iteration: 1193 | Episodes: 72700 | Median Reward: 35.08 | Max Reward: 49.27
Iteration: 1194 | Episodes: 72800 | Median Reward: 42.91 | Max Reward: 49.27
Iteration: 1196 | Episodes: 72900 | Median Reward: 42.76 | Max Reward: 49.27
Iteration: 1198 | Episodes: 73000 | Median Reward: 42.10 | Max Reward: 49.27
Iteration: 1199 | Episodes: 73100 | Median Reward: 38.23 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -63.4      |
| time/                   |            |
|    fps                  | 336        |
|    iterations           | 1200       |
|    time_elapsed         | 21912      |
|    total_timesteps      | 7372800    |
| train/                  |            |
|    approx_kl            | 0.81866026 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.4        |
|    entropy_loss         | -91.3      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 170        |
|    n_updates            | 11990      |
|    policy_gradient_loss | 0.032      |
|    std                  | 2.56       |
|    value_loss           | 311        |
----------------------------------------
Iteration: 1201 | Episodes: 73200 | Median Reward: 30.93 | Max Reward: 49.27
Iteration: 1203 | Episodes: 73300 | Median Reward: 22.30 | Max Reward: 49.27
Iteration: 1204 | Episodes: 73400 | Median Reward: 34.30 | Max Reward: 49.27
Iteration: 1206 | Episodes: 73500 | Median Reward: 35.02 | Max Reward: 49.27
Iteration: 1208 | Episodes: 73600 | Median Reward: 24.54 | Max Reward: 49.27
Iteration: 1209 | Episodes: 73700 | Median Reward: 9.75 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -83.1      |
| time/                   |            |
|    fps                  | 336        |
|    iterations           | 1210       |
|    time_elapsed         | 22104      |
|    total_timesteps      | 7434240    |
| train/                  |            |
|    approx_kl            | 0.37132573 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.4        |
|    entropy_loss         | -88.5      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 71.7       |
|    n_updates            | 12090      |
|    policy_gradient_loss | 0.0254     |
|    std                  | 2.57       |
|    value_loss           | 196        |
----------------------------------------
Iteration: 1211 | Episodes: 73800 | Median Reward: 35.44 | Max Reward: 49.27
Iteration: 1212 | Episodes: 73900 | Median Reward: 26.72 | Max Reward: 49.27
Iteration: 1214 | Episodes: 74000 | Median Reward: 31.72 | Max Reward: 49.27
Iteration: 1216 | Episodes: 74100 | Median Reward: 26.73 | Max Reward: 49.27
Iteration: 1217 | Episodes: 74200 | Median Reward: 33.25 | Max Reward: 49.27
Iteration: 1219 | Episodes: 74300 | Median Reward: 20.63 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -83.1      |
| time/                   |            |
|    fps                  | 336        |
|    iterations           | 1220       |
|    time_elapsed         | 22283      |
|    total_timesteps      | 7495680    |
| train/                  |            |
|    approx_kl            | 0.98593456 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.4        |
|    entropy_loss         | -86.5      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 147        |
|    n_updates            | 12190      |
|    policy_gradient_loss | -0.0071    |
|    std                  | 2.57       |
|    value_loss           | 227        |
----------------------------------------
Iteration: 1221 | Episodes: 74400 | Median Reward: 14.69 | Max Reward: 49.27
Iteration: 1222 | Episodes: 74500 | Median Reward: 22.47 | Max Reward: 49.27
Iteration: 1224 | Episodes: 74600 | Median Reward: 30.07 | Max Reward: 49.27
Iteration: 1226 | Episodes: 74700 | Median Reward: 24.66 | Max Reward: 49.27
Iteration: 1227 | Episodes: 74800 | Median Reward: 37.33 | Max Reward: 49.27
Iteration: 1229 | Episodes: 74900 | Median Reward: 34.31 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -71.4    |
| time/                   |          |
|    fps                  | 336      |
|    iterations           | 1230     |
|    time_elapsed         | 22462    |
|    total_timesteps      | 7557120  |
| train/                  |          |
|    approx_kl            | 8.282578 |
|    clip_fraction        | 0.438    |
|    clip_range           | 0.4      |
|    entropy_loss         | -83.9    |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 188      |
|    n_updates            | 12290    |
|    policy_gradient_loss | -0.0107  |
|    std                  | 2.58     |
|    value_loss           | 323      |
--------------------------------------
Iteration: 1231 | Episodes: 75000 | Median Reward: 33.62 | Max Reward: 49.27
Iteration: 1232 | Episodes: 75100 | Median Reward: 40.17 | Max Reward: 49.27
Iteration: 1234 | Episodes: 75200 | Median Reward: 33.92 | Max Reward: 49.27
Iteration: 1235 | Episodes: 75300 | Median Reward: 22.22 | Max Reward: 49.27
Iteration: 1237 | Episodes: 75400 | Median Reward: 30.07 | Max Reward: 49.27
Iteration: 1239 | Episodes: 75500 | Median Reward: 24.51 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -90       |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1240      |
|    time_elapsed         | 22647     |
|    total_timesteps      | 7618560   |
| train/                  |           |
|    approx_kl            | 1.0171059 |
|    clip_fraction        | 0.193     |
|    clip_range           | 0.4       |
|    entropy_loss         | -81.2     |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 113       |
|    n_updates            | 12390     |
|    policy_gradient_loss | 0.00645   |
|    std                  | 2.58      |
|    value_loss           | 280       |
---------------------------------------
Iteration: 1240 | Episodes: 75600 | Median Reward: 1.85 | Max Reward: 49.27
Iteration: 1242 | Episodes: 75700 | Median Reward: -3.77 | Max Reward: 49.27
Iteration: 1244 | Episodes: 75800 | Median Reward: 10.99 | Max Reward: 49.27
Iteration: 1245 | Episodes: 75900 | Median Reward: 24.43 | Max Reward: 49.27
Iteration: 1247 | Episodes: 76000 | Median Reward: 13.42 | Max Reward: 49.27
Iteration: 1249 | Episodes: 76100 | Median Reward: 13.23 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -97.4      |
| time/                   |            |
|    fps                  | 336        |
|    iterations           | 1250       |
|    time_elapsed         | 22822      |
|    total_timesteps      | 7680000    |
| train/                  |            |
|    approx_kl            | 0.15376727 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.4        |
|    entropy_loss         | -80.5      |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 78.6       |
|    n_updates            | 12490      |
|    policy_gradient_loss | -0.0145    |
|    std                  | 2.59       |
|    value_loss           | 192        |
----------------------------------------
Iteration: 1250 | Episodes: 76200 | Median Reward: 1.77 | Max Reward: 49.27
Iteration: 1252 | Episodes: 76300 | Median Reward: 7.67 | Max Reward: 49.27
Iteration: 1254 | Episodes: 76400 | Median Reward: 18.75 | Max Reward: 49.27
Iteration: 1255 | Episodes: 76500 | Median Reward: 18.47 | Max Reward: 49.27
Iteration: 1257 | Episodes: 76600 | Median Reward: 13.73 | Max Reward: 49.27
Iteration: 1258 | Episodes: 76700 | Median Reward: 4.69 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -93.5       |
| time/                   |             |
|    fps                  | 336         |
|    iterations           | 1260        |
|    time_elapsed         | 22998       |
|    total_timesteps      | 7741440     |
| train/                  |             |
|    approx_kl            | 0.056580294 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.4         |
|    entropy_loss         | -80.7       |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 98.1        |
|    n_updates            | 12590       |
|    policy_gradient_loss | -0.0114     |
|    std                  | 2.59        |
|    value_loss           | 204         |
-----------------------------------------
Iteration: 1260 | Episodes: 76800 | Median Reward: 7.28 | Max Reward: 49.27
Iteration: 1262 | Episodes: 76900 | Median Reward: 14.70 | Max Reward: 49.27
Iteration: 1263 | Episodes: 77000 | Median Reward: 24.35 | Max Reward: 49.27
Iteration: 1265 | Episodes: 77100 | Median Reward: 17.53 | Max Reward: 49.27
Iteration: 1267 | Episodes: 77200 | Median Reward: 10.73 | Max Reward: 49.27
Iteration: 1268 | Episodes: 77300 | Median Reward: 4.28 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -86.4     |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1270      |
|    time_elapsed         | 23184     |
|    total_timesteps      | 7802880   |
| train/                  |           |
|    approx_kl            | 0.5309235 |
|    clip_fraction        | 0.268     |
|    clip_range           | 0.4       |
|    entropy_loss         | -78.6     |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 74.4      |
|    n_updates            | 12690     |
|    policy_gradient_loss | 0.0626    |
|    std                  | 2.6       |
|    value_loss           | 165       |
---------------------------------------
Iteration: 1270 | Episodes: 77400 | Median Reward: 16.47 | Max Reward: 49.27
Iteration: 1272 | Episodes: 77500 | Median Reward: 12.52 | Max Reward: 49.27
Iteration: 1273 | Episodes: 77600 | Median Reward: 9.42 | Max Reward: 49.27
Iteration: 1275 | Episodes: 77700 | Median Reward: 9.42 | Max Reward: 49.27
Iteration: 1277 | Episodes: 77800 | Median Reward: 3.03 | Max Reward: 49.27
Iteration: 1278 | Episodes: 77900 | Median Reward: 3.77 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -87       |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1280      |
|    time_elapsed         | 23359     |
|    total_timesteps      | 7864320   |
| train/                  |           |
|    approx_kl            | 0.6716013 |
|    clip_fraction        | 0.274     |
|    clip_range           | 0.4       |
|    entropy_loss         | -86.5     |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 156       |
|    n_updates            | 12790     |
|    policy_gradient_loss | 0.0198    |
|    std                  | 2.61      |
|    value_loss           | 297       |
---------------------------------------
Iteration: 1280 | Episodes: 78000 | Median Reward: 11.20 | Max Reward: 49.27
Iteration: 1282 | Episodes: 78100 | Median Reward: 2.19 | Max Reward: 49.27
Iteration: 1283 | Episodes: 78200 | Median Reward: 21.35 | Max Reward: 49.27
Iteration: 1285 | Episodes: 78300 | Median Reward: 9.50 | Max Reward: 49.27
Iteration: 1286 | Episodes: 78400 | Median Reward: 16.33 | Max Reward: 49.27
Iteration: 1288 | Episodes: 78500 | Median Reward: 12.55 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -94.9      |
| time/                   |            |
|    fps                  | 336        |
|    iterations           | 1290       |
|    time_elapsed         | 23534      |
|    total_timesteps      | 7925760    |
| train/                  |            |
|    approx_kl            | 0.12576711 |
|    clip_fraction        | 0.0609     |
|    clip_range           | 0.4        |
|    entropy_loss         | -92.5      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 64.2       |
|    n_updates            | 12890      |
|    policy_gradient_loss | 0.0184     |
|    std                  | 2.61       |
|    value_loss           | 185        |
----------------------------------------
Iteration: 1290 | Episodes: 78600 | Median Reward: -4.42 | Max Reward: 49.27
Iteration: 1291 | Episodes: 78700 | Median Reward: 14.15 | Max Reward: 49.27
Iteration: 1293 | Episodes: 78800 | Median Reward: 30.95 | Max Reward: 49.27
Iteration: 1295 | Episodes: 78900 | Median Reward: 34.03 | Max Reward: 49.27
Iteration: 1296 | Episodes: 79000 | Median Reward: 33.49 | Max Reward: 49.27
Iteration: 1298 | Episodes: 79100 | Median Reward: 26.38 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -72.6     |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1300      |
|    time_elapsed         | 23722     |
|    total_timesteps      | 7987200   |
| train/                  |           |
|    approx_kl            | 3.6980608 |
|    clip_fraction        | 0.232     |
|    clip_range           | 0.4       |
|    entropy_loss         | -94.2     |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 144       |
|    n_updates            | 12990     |
|    policy_gradient_loss | 0.0133    |
|    std                  | 2.62      |
|    value_loss           | 398       |
---------------------------------------
Iteration: 1300 | Episodes: 79200 | Median Reward: 36.79 | Max Reward: 49.27
Iteration: 1301 | Episodes: 79300 | Median Reward: 41.06 | Max Reward: 49.27
Iteration: 1303 | Episodes: 79400 | Median Reward: 35.57 | Max Reward: 49.27
Iteration: 1305 | Episodes: 79500 | Median Reward: 25.98 | Max Reward: 49.27
Iteration: 1306 | Episodes: 79600 | Median Reward: 35.55 | Max Reward: 49.27
Iteration: 1308 | Episodes: 79700 | Median Reward: 39.27 | Max Reward: 49.27
Iteration: 1309 | Episodes: 79800 | Median Reward: 35.44 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -73.1     |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1310      |
|    time_elapsed         | 23897     |
|    total_timesteps      | 8048640   |
| train/                  |           |
|    approx_kl            | 1.6671776 |
|    clip_fraction        | 0.213     |
|    clip_range           | 0.4       |
|    entropy_loss         | -94.9     |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 117       |
|    n_updates            | 13090     |
|    policy_gradient_loss | 0.0305    |
|    std                  | 2.63      |
|    value_loss           | 306       |
---------------------------------------
Iteration: 1311 | Episodes: 79900 | Median Reward: 16.76 | Max Reward: 49.27
Iteration: 1313 | Episodes: 80000 | Median Reward: 34.87 | Max Reward: 49.27
Iteration: 1314 | Episodes: 80100 | Median Reward: 3.44 | Max Reward: 49.27
Iteration: 1316 | Episodes: 80200 | Median Reward: 39.67 | Max Reward: 49.27
Iteration: 1318 | Episodes: 80300 | Median Reward: 38.03 | Max Reward: 49.27
Iteration: 1319 | Episodes: 80400 | Median Reward: 42.85 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -71.1     |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1320      |
|    time_elapsed         | 24072     |
|    total_timesteps      | 8110080   |
| train/                  |           |
|    approx_kl            | 0.6930572 |
|    clip_fraction        | 0.147     |
|    clip_range           | 0.4       |
|    entropy_loss         | -97.2     |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 124       |
|    n_updates            | 13190     |
|    policy_gradient_loss | 0.00456   |
|    std                  | 2.64      |
|    value_loss           | 317       |
---------------------------------------
Iteration: 1321 | Episodes: 80500 | Median Reward: 41.01 | Max Reward: 49.27
Iteration: 1323 | Episodes: 80600 | Median Reward: 29.89 | Max Reward: 49.27
Iteration: 1324 | Episodes: 80700 | Median Reward: 35.01 | Max Reward: 49.27
Iteration: 1326 | Episodes: 80800 | Median Reward: 33.02 | Max Reward: 49.27
Iteration: 1328 | Episodes: 80900 | Median Reward: 30.36 | Max Reward: 49.27
Iteration: 1329 | Episodes: 81000 | Median Reward: 39.86 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -65.9      |
| time/                   |            |
|    fps                  | 336        |
|    iterations           | 1330       |
|    time_elapsed         | 24254      |
|    total_timesteps      | 8171520    |
| train/                  |            |
|    approx_kl            | 0.18449233 |
|    clip_fraction        | 0.0592     |
|    clip_range           | 0.4        |
|    entropy_loss         | -97.8      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 230        |
|    n_updates            | 13290      |
|    policy_gradient_loss | -0.0126    |
|    std                  | 2.65       |
|    value_loss           | 395        |
----------------------------------------
Iteration: 1331 | Episodes: 81100 | Median Reward: 42.09 | Max Reward: 49.27
Iteration: 1332 | Episodes: 81200 | Median Reward: 37.89 | Max Reward: 49.27
Iteration: 1334 | Episodes: 81300 | Median Reward: 34.05 | Max Reward: 49.27
Iteration: 1336 | Episodes: 81400 | Median Reward: 20.37 | Max Reward: 49.27
Iteration: 1337 | Episodes: 81500 | Median Reward: 35.20 | Max Reward: 49.27
Iteration: 1339 | Episodes: 81600 | Median Reward: 27.74 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -75.5     |
| time/                   |           |
|    fps                  | 336       |
|    iterations           | 1340      |
|    time_elapsed         | 24436     |
|    total_timesteps      | 8232960   |
| train/                  |           |
|    approx_kl            | 0.9447833 |
|    clip_fraction        | 0.223     |
|    clip_range           | 0.4       |
|    entropy_loss         | -98.2     |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 132       |
|    n_updates            | 13390     |
|    policy_gradient_loss | 0.0938    |
|    std                  | 2.66      |
|    value_loss           | 334       |
---------------------------------------
Iteration: 1341 | Episodes: 81700 | Median Reward: 14.14 | Max Reward: 49.27
Iteration: 1342 | Episodes: 81800 | Median Reward: 42.68 | Max Reward: 49.27
Iteration: 1344 | Episodes: 81900 | Median Reward: 35.27 | Max Reward: 49.27
Iteration: 1346 | Episodes: 82000 | Median Reward: 35.27 | Max Reward: 49.27
Iteration: 1347 | Episodes: 82100 | Median Reward: 35.87 | Max Reward: 49.27
Iteration: 1349 | Episodes: 82200 | Median Reward: 39.49 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 1350      |
|    time_elapsed         | 24610     |
|    total_timesteps      | 8294400   |
| train/                  |           |
|    approx_kl            | 4.672376  |
|    clip_fraction        | 0.424     |
|    clip_range           | 0.4       |
|    entropy_loss         | -96.8     |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 212       |
|    n_updates            | 13490     |
|    policy_gradient_loss | 0.0467    |
|    std                  | 2.67      |
|    value_loss           | 352       |
---------------------------------------
Iteration: 1351 | Episodes: 82300 | Median Reward: 33.55 | Max Reward: 49.27
Iteration: 1352 | Episodes: 82400 | Median Reward: 40.50 | Max Reward: 49.27
Iteration: 1354 | Episodes: 82500 | Median Reward: 42.82 | Max Reward: 49.27
Iteration: 1355 | Episodes: 82600 | Median Reward: 30.51 | Max Reward: 49.27
Iteration: 1357 | Episodes: 82700 | Median Reward: 39.94 | Max Reward: 49.27
Iteration: 1359 | Episodes: 82800 | Median Reward: 29.58 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -81.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 1360      |
|    time_elapsed         | 24793     |
|    total_timesteps      | 8355840   |
| train/                  |           |
|    approx_kl            | 1.7515103 |
|    clip_fraction        | 0.264     |
|    clip_range           | 0.4       |
|    entropy_loss         | -95.1     |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 98.5      |
|    n_updates            | 13590     |
|    policy_gradient_loss | 0.0518    |
|    std                  | 2.68      |
|    value_loss           | 290       |
---------------------------------------
Iteration: 1360 | Episodes: 82900 | Median Reward: 10.99 | Max Reward: 49.27
Iteration: 1362 | Episodes: 83000 | Median Reward: 13.58 | Max Reward: 49.27
Iteration: 1364 | Episodes: 83100 | Median Reward: 10.00 | Max Reward: 49.27
Iteration: 1365 | Episodes: 83200 | Median Reward: 31.16 | Max Reward: 49.27
Iteration: 1367 | Episodes: 83300 | Median Reward: 28.74 | Max Reward: 49.27
Iteration: 1369 | Episodes: 83400 | Median Reward: 31.01 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -76.5      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 1370       |
|    time_elapsed         | 24975      |
|    total_timesteps      | 8417280    |
| train/                  |            |
|    approx_kl            | 0.18297447 |
|    clip_fraction        | 0.0834     |
|    clip_range           | 0.4        |
|    entropy_loss         | -96.1      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 122        |
|    n_updates            | 13690      |
|    policy_gradient_loss | 0.0215     |
|    std                  | 2.69       |
|    value_loss           | 204        |
----------------------------------------
Iteration: 1370 | Episodes: 83500 | Median Reward: 36.32 | Max Reward: 49.27
Iteration: 1372 | Episodes: 83600 | Median Reward: 32.57 | Max Reward: 49.27
Iteration: 1374 | Episodes: 83700 | Median Reward: 33.80 | Max Reward: 49.27
Iteration: 1375 | Episodes: 83800 | Median Reward: 34.67 | Max Reward: 49.27
Iteration: 1377 | Episodes: 83900 | Median Reward: 36.02 | Max Reward: 49.27
Iteration: 1378 | Episodes: 84000 | Median Reward: 35.47 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -69.8      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 1380       |
|    time_elapsed         | 25150      |
|    total_timesteps      | 8478720    |
| train/                  |            |
|    approx_kl            | 0.26613486 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.4        |
|    entropy_loss         | -92.4      |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 187        |
|    n_updates            | 13790      |
|    policy_gradient_loss | -0.000628  |
|    std                  | 2.7        |
|    value_loss           | 369        |
----------------------------------------
Iteration: 1380 | Episodes: 84100 | Median Reward: 34.25 | Max Reward: 49.27
Iteration: 1382 | Episodes: 84200 | Median Reward: 40.38 | Max Reward: 49.27
Iteration: 1383 | Episodes: 84300 | Median Reward: 40.79 | Max Reward: 49.27
Iteration: 1385 | Episodes: 84400 | Median Reward: 43.92 | Max Reward: 49.27
Iteration: 1387 | Episodes: 84500 | Median Reward: 36.43 | Max Reward: 49.27
Iteration: 1388 | Episodes: 84600 | Median Reward: 38.56 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -61.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 1390      |
|    time_elapsed         | 25335     |
|    total_timesteps      | 8540160   |
| train/                  |           |
|    approx_kl            | 1.0905266 |
|    clip_fraction        | 0.271     |
|    clip_range           | 0.4       |
|    entropy_loss         | -97       |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 136       |
|    n_updates            | 13890     |
|    policy_gradient_loss | 0.0539    |
|    std                  | 2.71      |
|    value_loss           | 351       |
---------------------------------------
Iteration: 1390 | Episodes: 84700 | Median Reward: 41.07 | Max Reward: 49.27
Iteration: 1392 | Episodes: 84800 | Median Reward: 30.13 | Max Reward: 49.27
Iteration: 1393 | Episodes: 84900 | Median Reward: 39.40 | Max Reward: 49.27
Iteration: 1395 | Episodes: 85000 | Median Reward: 38.93 | Max Reward: 49.27
Iteration: 1397 | Episodes: 85100 | Median Reward: 36.65 | Max Reward: 49.27
Iteration: 1398 | Episodes: 85200 | Median Reward: 38.01 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -62.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 1400      |
|    time_elapsed         | 25512     |
|    total_timesteps      | 8601600   |
| train/                  |           |
|    approx_kl            | 3.4409437 |
|    clip_fraction        | 0.267     |
|    clip_range           | 0.4       |
|    entropy_loss         | -97       |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 179       |
|    n_updates            | 13990     |
|    policy_gradient_loss | 0.0181    |
|    std                  | 2.72      |
|    value_loss           | 351       |
---------------------------------------
Iteration: 1400 | Episodes: 85300 | Median Reward: 39.45 | Max Reward: 49.27
Iteration: 1401 | Episodes: 85400 | Median Reward: 38.54 | Max Reward: 49.27
Iteration: 1403 | Episodes: 85500 | Median Reward: 26.27 | Max Reward: 49.27
Iteration: 1405 | Episodes: 85600 | Median Reward: 33.32 | Max Reward: 49.27
Iteration: 1406 | Episodes: 85700 | Median Reward: 33.18 | Max Reward: 49.27
Iteration: 1408 | Episodes: 85800 | Median Reward: 38.49 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -63.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 1410      |
|    time_elapsed         | 25687     |
|    total_timesteps      | 8663040   |
| train/                  |           |
|    approx_kl            | 0.8835524 |
|    clip_fraction        | 0.127     |
|    clip_range           | 0.4       |
|    entropy_loss         | -101      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 204       |
|    n_updates            | 14090     |
|    policy_gradient_loss | -0.00613  |
|    std                  | 2.73      |
|    value_loss           | 390       |
---------------------------------------
Iteration: 1410 | Episodes: 85900 | Median Reward: 39.26 | Max Reward: 49.27
Iteration: 1411 | Episodes: 86000 | Median Reward: 37.31 | Max Reward: 49.27
Iteration: 1413 | Episodes: 86100 | Median Reward: 39.34 | Max Reward: 49.27
Iteration: 1415 | Episodes: 86200 | Median Reward: 39.67 | Max Reward: 49.27
Iteration: 1416 | Episodes: 86300 | Median Reward: 36.83 | Max Reward: 49.27
Iteration: 1418 | Episodes: 86400 | Median Reward: 39.32 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.8        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1420         |
|    time_elapsed         | 25873        |
|    total_timesteps      | 8724480      |
| train/                  |              |
|    approx_kl            | 9.930203e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -108         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0001       |
|    loss                 | 191          |
|    n_updates            | 14190        |
|    policy_gradient_loss | -0.000805    |
|    std                  | 2.75         |
|    value_loss           | 384          |
------------------------------------------
Iteration: 1420 | Episodes: 86500 | Median Reward: 34.99 | Max Reward: 49.27
Iteration: 1421 | Episodes: 86600 | Median Reward: 40.20 | Max Reward: 49.27
Iteration: 1423 | Episodes: 86700 | Median Reward: 42.14 | Max Reward: 49.27
Iteration: 1425 | Episodes: 86800 | Median Reward: 40.76 | Max Reward: 49.27
Iteration: 1426 | Episodes: 86900 | Median Reward: 35.28 | Max Reward: 49.27
Iteration: 1428 | Episodes: 87000 | Median Reward: 31.51 | Max Reward: 49.27
Iteration: 1429 | Episodes: 87100 | Median Reward: 44.67 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -57      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 1430     |
|    time_elapsed         | 26052    |
|    total_timesteps      | 8785920  |
| train/                  |          |
|    approx_kl            | 4.811292 |
|    clip_fraction        | 0.111    |
|    clip_range           | 0.4      |
|    entropy_loss         | -107     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 182      |
|    n_updates            | 14290    |
|    policy_gradient_loss | 0.0443   |
|    std                  | 2.77     |
|    value_loss           | 387      |
--------------------------------------
Iteration: 1431 | Episodes: 87200 | Median Reward: 36.22 | Max Reward: 49.27
Iteration: 1433 | Episodes: 87300 | Median Reward: 46.12 | Max Reward: 49.27
Iteration: 1434 | Episodes: 87400 | Median Reward: 35.34 | Max Reward: 49.27
Iteration: 1436 | Episodes: 87500 | Median Reward: 38.21 | Max Reward: 49.27
Iteration: 1438 | Episodes: 87600 | Median Reward: 46.44 | Max Reward: 49.27
Iteration: 1439 | Episodes: 87700 | Median Reward: 37.47 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -67.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 1440      |
|    time_elapsed         | 26226     |
|    total_timesteps      | 8847360   |
| train/                  |           |
|    approx_kl            | 7.5137486 |
|    clip_fraction        | 0.0839    |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 170       |
|    n_updates            | 14390     |
|    policy_gradient_loss | 0.0414    |
|    std                  | 2.79      |
|    value_loss           | 344       |
---------------------------------------
Iteration: 1441 | Episodes: 87800 | Median Reward: 22.05 | Max Reward: 49.27
Iteration: 1443 | Episodes: 87900 | Median Reward: 14.33 | Max Reward: 49.27
Iteration: 1444 | Episodes: 88000 | Median Reward: 15.65 | Max Reward: 49.27
Iteration: 1446 | Episodes: 88100 | Median Reward: 32.28 | Max Reward: 49.27
Iteration: 1448 | Episodes: 88200 | Median Reward: 38.35 | Max Reward: 49.27
Iteration: 1449 | Episodes: 88300 | Median Reward: 42.63 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -60.4    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 1450     |
|    time_elapsed         | 26402    |
|    total_timesteps      | 8908800  |
| train/                  |          |
|    approx_kl            | 3.358704 |
|    clip_fraction        | 0.101    |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 127      |
|    n_updates            | 14490    |
|    policy_gradient_loss | 0.0282   |
|    std                  | 2.8      |
|    value_loss           | 333      |
--------------------------------------
Iteration: 1451 | Episodes: 88400 | Median Reward: 42.48 | Max Reward: 49.27
Iteration: 1452 | Episodes: 88500 | Median Reward: 18.83 | Max Reward: 49.27
Iteration: 1454 | Episodes: 88600 | Median Reward: 31.85 | Max Reward: 49.27
Iteration: 1456 | Episodes: 88700 | Median Reward: 45.33 | Max Reward: 49.27
Iteration: 1457 | Episodes: 88800 | Median Reward: 45.69 | Max Reward: 49.27
Iteration: 1459 | Episodes: 88900 | Median Reward: 46.83 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -58.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 1460      |
|    time_elapsed         | 26584     |
|    total_timesteps      | 8970240   |
| train/                  |           |
|    approx_kl            | 2.3695698 |
|    clip_fraction        | 0.0779    |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 206       |
|    n_updates            | 14590     |
|    policy_gradient_loss | 0.00226   |
|    std                  | 2.81      |
|    value_loss           | 429       |
---------------------------------------
Iteration: 1461 | Episodes: 89000 | Median Reward: 44.33 | Max Reward: 49.27
Iteration: 1462 | Episodes: 89100 | Median Reward: 42.76 | Max Reward: 49.27
Iteration: 1464 | Episodes: 89200 | Median Reward: 43.38 | Max Reward: 49.27
Iteration: 1466 | Episodes: 89300 | Median Reward: 32.77 | Max Reward: 49.27
Iteration: 1467 | Episodes: 89400 | Median Reward: 2.82 | Max Reward: 49.27
Iteration: 1469 | Episodes: 89500 | Median Reward: 37.73 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 1470      |
|    time_elapsed         | 26761     |
|    total_timesteps      | 9031680   |
| train/                  |           |
|    approx_kl            | 3.5116541 |
|    clip_fraction        | 0.0614    |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 185       |
|    n_updates            | 14690     |
|    policy_gradient_loss | 0.00879   |
|    std                  | 2.82      |
|    value_loss           | 285       |
---------------------------------------
Iteration: 1470 | Episodes: 89600 | Median Reward: 22.00 | Max Reward: 49.27
Iteration: 1472 | Episodes: 89700 | Median Reward: 33.52 | Max Reward: 49.27
Iteration: 1474 | Episodes: 89800 | Median Reward: 40.48 | Max Reward: 49.27
Iteration: 1475 | Episodes: 89900 | Median Reward: 33.08 | Max Reward: 49.27
Iteration: 1477 | Episodes: 90000 | Median Reward: 40.59 | Max Reward: 49.27
Iteration: 1479 | Episodes: 90100 | Median Reward: 38.53 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63           |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1480          |
|    time_elapsed         | 26944         |
|    total_timesteps      | 9093120       |
| train/                  |               |
|    approx_kl            | 0.00016993744 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -109          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 169           |
|    n_updates            | 14790         |
|    policy_gradient_loss | -0.000494     |
|    std                  | 2.84          |
|    value_loss           | 347           |
-------------------------------------------
Iteration: 1480 | Episodes: 90200 | Median Reward: 38.78 | Max Reward: 49.27
Iteration: 1482 | Episodes: 90300 | Median Reward: 38.39 | Max Reward: 49.27
Iteration: 1484 | Episodes: 90400 | Median Reward: 41.43 | Max Reward: 49.27
Iteration: 1485 | Episodes: 90500 | Median Reward: 41.00 | Max Reward: 49.27
Iteration: 1487 | Episodes: 90600 | Median Reward: 39.04 | Max Reward: 49.27
Iteration: 1489 | Episodes: 90700 | Median Reward: 38.88 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.8        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1490         |
|    time_elapsed         | 27128        |
|    total_timesteps      | 9154560      |
| train/                  |              |
|    approx_kl            | 0.0012492832 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -109         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0001       |
|    loss                 | 162          |
|    n_updates            | 14890        |
|    policy_gradient_loss | -0.00135     |
|    std                  | 2.88         |
|    value_loss           | 329          |
------------------------------------------
Iteration: 1490 | Episodes: 90800 | Median Reward: 39.03 | Max Reward: 49.27
Iteration: 1492 | Episodes: 90900 | Median Reward: 37.04 | Max Reward: 49.27
Iteration: 1494 | Episodes: 91000 | Median Reward: 38.98 | Max Reward: 49.27
Iteration: 1495 | Episodes: 91100 | Median Reward: 41.83 | Max Reward: 49.27
Iteration: 1497 | Episodes: 91200 | Median Reward: 37.46 | Max Reward: 49.27
Iteration: 1498 | Episodes: 91300 | Median Reward: 41.43 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.5         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1500          |
|    time_elapsed         | 27302         |
|    total_timesteps      | 9216000       |
| train/                  |               |
|    approx_kl            | 0.00030386148 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -110          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 148           |
|    n_updates            | 14990         |
|    policy_gradient_loss | -0.000237     |
|    std                  | 2.93          |
|    value_loss           | 334           |
-------------------------------------------
Iteration: 1500 | Episodes: 91400 | Median Reward: 42.08 | Max Reward: 49.27
Iteration: 1502 | Episodes: 91500 | Median Reward: 39.77 | Max Reward: 49.27
Iteration: 1503 | Episodes: 91600 | Median Reward: 41.33 | Max Reward: 49.27
Iteration: 1505 | Episodes: 91700 | Median Reward: 39.81 | Max Reward: 49.27
Iteration: 1507 | Episodes: 91800 | Median Reward: 38.98 | Max Reward: 49.27
Iteration: 1508 | Episodes: 91900 | Median Reward: 38.89 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.4        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1510         |
|    time_elapsed         | 27488        |
|    total_timesteps      | 9277440      |
| train/                  |              |
|    approx_kl            | 0.0010847261 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -110         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 152          |
|    n_updates            | 15090        |
|    policy_gradient_loss | -0.000861    |
|    std                  | 2.97         |
|    value_loss           | 341          |
------------------------------------------
Iteration: 1510 | Episodes: 92000 | Median Reward: 34.50 | Max Reward: 49.27
Iteration: 1512 | Episodes: 92100 | Median Reward: 41.76 | Max Reward: 49.27
Iteration: 1513 | Episodes: 92200 | Median Reward: 39.42 | Max Reward: 49.27
Iteration: 1515 | Episodes: 92300 | Median Reward: 40.19 | Max Reward: 49.27
Iteration: 1517 | Episodes: 92400 | Median Reward: 38.87 | Max Reward: 49.27
Iteration: 1518 | Episodes: 92500 | Median Reward: 31.25 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.5        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1520         |
|    time_elapsed         | 27666        |
|    total_timesteps      | 9338880      |
| train/                  |              |
|    approx_kl            | 0.0004941699 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -110         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 165          |
|    n_updates            | 15190        |
|    policy_gradient_loss | -0.000591    |
|    std                  | 3.01         |
|    value_loss           | 337          |
------------------------------------------
Iteration: 1520 | Episodes: 92600 | Median Reward: 38.86 | Max Reward: 49.27
Iteration: 1521 | Episodes: 92700 | Median Reward: 38.11 | Max Reward: 49.27
Iteration: 1523 | Episodes: 92800 | Median Reward: 33.45 | Max Reward: 49.27
Iteration: 1525 | Episodes: 92900 | Median Reward: 36.05 | Max Reward: 49.27
Iteration: 1526 | Episodes: 93000 | Median Reward: 36.34 | Max Reward: 49.27
Iteration: 1528 | Episodes: 93100 | Median Reward: 37.26 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.6        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1530         |
|    time_elapsed         | 27840        |
|    total_timesteps      | 9400320      |
| train/                  |              |
|    approx_kl            | 0.0004901489 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0001       |
|    loss                 | 171          |
|    n_updates            | 15290        |
|    policy_gradient_loss | -0.000502    |
|    std                  | 3.05         |
|    value_loss           | 337          |
------------------------------------------
Iteration: 1530 | Episodes: 93200 | Median Reward: 40.44 | Max Reward: 49.27
Iteration: 1531 | Episodes: 93300 | Median Reward: 37.18 | Max Reward: 49.27
Iteration: 1533 | Episodes: 93400 | Median Reward: 39.01 | Max Reward: 49.27
Iteration: 1535 | Episodes: 93500 | Median Reward: 39.01 | Max Reward: 49.27
Iteration: 1536 | Episodes: 93600 | Median Reward: 38.83 | Max Reward: 49.27
Iteration: 1538 | Episodes: 93700 | Median Reward: 31.83 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.1         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1540          |
|    time_elapsed         | 28028         |
|    total_timesteps      | 9461760       |
| train/                  |               |
|    approx_kl            | 4.6567104e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -111          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 118           |
|    n_updates            | 15390         |
|    policy_gradient_loss | -0.000267     |
|    std                  | 3.1           |
|    value_loss           | 282           |
-------------------------------------------
Iteration: 1540 | Episodes: 93800 | Median Reward: 37.00 | Max Reward: 49.27
Iteration: 1541 | Episodes: 93900 | Median Reward: 33.94 | Max Reward: 49.27
Iteration: 1543 | Episodes: 94000 | Median Reward: 37.27 | Max Reward: 49.27
Iteration: 1544 | Episodes: 94100 | Median Reward: 36.22 | Max Reward: 49.27
Iteration: 1546 | Episodes: 94200 | Median Reward: 39.02 | Max Reward: 49.27
Iteration: 1548 | Episodes: 94300 | Median Reward: 40.60 | Max Reward: 49.27
Iteration: 1549 | Episodes: 94400 | Median Reward: 39.09 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.9        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1550         |
|    time_elapsed         | 28204        |
|    total_timesteps      | 9523200      |
| train/                  |              |
|    approx_kl            | 0.0007215518 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0001       |
|    loss                 | 155          |
|    n_updates            | 15490        |
|    policy_gradient_loss | -0.000811    |
|    std                  | 3.14         |
|    value_loss           | 333          |
------------------------------------------
Iteration: 1551 | Episodes: 94500 | Median Reward: 41.87 | Max Reward: 49.27
Iteration: 1553 | Episodes: 94600 | Median Reward: 35.41 | Max Reward: 49.27
Iteration: 1554 | Episodes: 94700 | Median Reward: 38.77 | Max Reward: 49.27
Iteration: 1556 | Episodes: 94800 | Median Reward: 38.44 | Max Reward: 49.27
Iteration: 1558 | Episodes: 94900 | Median Reward: 34.98 | Max Reward: 49.27
Iteration: 1559 | Episodes: 95000 | Median Reward: 35.93 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.7         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1560          |
|    time_elapsed         | 28383         |
|    total_timesteps      | 9584640       |
| train/                  |               |
|    approx_kl            | 0.00017274331 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 149           |
|    n_updates            | 15590         |
|    policy_gradient_loss | -0.000335     |
|    std                  | 3.18          |
|    value_loss           | 332           |
-------------------------------------------
Iteration: 1561 | Episodes: 95100 | Median Reward: 32.84 | Max Reward: 49.27
Iteration: 1563 | Episodes: 95200 | Median Reward: 42.45 | Max Reward: 49.27
Iteration: 1564 | Episodes: 95300 | Median Reward: 34.04 | Max Reward: 49.27
Iteration: 1566 | Episodes: 95400 | Median Reward: 38.95 | Max Reward: 49.27
Iteration: 1567 | Episodes: 95500 | Median Reward: 34.67 | Max Reward: 49.27
Iteration: 1569 | Episodes: 95600 | Median Reward: 38.88 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.4        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1570         |
|    time_elapsed         | 28571        |
|    total_timesteps      | 9646080      |
| train/                  |              |
|    approx_kl            | 0.0005512268 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 157          |
|    n_updates            | 15690        |
|    policy_gradient_loss | -0.000324    |
|    std                  | 3.21         |
|    value_loss           | 331          |
------------------------------------------
Iteration: 1571 | Episodes: 95700 | Median Reward: 39.78 | Max Reward: 49.27
Iteration: 1572 | Episodes: 95800 | Median Reward: 42.53 | Max Reward: 49.27
Iteration: 1574 | Episodes: 95900 | Median Reward: 38.49 | Max Reward: 49.27
Iteration: 1576 | Episodes: 96000 | Median Reward: 39.53 | Max Reward: 49.27
Iteration: 1577 | Episodes: 96100 | Median Reward: 40.36 | Max Reward: 49.27
Iteration: 1579 | Episodes: 96200 | Median Reward: 40.69 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60           |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1580          |
|    time_elapsed         | 28748         |
|    total_timesteps      | 9707520       |
| train/                  |               |
|    approx_kl            | 0.00010744346 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 170           |
|    n_updates            | 15790         |
|    policy_gradient_loss | -0.000142     |
|    std                  | 3.24          |
|    value_loss           | 358           |
-------------------------------------------
Iteration: 1581 | Episodes: 96300 | Median Reward: 39.85 | Max Reward: 49.27
Iteration: 1582 | Episodes: 96400 | Median Reward: 37.39 | Max Reward: 49.27
Iteration: 1584 | Episodes: 96500 | Median Reward: 40.19 | Max Reward: 49.27
Iteration: 1586 | Episodes: 96600 | Median Reward: 39.49 | Max Reward: 49.27
Iteration: 1587 | Episodes: 96700 | Median Reward: 38.92 | Max Reward: 49.27
Iteration: 1589 | Episodes: 96800 | Median Reward: 34.40 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.2         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1590          |
|    time_elapsed         | 28924         |
|    total_timesteps      | 9768960       |
| train/                  |               |
|    approx_kl            | 0.00012463165 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 149           |
|    n_updates            | 15890         |
|    policy_gradient_loss | -0.000419     |
|    std                  | 3.29          |
|    value_loss           | 290           |
-------------------------------------------
Iteration: 1590 | Episodes: 96900 | Median Reward: 35.66 | Max Reward: 49.27
Iteration: 1592 | Episodes: 97000 | Median Reward: 39.53 | Max Reward: 49.27
Iteration: 1594 | Episodes: 97100 | Median Reward: 41.89 | Max Reward: 49.27
Iteration: 1595 | Episodes: 97200 | Median Reward: 41.02 | Max Reward: 49.27
Iteration: 1597 | Episodes: 97300 | Median Reward: 39.34 | Max Reward: 49.27
Iteration: 1599 | Episodes: 97400 | Median Reward: 38.73 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.2        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1600         |
|    time_elapsed         | 29111        |
|    total_timesteps      | 9830400      |
| train/                  |              |
|    approx_kl            | 0.0001742431 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -113         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0001       |
|    loss                 | 152          |
|    n_updates            | 15990        |
|    policy_gradient_loss | -0.000429    |
|    std                  | 3.32         |
|    value_loss           | 318          |
------------------------------------------
Iteration: 1600 | Episodes: 97500 | Median Reward: 42.68 | Max Reward: 49.27
Iteration: 1602 | Episodes: 97600 | Median Reward: 32.91 | Max Reward: 49.27
Iteration: 1604 | Episodes: 97700 | Median Reward: 39.27 | Max Reward: 49.27
Iteration: 1605 | Episodes: 97800 | Median Reward: 37.33 | Max Reward: 49.27
Iteration: 1607 | Episodes: 97900 | Median Reward: 36.43 | Max Reward: 49.27
Iteration: 1609 | Episodes: 98000 | Median Reward: 38.65 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.1        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1610         |
|    time_elapsed         | 29289        |
|    total_timesteps      | 9891840      |
| train/                  |              |
|    approx_kl            | 0.0016237628 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -113         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 160          |
|    n_updates            | 16090        |
|    policy_gradient_loss | -0.00239     |
|    std                  | 3.35         |
|    value_loss           | 337          |
------------------------------------------
Iteration: 1610 | Episodes: 98100 | Median Reward: 37.18 | Max Reward: 49.27
Iteration: 1612 | Episodes: 98200 | Median Reward: 39.81 | Max Reward: 49.27
Iteration: 1614 | Episodes: 98300 | Median Reward: 40.49 | Max Reward: 49.27
Iteration: 1615 | Episodes: 98400 | Median Reward: 39.11 | Max Reward: 49.27
Iteration: 1617 | Episodes: 98500 | Median Reward: 40.47 | Max Reward: 49.27
Iteration: 1618 | Episodes: 98600 | Median Reward: 39.33 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.8        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1620         |
|    time_elapsed         | 29463        |
|    total_timesteps      | 9953280      |
| train/                  |              |
|    approx_kl            | 0.0003314325 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -113         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 158          |
|    n_updates            | 16190        |
|    policy_gradient_loss | -0.000439    |
|    std                  | 3.4          |
|    value_loss           | 331          |
------------------------------------------
Iteration: 1620 | Episodes: 98700 | Median Reward: 33.98 | Max Reward: 49.27
Iteration: 1622 | Episodes: 98800 | Median Reward: 40.69 | Max Reward: 49.27
Iteration: 1623 | Episodes: 98900 | Median Reward: 35.62 | Max Reward: 49.27
Iteration: 1625 | Episodes: 99000 | Median Reward: 37.94 | Max Reward: 49.27
Iteration: 1627 | Episodes: 99100 | Median Reward: 38.80 | Max Reward: 49.27
Iteration: 1628 | Episodes: 99200 | Median Reward: 38.55 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.9         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1630          |
|    time_elapsed         | 29649         |
|    total_timesteps      | 10014720      |
| train/                  |               |
|    approx_kl            | 8.1638325e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 165           |
|    n_updates            | 16290         |
|    policy_gradient_loss | -0.000208     |
|    std                  | 3.44          |
|    value_loss           | 299           |
-------------------------------------------
Iteration: 1630 | Episodes: 99300 | Median Reward: 36.63 | Max Reward: 49.27
Iteration: 1632 | Episodes: 99400 | Median Reward: 38.27 | Max Reward: 49.27
Iteration: 1633 | Episodes: 99500 | Median Reward: 37.33 | Max Reward: 49.27
Iteration: 1635 | Episodes: 99600 | Median Reward: 37.91 | Max Reward: 49.27
Iteration: 1637 | Episodes: 99700 | Median Reward: 32.71 | Max Reward: 49.27
Iteration: 1638 | Episodes: 99800 | Median Reward: 43.66 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.7         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1640          |
|    time_elapsed         | 29825         |
|    total_timesteps      | 10076160      |
| train/                  |               |
|    approx_kl            | 2.5732814e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -114          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 177           |
|    n_updates            | 16390         |
|    policy_gradient_loss | -0.000101     |
|    std                  | 3.48          |
|    value_loss           | 371           |
-------------------------------------------
Iteration: 1640 | Episodes: 99900 | Median Reward: 34.82 | Max Reward: 49.27
Iteration: 1641 | Episodes: 100000 | Median Reward: 39.06 | Max Reward: 49.27
Iteration: 1643 | Episodes: 100100 | Median Reward: 37.65 | Max Reward: 49.27
Iteration: 1645 | Episodes: 100200 | Median Reward: 31.84 | Max Reward: 49.27
Iteration: 1646 | Episodes: 100300 | Median Reward: 41.34 | Max Reward: 49.27
Iteration: 1648 | Episodes: 100400 | Median Reward: 39.11 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.2         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1650          |
|    time_elapsed         | 30004         |
|    total_timesteps      | 10137600      |
| train/                  |               |
|    approx_kl            | 0.00013510836 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -114          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 174           |
|    n_updates            | 16490         |
|    policy_gradient_loss | -0.000315     |
|    std                  | 3.51          |
|    value_loss           | 353           |
-------------------------------------------
Iteration: 1650 | Episodes: 100500 | Median Reward: 37.08 | Max Reward: 49.27
Iteration: 1651 | Episodes: 100600 | Median Reward: 39.56 | Max Reward: 49.27
Iteration: 1653 | Episodes: 100700 | Median Reward: 40.46 | Max Reward: 49.27
Iteration: 1655 | Episodes: 100800 | Median Reward: 42.73 | Max Reward: 49.27
Iteration: 1656 | Episodes: 100900 | Median Reward: 40.31 | Max Reward: 49.27
Iteration: 1658 | Episodes: 101000 | Median Reward: 39.67 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.6         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1660          |
|    time_elapsed         | 30191         |
|    total_timesteps      | 10199040      |
| train/                  |               |
|    approx_kl            | 0.00025730405 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -114          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 159           |
|    n_updates            | 16590         |
|    policy_gradient_loss | -0.000373     |
|    std                  | 3.54          |
|    value_loss           | 316           |
-------------------------------------------
Iteration: 1660 | Episodes: 101100 | Median Reward: 40.08 | Max Reward: 49.27
Iteration: 1661 | Episodes: 101200 | Median Reward: 38.98 | Max Reward: 49.27
Iteration: 1663 | Episodes: 101300 | Median Reward: 38.44 | Max Reward: 49.27
Iteration: 1664 | Episodes: 101400 | Median Reward: 41.24 | Max Reward: 49.27
Iteration: 1666 | Episodes: 101500 | Median Reward: 34.83 | Max Reward: 49.27
Iteration: 1668 | Episodes: 101600 | Median Reward: 38.23 | Max Reward: 49.27
Iteration: 1669 | Episodes: 101700 | Median Reward: 40.15 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.4         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1670          |
|    time_elapsed         | 30370         |
|    total_timesteps      | 10260480      |
| train/                  |               |
|    approx_kl            | 0.00012630993 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -114          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 149           |
|    n_updates            | 16690         |
|    policy_gradient_loss | -1.51e-05     |
|    std                  | 3.57          |
|    value_loss           | 357           |
-------------------------------------------
Iteration: 1671 | Episodes: 101800 | Median Reward: 38.75 | Max Reward: 49.27
Iteration: 1673 | Episodes: 101900 | Median Reward: 35.75 | Max Reward: 49.27
Iteration: 1674 | Episodes: 102000 | Median Reward: 35.35 | Max Reward: 49.27
Iteration: 1676 | Episodes: 102100 | Median Reward: 34.39 | Max Reward: 49.27
Iteration: 1678 | Episodes: 102200 | Median Reward: 37.26 | Max Reward: 49.27
Iteration: 1679 | Episodes: 102300 | Median Reward: 35.00 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -65.4      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 1680       |
|    time_elapsed         | 30546      |
|    total_timesteps      | 10321920   |
| train/                  |            |
|    approx_kl            | 0.09417166 |
|    clip_fraction        | 0.0224     |
|    clip_range           | 0.4        |
|    entropy_loss         | -114       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 134        |
|    n_updates            | 16790      |
|    policy_gradient_loss | -0.00591   |
|    std                  | 3.6        |
|    value_loss           | 288        |
----------------------------------------
Iteration: 1681 | Episodes: 102400 | Median Reward: 38.69 | Max Reward: 49.27
Iteration: 1683 | Episodes: 102500 | Median Reward: 38.80 | Max Reward: 49.27
Iteration: 1684 | Episodes: 102600 | Median Reward: 37.15 | Max Reward: 49.27
Iteration: 1686 | Episodes: 102700 | Median Reward: 41.30 | Max Reward: 49.27
Iteration: 1687 | Episodes: 102800 | Median Reward: 35.93 | Max Reward: 49.27
Iteration: 1689 | Episodes: 102900 | Median Reward: 38.03 | Max Reward: 49.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.1        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1690         |
|    time_elapsed         | 30734        |
|    total_timesteps      | 10383360     |
| train/                  |              |
|    approx_kl            | 0.0005047033 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0001       |
|    loss                 | 170          |
|    n_updates            | 16890        |
|    policy_gradient_loss | 0.00091      |
|    std                  | 3.64         |
|    value_loss           | 308          |
------------------------------------------
Iteration: 1691 | Episodes: 103000 | Median Reward: 35.29 | Max Reward: 49.27
Iteration: 1692 | Episodes: 103100 | Median Reward: 39.01 | Max Reward: 49.27
Iteration: 1694 | Episodes: 103200 | Median Reward: 38.88 | Max Reward: 49.27
Iteration: 1696 | Episodes: 103300 | Median Reward: 36.58 | Max Reward: 49.27
Iteration: 1697 | Episodes: 103400 | Median Reward: 35.95 | Max Reward: 49.27
Iteration: 1699 | Episodes: 103500 | Median Reward: 40.38 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.7         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1700          |
|    time_elapsed         | 30910         |
|    total_timesteps      | 10444800      |
| train/                  |               |
|    approx_kl            | 3.4120705e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -115          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 182           |
|    n_updates            | 16990         |
|    policy_gradient_loss | -8.94e-05     |
|    std                  | 3.67          |
|    value_loss           | 360           |
-------------------------------------------
Iteration: 1701 | Episodes: 103600 | Median Reward: 39.86 | Max Reward: 49.27
Iteration: 1702 | Episodes: 103700 | Median Reward: 39.38 | Max Reward: 49.27
Iteration: 1704 | Episodes: 103800 | Median Reward: 32.59 | Max Reward: 49.27
Iteration: 1706 | Episodes: 103900 | Median Reward: 38.28 | Max Reward: 49.27
Iteration: 1707 | Episodes: 104000 | Median Reward: 35.81 | Max Reward: 49.27
Iteration: 1709 | Episodes: 104100 | Median Reward: 35.36 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.2         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1710          |
|    time_elapsed         | 31096         |
|    total_timesteps      | 10506240      |
| train/                  |               |
|    approx_kl            | 0.00041163428 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -115          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0001        |
|    loss                 | 165           |
|    n_updates            | 17090         |
|    policy_gradient_loss | -0.000868     |
|    std                  | 3.72          |
|    value_loss           | 335           |
-------------------------------------------
Iteration: 1711 | Episodes: 104200 | Median Reward: 34.62 | Max Reward: 49.27
Iteration: 1712 | Episodes: 104300 | Median Reward: 35.29 | Max Reward: 49.27
Iteration: 1714 | Episodes: 104400 | Median Reward: 36.69 | Max Reward: 49.27
Iteration: 1715 | Episodes: 104500 | Median Reward: 38.82 | Max Reward: 49.27
Iteration: 1717 | Episodes: 104600 | Median Reward: 40.01 | Max Reward: 49.27
Iteration: 1719 | Episodes: 104700 | Median Reward: 38.84 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.2         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1720          |
|    time_elapsed         | 31275         |
|    total_timesteps      | 10567680      |
| train/                  |               |
|    approx_kl            | 0.00068912865 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.4           |
|    entropy_loss         | -116          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 177           |
|    n_updates            | 17190         |
|    policy_gradient_loss | -0.000525     |
|    std                  | 3.76          |
|    value_loss           | 348           |
-------------------------------------------
Iteration: 1720 | Episodes: 104800 | Median Reward: 39.46 | Max Reward: 49.27
Iteration: 1722 | Episodes: 104900 | Median Reward: 37.93 | Max Reward: 49.27
Iteration: 1724 | Episodes: 105000 | Median Reward: 36.04 | Max Reward: 49.27
Iteration: 1725 | Episodes: 105100 | Median Reward: 37.99 | Max Reward: 49.27
Iteration: 1727 | Episodes: 105200 | Median Reward: 37.95 | Max Reward: 49.27
Iteration: 1729 | Episodes: 105300 | Median Reward: 36.03 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.6         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1730          |
|    time_elapsed         | 31450         |
|    total_timesteps      | 10629120      |
| train/                  |               |
|    approx_kl            | 0.00015498188 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -116          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 168           |
|    n_updates            | 17290         |
|    policy_gradient_loss | -0.00045      |
|    std                  | 3.8           |
|    value_loss           | 333           |
-------------------------------------------
Iteration: 1730 | Episodes: 105400 | Median Reward: 37.05 | Max Reward: 49.27
Iteration: 1732 | Episodes: 105500 | Median Reward: 41.61 | Max Reward: 49.27
Iteration: 1734 | Episodes: 105600 | Median Reward: 41.06 | Max Reward: 49.27
Iteration: 1735 | Episodes: 105700 | Median Reward: 40.01 | Max Reward: 49.27
Iteration: 1737 | Episodes: 105800 | Median Reward: 39.59 | Max Reward: 49.27
Iteration: 1738 | Episodes: 105900 | Median Reward: 39.72 | Max Reward: 49.27
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.2         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 1740          |
|    time_elapsed         | 31641         |
|    total_timesteps      | 10690560      |
| train/                  |               |
|    approx_kl            | 0.00011134376 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -116          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 189           |
|    n_updates            | 17390         |
|    policy_gradient_loss | -0.000115     |
|    std                  | 3.84          |
|    value_loss           | 331           |
-------------------------------------------
Iteration: 1740 | Episodes: 106000 | Median Reward: 36.37 | Max Reward: 49.27
Iteration: 1742 | Episodes: 106100 | Median Reward: 35.81 | Max Reward: 49.27
Iteration: 1743 | Episodes: 106200 | Median Reward: 33.81 | Max Reward: 49.27
Iteration: 1745 | Episodes: 106300 | Median Reward: 40.19 | Max Reward: 49.27
Iteration: 1747 | Episodes: 106400 | Median Reward: 39.15 | Max Reward: 49.27
Iteration: 1748 | Episodes: 106500 | Median Reward: 38.23 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 1750      |
|    time_elapsed         | 31817     |
|    total_timesteps      | 10752000  |
| train/                  |           |
|    approx_kl            | 1.2156239 |
|    clip_fraction        | 0.0865    |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 174       |
|    n_updates            | 17490     |
|    policy_gradient_loss | -0.00439  |
|    std                  | 3.88      |
|    value_loss           | 353       |
---------------------------------------
Iteration: 1750 | Episodes: 106600 | Median Reward: 36.01 | Max Reward: 49.27
Iteration: 1752 | Episodes: 106700 | Median Reward: 33.97 | Max Reward: 49.27
Iteration: 1753 | Episodes: 106800 | Median Reward: 33.43 | Max Reward: 49.27
Iteration: 1755 | Episodes: 106900 | Median Reward: 35.86 | Max Reward: 49.27
Iteration: 1757 | Episodes: 107000 | Median Reward: 38.00 | Max Reward: 49.27
Iteration: 1758 | Episodes: 107100 | Median Reward: 34.31 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -69.7      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 1760       |
|    time_elapsed         | 31993      |
|    total_timesteps      | 10813440   |
| train/                  |            |
|    approx_kl            | 0.06674571 |
|    clip_fraction        | 0.0111     |
|    clip_range           | 0.4        |
|    entropy_loss         | -116       |
|    explained_variance   | -2.38e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 145        |
|    n_updates            | 17590      |
|    policy_gradient_loss | -0.00282   |
|    std                  | 3.92       |
|    value_loss           | 289        |
----------------------------------------
Iteration: 1760 | Episodes: 107200 | Median Reward: 31.03 | Max Reward: 49.27
Iteration: 1761 | Episodes: 107300 | Median Reward: 36.37 | Max Reward: 49.27
Iteration: 1763 | Episodes: 107400 | Median Reward: 37.08 | Max Reward: 49.27
Iteration: 1765 | Episodes: 107500 | Median Reward: 33.27 | Max Reward: 49.27
Iteration: 1766 | Episodes: 107600 | Median Reward: 33.62 | Max Reward: 49.27
Iteration: 1768 | Episodes: 107700 | Median Reward: 32.67 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.2       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 1770        |
|    time_elapsed         | 32181       |
|    total_timesteps      | 10874880    |
| train/                  |             |
|    approx_kl            | 0.007800402 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.4         |
|    entropy_loss         | -116        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 162         |
|    n_updates            | 17690       |
|    policy_gradient_loss | -0.000716   |
|    std                  | 3.95        |
|    value_loss           | 330         |
-----------------------------------------
Iteration: 1770 | Episodes: 107800 | Median Reward: 37.98 | Max Reward: 49.27
Iteration: 1771 | Episodes: 107900 | Median Reward: 39.09 | Max Reward: 49.27
Iteration: 1773 | Episodes: 108000 | Median Reward: 43.29 | Max Reward: 49.27
Iteration: 1775 | Episodes: 108100 | Median Reward: 38.15 | Max Reward: 49.27
Iteration: 1776 | Episodes: 108200 | Median Reward: 35.89 | Max Reward: 49.27
Iteration: 1778 | Episodes: 108300 | Median Reward: 36.81 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -63.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1780      |
|    time_elapsed         | 32355     |
|    total_timesteps      | 10936320  |
| train/                  |           |
|    approx_kl            | 0.5569066 |
|    clip_fraction        | 0.0155    |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 140       |
|    n_updates            | 17790     |
|    policy_gradient_loss | -0.000225 |
|    std                  | 3.99      |
|    value_loss           | 307       |
---------------------------------------
Iteration: 1780 | Episodes: 108400 | Median Reward: 34.41 | Max Reward: 49.27
Iteration: 1781 | Episodes: 108500 | Median Reward: 40.58 | Max Reward: 49.27
Iteration: 1783 | Episodes: 108600 | Median Reward: 37.58 | Max Reward: 49.27
Iteration: 1784 | Episodes: 108700 | Median Reward: 36.34 | Max Reward: 49.27
Iteration: 1786 | Episodes: 108800 | Median Reward: 41.07 | Max Reward: 49.27
Iteration: 1788 | Episodes: 108900 | Median Reward: 0.42 | Max Reward: 49.27
Iteration: 1789 | Episodes: 109000 | Median Reward: 13.21 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -78.7    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1790     |
|    time_elapsed         | 32530    |
|    total_timesteps      | 10997760 |
| train/                  |          |
|    approx_kl            | 4.425039 |
|    clip_fraction        | 0.193    |
|    clip_range           | 0.4      |
|    entropy_loss         | -110     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 149      |
|    n_updates            | 17890    |
|    policy_gradient_loss | 0.0345   |
|    std                  | 4.01     |
|    value_loss           | 242      |
--------------------------------------
Iteration: 1791 | Episodes: 109100 | Median Reward: 19.08 | Max Reward: 49.27
Iteration: 1793 | Episodes: 109200 | Median Reward: 14.53 | Max Reward: 49.27
Iteration: 1794 | Episodes: 109300 | Median Reward: -4.36 | Max Reward: 49.27
Iteration: 1796 | Episodes: 109400 | Median Reward: -11.85 | Max Reward: 49.27
Iteration: 1798 | Episodes: 109500 | Median Reward: 9.52 | Max Reward: 49.27
Iteration: 1799 | Episodes: 109600 | Median Reward: -9.27 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -110      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1800      |
|    time_elapsed         | 32718     |
|    total_timesteps      | 11059200  |
| train/                  |           |
|    approx_kl            | 0.6604509 |
|    clip_fraction        | 0.0734    |
|    clip_range           | 0.4       |
|    entropy_loss         | -113      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 46.1      |
|    n_updates            | 17990     |
|    policy_gradient_loss | -0.00913  |
|    std                  | 4.01      |
|    value_loss           | 143       |
---------------------------------------
Iteration: 1801 | Episodes: 109700 | Median Reward: 23.58 | Max Reward: 49.27
Iteration: 1803 | Episodes: 109800 | Median Reward: -2.08 | Max Reward: 49.27
Iteration: 1804 | Episodes: 109900 | Median Reward: 0.12 | Max Reward: 49.27
Iteration: 1806 | Episodes: 110000 | Median Reward: 3.81 | Max Reward: 49.27
Iteration: 1807 | Episodes: 110100 | Median Reward: 3.81 | Max Reward: 49.27
Iteration: 1809 | Episodes: 110200 | Median Reward: -6.84 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -106       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1810       |
|    time_elapsed         | 32892      |
|    total_timesteps      | 11120640   |
| train/                  |            |
|    approx_kl            | 0.08767973 |
|    clip_fraction        | 0.0634     |
|    clip_range           | 0.4        |
|    entropy_loss         | -113       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 59.5       |
|    n_updates            | 18090      |
|    policy_gradient_loss | 0.00405    |
|    std                  | 4.02       |
|    value_loss           | 163        |
----------------------------------------
Iteration: 1811 | Episodes: 110300 | Median Reward: 7.72 | Max Reward: 49.27
Iteration: 1812 | Episodes: 110400 | Median Reward: -6.72 | Max Reward: 49.27
Iteration: 1814 | Episodes: 110500 | Median Reward: -2.62 | Max Reward: 49.27
Iteration: 1816 | Episodes: 110600 | Median Reward: -14.59 | Max Reward: 49.27
Iteration: 1817 | Episodes: 110700 | Median Reward: -5.24 | Max Reward: 49.27
Iteration: 1819 | Episodes: 110800 | Median Reward: -11.59 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -104        |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 1820        |
|    time_elapsed         | 33068       |
|    total_timesteps      | 11182080    |
| train/                  |             |
|    approx_kl            | 0.066385925 |
|    clip_fraction        | 0.0268      |
|    clip_range           | 0.4         |
|    entropy_loss         | -114        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 92          |
|    n_updates            | 18190       |
|    policy_gradient_loss | -0.000783   |
|    std                  | 4.02        |
|    value_loss           | 141         |
-----------------------------------------
Iteration: 1821 | Episodes: 110900 | Median Reward: -14.92 | Max Reward: 49.27
Iteration: 1822 | Episodes: 111000 | Median Reward: -12.04 | Max Reward: 49.27
Iteration: 1824 | Episodes: 111100 | Median Reward: -9.41 | Max Reward: 49.27
Iteration: 1826 | Episodes: 111200 | Median Reward: -8.79 | Max Reward: 49.27
Iteration: 1827 | Episodes: 111300 | Median Reward: -2.76 | Max Reward: 49.27
Iteration: 1829 | Episodes: 111400 | Median Reward: -9.63 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -90.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1830      |
|    time_elapsed         | 33256     |
|    total_timesteps      | 11243520  |
| train/                  |           |
|    approx_kl            | 0.6606471 |
|    clip_fraction        | 0.0769    |
|    clip_range           | 0.4       |
|    entropy_loss         | -112      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 37.8      |
|    n_updates            | 18290     |
|    policy_gradient_loss | 0.00562   |
|    std                  | 4.03      |
|    value_loss           | 137       |
---------------------------------------
Iteration: 1830 | Episodes: 111500 | Median Reward: -0.55 | Max Reward: 49.27
Iteration: 1832 | Episodes: 111600 | Median Reward: 30.75 | Max Reward: 49.27
Iteration: 1834 | Episodes: 111700 | Median Reward: 8.78 | Max Reward: 49.27
Iteration: 1835 | Episodes: 111800 | Median Reward: -15.50 | Max Reward: 49.27
Iteration: 1837 | Episodes: 111900 | Median Reward: -11.19 | Max Reward: 49.27
Iteration: 1839 | Episodes: 112000 | Median Reward: -4.37 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -96.7    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1840     |
|    time_elapsed         | 33430    |
|    total_timesteps      | 11304960 |
| train/                  |          |
|    approx_kl            | 8.979647 |
|    clip_fraction        | 0.276    |
|    clip_range           | 0.4      |
|    entropy_loss         | -111     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 37.2     |
|    n_updates            | 18390    |
|    policy_gradient_loss | 0.0583   |
|    std                  | 4.04     |
|    value_loss           | 185      |
--------------------------------------
Iteration: 1840 | Episodes: 112100 | Median Reward: -9.72 | Max Reward: 49.27
Iteration: 1842 | Episodes: 112200 | Median Reward: -19.87 | Max Reward: 49.27
Iteration: 1844 | Episodes: 112300 | Median Reward: -0.44 | Max Reward: 49.27
Iteration: 1845 | Episodes: 112400 | Median Reward: -15.04 | Max Reward: 49.27
Iteration: 1847 | Episodes: 112500 | Median Reward: -13.90 | Max Reward: 49.27
Iteration: 1849 | Episodes: 112600 | Median Reward: 21.96 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -83.2      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1850       |
|    time_elapsed         | 33607      |
|    total_timesteps      | 11366400   |
| train/                  |            |
|    approx_kl            | 0.41445783 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.4        |
|    entropy_loss         | -112       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 183        |
|    n_updates            | 18490      |
|    policy_gradient_loss | 0.0155     |
|    std                  | 4.04       |
|    value_loss           | 355        |
----------------------------------------
Iteration: 1850 | Episodes: 112700 | Median Reward: -6.47 | Max Reward: 49.27
Iteration: 1852 | Episodes: 112800 | Median Reward: -10.70 | Max Reward: 49.27
Iteration: 1854 | Episodes: 112900 | Median Reward: -7.41 | Max Reward: 49.27
Iteration: 1855 | Episodes: 113000 | Median Reward: -14.13 | Max Reward: 49.27
Iteration: 1857 | Episodes: 113100 | Median Reward: -16.62 | Max Reward: 49.27
Iteration: 1858 | Episodes: 113200 | Median Reward: -9.15 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -110       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1860       |
|    time_elapsed         | 33794      |
|    total_timesteps      | 11427840   |
| train/                  |            |
|    approx_kl            | 0.16046375 |
|    clip_fraction        | 0.0835     |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 38.5       |
|    n_updates            | 18590      |
|    policy_gradient_loss | -0.000648  |
|    std                  | 4.05       |
|    value_loss           | 108        |
----------------------------------------
Iteration: 1860 | Episodes: 113300 | Median Reward: -9.25 | Max Reward: 49.27
Iteration: 1862 | Episodes: 113400 | Median Reward: -9.51 | Max Reward: 49.27
Iteration: 1863 | Episodes: 113500 | Median Reward: -10.52 | Max Reward: 49.27
Iteration: 1865 | Episodes: 113600 | Median Reward: -5.93 | Max Reward: 49.27
Iteration: 1867 | Episodes: 113700 | Median Reward: -13.32 | Max Reward: 49.27
Iteration: 1868 | Episodes: 113800 | Median Reward: 0.64 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -102       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1870       |
|    time_elapsed         | 33970      |
|    total_timesteps      | 11489280   |
| train/                  |            |
|    approx_kl            | 0.16727397 |
|    clip_fraction        | 0.042      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 92.1       |
|    n_updates            | 18690      |
|    policy_gradient_loss | -0.00169   |
|    std                  | 4.05       |
|    value_loss           | 167        |
----------------------------------------
Iteration: 1870 | Episodes: 113900 | Median Reward: -9.99 | Max Reward: 49.27
Iteration: 1872 | Episodes: 114000 | Median Reward: -18.08 | Max Reward: 49.27
Iteration: 1873 | Episodes: 114100 | Median Reward: -9.13 | Max Reward: 49.27
Iteration: 1875 | Episodes: 114200 | Median Reward: 3.88 | Max Reward: 49.27
Iteration: 1877 | Episodes: 114300 | Median Reward: -6.85 | Max Reward: 49.27
Iteration: 1878 | Episodes: 114400 | Median Reward: -6.21 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -104       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1880       |
|    time_elapsed         | 34145      |
|    total_timesteps      | 11550720   |
| train/                  |            |
|    approx_kl            | 0.17535883 |
|    clip_fraction        | 0.0495     |
|    clip_range           | 0.4        |
|    entropy_loss         | -114       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 72.3       |
|    n_updates            | 18790      |
|    policy_gradient_loss | 0.0157     |
|    std                  | 4.07       |
|    value_loss           | 129        |
----------------------------------------
Iteration: 1880 | Episodes: 114500 | Median Reward: -11.94 | Max Reward: 49.27
Iteration: 1881 | Episodes: 114600 | Median Reward: -8.88 | Max Reward: 49.27
Iteration: 1883 | Episodes: 114700 | Median Reward: -4.61 | Max Reward: 49.27
Iteration: 1885 | Episodes: 114800 | Median Reward: -4.48 | Max Reward: 49.27
Iteration: 1886 | Episodes: 114900 | Median Reward: -13.58 | Max Reward: 49.27
Iteration: 1888 | Episodes: 115000 | Median Reward: -15.47 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -105       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1890       |
|    time_elapsed         | 34331      |
|    total_timesteps      | 11612160   |
| train/                  |            |
|    approx_kl            | 0.24548315 |
|    clip_fraction        | 0.0703     |
|    clip_range           | 0.4        |
|    entropy_loss         | -113       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 41.7       |
|    n_updates            | 18890      |
|    policy_gradient_loss | -0.0063    |
|    std                  | 4.07       |
|    value_loss           | 93.5       |
----------------------------------------
Iteration: 1890 | Episodes: 115100 | Median Reward: -13.01 | Max Reward: 49.27
Iteration: 1891 | Episodes: 115200 | Median Reward: -9.90 | Max Reward: 49.27
Iteration: 1893 | Episodes: 115300 | Median Reward: -7.98 | Max Reward: 49.27
Iteration: 1895 | Episodes: 115400 | Median Reward: -10.73 | Max Reward: 49.27
Iteration: 1896 | Episodes: 115500 | Median Reward: -7.53 | Max Reward: 49.27
Iteration: 1898 | Episodes: 115600 | Median Reward: -7.67 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -98.3    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1900     |
|    time_elapsed         | 34505    |
|    total_timesteps      | 11673600 |
| train/                  |          |
|    approx_kl            | 6.636175 |
|    clip_fraction        | 0.0787   |
|    clip_range           | 0.4      |
|    entropy_loss         | -115     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 127      |
|    n_updates            | 18990    |
|    policy_gradient_loss | 0.0115   |
|    std                  | 4.08     |
|    value_loss           | 220      |
--------------------------------------
Iteration: 1900 | Episodes: 115700 | Median Reward: -10.83 | Max Reward: 49.27
Iteration: 1901 | Episodes: 115800 | Median Reward: -19.83 | Max Reward: 49.27
Iteration: 1903 | Episodes: 115900 | Median Reward: -17.89 | Max Reward: 49.27
Iteration: 1904 | Episodes: 116000 | Median Reward: -14.56 | Max Reward: 49.27
Iteration: 1906 | Episodes: 116100 | Median Reward: -7.33 | Max Reward: 49.27
Iteration: 1908 | Episodes: 116200 | Median Reward: -15.59 | Max Reward: 49.27
Iteration: 1909 | Episodes: 116300 | Median Reward: -15.59 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -108      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1910      |
|    time_elapsed         | 34681     |
|    total_timesteps      | 11735040  |
| train/                  |           |
|    approx_kl            | 2.4346366 |
|    clip_fraction        | 0.228     |
|    clip_range           | 0.4       |
|    entropy_loss         | -113      |
|    explained_variance   | 2.38e-07  |
|    learning_rate        | 0.0001    |
|    loss                 | 37.4      |
|    n_updates            | 19090     |
|    policy_gradient_loss | 0.0627    |
|    std                  | 4.09      |
|    value_loss           | 144       |
---------------------------------------
Iteration: 1911 | Episodes: 116400 | Median Reward: -14.62 | Max Reward: 49.27
Iteration: 1913 | Episodes: 116500 | Median Reward: -11.89 | Max Reward: 49.27
Iteration: 1914 | Episodes: 116600 | Median Reward: -7.09 | Max Reward: 49.27
Iteration: 1916 | Episodes: 116700 | Median Reward: 1.95 | Max Reward: 49.27
Iteration: 1918 | Episodes: 116800 | Median Reward: 4.75 | Max Reward: 49.27
Iteration: 1919 | Episodes: 116900 | Median Reward: 28.10 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -80.7     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1920      |
|    time_elapsed         | 34867     |
|    total_timesteps      | 11796480  |
| train/                  |           |
|    approx_kl            | 3.6958778 |
|    clip_fraction        | 0.0622    |
|    clip_range           | 0.4       |
|    entropy_loss         | -116      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 142       |
|    n_updates            | 19190     |
|    policy_gradient_loss | 0.00339   |
|    std                  | 4.1       |
|    value_loss           | 316       |
---------------------------------------
Iteration: 1921 | Episodes: 117000 | Median Reward: 14.54 | Max Reward: 49.27
Iteration: 1923 | Episodes: 117100 | Median Reward: 38.97 | Max Reward: 49.27
Iteration: 1924 | Episodes: 117200 | Median Reward: 5.98 | Max Reward: 49.27
Iteration: 1926 | Episodes: 117300 | Median Reward: -2.16 | Max Reward: 49.27
Iteration: 1928 | Episodes: 117400 | Median Reward: 19.65 | Max Reward: 49.27
Iteration: 1929 | Episodes: 117500 | Median Reward: -8.22 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -99.1      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1930       |
|    time_elapsed         | 35043      |
|    total_timesteps      | 11857920   |
| train/                  |            |
|    approx_kl            | 0.07788673 |
|    clip_fraction        | 0.014      |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 48.3       |
|    n_updates            | 19290      |
|    policy_gradient_loss | -0.00216   |
|    std                  | 4.11       |
|    value_loss           | 117        |
----------------------------------------
Iteration: 1931 | Episodes: 117600 | Median Reward: -7.66 | Max Reward: 49.27
Iteration: 1932 | Episodes: 117700 | Median Reward: -14.22 | Max Reward: 49.27
Iteration: 1934 | Episodes: 117800 | Median Reward: -7.13 | Max Reward: 49.27
Iteration: 1936 | Episodes: 117900 | Median Reward: -7.06 | Max Reward: 49.27
Iteration: 1937 | Episodes: 118000 | Median Reward: -12.17 | Max Reward: 49.27
Iteration: 1939 | Episodes: 118100 | Median Reward: -8.82 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -106      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1940      |
|    time_elapsed         | 35217     |
|    total_timesteps      | 11919360  |
| train/                  |           |
|    approx_kl            | 0.3177781 |
|    clip_fraction        | 0.0882    |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 98.8      |
|    n_updates            | 19390     |
|    policy_gradient_loss | -0.00402  |
|    std                  | 4.12      |
|    value_loss           | 140       |
---------------------------------------
Iteration: 1941 | Episodes: 118200 | Median Reward: -8.82 | Max Reward: 49.27
Iteration: 1942 | Episodes: 118300 | Median Reward: -10.70 | Max Reward: 49.27
Iteration: 1944 | Episodes: 118400 | Median Reward: -11.99 | Max Reward: 49.27
Iteration: 1946 | Episodes: 118500 | Median Reward: -10.07 | Max Reward: 49.27
Iteration: 1947 | Episodes: 118600 | Median Reward: 4.21 | Max Reward: 49.27
Iteration: 1949 | Episodes: 118700 | Median Reward: -4.24 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -98.9      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1950       |
|    time_elapsed         | 35404      |
|    total_timesteps      | 11980800   |
| train/                  |            |
|    approx_kl            | 0.54508114 |
|    clip_fraction        | 0.0252     |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 67.5       |
|    n_updates            | 19490      |
|    policy_gradient_loss | 0.0125     |
|    std                  | 4.14       |
|    value_loss           | 126        |
----------------------------------------
Iteration: 1950 | Episodes: 118800 | Median Reward: 20.74 | Max Reward: 49.27
Iteration: 1952 | Episodes: 118900 | Median Reward: -5.78 | Max Reward: 49.27
Iteration: 1954 | Episodes: 119000 | Median Reward: -15.25 | Max Reward: 49.27
Iteration: 1955 | Episodes: 119100 | Median Reward: -5.29 | Max Reward: 49.27
Iteration: 1957 | Episodes: 119200 | Median Reward: 3.96 | Max Reward: 49.27
Iteration: 1959 | Episodes: 119300 | Median Reward: -5.58 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -95.9    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1960     |
|    time_elapsed         | 35578    |
|    total_timesteps      | 12042240 |
| train/                  |          |
|    approx_kl            | 3.515035 |
|    clip_fraction        | 0.13     |
|    clip_range           | 0.4      |
|    entropy_loss         | -115     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 181      |
|    n_updates            | 19590    |
|    policy_gradient_loss | 0.0208   |
|    std                  | 4.15     |
|    value_loss           | 234      |
--------------------------------------
Iteration: 1960 | Episodes: 119400 | Median Reward: 6.41 | Max Reward: 49.27
Iteration: 1962 | Episodes: 119500 | Median Reward: -6.70 | Max Reward: 49.27
Iteration: 1964 | Episodes: 119600 | Median Reward: -13.85 | Max Reward: 49.27
Iteration: 1965 | Episodes: 119700 | Median Reward: -13.50 | Max Reward: 49.27
Iteration: 1967 | Episodes: 119800 | Median Reward: -13.02 | Max Reward: 49.27
Iteration: 1969 | Episodes: 119900 | Median Reward: -7.64 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -92.1    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1970     |
|    time_elapsed         | 35751    |
|    total_timesteps      | 12103680 |
| train/                  |          |
|    approx_kl            | 17.49923 |
|    clip_fraction        | 0.17     |
|    clip_range           | 0.4      |
|    entropy_loss         | -114     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 142      |
|    n_updates            | 19690    |
|    policy_gradient_loss | 0.00421  |
|    std                  | 4.17     |
|    value_loss           | 286      |
--------------------------------------
Iteration: 1970 | Episodes: 120000 | Median Reward: -10.52 | Max Reward: 49.27
Iteration: 1972 | Episodes: 120100 | Median Reward: -10.86 | Max Reward: 49.27
Iteration: 1974 | Episodes: 120200 | Median Reward: -10.34 | Max Reward: 49.27
Iteration: 1975 | Episodes: 120300 | Median Reward: -0.76 | Max Reward: 49.27
Iteration: 1977 | Episodes: 120400 | Median Reward: -8.38 | Max Reward: 49.27
Iteration: 1978 | Episodes: 120500 | Median Reward: -6.72 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -94.7      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1980       |
|    time_elapsed         | 35936      |
|    total_timesteps      | 12165120   |
| train/                  |            |
|    approx_kl            | 0.34565187 |
|    clip_fraction        | 0.0669     |
|    clip_range           | 0.4        |
|    entropy_loss         | -115       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 77         |
|    n_updates            | 19790      |
|    policy_gradient_loss | -0.000824  |
|    std                  | 4.19       |
|    value_loss           | 253        |
----------------------------------------
Iteration: 1980 | Episodes: 120600 | Median Reward: -14.16 | Max Reward: 49.27
Iteration: 1982 | Episodes: 120700 | Median Reward: -16.69 | Max Reward: 49.27
Iteration: 1983 | Episodes: 120800 | Median Reward: -14.83 | Max Reward: 49.27
Iteration: 1985 | Episodes: 120900 | Median Reward: -13.65 | Max Reward: 49.27
Iteration: 1987 | Episodes: 121000 | Median Reward: -13.05 | Max Reward: 49.27
Iteration: 1988 | Episodes: 121100 | Median Reward: -10.07 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -103       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1990       |
|    time_elapsed         | 36108      |
|    total_timesteps      | 12226560   |
| train/                  |            |
|    approx_kl            | 0.20359835 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.4        |
|    entropy_loss         | -115       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 40.9       |
|    n_updates            | 19890      |
|    policy_gradient_loss | 0.00691    |
|    std                  | 4.2        |
|    value_loss           | 137        |
----------------------------------------
Iteration: 1990 | Episodes: 121200 | Median Reward: -11.59 | Max Reward: 49.27
Iteration: 1992 | Episodes: 121300 | Median Reward: 4.23 | Max Reward: 49.27
Iteration: 1993 | Episodes: 121400 | Median Reward: -13.67 | Max Reward: 49.27
Iteration: 1995 | Episodes: 121500 | Median Reward: -4.78 | Max Reward: 49.27
Iteration: 1997 | Episodes: 121600 | Median Reward: -10.70 | Max Reward: 49.27
Iteration: 1998 | Episodes: 121700 | Median Reward: -7.70 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -112       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2000       |
|    time_elapsed         | 36281      |
|    total_timesteps      | 12288000   |
| train/                  |            |
|    approx_kl            | 0.12333702 |
|    clip_fraction        | 0.029      |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 48.4       |
|    n_updates            | 19990      |
|    policy_gradient_loss | -0.0063    |
|    std                  | 4.21       |
|    value_loss           | 150        |
----------------------------------------
Iteration: 2000 | Episodes: 121800 | Median Reward: -13.82 | Max Reward: 49.27
Iteration: 2001 | Episodes: 121900 | Median Reward: -6.41 | Max Reward: 49.27
Iteration: 2003 | Episodes: 122000 | Median Reward: -11.21 | Max Reward: 49.27
Iteration: 2005 | Episodes: 122100 | Median Reward: -8.23 | Max Reward: 49.27
Iteration: 2006 | Episodes: 122200 | Median Reward: -12.72 | Max Reward: 49.27
Iteration: 2008 | Episodes: 122300 | Median Reward: -17.35 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -99.1    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2010     |
|    time_elapsed         | 36468    |
|    total_timesteps      | 12349440 |
| train/                  |          |
|    approx_kl            | 3.184101 |
|    clip_fraction        | 0.094    |
|    clip_range           | 0.4      |
|    entropy_loss         | -116     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 126      |
|    n_updates            | 20090    |
|    policy_gradient_loss | 0.00718  |
|    std                  | 4.22     |
|    value_loss           | 240      |
--------------------------------------
Iteration: 2010 | Episodes: 122400 | Median Reward: -9.12 | Max Reward: 49.27
Iteration: 2011 | Episodes: 122500 | Median Reward: 14.25 | Max Reward: 49.27
Iteration: 2013 | Episodes: 122600 | Median Reward: -0.88 | Max Reward: 49.27
Iteration: 2015 | Episodes: 122700 | Median Reward: -7.91 | Max Reward: 49.27
Iteration: 2016 | Episodes: 122800 | Median Reward: -3.86 | Max Reward: 49.27
Iteration: 2018 | Episodes: 122900 | Median Reward: 3.13 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2020      |
|    time_elapsed         | 36640     |
|    total_timesteps      | 12410880  |
| train/                  |           |
|    approx_kl            | 1.5247979 |
|    clip_fraction        | 0.145     |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 170       |
|    n_updates            | 20190     |
|    policy_gradient_loss | 0.032     |
|    std                  | 4.23      |
|    value_loss           | 391       |
---------------------------------------
Iteration: 2020 | Episodes: 123000 | Median Reward: 0.39 | Max Reward: 49.27
Iteration: 2021 | Episodes: 123100 | Median Reward: 2.65 | Max Reward: 49.27
Iteration: 2023 | Episodes: 123200 | Median Reward: -9.94 | Max Reward: 49.27
Iteration: 2024 | Episodes: 123300 | Median Reward: 1.02 | Max Reward: 49.27
Iteration: 2026 | Episodes: 123400 | Median Reward: 26.17 | Max Reward: 49.27
Iteration: 2028 | Episodes: 123500 | Median Reward: -9.44 | Max Reward: 49.27
Iteration: 2029 | Episodes: 123600 | Median Reward: -8.43 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -93.6      |
| time/                   |            |
|    fps                  | 339        |
|    iterations           | 2030       |
|    time_elapsed         | 36763      |
|    total_timesteps      | 12472320   |
| train/                  |            |
|    approx_kl            | 0.36656222 |
|    clip_fraction        | 0.078      |
|    clip_range           | 0.4        |
|    entropy_loss         | -116       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 45.7       |
|    n_updates            | 20290      |
|    policy_gradient_loss | -0.00545   |
|    std                  | 4.25       |
|    value_loss           | 336        |
----------------------------------------
Iteration: 2031 | Episodes: 123700 | Median Reward: -5.44 | Max Reward: 49.27
Iteration: 2033 | Episodes: 123800 | Median Reward: -6.19 | Max Reward: 49.27
Iteration: 2034 | Episodes: 123900 | Median Reward: -6.83 | Max Reward: 49.27
Iteration: 2036 | Episodes: 124000 | Median Reward: -19.21 | Max Reward: 49.27
Iteration: 2038 | Episodes: 124100 | Median Reward: -11.93 | Max Reward: 49.27
Iteration: 2039 | Episodes: 124200 | Median Reward: -11.88 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -101      |
| time/                   |           |
|    fps                  | 339       |
|    iterations           | 2040      |
|    time_elapsed         | 36946     |
|    total_timesteps      | 12533760  |
| train/                  |           |
|    approx_kl            | 1.9509556 |
|    clip_fraction        | 0.0613    |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 52.1      |
|    n_updates            | 20390     |
|    policy_gradient_loss | 0.00117   |
|    std                  | 4.27      |
|    value_loss           | 136       |
---------------------------------------
Iteration: 2041 | Episodes: 124300 | Median Reward: -9.47 | Max Reward: 49.27
Iteration: 2043 | Episodes: 124400 | Median Reward: -14.16 | Max Reward: 49.27
Iteration: 2044 | Episodes: 124500 | Median Reward: -12.31 | Max Reward: 49.27
Iteration: 2046 | Episodes: 124600 | Median Reward: -13.16 | Max Reward: 49.27
Iteration: 2047 | Episodes: 124700 | Median Reward: -6.78 | Max Reward: 49.27
Iteration: 2049 | Episodes: 124800 | Median Reward: -0.81 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -97.5    |
| time/                   |          |
|    fps                  | 339      |
|    iterations           | 2050     |
|    time_elapsed         | 37127    |
|    total_timesteps      | 12595200 |
| train/                  |          |
|    approx_kl            | 1.498627 |
|    clip_fraction        | 0.0718   |
|    clip_range           | 0.4      |
|    entropy_loss         | -117     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 56.2     |
|    n_updates            | 20490    |
|    policy_gradient_loss | 0.00242  |
|    std                  | 4.28     |
|    value_loss           | 234      |
--------------------------------------
Iteration: 2051 | Episodes: 124900 | Median Reward: -4.16 | Max Reward: 49.27
Iteration: 2052 | Episodes: 125000 | Median Reward: -6.97 | Max Reward: 49.27
Iteration: 2054 | Episodes: 125100 | Median Reward: 8.42 | Max Reward: 49.27
Iteration: 2056 | Episodes: 125200 | Median Reward: -20.01 | Max Reward: 49.27
Iteration: 2057 | Episodes: 125300 | Median Reward: -7.12 | Max Reward: 49.27
Iteration: 2059 | Episodes: 125400 | Median Reward: -14.31 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -113      |
| time/                   |           |
|    fps                  | 339       |
|    iterations           | 2060      |
|    time_elapsed         | 37325     |
|    total_timesteps      | 12656640  |
| train/                  |           |
|    approx_kl            | 1.3008647 |
|    clip_fraction        | 0.132     |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 38.4      |
|    n_updates            | 20590     |
|    policy_gradient_loss | 0.00765   |
|    std                  | 4.3       |
|    value_loss           | 169       |
---------------------------------------
Iteration: 2061 | Episodes: 125500 | Median Reward: -24.04 | Max Reward: 49.27
Iteration: 2062 | Episodes: 125600 | Median Reward: -15.78 | Max Reward: 49.27
Iteration: 2064 | Episodes: 125700 | Median Reward: -1.86 | Max Reward: 49.27
Iteration: 2066 | Episodes: 125800 | Median Reward: -5.54 | Max Reward: 49.27
Iteration: 2067 | Episodes: 125900 | Median Reward: -5.38 | Max Reward: 49.27
Iteration: 2069 | Episodes: 126000 | Median Reward: 13.43 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -86.3      |
| time/                   |            |
|    fps                  | 339        |
|    iterations           | 2070       |
|    time_elapsed         | 37507      |
|    total_timesteps      | 12718080   |
| train/                  |            |
|    approx_kl            | 0.13565825 |
|    clip_fraction        | 0.0262     |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 187        |
|    n_updates            | 20690      |
|    policy_gradient_loss | -0.00744   |
|    std                  | 4.3        |
|    value_loss           | 361        |
----------------------------------------
Iteration: 2071 | Episodes: 126100 | Median Reward: -7.86 | Max Reward: 49.27
Iteration: 2072 | Episodes: 126200 | Median Reward: 9.22 | Max Reward: 49.27
Iteration: 2074 | Episodes: 126300 | Median Reward: 3.17 | Max Reward: 49.27
Iteration: 2075 | Episodes: 126400 | Median Reward: 28.94 | Max Reward: 49.27
Iteration: 2077 | Episodes: 126500 | Median Reward: -1.23 | Max Reward: 49.27
Iteration: 2079 | Episodes: 126600 | Median Reward: -7.58 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91.6     |
| time/                   |           |
|    fps                  | 339       |
|    iterations           | 2080      |
|    time_elapsed         | 37689     |
|    total_timesteps      | 12779520  |
| train/                  |           |
|    approx_kl            | 1.4544592 |
|    clip_fraction        | 0.136     |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 101       |
|    n_updates            | 20790     |
|    policy_gradient_loss | -0.00823  |
|    std                  | 4.32      |
|    value_loss           | 179       |
---------------------------------------
Iteration: 2080 | Episodes: 126700 | Median Reward: 19.41 | Max Reward: 49.27
Iteration: 2082 | Episodes: 126800 | Median Reward: -4.25 | Max Reward: 49.27
Iteration: 2084 | Episodes: 126900 | Median Reward: -9.87 | Max Reward: 49.27
Iteration: 2085 | Episodes: 127000 | Median Reward: 45.46 | Max Reward: 49.27
Iteration: 2087 | Episodes: 127100 | Median Reward: -4.35 | Max Reward: 49.27
Iteration: 2089 | Episodes: 127200 | Median Reward: -2.10 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -92.7      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2090       |
|    time_elapsed         | 37884      |
|    total_timesteps      | 12840960   |
| train/                  |            |
|    approx_kl            | 0.66421723 |
|    clip_fraction        | 0.0392     |
|    clip_range           | 0.4        |
|    entropy_loss         | -118       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 141        |
|    n_updates            | 20890      |
|    policy_gradient_loss | -0.00308   |
|    std                  | 4.33       |
|    value_loss           | 318        |
----------------------------------------
Iteration: 2090 | Episodes: 127300 | Median Reward: -9.58 | Max Reward: 49.27
Iteration: 2092 | Episodes: 127400 | Median Reward: 22.49 | Max Reward: 49.27
Iteration: 2093 | Episodes: 127500 | Median Reward: -2.88 | Max Reward: 49.27
Iteration: 2095 | Episodes: 127600 | Median Reward: -1.30 | Max Reward: 49.27
Iteration: 2097 | Episodes: 127700 | Median Reward: -1.98 | Max Reward: 49.27
Iteration: 2098 | Episodes: 127800 | Median Reward: -5.49 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.8     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2100      |
|    time_elapsed         | 38065     |
|    total_timesteps      | 12902400  |
| train/                  |           |
|    approx_kl            | 1.5860125 |
|    clip_fraction        | 0.109     |
|    clip_range           | 0.4       |
|    entropy_loss         | -118      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 68.3      |
|    n_updates            | 20990     |
|    policy_gradient_loss | 0.0073    |
|    std                  | 4.34      |
|    value_loss           | 228       |
---------------------------------------
Iteration: 2100 | Episodes: 127900 | Median Reward: 33.54 | Max Reward: 49.27
Iteration: 2102 | Episodes: 128000 | Median Reward: 5.76 | Max Reward: 49.27
Iteration: 2103 | Episodes: 128100 | Median Reward: 44.19 | Max Reward: 49.27
Iteration: 2105 | Episodes: 128200 | Median Reward: 30.39 | Max Reward: 49.27
Iteration: 2107 | Episodes: 128300 | Median Reward: 43.55 | Max Reward: 49.27
Iteration: 2108 | Episodes: 128400 | Median Reward: 12.48 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -97.6      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2110       |
|    time_elapsed         | 38247      |
|    total_timesteps      | 12963840   |
| train/                  |            |
|    approx_kl            | 0.29441035 |
|    clip_fraction        | 0.0602     |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 135        |
|    n_updates            | 21090      |
|    policy_gradient_loss | -0.00473   |
|    std                  | 4.35       |
|    value_loss           | 373        |
----------------------------------------
Iteration: 2110 | Episodes: 128500 | Median Reward: -5.24 | Max Reward: 49.27
Iteration: 2112 | Episodes: 128600 | Median Reward: 6.62 | Max Reward: 49.27
Iteration: 2113 | Episodes: 128700 | Median Reward: -14.00 | Max Reward: 49.27
Iteration: 2115 | Episodes: 128800 | Median Reward: -10.49 | Max Reward: 49.27
Iteration: 2116 | Episodes: 128900 | Median Reward: 14.34 | Max Reward: 49.27
Iteration: 2118 | Episodes: 129000 | Median Reward: -20.95 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -114       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2120       |
|    time_elapsed         | 38443      |
|    total_timesteps      | 13025280   |
| train/                  |            |
|    approx_kl            | 0.27057874 |
|    clip_fraction        | 0.047      |
|    clip_range           | 0.4        |
|    entropy_loss         | -118       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 49.3       |
|    n_updates            | 21190      |
|    policy_gradient_loss | -0.0102    |
|    std                  | 4.37       |
|    value_loss           | 103        |
----------------------------------------
Iteration: 2120 | Episodes: 129100 | Median Reward: -15.20 | Max Reward: 49.27
Iteration: 2121 | Episodes: 129200 | Median Reward: -15.91 | Max Reward: 49.27
Iteration: 2123 | Episodes: 129300 | Median Reward: -12.66 | Max Reward: 49.27
Iteration: 2125 | Episodes: 129400 | Median Reward: -11.74 | Max Reward: 49.27
Iteration: 2126 | Episodes: 129500 | Median Reward: -5.82 | Max Reward: 49.27
Iteration: 2126 | Episodes: 129600 | Median Reward: 49.05 | Max Reward: 49.27
Iteration: 2127 | Episodes: 129700 | Median Reward: 44.85 | Max Reward: 49.27
Iteration: 2129 | Episodes: 129800 | Median Reward: -9.97 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -90        |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2130       |
|    time_elapsed         | 38627      |
|    total_timesteps      | 13086720   |
| train/                  |            |
|    approx_kl            | 0.22688742 |
|    clip_fraction        | 0.0594     |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 39.4       |
|    n_updates            | 21290      |
|    policy_gradient_loss | -0.00317   |
|    std                  | 4.38       |
|    value_loss           | 224        |
----------------------------------------
Iteration: 2130 | Episodes: 129900 | Median Reward: -2.51 | Max Reward: 49.27
Iteration: 2132 | Episodes: 130000 | Median Reward: -1.27 | Max Reward: 49.27
Iteration: 2134 | Episodes: 130100 | Median Reward: -0.94 | Max Reward: 49.27
Iteration: 2135 | Episodes: 130200 | Median Reward: -6.71 | Max Reward: 49.27
Iteration: 2137 | Episodes: 130300 | Median Reward: -10.34 | Max Reward: 49.27
Iteration: 2139 | Episodes: 130400 | Median Reward: -15.14 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -101     |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2140     |
|    time_elapsed         | 38810    |
|    total_timesteps      | 13148160 |
| train/                  |          |
|    approx_kl            | 1.682723 |
|    clip_fraction        | 0.0775   |
|    clip_range           | 0.4      |
|    entropy_loss         | -118     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 31.9     |
|    n_updates            | 21390    |
|    policy_gradient_loss | -0.0111  |
|    std                  | 4.4      |
|    value_loss           | 101      |
--------------------------------------
Iteration: 2140 | Episodes: 130500 | Median Reward: 0.79 | Max Reward: 49.27
Iteration: 2142 | Episodes: 130600 | Median Reward: -9.78 | Max Reward: 49.27
Iteration: 2144 | Episodes: 130700 | Median Reward: -11.72 | Max Reward: 49.27
Iteration: 2145 | Episodes: 130800 | Median Reward: -0.25 | Max Reward: 49.27
Iteration: 2147 | Episodes: 130900 | Median Reward: -5.37 | Max Reward: 49.27
Iteration: 2149 | Episodes: 131000 | Median Reward: -1.89 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91       |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2150      |
|    time_elapsed         | 38997     |
|    total_timesteps      | 13209600  |
| train/                  |           |
|    approx_kl            | 2.1086617 |
|    clip_fraction        | 0.114     |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0001    |
|    loss                 | 124       |
|    n_updates            | 21490     |
|    policy_gradient_loss | 0.00277   |
|    std                  | 4.41      |
|    value_loss           | 261       |
---------------------------------------
Iteration: 2150 | Episodes: 131100 | Median Reward: 7.61 | Max Reward: 49.27
Iteration: 2152 | Episodes: 131200 | Median Reward: 5.35 | Max Reward: 49.27
Iteration: 2153 | Episodes: 131300 | Median Reward: -9.91 | Max Reward: 49.27
Iteration: 2155 | Episodes: 131400 | Median Reward: 1.64 | Max Reward: 49.27
Iteration: 2157 | Episodes: 131500 | Median Reward: 44.24 | Max Reward: 49.27
Iteration: 2158 | Episodes: 131600 | Median Reward: 31.27 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 82.5        |
|    ep_rew_mean          | -44         |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 2160        |
|    time_elapsed         | 39180       |
|    total_timesteps      | 13271040    |
| train/                  |             |
|    approx_kl            | 0.018660352 |
|    clip_fraction        | 0.0274      |
|    clip_range           | 0.4         |
|    entropy_loss         | -117        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 249         |
|    n_updates            | 21590       |
|    policy_gradient_loss | -0.00483    |
|    std                  | 4.42        |
|    value_loss           | 440         |
-----------------------------------------
Iteration: 2160 | Episodes: 131700 | Median Reward: 46.98 | Max Reward: 49.27
Iteration: 2161 | Episodes: 131800 | Median Reward: 25.90 | Max Reward: 49.27
Iteration: 2163 | Episodes: 131900 | Median Reward: 41.85 | Max Reward: 49.27
Iteration: 2165 | Episodes: 132000 | Median Reward: 40.51 | Max Reward: 49.27
Iteration: 2166 | Episodes: 132100 | Median Reward: 39.46 | Max Reward: 49.27
Iteration: 2168 | Episodes: 132200 | Median Reward: 38.41 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -72.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2170      |
|    time_elapsed         | 39363     |
|    total_timesteps      | 13332480  |
| train/                  |           |
|    approx_kl            | 2.7021306 |
|    clip_fraction        | 0.136     |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 194       |
|    n_updates            | 21690     |
|    policy_gradient_loss | 0.0365    |
|    std                  | 4.43      |
|    value_loss           | 373       |
---------------------------------------
Iteration: 2170 | Episodes: 132300 | Median Reward: 38.75 | Max Reward: 49.27
Iteration: 2171 | Episodes: 132400 | Median Reward: 32.74 | Max Reward: 49.27
Iteration: 2173 | Episodes: 132500 | Median Reward: 44.42 | Max Reward: 49.27
Iteration: 2174 | Episodes: 132600 | Median Reward: 17.97 | Max Reward: 49.27
Iteration: 2176 | Episodes: 132700 | Median Reward: 36.92 | Max Reward: 49.27
Iteration: 2178 | Episodes: 132800 | Median Reward: 38.50 | Max Reward: 49.27
Iteration: 2179 | Episodes: 132900 | Median Reward: 36.81 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.1     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2180      |
|    time_elapsed         | 39558     |
|    total_timesteps      | 13393920  |
| train/                  |           |
|    approx_kl            | 0.3584999 |
|    clip_fraction        | 0.0864    |
|    clip_range           | 0.4       |
|    entropy_loss         | -116      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 178       |
|    n_updates            | 21790     |
|    policy_gradient_loss | -0.0109   |
|    std                  | 4.44      |
|    value_loss           | 383       |
---------------------------------------
Iteration: 2181 | Episodes: 133000 | Median Reward: 16.92 | Max Reward: 49.27
Iteration: 2183 | Episodes: 133100 | Median Reward: 15.61 | Max Reward: 49.27
Iteration: 2184 | Episodes: 133200 | Median Reward: 33.69 | Max Reward: 49.27
Iteration: 2186 | Episodes: 133300 | Median Reward: 14.38 | Max Reward: 49.27
Iteration: 2188 | Episodes: 133400 | Median Reward: 24.48 | Max Reward: 49.27
Iteration: 2189 | Episodes: 133500 | Median Reward: 0.14 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -95.1      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2190       |
|    time_elapsed         | 39739      |
|    total_timesteps      | 13455360   |
| train/                  |            |
|    approx_kl            | 0.23472056 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 134        |
|    n_updates            | 21890      |
|    policy_gradient_loss | 0.00331    |
|    std                  | 4.46       |
|    value_loss           | 315        |
----------------------------------------
Iteration: 2191 | Episodes: 133600 | Median Reward: -1.85 | Max Reward: 49.27
Iteration: 2193 | Episodes: 133700 | Median Reward: 2.99 | Max Reward: 49.27
Iteration: 2194 | Episodes: 133800 | Median Reward: -7.68 | Max Reward: 49.27
aaIteration: 2196 | Episodes: 133900 | Median Reward: -6.00 | Max Reward: 49.27
Iteration: 2198 | Episodes: 134000 | Median Reward: -6.22 | Max Reward: 49.27
Iteration: 2199 | Episodes: 134100 | Median Reward: -8.36 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -99.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2200      |
|    time_elapsed         | 39925     |
|    total_timesteps      | 13516800  |
| train/                  |           |
|    approx_kl            | 1.3871568 |
|    clip_fraction        | 0.158     |
|    clip_range           | 0.4       |
|    entropy_loss         | -116      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 50.4      |
|    n_updates            | 21990     |
|    policy_gradient_loss | 0.00841   |
|    std                  | 4.47      |
|    value_loss           | 194       |
---------------------------------------
Iteration: 2201 | Episodes: 134200 | Median Reward: -4.16 | Max Reward: 49.27
Iteration: 2202 | Episodes: 134300 | Median Reward: 9.39 | Max Reward: 49.27
Iteration: 2204 | Episodes: 134400 | Median Reward: -6.06 | Max Reward: 49.27
Iteration: 2206 | Episodes: 134500 | Median Reward: -11.05 | Max Reward: 49.27
Iteration: 2207 | Episodes: 134600 | Median Reward: -2.39 | Max Reward: 49.27
Iteration: 2209 | Episodes: 134700 | Median Reward: 3.84 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -86.8     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2210      |
|    time_elapsed         | 40119     |
|    total_timesteps      | 13578240  |
| train/                  |           |
|    approx_kl            | 5.0860643 |
|    clip_fraction        | 0.14      |
|    clip_range           | 0.4       |
|    entropy_loss         | -118      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 104       |
|    n_updates            | 22090     |
|    policy_gradient_loss | 0.0247    |
|    std                  | 4.49      |
|    value_loss           | 217       |
---------------------------------------
Iteration: 2211 | Episodes: 134800 | Median Reward: 4.75 | Max Reward: 49.27
Iteration: 2212 | Episodes: 134900 | Median Reward: 14.19 | Max Reward: 49.27
Iteration: 2214 | Episodes: 135000 | Median Reward: 20.33 | Max Reward: 49.27
Iteration: 2216 | Episodes: 135100 | Median Reward: -9.98 | Max Reward: 49.27
Iteration: 2217 | Episodes: 135200 | Median Reward: -2.78 | Max Reward: 49.27
Iteration: 2219 | Episodes: 135300 | Median Reward: -7.58 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2220       |
|    time_elapsed         | 40299      |
|    total_timesteps      | 13639680   |
| train/                  |            |
|    approx_kl            | 0.15871517 |
|    clip_fraction        | 0.0457     |
|    clip_range           | 0.4        |
|    entropy_loss         | -118       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 44.2       |
|    n_updates            | 22190      |
|    policy_gradient_loss | 0.00275    |
|    std                  | 4.5        |
|    value_loss           | 174        |
----------------------------------------
Iteration: 2221 | Episodes: 135400 | Median Reward: -9.24 | Max Reward: 49.27
Iteration: 2222 | Episodes: 135500 | Median Reward: -7.34 | Max Reward: 49.27
Iteration: 2224 | Episodes: 135600 | Median Reward: -2.63 | Max Reward: 49.27
Iteration: 2225 | Episodes: 135700 | Median Reward: 19.52 | Max Reward: 49.27
Iteration: 2227 | Episodes: 135800 | Median Reward: -5.57 | Max Reward: 49.27
Iteration: 2229 | Episodes: 135900 | Median Reward: 22.80 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2230      |
|    time_elapsed         | 40480     |
|    total_timesteps      | 13701120  |
| train/                  |           |
|    approx_kl            | 1.8081627 |
|    clip_fraction        | 0.144     |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 156       |
|    n_updates            | 22290     |
|    policy_gradient_loss | 0.0397    |
|    std                  | 4.52      |
|    value_loss           | 400       |
---------------------------------------
Iteration: 2230 | Episodes: 136000 | Median Reward: -9.54 | Max Reward: 49.27
Iteration: 2232 | Episodes: 136100 | Median Reward: 26.37 | Max Reward: 49.27
Iteration: 2234 | Episodes: 136200 | Median Reward: 26.37 | Max Reward: 49.27
Iteration: 2235 | Episodes: 136300 | Median Reward: 31.77 | Max Reward: 49.27
Iteration: 2237 | Episodes: 136400 | Median Reward: 42.24 | Max Reward: 49.27
Iteration: 2239 | Episodes: 136500 | Median Reward: 38.90 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.8       |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 2240        |
|    time_elapsed         | 40661       |
|    total_timesteps      | 13762560    |
| train/                  |             |
|    approx_kl            | 0.013838274 |
|    clip_fraction        | 0.0153      |
|    clip_range           | 0.4         |
|    entropy_loss         | -119        |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 138         |
|    n_updates            | 22390       |
|    policy_gradient_loss | -0.00227    |
|    std                  | 4.53        |
|    value_loss           | 445         |
-----------------------------------------
Iteration: 2240 | Episodes: 136600 | Median Reward: 44.31 | Max Reward: 49.27
Iteration: 2242 | Episodes: 136700 | Median Reward: 39.01 | Max Reward: 49.27
Iteration: 2244 | Episodes: 136800 | Median Reward: 32.61 | Max Reward: 49.27
Iteration: 2245 | Episodes: 136900 | Median Reward: 33.01 | Max Reward: 49.27
Iteration: 2247 | Episodes: 137000 | Median Reward: 31.87 | Max Reward: 49.27
Iteration: 2248 | Episodes: 137100 | Median Reward: -3.92 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -84.3     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2250      |
|    time_elapsed         | 40854     |
|    total_timesteps      | 13824000  |
| train/                  |           |
|    approx_kl            | 2.5613952 |
|    clip_fraction        | 0.0927    |
|    clip_range           | 0.4       |
|    entropy_loss         | -118      |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 196       |
|    n_updates            | 22490     |
|    policy_gradient_loss | 0.019     |
|    std                  | 4.54      |
|    value_loss           | 249       |
---------------------------------------
Iteration: 2250 | Episodes: 137200 | Median Reward: 29.65 | Max Reward: 49.27
Iteration: 2252 | Episodes: 137300 | Median Reward: 31.20 | Max Reward: 49.27
Iteration: 2253 | Episodes: 137400 | Median Reward: -13.40 | Max Reward: 49.27
Iteration: 2255 | Episodes: 137500 | Median Reward: -10.32 | Max Reward: 49.27
Iteration: 2257 | Episodes: 137600 | Median Reward: -12.50 | Max Reward: 49.27
Iteration: 2258 | Episodes: 137700 | Median Reward: 8.15 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -100       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2260       |
|    time_elapsed         | 41036      |
|    total_timesteps      | 13885440   |
| train/                  |            |
|    approx_kl            | 0.56464726 |
|    clip_fraction        | 0.0731     |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 135        |
|    n_updates            | 22590      |
|    policy_gradient_loss | 0.0115     |
|    std                  | 4.55       |
|    value_loss           | 219        |
----------------------------------------
Iteration: 2260 | Episodes: 137800 | Median Reward: -7.33 | Max Reward: 49.27
Iteration: 2262 | Episodes: 137900 | Median Reward: 6.12 | Max Reward: 49.27
Iteration: 2263 | Episodes: 138000 | Median Reward: -0.47 | Max Reward: 49.27
Iteration: 2265 | Episodes: 138100 | Median Reward: -7.95 | Max Reward: 49.27
Iteration: 2267 | Episodes: 138200 | Median Reward: 6.76 | Max Reward: 49.27
Iteration: 2268 | Episodes: 138300 | Median Reward: -15.41 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -99.1      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2270       |
|    time_elapsed         | 41215      |
|    total_timesteps      | 13946880   |
| train/                  |            |
|    approx_kl            | 0.10287446 |
|    clip_fraction        | 0.0646     |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 74.8       |
|    n_updates            | 22690      |
|    policy_gradient_loss | 0.00751    |
|    std                  | 4.56       |
|    value_loss           | 123        |
----------------------------------------
Iteration: 2270 | Episodes: 138400 | Median Reward: -9.20 | Max Reward: 49.27
Iteration: 2272 | Episodes: 138500 | Median Reward: -5.33 | Max Reward: 49.27
Iteration: 2273 | Episodes: 138600 | Median Reward: 1.76 | Max Reward: 49.27
Iteration: 2275 | Episodes: 138700 | Median Reward: -9.53 | Max Reward: 49.27
Iteration: 2276 | Episodes: 138800 | Median Reward: -6.21 | Max Reward: 49.27
Iteration: 2278 | Episodes: 138900 | Median Reward: 6.36 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -105      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2280      |
|    time_elapsed         | 41411     |
|    total_timesteps      | 14008320  |
| train/                  |           |
|    approx_kl            | 0.7985083 |
|    clip_fraction        | 0.0432    |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0001    |
|    loss                 | 119       |
|    n_updates            | 22790     |
|    policy_gradient_loss | 0.00896   |
|    std                  | 4.57      |
|    value_loss           | 164       |
---------------------------------------
Iteration: 2280 | Episodes: 139000 | Median Reward: -11.00 | Max Reward: 49.27
Iteration: 2281 | Episodes: 139100 | Median Reward: -9.99 | Max Reward: 49.27
Iteration: 2283 | Episodes: 139200 | Median Reward: 20.39 | Max Reward: 49.27
Iteration: 2285 | Episodes: 139300 | Median Reward: -15.99 | Max Reward: 49.27
Iteration: 2286 | Episodes: 139400 | Median Reward: -13.41 | Max Reward: 49.27
Iteration: 2288 | Episodes: 139500 | Median Reward: -7.18 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -93.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2290      |
|    time_elapsed         | 41594     |
|    total_timesteps      | 14069760  |
| train/                  |           |
|    approx_kl            | 2.4941025 |
|    clip_fraction        | 0.12      |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 204       |
|    n_updates            | 22890     |
|    policy_gradient_loss | 0.0224    |
|    std                  | 4.58      |
|    value_loss           | 389       |
---------------------------------------
Iteration: 2290 | Episodes: 139600 | Median Reward: -10.99 | Max Reward: 49.27
Iteration: 2291 | Episodes: 139700 | Median Reward: 24.76 | Max Reward: 49.27
Iteration: 2293 | Episodes: 139800 | Median Reward: -3.64 | Max Reward: 49.27
Iteration: 2295 | Episodes: 139900 | Median Reward: -9.85 | Max Reward: 49.27
Iteration: 2296 | Episodes: 140000 | Median Reward: 35.19 | Max Reward: 49.27
Iteration: 2298 | Episodes: 140100 | Median Reward: -15.32 | Max Reward: 49.27
Iteration: 2299 | Episodes: 140200 | Median Reward: -10.08 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -102        |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 2300        |
|    time_elapsed         | 41776       |
|    total_timesteps      | 14131200    |
| train/                  |             |
|    approx_kl            | 0.009430228 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.4         |
|    entropy_loss         | -119        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 145         |
|    n_updates            | 22990       |
|    policy_gradient_loss | 0.00629     |
|    std                  | 4.6         |
|    value_loss           | 165         |
-----------------------------------------
Iteration: 2301 | Episodes: 140300 | Median Reward: -3.09 | Max Reward: 49.27
Iteration: 2303 | Episodes: 140400 | Median Reward: -12.88 | Max Reward: 49.27
Iteration: 2304 | Episodes: 140500 | Median Reward: 24.83 | Max Reward: 49.27
Iteration: 2306 | Episodes: 140600 | Median Reward: -15.15 | Max Reward: 49.27
Iteration: 2308 | Episodes: 140700 | Median Reward: -14.45 | Max Reward: 49.27
Iteration: 2309 | Episodes: 140800 | Median Reward: -17.46 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -99.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2310      |
|    time_elapsed         | 41958     |
|    total_timesteps      | 14192640  |
| train/                  |           |
|    approx_kl            | 1.0455328 |
|    clip_fraction        | 0.0631    |
|    clip_range           | 0.4       |
|    entropy_loss         | -118      |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 114       |
|    n_updates            | 23090     |
|    policy_gradient_loss | 0.0117    |
|    std                  | 4.61      |
|    value_loss           | 233       |
---------------------------------------
Iteration: 2311 | Episodes: 140900 | Median Reward: 37.46 | Max Reward: 49.27
Iteration: 2313 | Episodes: 141000 | Median Reward: 4.70 | Max Reward: 49.27
Iteration: 2314 | Episodes: 141100 | Median Reward: 26.74 | Max Reward: 49.27
Iteration: 2316 | Episodes: 141200 | Median Reward: -11.29 | Max Reward: 49.27
Iteration: 2318 | Episodes: 141300 | Median Reward: -15.68 | Max Reward: 49.27
Iteration: 2319 | Episodes: 141400 | Median Reward: -14.37 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -108      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2320      |
|    time_elapsed         | 42153     |
|    total_timesteps      | 14254080  |
| train/                  |           |
|    approx_kl            | 1.3244145 |
|    clip_fraction        | 0.0908    |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 95.3      |
|    n_updates            | 23190     |
|    policy_gradient_loss | 0.00838   |
|    std                  | 4.62      |
|    value_loss           | 264       |
---------------------------------------
Iteration: 2321 | Episodes: 141500 | Median Reward: -12.53 | Max Reward: 49.27
Iteration: 2322 | Episodes: 141600 | Median Reward: -13.31 | Max Reward: 49.27
Iteration: 2324 | Episodes: 141700 | Median Reward: -15.07 | Max Reward: 49.27
Iteration: 2326 | Episodes: 141800 | Median Reward: 2.22 | Max Reward: 49.27
Iteration: 2327 | Episodes: 141900 | Median Reward: 32.98 | Max Reward: 49.27
Iteration: 2329 | Episodes: 142000 | Median Reward: 38.52 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -84.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2330      |
|    time_elapsed         | 42337     |
|    total_timesteps      | 14315520  |
| train/                  |           |
|    approx_kl            | 0.2609706 |
|    clip_fraction        | 0.0393    |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 280       |
|    n_updates            | 23290     |
|    policy_gradient_loss | -0.00587  |
|    std                  | 4.63      |
|    value_loss           | 345       |
---------------------------------------
Iteration: 2331 | Episodes: 142100 | Median Reward: 42.07 | Max Reward: 49.27
Iteration: 2332 | Episodes: 142200 | Median Reward: 6.53 | Max Reward: 49.27
Iteration: 2334 | Episodes: 142300 | Median Reward: 1.15 | Max Reward: 49.27
Iteration: 2336 | Episodes: 142400 | Median Reward: 37.65 | Max Reward: 49.27
Iteration: 2337 | Episodes: 142500 | Median Reward: 1.13 | Max Reward: 49.27
Iteration: 2339 | Episodes: 142600 | Median Reward: 33.60 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -79.1     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2340      |
|    time_elapsed         | 42519     |
|    total_timesteps      | 14376960  |
| train/                  |           |
|    approx_kl            | 1.8547821 |
|    clip_fraction        | 0.0854    |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 260       |
|    n_updates            | 23390     |
|    policy_gradient_loss | 0.00374   |
|    std                  | 4.65      |
|    value_loss           | 425       |
---------------------------------------
Iteration: 2341 | Episodes: 142700 | Median Reward: 30.70 | Max Reward: 49.27
Iteration: 2342 | Episodes: 142800 | Median Reward: -0.51 | Max Reward: 49.27
Iteration: 2344 | Episodes: 142900 | Median Reward: -2.98 | Max Reward: 49.27
Iteration: 2345 | Episodes: 143000 | Median Reward: 38.47 | Max Reward: 49.27
Iteration: 2347 | Episodes: 143100 | Median Reward: -2.52 | Max Reward: 49.27
Iteration: 2349 | Episodes: 143200 | Median Reward: -0.87 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -105      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2350      |
|    time_elapsed         | 42704     |
|    total_timesteps      | 14438400  |
| train/                  |           |
|    approx_kl            | 1.9793098 |
|    clip_fraction        | 0.0666    |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 36.3      |
|    n_updates            | 23490     |
|    policy_gradient_loss | 0.00229   |
|    std                  | 4.66      |
|    value_loss           | 177       |
---------------------------------------
Iteration: 2350 | Episodes: 143300 | Median Reward: -7.78 | Max Reward: 49.27
Iteration: 2352 | Episodes: 143400 | Median Reward: -10.09 | Max Reward: 49.27
Iteration: 2354 | Episodes: 143500 | Median Reward: -8.30 | Max Reward: 49.27
Iteration: 2355 | Episodes: 143600 | Median Reward: -8.38 | Max Reward: 49.27
Iteration: 2357 | Episodes: 143700 | Median Reward: -8.03 | Max Reward: 49.27
Iteration: 2359 | Episodes: 143800 | Median Reward: -3.61 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -107       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2360       |
|    time_elapsed         | 42891      |
|    total_timesteps      | 14499840   |
| train/                  |            |
|    approx_kl            | 0.06708268 |
|    clip_fraction        | 0.0247     |
|    clip_range           | 0.4        |
|    entropy_loss         | -119       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 60.7       |
|    n_updates            | 23590      |
|    policy_gradient_loss | 0.00358    |
|    std                  | 4.68       |
|    value_loss           | 135        |
----------------------------------------
Iteration: 2360 | Episodes: 143900 | Median Reward: -15.74 | Max Reward: 49.27
Iteration: 2362 | Episodes: 144000 | Median Reward: -9.82 | Max Reward: 49.27
Iteration: 2364 | Episodes: 144100 | Median Reward: -17.09 | Max Reward: 49.27
Iteration: 2365 | Episodes: 144200 | Median Reward: 21.48 | Max Reward: 49.27
Iteration: 2367 | Episodes: 144300 | Median Reward: -6.03 | Max Reward: 49.27
Iteration: 2368 | Episodes: 144400 | Median Reward: -9.60 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -96.4      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2370       |
|    time_elapsed         | 43073      |
|    total_timesteps      | 14561280   |
| train/                  |            |
|    approx_kl            | 0.51887393 |
|    clip_fraction        | 0.0429     |
|    clip_range           | 0.4        |
|    entropy_loss         | -119       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 30.5       |
|    n_updates            | 23690      |
|    policy_gradient_loss | 0.00466    |
|    std                  | 4.68       |
|    value_loss           | 197        |
----------------------------------------
Iteration: 2370 | Episodes: 144500 | Median Reward: 1.92 | Max Reward: 49.27
Iteration: 2372 | Episodes: 144600 | Median Reward: 31.02 | Max Reward: 49.27
Iteration: 2373 | Episodes: 144700 | Median Reward: -2.20 | Max Reward: 49.27
Iteration: 2375 | Episodes: 144800 | Median Reward: -14.63 | Max Reward: 49.27
Iteration: 2377 | Episodes: 144900 | Median Reward: -11.73 | Max Reward: 49.27
Iteration: 2378 | Episodes: 145000 | Median Reward: -13.70 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -107        |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 2380        |
|    time_elapsed         | 43255       |
|    total_timesteps      | 14622720    |
| train/                  |             |
|    approx_kl            | 0.100927316 |
|    clip_fraction        | 0.0707      |
|    clip_range           | 0.4         |
|    entropy_loss         | -118        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 36.8        |
|    n_updates            | 23790       |
|    policy_gradient_loss | -0.00909    |
|    std                  | 4.69        |
|    value_loss           | 135         |
-----------------------------------------
Iteration: 2380 | Episodes: 145100 | Median Reward: -9.91 | Max Reward: 49.27
Iteration: 2382 | Episodes: 145200 | Median Reward: -5.12 | Max Reward: 49.27
Iteration: 2383 | Episodes: 145300 | Median Reward: -5.90 | Max Reward: 49.27
Iteration: 2385 | Episodes: 145400 | Median Reward: -12.63 | Max Reward: 49.27
Iteration: 2387 | Episodes: 145500 | Median Reward: 1.79 | Max Reward: 49.27
Iteration: 2388 | Episodes: 145600 | Median Reward: -9.02 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -96.9    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2390     |
|    time_elapsed         | 43451    |
|    total_timesteps      | 14684160 |
| train/                  |          |
|    approx_kl            | 1.21162  |
|    clip_fraction        | 0.0788   |
|    clip_range           | 0.4      |
|    entropy_loss         | -119     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 78.9     |
|    n_updates            | 23890    |
|    policy_gradient_loss | -0.00615 |
|    std                  | 4.7      |
|    value_loss           | 233      |
--------------------------------------
Iteration: 2390 | Episodes: 145700 | Median Reward: -10.51 | Max Reward: 49.27
Iteration: 2391 | Episodes: 145800 | Median Reward: -8.99 | Max Reward: 49.27
Iteration: 2393 | Episodes: 145900 | Median Reward: -8.45 | Max Reward: 49.27
Iteration: 2395 | Episodes: 146000 | Median Reward: -8.45 | Max Reward: 49.27
Iteration: 2396 | Episodes: 146100 | Median Reward: -12.54 | Max Reward: 49.27
Iteration: 2398 | Episodes: 146200 | Median Reward: -1.36 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -89.4      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2400       |
|    time_elapsed         | 43635      |
|    total_timesteps      | 14745600   |
| train/                  |            |
|    approx_kl            | 0.87092966 |
|    clip_fraction        | 0.0706     |
|    clip_range           | 0.4        |
|    entropy_loss         | -119       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 95         |
|    n_updates            | 23990      |
|    policy_gradient_loss | 0.00578    |
|    std                  | 4.72       |
|    value_loss           | 249        |
----------------------------------------
Iteration: 2400 | Episodes: 146300 | Median Reward: -6.68 | Max Reward: 49.27
Iteration: 2401 | Episodes: 146400 | Median Reward: -6.72 | Max Reward: 49.27
Iteration: 2403 | Episodes: 146500 | Median Reward: -10.33 | Max Reward: 49.27
Iteration: 2405 | Episodes: 146600 | Median Reward: -11.60 | Max Reward: 49.27
Iteration: 2406 | Episodes: 146700 | Median Reward: -10.38 | Max Reward: 49.27
Iteration: 2408 | Episodes: 146800 | Median Reward: 2.78 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -103      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2410      |
|    time_elapsed         | 43816     |
|    total_timesteps      | 14807040  |
| train/                  |           |
|    approx_kl            | 0.8362583 |
|    clip_fraction        | 0.0821    |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 69        |
|    n_updates            | 24090     |
|    policy_gradient_loss | 0.028     |
|    std                  | 4.73      |
|    value_loss           | 230       |
---------------------------------------
Iteration: 2410 | Episodes: 146900 | Median Reward: -10.91 | Max Reward: 49.27
Iteration: 2411 | Episodes: 147000 | Median Reward: 3.80 | Max Reward: 49.27
Iteration: 2413 | Episodes: 147100 | Median Reward: -6.48 | Max Reward: 49.27
Iteration: 2415 | Episodes: 147200 | Median Reward: -6.97 | Max Reward: 49.27
Iteration: 2416 | Episodes: 147300 | Median Reward: 7.60 | Max Reward: 49.27
Iteration: 2418 | Episodes: 147400 | Median Reward: 32.04 | Max Reward: 49.27
Iteration: 2419 | Episodes: 147500 | Median Reward: -17.96 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -109      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2420      |
|    time_elapsed         | 43997     |
|    total_timesteps      | 14868480  |
| train/                  |           |
|    approx_kl            | 3.0955257 |
|    clip_fraction        | 0.0964    |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 63.5      |
|    n_updates            | 24190     |
|    policy_gradient_loss | 0.0225    |
|    std                  | 4.74      |
|    value_loss           | 246       |
---------------------------------------
Iteration: 2421 | Episodes: 147600 | Median Reward: -11.52 | Max Reward: 49.27
Iteration: 2423 | Episodes: 147700 | Median Reward: -8.65 | Max Reward: 49.27
Iteration: 2424 | Episodes: 147800 | Median Reward: -9.81 | Max Reward: 49.27
Iteration: 2426 | Episodes: 147900 | Median Reward: 25.76 | Max Reward: 49.27
Iteration: 2428 | Episodes: 148000 | Median Reward: 20.81 | Max Reward: 49.27
Iteration: 2429 | Episodes: 148100 | Median Reward: 29.27 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -80.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2430      |
|    time_elapsed         | 44187     |
|    total_timesteps      | 14929920  |
| train/                  |           |
|    approx_kl            | 2.9574835 |
|    clip_fraction        | 0.0914    |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 170       |
|    n_updates            | 24290     |
|    policy_gradient_loss | 0.0418    |
|    std                  | 4.76      |
|    value_loss           | 369       |
---------------------------------------
Iteration: 2431 | Episodes: 148200 | Median Reward: -9.26 | Max Reward: 49.27
Iteration: 2433 | Episodes: 148300 | Median Reward: 37.61 | Max Reward: 49.27
Iteration: 2434 | Episodes: 148400 | Median Reward: 38.12 | Max Reward: 49.27
Iteration: 2436 | Episodes: 148500 | Median Reward: 39.64 | Max Reward: 49.27
Iteration: 2438 | Episodes: 148600 | Median Reward: 34.27 | Max Reward: 49.27
Iteration: 2439 | Episodes: 148700 | Median Reward: 33.39 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2440      |
|    time_elapsed         | 44365     |
|    total_timesteps      | 14991360  |
| train/                  |           |
|    approx_kl            | 3.2153196 |
|    clip_fraction        | 0.0892    |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 220       |
|    n_updates            | 24390     |
|    policy_gradient_loss | 0.0308    |
|    std                  | 4.76      |
|    value_loss           | 415       |
---------------------------------------
Iteration: 2441 | Episodes: 148800 | Median Reward: 31.70 | Max Reward: 49.27
Iteration: 2442 | Episodes: 148900 | Median Reward: 40.44 | Max Reward: 49.27
Iteration: 2444 | Episodes: 149000 | Median Reward: 40.60 | Max Reward: 49.27
Iteration: 2446 | Episodes: 149100 | Median Reward: 33.65 | Max Reward: 49.27
Iteration: 2447 | Episodes: 149200 | Median Reward: 2.38 | Max Reward: 49.27
Iteration: 2449 | Episodes: 149300 | Median Reward: -11.00 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -104      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2450      |
|    time_elapsed         | 44547     |
|    total_timesteps      | 15052800  |
| train/                  |           |
|    approx_kl            | 1.1514971 |
|    clip_fraction        | 0.0765    |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 82.5      |
|    n_updates            | 24490     |
|    policy_gradient_loss | 0.0331    |
|    std                  | 4.77      |
|    value_loss           | 123       |
---------------------------------------
Iteration: 2451 | Episodes: 149400 | Median Reward: -1.18 | Max Reward: 49.27
Iteration: 2452 | Episodes: 149500 | Median Reward: -8.97 | Max Reward: 49.27
Iteration: 2454 | Episodes: 149600 | Median Reward: 24.86 | Max Reward: 49.27
Iteration: 2456 | Episodes: 149700 | Median Reward: 25.01 | Max Reward: 49.27
Iteration: 2457 | Episodes: 149800 | Median Reward: 21.50 | Max Reward: 49.27
Iteration: 2459 | Episodes: 149900 | Median Reward: 28.79 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2460      |
|    time_elapsed         | 44722     |
|    total_timesteps      | 15114240  |
| train/                  |           |
|    approx_kl            | 1.7355232 |
|    clip_fraction        | 0.0626    |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 116       |
|    n_updates            | 24590     |
|    policy_gradient_loss | 0.0149    |
|    std                  | 4.78      |
|    value_loss           | 382       |
---------------------------------------
Iteration: 2461 | Episodes: 150000 | Median Reward: 19.80 | Max Reward: 49.27
Iteration: 2462 | Episodes: 150100 | Median Reward: 38.09 | Max Reward: 49.27
Iteration: 2464 | Episodes: 150200 | Median Reward: 41.27 | Max Reward: 49.27
Iteration: 2465 | Episodes: 150300 | Median Reward: 36.69 | Max Reward: 49.27
Iteration: 2467 | Episodes: 150400 | Median Reward: 40.06 | Max Reward: 49.27
Iteration: 2469 | Episodes: 150500 | Median Reward: 35.41 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -71.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2470      |
|    time_elapsed         | 44913     |
|    total_timesteps      | 15175680  |
| train/                  |           |
|    approx_kl            | 0.9465078 |
|    clip_fraction        | 0.062     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0001    |
|    loss                 | 163       |
|    n_updates            | 24690     |
|    policy_gradient_loss | 0.00718   |
|    std                  | 4.79      |
|    value_loss           | 326       |
---------------------------------------
Iteration: 2470 | Episodes: 150600 | Median Reward: 35.49 | Max Reward: 49.27
Iteration: 2472 | Episodes: 150700 | Median Reward: 32.05 | Max Reward: 49.27
Iteration: 2474 | Episodes: 150800 | Median Reward: 16.09 | Max Reward: 49.27
Iteration: 2475 | Episodes: 150900 | Median Reward: 23.93 | Max Reward: 49.27
Iteration: 2477 | Episodes: 151000 | Median Reward: 22.79 | Max Reward: 49.27
Iteration: 2479 | Episodes: 151100 | Median Reward: 14.10 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -79.7    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2480     |
|    time_elapsed         | 45089    |
|    total_timesteps      | 15237120 |
| train/                  |          |
|    approx_kl            | 1.273957 |
|    clip_fraction        | 0.0625   |
|    clip_range           | 0.4      |
|    entropy_loss         | -121     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 142      |
|    n_updates            | 24790    |
|    policy_gradient_loss | 0.0179   |
|    std                  | 4.81     |
|    value_loss           | 286      |
--------------------------------------
Iteration: 2480 | Episodes: 151200 | Median Reward: 37.05 | Max Reward: 49.27
Iteration: 2482 | Episodes: 151300 | Median Reward: 23.39 | Max Reward: 49.27
Iteration: 2484 | Episodes: 151400 | Median Reward: -0.84 | Max Reward: 49.27
Iteration: 2485 | Episodes: 151500 | Median Reward: 5.17 | Max Reward: 49.27
Iteration: 2487 | Episodes: 151600 | Median Reward: 0.93 | Max Reward: 49.27
Iteration: 2488 | Episodes: 151700 | Median Reward: 9.82 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -74.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2490      |
|    time_elapsed         | 45269     |
|    total_timesteps      | 15298560  |
| train/                  |           |
|    approx_kl            | 1.4928685 |
|    clip_fraction        | 0.0592    |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 165       |
|    n_updates            | 24890     |
|    policy_gradient_loss | 0.0301    |
|    std                  | 4.83      |
|    value_loss           | 369       |
---------------------------------------
Iteration: 2490 | Episodes: 151800 | Median Reward: 38.40 | Max Reward: 49.27
Iteration: 2492 | Episodes: 151900 | Median Reward: 38.04 | Max Reward: 49.27
Iteration: 2493 | Episodes: 152000 | Median Reward: 36.92 | Max Reward: 49.27
Iteration: 2495 | Episodes: 152100 | Median Reward: 42.60 | Max Reward: 49.27
Iteration: 2497 | Episodes: 152200 | Median Reward: 35.32 | Max Reward: 49.27
Iteration: 2498 | Episodes: 152300 | Median Reward: 42.61 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -62.4    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2500     |
|    time_elapsed         | 45447    |
|    total_timesteps      | 15360000 |
| train/                  |          |
|    approx_kl            | 1.304359 |
|    clip_fraction        | 0.0654   |
|    clip_range           | 0.4      |
|    entropy_loss         | -121     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 231      |
|    n_updates            | 24990    |
|    policy_gradient_loss | 0.00928  |
|    std                  | 4.83     |
|    value_loss           | 513      |
--------------------------------------
Iteration: 2500 | Episodes: 152400 | Median Reward: 42.75 | Max Reward: 49.27
Iteration: 2502 | Episodes: 152500 | Median Reward: 39.05 | Max Reward: 49.27
Iteration: 2503 | Episodes: 152600 | Median Reward: 43.46 | Max Reward: 49.27
Iteration: 2505 | Episodes: 152700 | Median Reward: 45.55 | Max Reward: 49.27
Iteration: 2507 | Episodes: 152800 | Median Reward: 40.80 | Max Reward: 49.27
Iteration: 2508 | Episodes: 152900 | Median Reward: 39.24 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -86.2      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2510       |
|    time_elapsed         | 45624      |
|    total_timesteps      | 15421440   |
| train/                  |            |
|    approx_kl            | 0.30521774 |
|    clip_fraction        | 0.0867     |
|    clip_range           | 0.4        |
|    entropy_loss         | -119       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 187        |
|    n_updates            | 25090      |
|    policy_gradient_loss | 0.0364     |
|    std                  | 4.84       |
|    value_loss           | 385        |
----------------------------------------
Iteration: 2510 | Episodes: 153000 | Median Reward: 23.93 | Max Reward: 49.27
Iteration: 2512 | Episodes: 153100 | Median Reward: 40.36 | Max Reward: 49.27
Iteration: 2513 | Episodes: 153200 | Median Reward: 42.16 | Max Reward: 49.27
Iteration: 2515 | Episodes: 153300 | Median Reward: 38.96 | Max Reward: 49.27
Iteration: 2516 | Episodes: 153400 | Median Reward: 39.86 | Max Reward: 49.27
Iteration: 2518 | Episodes: 153500 | Median Reward: 36.13 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -86.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2520      |
|    time_elapsed         | 45816     |
|    total_timesteps      | 15482880  |
| train/                  |           |
|    approx_kl            | 2.3802211 |
|    clip_fraction        | 0.102     |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 159       |
|    n_updates            | 25190     |
|    policy_gradient_loss | 0.0103    |
|    std                  | 4.85      |
|    value_loss           | 289       |
---------------------------------------
Iteration: 2520 | Episodes: 153600 | Median Reward: 7.16 | Max Reward: 49.27
Iteration: 2521 | Episodes: 153700 | Median Reward: 9.93 | Max Reward: 49.27
Iteration: 2523 | Episodes: 153800 | Median Reward: 33.44 | Max Reward: 49.27
Iteration: 2525 | Episodes: 153900 | Median Reward: -5.00 | Max Reward: 49.27
Iteration: 2526 | Episodes: 154000 | Median Reward: 38.46 | Max Reward: 49.27
Iteration: 2528 | Episodes: 154100 | Median Reward: 33.94 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2530      |
|    time_elapsed         | 45998     |
|    total_timesteps      | 15544320  |
| train/                  |           |
|    approx_kl            | 2.3077955 |
|    clip_fraction        | 0.121     |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 247       |
|    n_updates            | 25290     |
|    policy_gradient_loss | 0.0191    |
|    std                  | 4.87      |
|    value_loss           | 384       |
---------------------------------------
Iteration: 2530 | Episodes: 154200 | Median Reward: 30.25 | Max Reward: 49.27
Iteration: 2531 | Episodes: 154300 | Median Reward: 39.56 | Max Reward: 49.27
Iteration: 2533 | Episodes: 154400 | Median Reward: 40.12 | Max Reward: 49.27
Iteration: 2535 | Episodes: 154500 | Median Reward: 42.20 | Max Reward: 49.27
Iteration: 2536 | Episodes: 154600 | Median Reward: 38.86 | Max Reward: 49.27
Iteration: 2538 | Episodes: 154700 | Median Reward: 39.78 | Max Reward: 49.27
Iteration: 2539 | Episodes: 154800 | Median Reward: 11.59 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -86.3      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2540       |
|    time_elapsed         | 46176      |
|    total_timesteps      | 15605760   |
| train/                  |            |
|    approx_kl            | 0.47183284 |
|    clip_fraction        | 0.0662     |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 63.8       |
|    n_updates            | 25390      |
|    policy_gradient_loss | 0.00222    |
|    std                  | 4.88       |
|    value_loss           | 247        |
----------------------------------------
Iteration: 2541 | Episodes: 154900 | Median Reward: -4.69 | Max Reward: 49.27
Iteration: 2543 | Episodes: 155000 | Median Reward: -5.45 | Max Reward: 49.27
Iteration: 2544 | Episodes: 155100 | Median Reward: 18.94 | Max Reward: 49.27
Iteration: 2546 | Episodes: 155200 | Median Reward: -8.93 | Max Reward: 49.27
Iteration: 2548 | Episodes: 155300 | Median Reward: -13.86 | Max Reward: 49.27
Iteration: 2549 | Episodes: 155400 | Median Reward: 39.69 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2550      |
|    time_elapsed         | 46360     |
|    total_timesteps      | 15667200  |
| train/                  |           |
|    approx_kl            | 3.4010315 |
|    clip_fraction        | 0.136     |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 141       |
|    n_updates            | 25490     |
|    policy_gradient_loss | 0.0317    |
|    std                  | 4.89      |
|    value_loss           | 351       |
---------------------------------------
Iteration: 2551 | Episodes: 155500 | Median Reward: 42.66 | Max Reward: 49.27
Iteration: 2553 | Episodes: 155600 | Median Reward: 43.18 | Max Reward: 49.27
Iteration: 2554 | Episodes: 155700 | Median Reward: 44.28 | Max Reward: 49.27
Iteration: 2556 | Episodes: 155800 | Median Reward: 43.24 | Max Reward: 49.27
Iteration: 2558 | Episodes: 155900 | Median Reward: 43.72 | Max Reward: 49.27
Iteration: 2559 | Episodes: 156000 | Median Reward: 24.69 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -76.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2560      |
|    time_elapsed         | 46547     |
|    total_timesteps      | 15728640  |
| train/                  |           |
|    approx_kl            | 4.2190323 |
|    clip_fraction        | 0.142     |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 142       |
|    n_updates            | 25590     |
|    policy_gradient_loss | 0.039     |
|    std                  | 4.9       |
|    value_loss           | 308       |
---------------------------------------
Iteration: 2561 | Episodes: 156100 | Median Reward: 8.12 | Max Reward: 49.27
Iteration: 2562 | Episodes: 156200 | Median Reward: 34.84 | Max Reward: 49.27
Iteration: 2564 | Episodes: 156300 | Median Reward: 33.40 | Max Reward: 49.27
Iteration: 2566 | Episodes: 156400 | Median Reward: 15.27 | Max Reward: 49.27
Iteration: 2567 | Episodes: 156500 | Median Reward: 6.06 | Max Reward: 49.27
Iteration: 2569 | Episodes: 156600 | Median Reward: 18.52 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -87.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2570      |
|    time_elapsed         | 46728     |
|    total_timesteps      | 15790080  |
| train/                  |           |
|    approx_kl            | 1.9756507 |
|    clip_fraction        | 0.147     |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 92.4      |
|    n_updates            | 25690     |
|    policy_gradient_loss | 0.0475    |
|    std                  | 4.91      |
|    value_loss           | 228       |
---------------------------------------
Iteration: 2571 | Episodes: 156700 | Median Reward: -0.77 | Max Reward: 49.27
Iteration: 2572 | Episodes: 156800 | Median Reward: 31.88 | Max Reward: 49.27
Iteration: 2574 | Episodes: 156900 | Median Reward: 22.78 | Max Reward: 49.27
Iteration: 2576 | Episodes: 157000 | Median Reward: 5.34 | Max Reward: 49.27
Iteration: 2577 | Episodes: 157100 | Median Reward: -0.69 | Max Reward: 49.27
Iteration: 2579 | Episodes: 157200 | Median Reward: 4.77 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -85       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2580      |
|    time_elapsed         | 46909     |
|    total_timesteps      | 15851520  |
| train/                  |           |
|    approx_kl            | 1.1608241 |
|    clip_fraction        | 0.0981    |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 205       |
|    n_updates            | 25790     |
|    policy_gradient_loss | 0.00304   |
|    std                  | 4.93      |
|    value_loss           | 309       |
---------------------------------------
Iteration: 2581 | Episodes: 157300 | Median Reward: 5.99 | Max Reward: 49.27
Iteration: 2582 | Episodes: 157400 | Median Reward: 33.90 | Max Reward: 49.27
Iteration: 2584 | Episodes: 157500 | Median Reward: 12.83 | Max Reward: 49.27
Iteration: 2585 | Episodes: 157600 | Median Reward: 8.30 | Max Reward: 49.27
Iteration: 2587 | Episodes: 157700 | Median Reward: 34.58 | Max Reward: 49.27
Iteration: 2589 | Episodes: 157800 | Median Reward: 39.02 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -71.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2590      |
|    time_elapsed         | 47088     |
|    total_timesteps      | 15912960  |
| train/                  |           |
|    approx_kl            | 6.3812037 |
|    clip_fraction        | 0.156     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 235       |
|    n_updates            | 25890     |
|    policy_gradient_loss | 0.0374    |
|    std                  | 4.94      |
|    value_loss           | 457       |
---------------------------------------
Iteration: 2590 | Episodes: 157900 | Median Reward: 16.63 | Max Reward: 49.27
Iteration: 2592 | Episodes: 158000 | Median Reward: 19.85 | Max Reward: 49.27
Iteration: 2594 | Episodes: 158100 | Median Reward: 19.85 | Max Reward: 49.27
Iteration: 2595 | Episodes: 158200 | Median Reward: -5.76 | Max Reward: 49.27
Iteration: 2597 | Episodes: 158300 | Median Reward: -8.69 | Max Reward: 49.27
Iteration: 2599 | Episodes: 158400 | Median Reward: 38.04 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2600      |
|    time_elapsed         | 47282     |
|    total_timesteps      | 15974400  |
| train/                  |           |
|    approx_kl            | 1.5975057 |
|    clip_fraction        | 0.191     |
|    clip_range           | 0.4       |
|    entropy_loss         | -118      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 236       |
|    n_updates            | 25990     |
|    policy_gradient_loss | 0.0359    |
|    std                  | 4.95      |
|    value_loss           | 422       |
---------------------------------------
Iteration: 2600 | Episodes: 158500 | Median Reward: 41.89 | Max Reward: 49.27
Iteration: 2602 | Episodes: 158600 | Median Reward: 42.20 | Max Reward: 49.27
Iteration: 2604 | Episodes: 158700 | Median Reward: 37.09 | Max Reward: 49.27
Iteration: 2605 | Episodes: 158800 | Median Reward: 39.50 | Max Reward: 49.27
Iteration: 2607 | Episodes: 158900 | Median Reward: 43.04 | Max Reward: 49.27
Iteration: 2608 | Episodes: 159000 | Median Reward: 38.14 | Max Reward: 49.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.1       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 2610        |
|    time_elapsed         | 47461       |
|    total_timesteps      | 16035840    |
| train/                  |             |
|    approx_kl            | 0.043496773 |
|    clip_fraction        | 0.0648      |
|    clip_range           | 0.4         |
|    entropy_loss         | -119        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 207         |
|    n_updates            | 26090       |
|    policy_gradient_loss | 0.0103      |
|    std                  | 4.96        |
|    value_loss           | 436         |
-----------------------------------------
Iteration: 2610 | Episodes: 159100 | Median Reward: 40.56 | Max Reward: 49.27
Iteration: 2612 | Episodes: 159200 | Median Reward: 40.12 | Max Reward: 49.27
Iteration: 2613 | Episodes: 159300 | Median Reward: 40.67 | Max Reward: 49.27
Iteration: 2615 | Episodes: 159400 | Median Reward: 42.47 | Max Reward: 49.27
Iteration: 2617 | Episodes: 159500 | Median Reward: 39.66 | Max Reward: 49.27
Iteration: 2618 | Episodes: 159600 | Median Reward: 36.96 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -74.5      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2620       |
|    time_elapsed         | 47642      |
|    total_timesteps      | 16097280   |
| train/                  |            |
|    approx_kl            | 0.16636354 |
|    clip_fraction        | 0.0659     |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 110        |
|    n_updates            | 26190      |
|    policy_gradient_loss | 0.000236   |
|    std                  | 4.97       |
|    value_loss           | 308        |
----------------------------------------
Iteration: 2620 | Episodes: 159700 | Median Reward: 36.24 | Max Reward: 49.27
Iteration: 2622 | Episodes: 159800 | Median Reward: 33.94 | Max Reward: 49.27
Iteration: 2623 | Episodes: 159900 | Median Reward: 32.53 | Max Reward: 49.27
Iteration: 2625 | Episodes: 160000 | Median Reward: 31.74 | Max Reward: 49.27
Iteration: 2627 | Episodes: 160100 | Median Reward: 29.36 | Max Reward: 49.27
Iteration: 2628 | Episodes: 160200 | Median Reward: 36.39 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -67.6      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2630       |
|    time_elapsed         | 47827      |
|    total_timesteps      | 16158720   |
| train/                  |            |
|    approx_kl            | 0.15685189 |
|    clip_fraction        | 0.0389     |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 182        |
|    n_updates            | 26290      |
|    policy_gradient_loss | -0.00719   |
|    std                  | 4.98       |
|    value_loss           | 369        |
----------------------------------------
Iteration: 2630 | Episodes: 160300 | Median Reward: 39.26 | Max Reward: 49.27
Iteration: 2632 | Episodes: 160400 | Median Reward: 16.22 | Max Reward: 49.27
Iteration: 2633 | Episodes: 160500 | Median Reward: 7.15 | Max Reward: 49.27
Iteration: 2635 | Episodes: 160600 | Median Reward: 33.60 | Max Reward: 49.27
Iteration: 2636 | Episodes: 160700 | Median Reward: 35.56 | Max Reward: 49.27
Iteration: 2638 | Episodes: 160800 | Median Reward: 32.62 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -68.1      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2640       |
|    time_elapsed         | 48011      |
|    total_timesteps      | 16220160   |
| train/                  |            |
|    approx_kl            | 0.85969067 |
|    clip_fraction        | 0.0806     |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 142        |
|    n_updates            | 26390      |
|    policy_gradient_loss | -0.00183   |
|    std                  | 5          |
|    value_loss           | 334        |
----------------------------------------
Iteration: 2640 | Episodes: 160900 | Median Reward: 34.50 | Max Reward: 49.27
Iteration: 2641 | Episodes: 161000 | Median Reward: 37.10 | Max Reward: 49.27
Iteration: 2643 | Episodes: 161100 | Median Reward: 37.20 | Max Reward: 49.27
Iteration: 2645 | Episodes: 161200 | Median Reward: 41.07 | Max Reward: 49.27
Iteration: 2646 | Episodes: 161300 | Median Reward: 36.32 | Max Reward: 49.27
Iteration: 2648 | Episodes: 161400 | Median Reward: 40.76 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -73.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2650      |
|    time_elapsed         | 48190     |
|    total_timesteps      | 16281600  |
| train/                  |           |
|    approx_kl            | 1.582128  |
|    clip_fraction        | 0.117     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 98.5      |
|    n_updates            | 26490     |
|    policy_gradient_loss | 0.0206    |
|    std                  | 5.02      |
|    value_loss           | 340       |
---------------------------------------
Iteration: 2650 | Episodes: 161500 | Median Reward: 41.98 | Max Reward: 49.27
Iteration: 2651 | Episodes: 161600 | Median Reward: 38.22 | Max Reward: 49.27
Iteration: 2653 | Episodes: 161700 | Median Reward: 32.04 | Max Reward: 49.27
Iteration: 2655 | Episodes: 161800 | Median Reward: 28.86 | Max Reward: 49.27
Iteration: 2656 | Episodes: 161900 | Median Reward: 37.18 | Max Reward: 49.27
Iteration: 2658 | Episodes: 162000 | Median Reward: 10.31 | Max Reward: 49.27
Iteration: 2659 | Episodes: 162100 | Median Reward: 25.82 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -76.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2660      |
|    time_elapsed         | 48370     |
|    total_timesteps      | 16343040  |
| train/                  |           |
|    approx_kl            | 1.8559473 |
|    clip_fraction        | 0.121     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 197       |
|    n_updates            | 26590     |
|    policy_gradient_loss | 0.00904   |
|    std                  | 5.04      |
|    value_loss           | 332       |
---------------------------------------
Iteration: 2661 | Episodes: 162200 | Median Reward: -1.14 | Max Reward: 49.27
Iteration: 2663 | Episodes: 162300 | Median Reward: 36.34 | Max Reward: 49.27
Iteration: 2664 | Episodes: 162400 | Median Reward: 34.52 | Max Reward: 49.27
Iteration: 2666 | Episodes: 162500 | Median Reward: 38.09 | Max Reward: 49.27
Iteration: 2668 | Episodes: 162600 | Median Reward: 44.67 | Max Reward: 49.27
Iteration: 2669 | Episodes: 162700 | Median Reward: 41.46 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -61       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2670      |
|    time_elapsed         | 48558     |
|    total_timesteps      | 16404480  |
| train/                  |           |
|    approx_kl            | 1.1935707 |
|    clip_fraction        | 0.079     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 203       |
|    n_updates            | 26690     |
|    policy_gradient_loss | 0.00799   |
|    std                  | 5.05      |
|    value_loss           | 391       |
---------------------------------------
Iteration: 2671 | Episodes: 162800 | Median Reward: 43.43 | Max Reward: 49.27
Iteration: 2673 | Episodes: 162900 | Median Reward: 44.17 | Max Reward: 49.27
Iteration: 2674 | Episodes: 163000 | Median Reward: 44.96 | Max Reward: 49.27
Iteration: 2676 | Episodes: 163100 | Median Reward: 44.10 | Max Reward: 49.27
Iteration: 2678 | Episodes: 163200 | Median Reward: 36.57 | Max Reward: 49.27
Iteration: 2679 | Episodes: 163300 | Median Reward: 41.35 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -68.3    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2680     |
|    time_elapsed         | 48743    |
|    total_timesteps      | 16465920 |
| train/                  |          |
|    approx_kl            | 3.061287 |
|    clip_fraction        | 0.134    |
|    clip_range           | 0.4      |
|    entropy_loss         | -121     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 211      |
|    n_updates            | 26790    |
|    policy_gradient_loss | 0.0197   |
|    std                  | 5.07     |
|    value_loss           | 390      |
--------------------------------------
Iteration: 2681 | Episodes: 163400 | Median Reward: 40.05 | Max Reward: 49.27
Iteration: 2682 | Episodes: 163500 | Median Reward: 42.85 | Max Reward: 49.27
Iteration: 2684 | Episodes: 163600 | Median Reward: 35.67 | Max Reward: 49.27
Iteration: 2686 | Episodes: 163700 | Median Reward: 38.97 | Max Reward: 49.27
Iteration: 2687 | Episodes: 163800 | Median Reward: 43.33 | Max Reward: 49.27
Iteration: 2689 | Episodes: 163900 | Median Reward: 44.35 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -63.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2690      |
|    time_elapsed         | 48922     |
|    total_timesteps      | 16527360  |
| train/                  |           |
|    approx_kl            | 4.8912086 |
|    clip_fraction        | 0.114     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 212       |
|    n_updates            | 26890     |
|    policy_gradient_loss | 0.0296    |
|    std                  | 5.09      |
|    value_loss           | 363       |
---------------------------------------
Iteration: 2691 | Episodes: 164000 | Median Reward: 42.12 | Max Reward: 49.27
Iteration: 2692 | Episodes: 164100 | Median Reward: 41.64 | Max Reward: 49.27
Iteration: 2694 | Episodes: 164200 | Median Reward: 39.52 | Max Reward: 49.27
Iteration: 2696 | Episodes: 164300 | Median Reward: 27.71 | Max Reward: 49.27
Iteration: 2697 | Episodes: 164400 | Median Reward: 24.35 | Max Reward: 49.27
Iteration: 2699 | Episodes: 164500 | Median Reward: 15.65 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -84       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2700      |
|    time_elapsed         | 49101     |
|    total_timesteps      | 16588800  |
| train/                  |           |
|    approx_kl            | 1.2336034 |
|    clip_fraction        | 0.177     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 54.8      |
|    n_updates            | 26990     |
|    policy_gradient_loss | 0.00584   |
|    std                  | 5.1       |
|    value_loss           | 209       |
---------------------------------------
Iteration: 2701 | Episodes: 164600 | Median Reward: 36.65 | Max Reward: 49.27
Iteration: 2702 | Episodes: 164700 | Median Reward: 32.83 | Max Reward: 49.27
Iteration: 2704 | Episodes: 164800 | Median Reward: 31.90 | Max Reward: 49.27
Iteration: 2705 | Episodes: 164900 | Median Reward: 17.90 | Max Reward: 49.27
Iteration: 2707 | Episodes: 165000 | Median Reward: 15.59 | Max Reward: 49.27
Iteration: 2709 | Episodes: 165100 | Median Reward: 14.66 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -89.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2710      |
|    time_elapsed         | 49288     |
|    total_timesteps      | 16650240  |
| train/                  |           |
|    approx_kl            | 0.7977469 |
|    clip_fraction        | 0.121     |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 136       |
|    n_updates            | 27090     |
|    policy_gradient_loss | 0.00489   |
|    std                  | 5.12      |
|    value_loss           | 269       |
---------------------------------------
Iteration: 2710 | Episodes: 165200 | Median Reward: 3.83 | Max Reward: 49.27
Iteration: 2712 | Episodes: 165300 | Median Reward: 4.29 | Max Reward: 49.27
Iteration: 2714 | Episodes: 165400 | Median Reward: 12.84 | Max Reward: 49.27
Iteration: 2715 | Episodes: 165500 | Median Reward: 21.00 | Max Reward: 49.27
Iteration: 2717 | Episodes: 165600 | Median Reward: 42.84 | Max Reward: 49.27
Iteration: 2719 | Episodes: 165700 | Median Reward: 37.09 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2720      |
|    time_elapsed         | 49474     |
|    total_timesteps      | 16711680  |
| train/                  |           |
|    approx_kl            | 5.0477657 |
|    clip_fraction        | 0.307     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 153       |
|    n_updates            | 27190     |
|    policy_gradient_loss | 0.041     |
|    std                  | 5.13      |
|    value_loss           | 289       |
---------------------------------------
Iteration: 2720 | Episodes: 165800 | Median Reward: 41.72 | Max Reward: 49.27
Iteration: 2722 | Episodes: 165900 | Median Reward: 30.48 | Max Reward: 49.27
Iteration: 2724 | Episodes: 166000 | Median Reward: 36.15 | Max Reward: 49.27
Iteration: 2725 | Episodes: 166100 | Median Reward: 33.25 | Max Reward: 49.27
Iteration: 2727 | Episodes: 166200 | Median Reward: 40.35 | Max Reward: 49.27
Iteration: 2729 | Episodes: 166300 | Median Reward: 41.72 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -68.3      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2730       |
|    time_elapsed         | 49652      |
|    total_timesteps      | 16773120   |
| train/                  |            |
|    approx_kl            | 0.45044285 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 231        |
|    n_updates            | 27290      |
|    policy_gradient_loss | -0.0343    |
|    std                  | 5.15       |
|    value_loss           | 425        |
----------------------------------------
Iteration: 2730 | Episodes: 166400 | Median Reward: 40.26 | Max Reward: 49.27
Iteration: 2732 | Episodes: 166500 | Median Reward: 32.79 | Max Reward: 49.27
Iteration: 2733 | Episodes: 166600 | Median Reward: 19.32 | Max Reward: 49.27
Iteration: 2735 | Episodes: 166700 | Median Reward: 38.81 | Max Reward: 49.27
Iteration: 2737 | Episodes: 166800 | Median Reward: 31.84 | Max Reward: 49.27
Iteration: 2738 | Episodes: 166900 | Median Reward: 15.41 | Max Reward: 49.27
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -78      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2740     |
|    time_elapsed         | 49832    |
|    total_timesteps      | 16834560 |
| train/                  |          |
|    approx_kl            | 2.813276 |
|    clip_fraction        | 0.413    |
|    clip_range           | 0.4      |
|    entropy_loss         | -119     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 139      |
|    n_updates            | 27390    |
|    policy_gradient_loss | 0.0407   |
|    std                  | 5.17     |
|    value_loss           | 283      |
--------------------------------------
Iteration: 2740 | Episodes: 167000 | Median Reward: 21.07 | Max Reward: 49.27
Iteration: 2742 | Episodes: 167100 | Median Reward: 27.96 | Max Reward: 49.27
Iteration: 2743 | Episodes: 167200 | Median Reward: 28.50 | Max Reward: 49.27
Iteration: 2745 | Episodes: 167300 | Median Reward: 31.83 | Max Reward: 49.27
Iteration: 2747 | Episodes: 167400 | Median Reward: 23.80 | Max Reward: 49.27
Iteration: 2748 | Episodes: 167500 | Median Reward: 30.22 | Max Reward: 49.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -61.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2750      |
|    time_elapsed         | 50012     |
|    total_timesteps      | 16896000  |
| train/                  |           |
|    approx_kl            | 3.4212232 |
|    clip_fraction        | 0.323     |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 141       |
|    n_updates            | 27490     |
|    policy_gradient_loss | -0.0435   |
|    std                  | 5.18      |
|    value_loss           | 369       |
---------------------------------------
Iteration: 2750 | Episodes: 167600 | Median Reward: 41.41 | Max Reward: 49.27
Iteration: 2751 | Episodes: 167700 | Median Reward: 33.15 | Max Reward: 49.27
Iteration: 2753 | Episodes: 167800 | Median Reward: 37.96 | Max Reward: 49.27
Iteration: 2755 | Episodes: 167900 | Median Reward: 37.28 | Max Reward: 49.27
Iteration: 2756 | Episodes: 168000 | Median Reward: 22.87 | Max Reward: 49.27
Iteration: 2758 | Episodes: 168100 | Median Reward: 35.71 | Max Reward: 49.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -63.1      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2760       |
|    time_elapsed         | 50196      |
|    total_timesteps      | 16957440   |
| train/                  |            |
|    approx_kl            | 0.35609826 |
|    clip_fraction        | 0.0699     |
|    clip_range           | 0.4        |
|    entropy_loss         | -119       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 179        |
|    n_updates            | 27590      |
|    policy_gradient_loss | -0.00916   |
|    std                  | 5.2        |
|    value_loss           | 357        |
----------------------------------------
Iteration: 2760 | Episodes: 168200 | Median Reward: 38.70 | Max Reward: 49.33
Iteration: 2761 | Episodes: 168300 | Median Reward: 37.97 | Max Reward: 49.33
Iteration: 2763 | Episodes: 168400 | Median Reward: 40.60 | Max Reward: 49.33
Iteration: 2765 | Episodes: 168500 | Median Reward: 35.08 | Max Reward: 49.33
Iteration: 2766 | Episodes: 168600 | Median Reward: 37.75 | Max Reward: 49.33
Iteration: 2768 | Episodes: 168700 | Median Reward: 44.64 | Max Reward: 49.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.8       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 2770        |
|    time_elapsed         | 50366       |
|    total_timesteps      | 17018880    |
| train/                  |             |
|    approx_kl            | 0.091714635 |
|    clip_fraction        | 0.0937      |
|    clip_range           | 0.4         |
|    entropy_loss         | -119        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 193         |
|    n_updates            | 27690       |
|    policy_gradient_loss | -0.00823    |
|    std                  | 5.22        |
|    value_loss           | 417         |
-----------------------------------------
Iteration: 2770 | Episodes: 168800 | Median Reward: 44.64 | Max Reward: 49.33
Iteration: 2771 | Episodes: 168900 | Median Reward: 43.30 | Max Reward: 49.33
Iteration: 2773 | Episodes: 169000 | Median Reward: 36.27 | Max Reward: 49.33
Iteration: 2774 | Episodes: 169100 | Median Reward: 36.80 | Max Reward: 49.33
Iteration: 2776 | Episodes: 169200 | Median Reward: 41.34 | Max Reward: 49.33
Iteration: 2778 | Episodes: 169300 | Median Reward: 35.22 | Max Reward: 49.33
Iteration: 2779 | Episodes: 169400 | Median Reward: 36.12 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -67.2      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2780       |
|    time_elapsed         | 50541      |
|    total_timesteps      | 17080320   |
| train/                  |            |
|    approx_kl            | 0.96167916 |
|    clip_fraction        | 0.26       |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 134        |
|    n_updates            | 27790      |
|    policy_gradient_loss | -0.0599    |
|    std                  | 5.24       |
|    value_loss           | 308        |
----------------------------------------
Iteration: 2781 | Episodes: 169500 | Median Reward: 29.27 | Max Reward: 49.33
Iteration: 2783 | Episodes: 169600 | Median Reward: 30.99 | Max Reward: 49.33
Iteration: 2784 | Episodes: 169700 | Median Reward: 30.67 | Max Reward: 49.33
Iteration: 2786 | Episodes: 169800 | Median Reward: 34.81 | Max Reward: 49.33
Iteration: 2788 | Episodes: 169900 | Median Reward: 28.75 | Max Reward: 49.33
Iteration: 2789 | Episodes: 170000 | Median Reward: 35.93 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -67.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2790      |
|    time_elapsed         | 50726     |
|    total_timesteps      | 17141760  |
| train/                  |           |
|    approx_kl            | 4.0410585 |
|    clip_fraction        | 0.337     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 162       |
|    n_updates            | 27890     |
|    policy_gradient_loss | -0.0733   |
|    std                  | 5.25      |
|    value_loss           | 352       |
---------------------------------------
Iteration: 2791 | Episodes: 170100 | Median Reward: 36.10 | Max Reward: 49.33
Iteration: 2793 | Episodes: 170200 | Median Reward: 36.70 | Max Reward: 49.33
Iteration: 2794 | Episodes: 170300 | Median Reward: 35.09 | Max Reward: 49.33
Iteration: 2796 | Episodes: 170400 | Median Reward: 41.08 | Max Reward: 49.33
Iteration: 2797 | Episodes: 170500 | Median Reward: 29.85 | Max Reward: 49.33
Iteration: 2799 | Episodes: 170600 | Median Reward: 41.22 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 99        |
|    ep_rew_mean          | -64.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2800      |
|    time_elapsed         | 50901     |
|    total_timesteps      | 17203200  |
| train/                  |           |
|    approx_kl            | 4.0514402 |
|    clip_fraction        | 0.281     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 198       |
|    n_updates            | 27990     |
|    policy_gradient_loss | -0.0254   |
|    std                  | 5.28      |
|    value_loss           | 359       |
---------------------------------------
Iteration: 2801 | Episodes: 170700 | Median Reward: 41.94 | Max Reward: 49.33
Iteration: 2802 | Episodes: 170800 | Median Reward: 40.57 | Max Reward: 49.33
Iteration: 2804 | Episodes: 170900 | Median Reward: 27.70 | Max Reward: 49.33
Iteration: 2806 | Episodes: 171000 | Median Reward: 17.64 | Max Reward: 49.33
Iteration: 2807 | Episodes: 171100 | Median Reward: 31.63 | Max Reward: 49.33
Iteration: 2809 | Episodes: 171200 | Median Reward: 38.88 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -71.1     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2810      |
|    time_elapsed         | 51077     |
|    total_timesteps      | 17264640  |
| train/                  |           |
|    approx_kl            | 2.2400613 |
|    clip_fraction        | 0.268     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 205       |
|    n_updates            | 28090     |
|    policy_gradient_loss | -0.0499   |
|    std                  | 5.29      |
|    value_loss           | 360       |
---------------------------------------
Iteration: 2811 | Episodes: 171300 | Median Reward: 25.33 | Max Reward: 49.33
Iteration: 2812 | Episodes: 171400 | Median Reward: 38.22 | Max Reward: 49.33
Iteration: 2814 | Episodes: 171500 | Median Reward: 36.05 | Max Reward: 49.33
Iteration: 2816 | Episodes: 171600 | Median Reward: 36.89 | Max Reward: 49.33
Iteration: 2817 | Episodes: 171700 | Median Reward: 40.82 | Max Reward: 49.33
Iteration: 2819 | Episodes: 171800 | Median Reward: 41.75 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -67.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2820      |
|    time_elapsed         | 51260     |
|    total_timesteps      | 17326080  |
| train/                  |           |
|    approx_kl            | 0.8547815 |
|    clip_fraction        | 0.176     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 122       |
|    n_updates            | 28190     |
|    policy_gradient_loss | -0.0181   |
|    std                  | 5.31      |
|    value_loss           | 359       |
---------------------------------------
Iteration: 2820 | Episodes: 171900 | Median Reward: 40.69 | Max Reward: 49.33
Iteration: 2822 | Episodes: 172000 | Median Reward: 40.16 | Max Reward: 49.33
Iteration: 2824 | Episodes: 172100 | Median Reward: 43.61 | Max Reward: 49.33
Iteration: 2825 | Episodes: 172200 | Median Reward: 42.00 | Max Reward: 49.33
Iteration: 2827 | Episodes: 172300 | Median Reward: 35.02 | Max Reward: 49.33
Iteration: 2829 | Episodes: 172400 | Median Reward: 39.98 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -60.8     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2830      |
|    time_elapsed         | 51434     |
|    total_timesteps      | 17387520  |
| train/                  |           |
|    approx_kl            | 0.8611157 |
|    clip_fraction        | 0.28      |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 171       |
|    n_updates            | 28290     |
|    policy_gradient_loss | -0.00584  |
|    std                  | 5.33      |
|    value_loss           | 372       |
---------------------------------------
Iteration: 2830 | Episodes: 172500 | Median Reward: 42.84 | Max Reward: 49.33
Iteration: 2832 | Episodes: 172600 | Median Reward: 35.78 | Max Reward: 49.33
Iteration: 2834 | Episodes: 172700 | Median Reward: 31.46 | Max Reward: 49.33
Iteration: 2835 | Episodes: 172800 | Median Reward: 35.25 | Max Reward: 49.33
Iteration: 2837 | Episodes: 172900 | Median Reward: 34.00 | Max Reward: 49.33
Iteration: 2839 | Episodes: 173000 | Median Reward: 35.78 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -64.9      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2840       |
|    time_elapsed         | 51609      |
|    total_timesteps      | 17448960   |
| train/                  |            |
|    approx_kl            | 0.44965762 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 196        |
|    n_updates            | 28390      |
|    policy_gradient_loss | -0.0944    |
|    std                  | 5.35       |
|    value_loss           | 335        |
----------------------------------------
Iteration: 2840 | Episodes: 173100 | Median Reward: 41.60 | Max Reward: 49.33
Iteration: 2842 | Episodes: 173200 | Median Reward: 40.70 | Max Reward: 49.33
Iteration: 2843 | Episodes: 173300 | Median Reward: 42.31 | Max Reward: 49.33
Iteration: 2845 | Episodes: 173400 | Median Reward: 40.85 | Max Reward: 49.33
Iteration: 2847 | Episodes: 173500 | Median Reward: 37.13 | Max Reward: 49.33
Iteration: 2848 | Episodes: 173600 | Median Reward: 37.32 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -67.3     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2850      |
|    time_elapsed         | 51779     |
|    total_timesteps      | 17510400  |
| train/                  |           |
|    approx_kl            | 0.5195181 |
|    clip_fraction        | 0.166     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 192       |
|    n_updates            | 28490     |
|    policy_gradient_loss | -0.0211   |
|    std                  | 5.36      |
|    value_loss           | 316       |
---------------------------------------
Iteration: 2850 | Episodes: 173700 | Median Reward: 42.48 | Max Reward: 49.33
Iteration: 2852 | Episodes: 173800 | Median Reward: 37.03 | Max Reward: 49.33
Iteration: 2853 | Episodes: 173900 | Median Reward: 42.06 | Max Reward: 49.33
Iteration: 2855 | Episodes: 174000 | Median Reward: 40.04 | Max Reward: 49.33
Iteration: 2857 | Episodes: 174100 | Median Reward: 37.43 | Max Reward: 49.33
Iteration: 2858 | Episodes: 174200 | Median Reward: 41.62 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -59.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2860      |
|    time_elapsed         | 51952     |
|    total_timesteps      | 17571840  |
| train/                  |           |
|    approx_kl            | 2.2438273 |
|    clip_fraction        | 0.409     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 149       |
|    n_updates            | 28590     |
|    policy_gradient_loss | 0.0761    |
|    std                  | 5.39      |
|    value_loss           | 361       |
---------------------------------------
Iteration: 2860 | Episodes: 174300 | Median Reward: 40.71 | Max Reward: 49.33
Iteration: 2862 | Episodes: 174400 | Median Reward: 35.38 | Max Reward: 49.33
Iteration: 2863 | Episodes: 174500 | Median Reward: 37.08 | Max Reward: 49.33
Iteration: 2865 | Episodes: 174600 | Median Reward: 42.78 | Max Reward: 49.33
Iteration: 2866 | Episodes: 174700 | Median Reward: 43.22 | Max Reward: 49.33
Iteration: 2868 | Episodes: 174800 | Median Reward: 31.09 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -80.6      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2870       |
|    time_elapsed         | 52116      |
|    total_timesteps      | 17633280   |
| train/                  |            |
|    approx_kl            | 0.34310654 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -121       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 135        |
|    n_updates            | 28690      |
|    policy_gradient_loss | -0.0473    |
|    std                  | 5.41       |
|    value_loss           | 206        |
----------------------------------------
Iteration: 2870 | Episodes: 174900 | Median Reward: 27.21 | Max Reward: 49.33
Iteration: 2871 | Episodes: 175000 | Median Reward: 34.25 | Max Reward: 49.33
Iteration: 2873 | Episodes: 175100 | Median Reward: 36.57 | Max Reward: 49.33
Iteration: 2875 | Episodes: 175200 | Median Reward: 35.60 | Max Reward: 49.33
Iteration: 2876 | Episodes: 175300 | Median Reward: 37.96 | Max Reward: 49.33
Iteration: 2878 | Episodes: 175400 | Median Reward: 39.34 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2880      |
|    time_elapsed         | 52287     |
|    total_timesteps      | 17694720  |
| train/                  |           |
|    approx_kl            | 3.8647156 |
|    clip_fraction        | 0.455     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 181       |
|    n_updates            | 28790     |
|    policy_gradient_loss | -0.113    |
|    std                  | 5.43      |
|    value_loss           | 313       |
---------------------------------------
Iteration: 2880 | Episodes: 175500 | Median Reward: 36.59 | Max Reward: 49.33
Iteration: 2881 | Episodes: 175600 | Median Reward: 41.15 | Max Reward: 49.33
Iteration: 2883 | Episodes: 175700 | Median Reward: 30.07 | Max Reward: 49.33
Iteration: 2885 | Episodes: 175800 | Median Reward: 31.89 | Max Reward: 49.33
Iteration: 2886 | Episodes: 175900 | Median Reward: 31.68 | Max Reward: 49.33
Iteration: 2888 | Episodes: 176000 | Median Reward: 39.13 | Max Reward: 49.33
Iteration: 2889 | Episodes: 176100 | Median Reward: 37.51 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -67.4     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2890      |
|    time_elapsed         | 52463     |
|    total_timesteps      | 17756160  |
| train/                  |           |
|    approx_kl            | 0.6208855 |
|    clip_fraction        | 0.217     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 169       |
|    n_updates            | 28890     |
|    policy_gradient_loss | -0.0684   |
|    std                  | 5.45      |
|    value_loss           | 304       |
---------------------------------------
Iteration: 2891 | Episodes: 176200 | Median Reward: 38.45 | Max Reward: 49.33
Iteration: 2893 | Episodes: 176300 | Median Reward: 39.05 | Max Reward: 49.33
Iteration: 2894 | Episodes: 176400 | Median Reward: 31.62 | Max Reward: 49.33
Iteration: 2896 | Episodes: 176500 | Median Reward: 34.72 | Max Reward: 49.33
Iteration: 2898 | Episodes: 176600 | Median Reward: 38.03 | Max Reward: 49.33
Iteration: 2899 | Episodes: 176700 | Median Reward: 24.34 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -79.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2900      |
|    time_elapsed         | 52649     |
|    total_timesteps      | 17817600  |
| train/                  |           |
|    approx_kl            | 7.1916895 |
|    clip_fraction        | 0.432     |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 108       |
|    n_updates            | 28990     |
|    policy_gradient_loss | 0.0102    |
|    std                  | 5.48      |
|    value_loss           | 288       |
---------------------------------------
Iteration: 2901 | Episodes: 176800 | Median Reward: 25.19 | Max Reward: 49.33
Iteration: 2903 | Episodes: 176900 | Median Reward: 29.42 | Max Reward: 49.33
Iteration: 2904 | Episodes: 177000 | Median Reward: -5.32 | Max Reward: 49.33
Iteration: 2906 | Episodes: 177100 | Median Reward: 35.09 | Max Reward: 49.33
Iteration: 2908 | Episodes: 177200 | Median Reward: 38.45 | Max Reward: 49.33
Iteration: 2909 | Episodes: 177300 | Median Reward: 37.25 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -61       |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2910      |
|    time_elapsed         | 52827     |
|    total_timesteps      | 17879040  |
| train/                  |           |
|    approx_kl            | 1.4534428 |
|    clip_fraction        | 0.183     |
|    clip_range           | 0.4       |
|    entropy_loss         | -123      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 186       |
|    n_updates            | 29090     |
|    policy_gradient_loss | 0.00468   |
|    std                  | 5.5       |
|    value_loss           | 349       |
---------------------------------------
Iteration: 2911 | Episodes: 177400 | Median Reward: 39.36 | Max Reward: 49.33
Iteration: 2913 | Episodes: 177500 | Median Reward: 34.30 | Max Reward: 49.33
Iteration: 2914 | Episodes: 177600 | Median Reward: 36.98 | Max Reward: 49.33
Iteration: 2916 | Episodes: 177700 | Median Reward: 35.32 | Max Reward: 49.33
Iteration: 2917 | Episodes: 177800 | Median Reward: 38.52 | Max Reward: 49.33
Iteration: 2919 | Episodes: 177900 | Median Reward: 35.82 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -71.8     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2920      |
|    time_elapsed         | 52999     |
|    total_timesteps      | 17940480  |
| train/                  |           |
|    approx_kl            | 11.016603 |
|    clip_fraction        | 0.326     |
|    clip_range           | 0.4       |
|    entropy_loss         | -123      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 168       |
|    n_updates            | 29190     |
|    policy_gradient_loss | -0.0249   |
|    std                  | 5.52      |
|    value_loss           | 312       |
---------------------------------------
Iteration: 2921 | Episodes: 178000 | Median Reward: 34.91 | Max Reward: 49.33
Iteration: 2922 | Episodes: 178100 | Median Reward: 32.46 | Max Reward: 49.33
Iteration: 2924 | Episodes: 178200 | Median Reward: 36.55 | Max Reward: 49.33
Iteration: 2926 | Episodes: 178300 | Median Reward: 36.56 | Max Reward: 49.33
Iteration: 2927 | Episodes: 178400 | Median Reward: 39.08 | Max Reward: 49.33
Iteration: 2929 | Episodes: 178500 | Median Reward: 39.14 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -62.3     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2930      |
|    time_elapsed         | 53183     |
|    total_timesteps      | 18001920  |
| train/                  |           |
|    approx_kl            | 2.9862096 |
|    clip_fraction        | 0.355     |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 196       |
|    n_updates            | 29290     |
|    policy_gradient_loss | -0.0579   |
|    std                  | 5.54      |
|    value_loss           | 360       |
---------------------------------------
Iteration: 2931 | Episodes: 178600 | Median Reward: 36.17 | Max Reward: 49.33
Iteration: 2932 | Episodes: 178700 | Median Reward: 38.60 | Max Reward: 49.33
Iteration: 2934 | Episodes: 178800 | Median Reward: 37.91 | Max Reward: 49.33
Iteration: 2936 | Episodes: 178900 | Median Reward: 35.24 | Max Reward: 49.33
Iteration: 2937 | Episodes: 179000 | Median Reward: 38.20 | Max Reward: 49.33
Iteration: 2939 | Episodes: 179100 | Median Reward: 38.07 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2940      |
|    time_elapsed         | 53364     |
|    total_timesteps      | 18063360  |
| train/                  |           |
|    approx_kl            | 16.865116 |
|    clip_fraction        | 0.423     |
|    clip_range           | 0.4       |
|    entropy_loss         | -123      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 176       |
|    n_updates            | 29390     |
|    policy_gradient_loss | 0.0799    |
|    std                  | 5.57      |
|    value_loss           | 340       |
---------------------------------------
Iteration: 2940 | Episodes: 179200 | Median Reward: 35.39 | Max Reward: 49.33
Iteration: 2942 | Episodes: 179300 | Median Reward: 25.11 | Max Reward: 49.33
Iteration: 2944 | Episodes: 179400 | Median Reward: 20.58 | Max Reward: 49.33
Iteration: 2945 | Episodes: 179500 | Median Reward: 36.93 | Max Reward: 49.33
Iteration: 2947 | Episodes: 179600 | Median Reward: 22.20 | Max Reward: 49.33
Iteration: 2949 | Episodes: 179700 | Median Reward: 32.44 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -75.1     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2950      |
|    time_elapsed         | 53544     |
|    total_timesteps      | 18124800  |
| train/                  |           |
|    approx_kl            | 0.2925695 |
|    clip_fraction        | 0.136     |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 161       |
|    n_updates            | 29490     |
|    policy_gradient_loss | -0.0603   |
|    std                  | 5.58      |
|    value_loss           | 263       |
---------------------------------------
Iteration: 2950 | Episodes: 179800 | Median Reward: 33.31 | Max Reward: 49.33
Iteration: 2952 | Episodes: 179900 | Median Reward: 34.21 | Max Reward: 49.33
Iteration: 2954 | Episodes: 180000 | Median Reward: 34.39 | Max Reward: 49.33
Iteration: 2955 | Episodes: 180100 | Median Reward: 39.30 | Max Reward: 49.33
Iteration: 2957 | Episodes: 180200 | Median Reward: 33.56 | Max Reward: 49.33
Iteration: 2959 | Episodes: 180300 | Median Reward: 32.48 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2960      |
|    time_elapsed         | 53729     |
|    total_timesteps      | 18186240  |
| train/                  |           |
|    approx_kl            | 0.8198726 |
|    clip_fraction        | 0.267     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 159       |
|    n_updates            | 29590     |
|    policy_gradient_loss | -0.0166   |
|    std                  | 5.6       |
|    value_loss           | 295       |
---------------------------------------
Iteration: 2960 | Episodes: 180400 | Median Reward: 33.41 | Max Reward: 49.33
Iteration: 2962 | Episodes: 180500 | Median Reward: 6.46 | Max Reward: 49.33
Iteration: 2963 | Episodes: 180600 | Median Reward: 30.22 | Max Reward: 49.33
Iteration: 2965 | Episodes: 180700 | Median Reward: 30.79 | Max Reward: 49.33
Iteration: 2967 | Episodes: 180800 | Median Reward: 32.81 | Max Reward: 49.33
Iteration: 2968 | Episodes: 180900 | Median Reward: 38.56 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -69.3    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2970     |
|    time_elapsed         | 53907    |
|    total_timesteps      | 18247680 |
| train/                  |          |
|    approx_kl            | 3.414072 |
|    clip_fraction        | 0.246    |
|    clip_range           | 0.4      |
|    entropy_loss         | -123     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 120      |
|    n_updates            | 29690    |
|    policy_gradient_loss | 0.0493   |
|    std                  | 5.61     |
|    value_loss           | 323      |
--------------------------------------
Iteration: 2970 | Episodes: 181000 | Median Reward: 35.41 | Max Reward: 49.33
Iteration: 2972 | Episodes: 181100 | Median Reward: 3.24 | Max Reward: 49.33
Iteration: 2973 | Episodes: 181200 | Median Reward: 34.58 | Max Reward: 49.33
Iteration: 2975 | Episodes: 181300 | Median Reward: 22.97 | Max Reward: 49.33
Iteration: 2977 | Episodes: 181400 | Median Reward: 37.07 | Max Reward: 49.33
Iteration: 2978 | Episodes: 181500 | Median Reward: 32.43 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -80.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2980      |
|    time_elapsed         | 54096     |
|    total_timesteps      | 18309120  |
| train/                  |           |
|    approx_kl            | 1.1309831 |
|    clip_fraction        | 0.231     |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 123       |
|    n_updates            | 29790     |
|    policy_gradient_loss | -0.0587   |
|    std                  | 5.63      |
|    value_loss           | 313       |
---------------------------------------
Iteration: 2980 | Episodes: 181600 | Median Reward: 31.93 | Max Reward: 49.33
Iteration: 2982 | Episodes: 181700 | Median Reward: 39.93 | Max Reward: 49.33
Iteration: 2983 | Episodes: 181800 | Median Reward: 32.96 | Max Reward: 49.33
Iteration: 2985 | Episodes: 181900 | Median Reward: 35.42 | Max Reward: 49.33
Iteration: 2986 | Episodes: 182000 | Median Reward: 34.85 | Max Reward: 49.33
Iteration: 2988 | Episodes: 182100 | Median Reward: 38.49 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2990      |
|    time_elapsed         | 54274     |
|    total_timesteps      | 18370560  |
| train/                  |           |
|    approx_kl            | 7.0369368 |
|    clip_fraction        | 0.554     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 151       |
|    n_updates            | 29890     |
|    policy_gradient_loss | 0.0204    |
|    std                  | 5.64      |
|    value_loss           | 342       |
---------------------------------------
Iteration: 2990 | Episodes: 182200 | Median Reward: 37.15 | Max Reward: 49.33
Iteration: 2991 | Episodes: 182300 | Median Reward: 37.02 | Max Reward: 49.33
Iteration: 2993 | Episodes: 182400 | Median Reward: 33.92 | Max Reward: 49.33
Iteration: 2995 | Episodes: 182500 | Median Reward: 32.28 | Max Reward: 49.33
Iteration: 2996 | Episodes: 182600 | Median Reward: 35.49 | Max Reward: 49.33
Iteration: 2998 | Episodes: 182700 | Median Reward: 35.47 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -73.3      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3000       |
|    time_elapsed         | 54452      |
|    total_timesteps      | 18432000   |
| train/                  |            |
|    approx_kl            | 0.45017493 |
|    clip_fraction        | 0.247      |
|    clip_range           | 0.4        |
|    entropy_loss         | -122       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 179        |
|    n_updates            | 29990      |
|    policy_gradient_loss | -0.0404    |
|    std                  | 5.66       |
|    value_loss           | 342        |
----------------------------------------
Iteration: 3000 | Episodes: 182800 | Median Reward: 28.98 | Max Reward: 49.33
Iteration: 3001 | Episodes: 182900 | Median Reward: 33.87 | Max Reward: 49.33
Iteration: 3003 | Episodes: 183000 | Median Reward: 38.59 | Max Reward: 49.33
Iteration: 3005 | Episodes: 183100 | Median Reward: 39.93 | Max Reward: 49.33
Iteration: 3006 | Episodes: 183200 | Median Reward: 38.27 | Max Reward: 49.33
Iteration: 3008 | Episodes: 183300 | Median Reward: 25.04 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -71       |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3010      |
|    time_elapsed         | 54641     |
|    total_timesteps      | 18493440  |
| train/                  |           |
|    approx_kl            | 1.2844396 |
|    clip_fraction        | 0.343     |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 143       |
|    n_updates            | 30090     |
|    policy_gradient_loss | -0.0765   |
|    std                  | 5.69      |
|    value_loss           | 278       |
---------------------------------------
Iteration: 3010 | Episodes: 183400 | Median Reward: 31.12 | Max Reward: 49.33
Iteration: 3011 | Episodes: 183500 | Median Reward: 30.85 | Max Reward: 49.33
Iteration: 3013 | Episodes: 183600 | Median Reward: 30.69 | Max Reward: 49.33
Iteration: 3014 | Episodes: 183700 | Median Reward: 29.71 | Max Reward: 49.33
Iteration: 3016 | Episodes: 183800 | Median Reward: 38.92 | Max Reward: 49.33
Iteration: 3018 | Episodes: 183900 | Median Reward: 38.08 | Max Reward: 49.33
Iteration: 3019 | Episodes: 184000 | Median Reward: 42.84 | Max Reward: 49.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59         |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 3020        |
|    time_elapsed         | 54819       |
|    total_timesteps      | 18554880    |
| train/                  |             |
|    approx_kl            | 0.090745136 |
|    clip_fraction        | 0.0726      |
|    clip_range           | 0.4         |
|    entropy_loss         | -122        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 177         |
|    n_updates            | 30190       |
|    policy_gradient_loss | -0.0149     |
|    std                  | 5.71        |
|    value_loss           | 406         |
-----------------------------------------
Iteration: 3021 | Episodes: 184100 | Median Reward: 42.56 | Max Reward: 49.33
Iteration: 3023 | Episodes: 184200 | Median Reward: 37.40 | Max Reward: 49.33
Iteration: 3024 | Episodes: 184300 | Median Reward: 28.34 | Max Reward: 49.33
Iteration: 3026 | Episodes: 184400 | Median Reward: 38.64 | Max Reward: 49.33
Iteration: 3028 | Episodes: 184500 | Median Reward: 30.92 | Max Reward: 49.33
Iteration: 3029 | Episodes: 184600 | Median Reward: 36.04 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -70.2    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 3030     |
|    time_elapsed         | 54998    |
|    total_timesteps      | 18616320 |
| train/                  |          |
|    approx_kl            | 0.276587 |
|    clip_fraction        | 0.224    |
|    clip_range           | 0.4      |
|    entropy_loss         | -122     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 158      |
|    n_updates            | 30290    |
|    policy_gradient_loss | 0.0147   |
|    std                  | 5.73     |
|    value_loss           | 314      |
--------------------------------------
Iteration: 3031 | Episodes: 184700 | Median Reward: 29.51 | Max Reward: 49.33
Iteration: 3032 | Episodes: 184800 | Median Reward: 21.46 | Max Reward: 49.33
Iteration: 3034 | Episodes: 184900 | Median Reward: 39.53 | Max Reward: 49.33
Iteration: 3036 | Episodes: 185000 | Median Reward: 31.61 | Max Reward: 49.33
Iteration: 3037 | Episodes: 185100 | Median Reward: 38.35 | Max Reward: 49.33
Iteration: 3039 | Episodes: 185200 | Median Reward: 40.94 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -60.1     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3040      |
|    time_elapsed         | 55187     |
|    total_timesteps      | 18677760  |
| train/                  |           |
|    approx_kl            | 2.1524181 |
|    clip_fraction        | 0.222     |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 179       |
|    n_updates            | 30390     |
|    policy_gradient_loss | -0.0254   |
|    std                  | 5.75      |
|    value_loss           | 386       |
---------------------------------------
Iteration: 3041 | Episodes: 185300 | Median Reward: 33.26 | Max Reward: 49.33
Iteration: 3042 | Episodes: 185400 | Median Reward: 32.39 | Max Reward: 49.33
Iteration: 3044 | Episodes: 185500 | Median Reward: 31.15 | Max Reward: 49.33
Iteration: 3046 | Episodes: 185600 | Median Reward: 28.71 | Max Reward: 49.33
Iteration: 3047 | Episodes: 185700 | Median Reward: 36.34 | Max Reward: 49.33
Iteration: 3049 | Episodes: 185800 | Median Reward: 33.82 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -72.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3050      |
|    time_elapsed         | 55365     |
|    total_timesteps      | 18739200  |
| train/                  |           |
|    approx_kl            | 3.0356693 |
|    clip_fraction        | 0.197     |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 197       |
|    n_updates            | 30490     |
|    policy_gradient_loss | 0.0632    |
|    std                  | 5.77      |
|    value_loss           | 322       |
---------------------------------------
Iteration: 3051 | Episodes: 185900 | Median Reward: 37.41 | Max Reward: 49.33
Iteration: 3052 | Episodes: 186000 | Median Reward: 34.97 | Max Reward: 49.33
Iteration: 3054 | Episodes: 186100 | Median Reward: 36.57 | Max Reward: 49.33
Iteration: 3056 | Episodes: 186200 | Median Reward: 35.02 | Max Reward: 49.33
Iteration: 3056 | Episodes: 186300 | Median Reward: 49.02 | Max Reward: 49.33
Iteration: 3058 | Episodes: 186400 | Median Reward: 37.01 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3060      |
|    time_elapsed         | 55546     |
|    total_timesteps      | 18800640  |
| train/                  |           |
|    approx_kl            | 12.298073 |
|    clip_fraction        | 0.308     |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 153       |
|    n_updates            | 30590     |
|    policy_gradient_loss | 0.125     |
|    std                  | 5.8       |
|    value_loss           | 356       |
---------------------------------------
Iteration: 3060 | Episodes: 186500 | Median Reward: 37.09 | Max Reward: 49.33
Iteration: 3061 | Episodes: 186600 | Median Reward: 38.50 | Max Reward: 49.33
Iteration: 3063 | Episodes: 186700 | Median Reward: 33.80 | Max Reward: 49.33
Iteration: 3065 | Episodes: 186800 | Median Reward: 44.14 | Max Reward: 49.33
Iteration: 3066 | Episodes: 186900 | Median Reward: 37.91 | Max Reward: 49.33
Iteration: 3067 | Episodes: 187000 | Median Reward: 49.06 | Max Reward: 49.33
Iteration: 3067 | Episodes: 187100 | Median Reward: 49.06 | Max Reward: 49.33
Iteration: 3069 | Episodes: 187200 | Median Reward: 45.01 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -63.7     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3070      |
|    time_elapsed         | 55735     |
|    total_timesteps      | 18862080  |
| train/                  |           |
|    approx_kl            | 1.5379863 |
|    clip_fraction        | 0.181     |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 148       |
|    n_updates            | 30690     |
|    policy_gradient_loss | 0.0634    |
|    std                  | 5.83      |
|    value_loss           | 375       |
---------------------------------------
Iteration: 3070 | Episodes: 187300 | Median Reward: 41.39 | Max Reward: 49.33
Iteration: 3072 | Episodes: 187400 | Median Reward: 46.68 | Max Reward: 49.33
Iteration: 3074 | Episodes: 187500 | Median Reward: 44.91 | Max Reward: 49.33
Iteration: 3074 | Episodes: 187600 | Median Reward: 49.03 | Max Reward: 49.33
Iteration: 3074 | Episodes: 187700 | Median Reward: 49.03 | Max Reward: 49.33
Iteration: 3076 | Episodes: 187800 | Median Reward: 44.01 | Max Reward: 49.33
Iteration: 3078 | Episodes: 187900 | Median Reward: 44.03 | Max Reward: 49.33
Iteration: 3079 | Episodes: 188000 | Median Reward: 43.54 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -59.6      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3080       |
|    time_elapsed         | 55919      |
|    total_timesteps      | 18923520   |
| train/                  |            |
|    approx_kl            | 0.64240915 |
|    clip_fraction        | 0.0806     |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 202        |
|    n_updates            | 30790      |
|    policy_gradient_loss | 0.0317     |
|    std                  | 5.85       |
|    value_loss           | 414        |
----------------------------------------
Iteration: 3081 | Episodes: 188100 | Median Reward: 42.35 | Max Reward: 49.33
Iteration: 3082 | Episodes: 188200 | Median Reward: 46.35 | Max Reward: 49.33
Iteration: 3082 | Episodes: 188300 | Median Reward: 49.00 | Max Reward: 49.33
Iteration: 3083 | Episodes: 188400 | Median Reward: 46.34 | Max Reward: 49.33
Iteration: 3085 | Episodes: 188500 | Median Reward: 42.60 | Max Reward: 49.33
Iteration: 3086 | Episodes: 188600 | Median Reward: 39.59 | Max Reward: 49.33
Iteration: 3088 | Episodes: 188700 | Median Reward: 35.64 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3090      |
|    time_elapsed         | 56112     |
|    total_timesteps      | 18984960  |
| train/                  |           |
|    approx_kl            | 0.5054114 |
|    clip_fraction        | 0.1       |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 178       |
|    n_updates            | 30890     |
|    policy_gradient_loss | -0.00243  |
|    std                  | 5.89      |
|    value_loss           | 354       |
---------------------------------------
Iteration: 3090 | Episodes: 188800 | Median Reward: 40.22 | Max Reward: 49.33
Iteration: 3091 | Episodes: 188900 | Median Reward: 44.34 | Max Reward: 49.33
Iteration: 3093 | Episodes: 189000 | Median Reward: 42.40 | Max Reward: 49.33
Iteration: 3095 | Episodes: 189100 | Median Reward: 43.17 | Max Reward: 49.33
Iteration: 3096 | Episodes: 189200 | Median Reward: 44.74 | Max Reward: 49.33
Iteration: 3098 | Episodes: 189300 | Median Reward: 35.77 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -58.8      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3100       |
|    time_elapsed         | 56291      |
|    total_timesteps      | 19046400   |
| train/                  |            |
|    approx_kl            | 0.74390435 |
|    clip_fraction        | 0.0575     |
|    clip_range           | 0.4        |
|    entropy_loss         | -124       |
|    explained_variance   | 5.96e-08   |
|    learning_rate        | 0.0001     |
|    loss                 | 199        |
|    n_updates            | 30990      |
|    policy_gradient_loss | 0.00258    |
|    std                  | 5.92       |
|    value_loss           | 388        |
----------------------------------------
Iteration: 3100 | Episodes: 189400 | Median Reward: 42.80 | Max Reward: 49.33
Iteration: 3101 | Episodes: 189500 | Median Reward: 45.04 | Max Reward: 49.33
Iteration: 3103 | Episodes: 189600 | Median Reward: 45.03 | Max Reward: 49.33
Iteration: 3105 | Episodes: 189700 | Median Reward: 44.04 | Max Reward: 49.33
Iteration: 3106 | Episodes: 189800 | Median Reward: 45.65 | Max Reward: 49.33
Iteration: 3108 | Episodes: 189900 | Median Reward: 44.11 | Max Reward: 49.33
Iteration: 3109 | Episodes: 190000 | Median Reward: 44.81 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -58.5      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3110       |
|    time_elapsed         | 56471      |
|    total_timesteps      | 19107840   |
| train/                  |            |
|    approx_kl            | 0.13415289 |
|    clip_fraction        | 0.0712     |
|    clip_range           | 0.4        |
|    entropy_loss         | -124       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 197        |
|    n_updates            | 31090      |
|    policy_gradient_loss | -0.00714   |
|    std                  | 5.95       |
|    value_loss           | 357        |
----------------------------------------
Iteration: 3111 | Episodes: 190100 | Median Reward: 44.58 | Max Reward: 49.33
Iteration: 3113 | Episodes: 190200 | Median Reward: 44.39 | Max Reward: 49.33
Iteration: 3114 | Episodes: 190300 | Median Reward: 34.67 | Max Reward: 49.33
Iteration: 3116 | Episodes: 190400 | Median Reward: 42.64 | Max Reward: 49.33
Iteration: 3118 | Episodes: 190500 | Median Reward: 43.66 | Max Reward: 49.33
Iteration: 3119 | Episodes: 190600 | Median Reward: 46.43 | Max Reward: 49.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 53.4        |
|    ep_rew_mean          | -9.61       |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 3120        |
|    time_elapsed         | 56662       |
|    total_timesteps      | 19169280    |
| train/                  |             |
|    approx_kl            | 0.099266775 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.4         |
|    entropy_loss         | -125        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 185         |
|    n_updates            | 31190       |
|    policy_gradient_loss | 0.0122      |
|    std                  | 5.99        |
|    value_loss           | 386         |
-----------------------------------------
Iteration: 3120 | Episodes: 190700 | Median Reward: 44.03 | Max Reward: 49.33
Iteration: 3122 | Episodes: 190800 | Median Reward: 44.19 | Max Reward: 49.33
Iteration: 3123 | Episodes: 190900 | Median Reward: 41.86 | Max Reward: 49.33
Iteration: 3125 | Episodes: 191000 | Median Reward: 27.68 | Max Reward: 49.33
Iteration: 3127 | Episodes: 191100 | Median Reward: 43.83 | Max Reward: 49.33
Iteration: 3128 | Episodes: 191200 | Median Reward: 37.56 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -68.9      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3130       |
|    time_elapsed         | 56839      |
|    total_timesteps      | 19230720   |
| train/                  |            |
|    approx_kl            | 0.34477735 |
|    clip_fraction        | 0.178      |
|    clip_range           | 0.4        |
|    entropy_loss         | -124       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 138        |
|    n_updates            | 31290      |
|    policy_gradient_loss | -0.00185   |
|    std                  | 6.02       |
|    value_loss           | 329        |
----------------------------------------
Iteration: 3130 | Episodes: 191300 | Median Reward: 34.20 | Max Reward: 49.33
Iteration: 3132 | Episodes: 191400 | Median Reward: 31.62 | Max Reward: 49.33
Iteration: 3133 | Episodes: 191500 | Median Reward: 38.27 | Max Reward: 49.33
Iteration: 3135 | Episodes: 191600 | Median Reward: 43.47 | Max Reward: 49.33
Iteration: 3137 | Episodes: 191700 | Median Reward: 41.27 | Max Reward: 49.33
Iteration: 3138 | Episodes: 191800 | Median Reward: 41.82 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -59.5      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3140       |
|    time_elapsed         | 57021      |
|    total_timesteps      | 19292160   |
| train/                  |            |
|    approx_kl            | 0.73273224 |
|    clip_fraction        | 0.0877     |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 149        |
|    n_updates            | 31390      |
|    policy_gradient_loss | 0.000673   |
|    std                  | 6.06       |
|    value_loss           | 364        |
----------------------------------------
Iteration: 3140 | Episodes: 191900 | Median Reward: 41.84 | Max Reward: 49.33
Iteration: 3141 | Episodes: 192000 | Median Reward: 26.27 | Max Reward: 49.33
Iteration: 3143 | Episodes: 192100 | Median Reward: 33.60 | Max Reward: 49.33
Iteration: 3145 | Episodes: 192200 | Median Reward: 0.41 | Max Reward: 49.33
Iteration: 3146 | Episodes: 192300 | Median Reward: 28.09 | Max Reward: 49.33
Iteration: 3148 | Episodes: 192400 | Median Reward: 6.09 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -83        |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3150       |
|    time_elapsed         | 57208      |
|    total_timesteps      | 19353600   |
| train/                  |            |
|    approx_kl            | 0.10891117 |
|    clip_fraction        | 0.0676     |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 127        |
|    n_updates            | 31490      |
|    policy_gradient_loss | -0.0121    |
|    std                  | 6.08       |
|    value_loss           | 214        |
----------------------------------------
Iteration: 3150 | Episodes: 192500 | Median Reward: 15.45 | Max Reward: 49.33
Iteration: 3151 | Episodes: 192600 | Median Reward: 18.42 | Max Reward: 49.33
Iteration: 3153 | Episodes: 192700 | Median Reward: 20.47 | Max Reward: 49.33
Iteration: 3154 | Episodes: 192800 | Median Reward: 4.59 | Max Reward: 49.33
Iteration: 3156 | Episodes: 192900 | Median Reward: 13.70 | Max Reward: 49.33
Iteration: 3158 | Episodes: 193000 | Median Reward: 19.74 | Max Reward: 49.33
Iteration: 3159 | Episodes: 193100 | Median Reward: 8.22 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -84.6      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3160       |
|    time_elapsed         | 57392      |
|    total_timesteps      | 19415040   |
| train/                  |            |
|    approx_kl            | 0.20012054 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 98.7       |
|    n_updates            | 31590      |
|    policy_gradient_loss | -0.00485   |
|    std                  | 6.1        |
|    value_loss           | 234        |
----------------------------------------
Iteration: 3161 | Episodes: 193200 | Median Reward: 30.57 | Max Reward: 49.33
Iteration: 3163 | Episodes: 193300 | Median Reward: 19.04 | Max Reward: 49.33
Iteration: 3164 | Episodes: 193400 | Median Reward: 25.07 | Max Reward: 49.33
Iteration: 3165 | Episodes: 193500 | Median Reward: 49.01 | Max Reward: 49.33
Iteration: 3167 | Episodes: 193600 | Median Reward: 42.25 | Max Reward: 49.33
Iteration: 3168 | Episodes: 193700 | Median Reward: 34.97 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -66.1     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3170      |
|    time_elapsed         | 57578     |
|    total_timesteps      | 19476480  |
| train/                  |           |
|    approx_kl            | 1.1555017 |
|    clip_fraction        | 0.235     |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 99.5      |
|    n_updates            | 31690     |
|    policy_gradient_loss | 0.0191    |
|    std                  | 6.13      |
|    value_loss           | 308       |
---------------------------------------
Iteration: 3170 | Episodes: 193800 | Median Reward: 37.10 | Max Reward: 49.33
Iteration: 3171 | Episodes: 193900 | Median Reward: 41.12 | Max Reward: 49.33
Iteration: 3173 | Episodes: 194000 | Median Reward: 40.40 | Max Reward: 49.33
Iteration: 3175 | Episodes: 194100 | Median Reward: 40.67 | Max Reward: 49.33
Iteration: 3176 | Episodes: 194200 | Median Reward: 40.67 | Max Reward: 49.33
Iteration: 3178 | Episodes: 194300 | Median Reward: 38.01 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -74.4     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3180      |
|    time_elapsed         | 57761     |
|    total_timesteps      | 19537920  |
| train/                  |           |
|    approx_kl            | 6.45016   |
|    clip_fraction        | 0.331     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 174       |
|    n_updates            | 31790     |
|    policy_gradient_loss | 0.0689    |
|    std                  | 6.17      |
|    value_loss           | 334       |
---------------------------------------
Iteration: 3180 | Episodes: 194400 | Median Reward: 32.98 | Max Reward: 49.33
Iteration: 3181 | Episodes: 194500 | Median Reward: 31.13 | Max Reward: 49.33
Iteration: 3183 | Episodes: 194600 | Median Reward: 29.95 | Max Reward: 49.33
Iteration: 3185 | Episodes: 194700 | Median Reward: 36.36 | Max Reward: 49.33
Iteration: 3186 | Episodes: 194800 | Median Reward: 38.51 | Max Reward: 49.33
Iteration: 3188 | Episodes: 194900 | Median Reward: 36.94 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -67.2    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 3190     |
|    time_elapsed         | 57939    |
|    total_timesteps      | 19599360 |
| train/                  |          |
|    approx_kl            | 4.068637 |
|    clip_fraction        | 0.388    |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 156      |
|    n_updates            | 31890    |
|    policy_gradient_loss | 0.0176   |
|    std                  | 6.2      |
|    value_loss           | 378      |
--------------------------------------
Iteration: 3190 | Episodes: 195000 | Median Reward: 34.21 | Max Reward: 49.33
Iteration: 3191 | Episodes: 195100 | Median Reward: 39.87 | Max Reward: 49.33
Iteration: 3193 | Episodes: 195200 | Median Reward: 39.53 | Max Reward: 49.33
Iteration: 3194 | Episodes: 195300 | Median Reward: 35.65 | Max Reward: 49.33
Iteration: 3196 | Episodes: 195400 | Median Reward: 41.71 | Max Reward: 49.33
Iteration: 3198 | Episodes: 195500 | Median Reward: 42.48 | Max Reward: 49.33
Iteration: 3199 | Episodes: 195600 | Median Reward: 46.79 | Max Reward: 49.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.2       |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 3200        |
|    time_elapsed         | 58127       |
|    total_timesteps      | 19660800    |
| train/                  |             |
|    approx_kl            | 0.042888653 |
|    clip_fraction        | 0.0351      |
|    clip_range           | 0.4         |
|    entropy_loss         | -126        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 210         |
|    n_updates            | 31990       |
|    policy_gradient_loss | -0.00562    |
|    std                  | 6.24        |
|    value_loss           | 412         |
-----------------------------------------
Iteration: 3201 | Episodes: 195700 | Median Reward: 43.10 | Max Reward: 49.33
Iteration: 3203 | Episodes: 195800 | Median Reward: 38.99 | Max Reward: 49.33
Iteration: 3204 | Episodes: 195900 | Median Reward: 43.96 | Max Reward: 49.33
Iteration: 3206 | Episodes: 196000 | Median Reward: 38.91 | Max Reward: 49.33
Iteration: 3208 | Episodes: 196100 | Median Reward: 40.53 | Max Reward: 49.33
Iteration: 3209 | Episodes: 196200 | Median Reward: 40.22 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -64.8      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3210       |
|    time_elapsed         | 58305      |
|    total_timesteps      | 19722240   |
| train/                  |            |
|    approx_kl            | 0.29510936 |
|    clip_fraction        | 0.0924     |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 163        |
|    n_updates            | 32090      |
|    policy_gradient_loss | -0.0172    |
|    std                  | 6.29       |
|    value_loss           | 357        |
----------------------------------------
Iteration: 3211 | Episodes: 196300 | Median Reward: 35.61 | Max Reward: 49.33
Iteration: 3213 | Episodes: 196400 | Median Reward: 40.86 | Max Reward: 49.33
Iteration: 3214 | Episodes: 196500 | Median Reward: 41.20 | Max Reward: 49.33
Iteration: 3216 | Episodes: 196600 | Median Reward: 34.92 | Max Reward: 49.33
Iteration: 3217 | Episodes: 196700 | Median Reward: 42.54 | Max Reward: 49.33
Iteration: 3219 | Episodes: 196800 | Median Reward: 42.27 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -59.3     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3220      |
|    time_elapsed         | 58485     |
|    total_timesteps      | 19783680  |
| train/                  |           |
|    approx_kl            | 0.8823116 |
|    clip_fraction        | 0.135     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 160       |
|    n_updates            | 32190     |
|    policy_gradient_loss | 0.0146    |
|    std                  | 6.34      |
|    value_loss           | 336       |
---------------------------------------
Iteration: 3221 | Episodes: 196900 | Median Reward: 44.29 | Max Reward: 49.33
Iteration: 3221 | Episodes: 197000 | Median Reward: 49.06 | Max Reward: 49.33
Iteration: 3223 | Episodes: 197100 | Median Reward: 43.99 | Max Reward: 49.33
Iteration: 3225 | Episodes: 197200 | Median Reward: 41.29 | Max Reward: 49.33
Iteration: 3226 | Episodes: 197300 | Median Reward: 35.30 | Max Reward: 49.33
Iteration: 3228 | Episodes: 197400 | Median Reward: 41.04 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3230      |
|    time_elapsed         | 58676     |
|    total_timesteps      | 19845120  |
| train/                  |           |
|    approx_kl            | 1.1680357 |
|    clip_fraction        | 0.15      |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 150       |
|    n_updates            | 32290     |
|    policy_gradient_loss | 0.0147    |
|    std                  | 6.38      |
|    value_loss           | 320       |
---------------------------------------
Iteration: 3230 | Episodes: 197500 | Median Reward: 27.14 | Max Reward: 49.33
Iteration: 3231 | Episodes: 197600 | Median Reward: 36.65 | Max Reward: 49.33
Iteration: 3233 | Episodes: 197700 | Median Reward: 36.35 | Max Reward: 49.33
Iteration: 3234 | Episodes: 197800 | Median Reward: 41.18 | Max Reward: 49.33
Iteration: 3236 | Episodes: 197900 | Median Reward: 42.32 | Max Reward: 49.33
Iteration: 3238 | Episodes: 198000 | Median Reward: 41.73 | Max Reward: 49.33
Iteration: 3239 | Episodes: 198100 | Median Reward: 42.19 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -59.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3240      |
|    time_elapsed         | 58853     |
|    total_timesteps      | 19906560  |
| train/                  |           |
|    approx_kl            | 1.8537884 |
|    clip_fraction        | 0.211     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 144       |
|    n_updates            | 32390     |
|    policy_gradient_loss | 0.0645    |
|    std                  | 6.42      |
|    value_loss           | 338       |
---------------------------------------
Iteration: 3241 | Episodes: 198200 | Median Reward: 42.14 | Max Reward: 49.33
Iteration: 3243 | Episodes: 198300 | Median Reward: 38.61 | Max Reward: 49.33
Iteration: 3243 | Episodes: 198400 | Median Reward: 49.03 | Max Reward: 49.33
Iteration: 3245 | Episodes: 198500 | Median Reward: 39.85 | Max Reward: 49.33
Iteration: 3247 | Episodes: 198600 | Median Reward: 32.09 | Max Reward: 49.33
Iteration: 3248 | Episodes: 198700 | Median Reward: 36.52 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -61.6      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3250       |
|    time_elapsed         | 59036      |
|    total_timesteps      | 19968000   |
| train/                  |            |
|    approx_kl            | 0.36737567 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 137        |
|    n_updates            | 32490      |
|    policy_gradient_loss | -0.0251    |
|    std                  | 6.47       |
|    value_loss           | 336        |
----------------------------------------
Iteration: 3250 | Episodes: 198800 | Median Reward: 39.71 | Max Reward: 49.33
Iteration: 3252 | Episodes: 198900 | Median Reward: 35.14 | Max Reward: 49.33
Iteration: 3253 | Episodes: 199000 | Median Reward: 31.39 | Max Reward: 49.33
Iteration: 3255 | Episodes: 199100 | Median Reward: 34.98 | Max Reward: 49.33
Iteration: 3256 | Episodes: 199200 | Median Reward: 31.21 | Max Reward: 49.33
Iteration: 3258 | Episodes: 199300 | Median Reward: 34.98 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -63.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3260      |
|    time_elapsed         | 59222     |
|    total_timesteps      | 20029440  |
| train/                  |           |
|    approx_kl            | 4.4704504 |
|    clip_fraction        | 0.268     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0001    |
|    loss                 | 138       |
|    n_updates            | 32590     |
|    policy_gradient_loss | 0.0428    |
|    std                  | 6.5       |
|    value_loss           | 308       |
---------------------------------------
Iteration: 3260 | Episodes: 199400 | Median Reward: 35.25 | Max Reward: 49.33
Iteration: 3261 | Episodes: 199500 | Median Reward: 35.25 | Max Reward: 49.33
Iteration: 3263 | Episodes: 199600 | Median Reward: 36.42 | Max Reward: 49.33
Iteration: 3265 | Episodes: 199700 | Median Reward: 36.42 | Max Reward: 49.33
Iteration: 3266 | Episodes: 199800 | Median Reward: 35.46 | Max Reward: 49.33
Iteration: 3268 | Episodes: 199900 | Median Reward: 38.71 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -62.6      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3270       |
|    time_elapsed         | 59402      |
|    total_timesteps      | 20090880   |
| train/                  |            |
|    approx_kl            | 0.09657987 |
|    clip_fraction        | 0.0756     |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 191        |
|    n_updates            | 32690      |
|    policy_gradient_loss | -0.000837  |
|    std                  | 6.55       |
|    value_loss           | 323        |
----------------------------------------
Iteration: 3270 | Episodes: 200000 | Median Reward: 36.31 | Max Reward: 49.33
Iteration: 3271 | Episodes: 200100 | Median Reward: 39.45 | Max Reward: 49.33
Iteration: 3273 | Episodes: 200200 | Median Reward: 42.40 | Max Reward: 49.33
Iteration: 3275 | Episodes: 200300 | Median Reward: 36.48 | Max Reward: 49.33
Iteration: 3276 | Episodes: 200400 | Median Reward: 40.33 | Max Reward: 49.33
Iteration: 3278 | Episodes: 200500 | Median Reward: 39.65 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -60.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3280      |
|    time_elapsed         | 59581     |
|    total_timesteps      | 20152320  |
| train/                  |           |
|    approx_kl            | 1.1813393 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 151       |
|    n_updates            | 32790     |
|    policy_gradient_loss | -0.0513   |
|    std                  | 6.59      |
|    value_loss           | 324       |
---------------------------------------
Iteration: 3280 | Episodes: 200600 | Median Reward: 40.12 | Max Reward: 49.33
Iteration: 3281 | Episodes: 200700 | Median Reward: 38.70 | Max Reward: 49.33
Iteration: 3283 | Episodes: 200800 | Median Reward: 42.47 | Max Reward: 49.33
Iteration: 3284 | Episodes: 200900 | Median Reward: 43.43 | Max Reward: 49.33
Iteration: 3286 | Episodes: 201000 | Median Reward: 39.97 | Max Reward: 49.33
Iteration: 3288 | Episodes: 201100 | Median Reward: 43.66 | Max Reward: 49.33
Iteration: 3289 | Episodes: 201200 | Median Reward: 42.87 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -59.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3290      |
|    time_elapsed         | 59771     |
|    total_timesteps      | 20213760  |
| train/                  |           |
|    approx_kl            | 2.1634974 |
|    clip_fraction        | 0.17      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 166       |
|    n_updates            | 32890     |
|    policy_gradient_loss | -0.0134   |
|    std                  | 6.64      |
|    value_loss           | 353       |
---------------------------------------
Iteration: 3291 | Episodes: 201300 | Median Reward: 42.04 | Max Reward: 49.33
Iteration: 3293 | Episodes: 201400 | Median Reward: 38.74 | Max Reward: 49.33
Iteration: 3294 | Episodes: 201500 | Median Reward: 31.43 | Max Reward: 49.33
Iteration: 3296 | Episodes: 201600 | Median Reward: 32.44 | Max Reward: 49.33
Iteration: 3297 | Episodes: 201700 | Median Reward: 35.94 | Max Reward: 49.33
Iteration: 3299 | Episodes: 201800 | Median Reward: 38.07 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.4     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3300      |
|    time_elapsed         | 59951     |
|    total_timesteps      | 20275200  |
| train/                  |           |
|    approx_kl            | 0.1323783 |
|    clip_fraction        | 0.109     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 126       |
|    n_updates            | 32990     |
|    policy_gradient_loss | -0.0152   |
|    std                  | 6.68      |
|    value_loss           | 290       |
---------------------------------------
Iteration: 3301 | Episodes: 201900 | Median Reward: 38.08 | Max Reward: 49.33
Iteration: 3302 | Episodes: 202000 | Median Reward: 38.66 | Max Reward: 49.33
Iteration: 3304 | Episodes: 202100 | Median Reward: 39.16 | Max Reward: 49.33
Iteration: 3306 | Episodes: 202200 | Median Reward: 35.30 | Max Reward: 49.33
Iteration: 3307 | Episodes: 202300 | Median Reward: 40.51 | Max Reward: 49.33
Iteration: 3309 | Episodes: 202400 | Median Reward: 40.51 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -64.7      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3310       |
|    time_elapsed         | 60142      |
|    total_timesteps      | 20336640   |
| train/                  |            |
|    approx_kl            | 0.36132476 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 97.9       |
|    n_updates            | 33090      |
|    policy_gradient_loss | -0.0186    |
|    std                  | 6.71       |
|    value_loss           | 321        |
----------------------------------------
Iteration: 3311 | Episodes: 202500 | Median Reward: 33.00 | Max Reward: 49.33
Iteration: 3312 | Episodes: 202600 | Median Reward: 25.09 | Max Reward: 49.33
Iteration: 3314 | Episodes: 202700 | Median Reward: 30.98 | Max Reward: 49.33
Iteration: 3316 | Episodes: 202800 | Median Reward: 39.96 | Max Reward: 49.33
Iteration: 3317 | Episodes: 202900 | Median Reward: 25.75 | Max Reward: 49.33
Iteration: 3319 | Episodes: 203000 | Median Reward: 35.94 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -72.6      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3320       |
|    time_elapsed         | 60324      |
|    total_timesteps      | 20398080   |
| train/                  |            |
|    approx_kl            | 0.32602018 |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 131        |
|    n_updates            | 33190      |
|    policy_gradient_loss | -0.00124   |
|    std                  | 6.72       |
|    value_loss           | 315        |
----------------------------------------
Iteration: 3320 | Episodes: 203100 | Median Reward: 9.89 | Max Reward: 49.33
Iteration: 3322 | Episodes: 203200 | Median Reward: 19.65 | Max Reward: 49.33
Iteration: 3324 | Episodes: 203300 | Median Reward: 40.32 | Max Reward: 49.33
Iteration: 3325 | Episodes: 203400 | Median Reward: 41.37 | Max Reward: 49.33
Iteration: 3327 | Episodes: 203500 | Median Reward: 43.80 | Max Reward: 49.33
Iteration: 3329 | Episodes: 203600 | Median Reward: 29.24 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -75      |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 3330     |
|    time_elapsed         | 60502    |
|    total_timesteps      | 20459520 |
| train/                  |          |
|    approx_kl            | 0.094801 |
|    clip_fraction        | 0.304    |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 148      |
|    n_updates            | 33290    |
|    policy_gradient_loss | -0.0343  |
|    std                  | 6.75     |
|    value_loss           | 274      |
--------------------------------------
Iteration: 3330 | Episodes: 203700 | Median Reward: 26.51 | Max Reward: 49.33
Iteration: 3332 | Episodes: 203800 | Median Reward: 43.90 | Max Reward: 49.33
Iteration: 3334 | Episodes: 203900 | Median Reward: 28.29 | Max Reward: 49.33
Iteration: 3335 | Episodes: 204000 | Median Reward: 28.83 | Max Reward: 49.33
Iteration: 3337 | Episodes: 204100 | Median Reward: 12.73 | Max Reward: 49.33
Iteration: 3339 | Episodes: 204200 | Median Reward: 8.09 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -85.3      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3340       |
|    time_elapsed         | 60686      |
|    total_timesteps      | 20520960   |
| train/                  |            |
|    approx_kl            | 0.30811155 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 98.5       |
|    n_updates            | 33390      |
|    policy_gradient_loss | -0.0239    |
|    std                  | 6.76       |
|    value_loss           | 238        |
----------------------------------------
Iteration: 3340 | Episodes: 204300 | Median Reward: 19.35 | Max Reward: 49.33
Iteration: 3342 | Episodes: 204400 | Median Reward: 37.28 | Max Reward: 49.33
Iteration: 3343 | Episodes: 204500 | Median Reward: 42.68 | Max Reward: 49.33
Iteration: 3345 | Episodes: 204600 | Median Reward: -6.68 | Max Reward: 49.33
Iteration: 3347 | Episodes: 204700 | Median Reward: -0.83 | Max Reward: 49.33
Iteration: 3348 | Episodes: 204800 | Median Reward: -15.70 | Max Reward: 49.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -107        |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 3350        |
|    time_elapsed         | 60865       |
|    total_timesteps      | 20582400    |
| train/                  |             |
|    approx_kl            | 0.061836682 |
|    clip_fraction        | 0.052       |
|    clip_range           | 0.4         |
|    entropy_loss         | -126        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 57.5        |
|    n_updates            | 33490       |
|    policy_gradient_loss | -0.0189     |
|    std                  | 6.77        |
|    value_loss           | 119         |
-----------------------------------------
Iteration: 3350 | Episodes: 204900 | Median Reward: -12.39 | Max Reward: 49.33
Iteration: 3352 | Episodes: 205000 | Median Reward: -15.03 | Max Reward: 49.33
Iteration: 3353 | Episodes: 205100 | Median Reward: -9.27 | Max Reward: 49.33
Iteration: 3355 | Episodes: 205200 | Median Reward: -0.41 | Max Reward: 49.33
Iteration: 3357 | Episodes: 205300 | Median Reward: -2.92 | Max Reward: 49.33
Iteration: 3358 | Episodes: 205400 | Median Reward: 8.70 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -100     |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 3360     |
|    time_elapsed         | 61049    |
|    total_timesteps      | 20643840 |
| train/                  |          |
|    approx_kl            | 0.169332 |
|    clip_fraction        | 0.216    |
|    clip_range           | 0.4      |
|    entropy_loss         | -126     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 169      |
|    n_updates            | 33590    |
|    policy_gradient_loss | -0.0435  |
|    std                  | 6.78     |
|    value_loss           | 186      |
--------------------------------------
Iteration: 3360 | Episodes: 205500 | Median Reward: -9.94 | Max Reward: 49.33
Iteration: 3362 | Episodes: 205600 | Median Reward: -12.63 | Max Reward: 49.33
Iteration: 3363 | Episodes: 205700 | Median Reward: -8.08 | Max Reward: 49.33
Iteration: 3365 | Episodes: 205800 | Median Reward: -6.22 | Max Reward: 49.33
Iteration: 3367 | Episodes: 205900 | Median Reward: 20.18 | Max Reward: 49.33
Iteration: 3368 | Episodes: 206000 | Median Reward: 31.14 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -88.3     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3370      |
|    time_elapsed         | 61234     |
|    total_timesteps      | 20705280  |
| train/                  |           |
|    approx_kl            | 1.3088772 |
|    clip_fraction        | 0.313     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 124       |
|    n_updates            | 33690     |
|    policy_gradient_loss | 0.0371    |
|    std                  | 6.8       |
|    value_loss           | 310       |
---------------------------------------
Iteration: 3370 | Episodes: 206100 | Median Reward: -1.16 | Max Reward: 49.33
Iteration: 3371 | Episodes: 206200 | Median Reward: -12.98 | Max Reward: 49.33
Iteration: 3373 | Episodes: 206300 | Median Reward: 8.68 | Max Reward: 49.33
Iteration: 3375 | Episodes: 206400 | Median Reward: 26.68 | Max Reward: 49.33
Iteration: 3376 | Episodes: 206500 | Median Reward: 27.59 | Max Reward: 49.33
Iteration: 3378 | Episodes: 206600 | Median Reward: 5.22 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -105      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3380      |
|    time_elapsed         | 61416     |
|    total_timesteps      | 20766720  |
| train/                  |           |
|    approx_kl            | 2.6910968 |
|    clip_fraction        | 0.169     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 110       |
|    n_updates            | 33790     |
|    policy_gradient_loss | -0.0236   |
|    std                  | 6.81      |
|    value_loss           | 160       |
---------------------------------------
Iteration: 3380 | Episodes: 206700 | Median Reward: -11.63 | Max Reward: 49.33
Iteration: 3381 | Episodes: 206800 | Median Reward: -6.66 | Max Reward: 49.33
Iteration: 3383 | Episodes: 206900 | Median Reward: -2.61 | Max Reward: 49.33
Iteration: 3385 | Episodes: 207000 | Median Reward: 6.04 | Max Reward: 49.33
Iteration: 3386 | Episodes: 207100 | Median Reward: 21.26 | Max Reward: 49.33
Iteration: 3388 | Episodes: 207200 | Median Reward: -11.48 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -95.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3390      |
|    time_elapsed         | 61601     |
|    total_timesteps      | 20828160  |
| train/                  |           |
|    approx_kl            | 9.189766  |
|    clip_fraction        | 0.53      |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 90        |
|    n_updates            | 33890     |
|    policy_gradient_loss | 0.0601    |
|    std                  | 6.82      |
|    value_loss           | 157       |
---------------------------------------
Iteration: 3390 | Episodes: 207300 | Median Reward: -1.45 | Max Reward: 49.33
Iteration: 3391 | Episodes: 207400 | Median Reward: 28.89 | Max Reward: 49.33
Iteration: 3393 | Episodes: 207500 | Median Reward: 26.40 | Max Reward: 49.33
Iteration: 3394 | Episodes: 207600 | Median Reward: 20.41 | Max Reward: 49.33
Iteration: 3396 | Episodes: 207700 | Median Reward: 23.45 | Max Reward: 49.33
Iteration: 3398 | Episodes: 207800 | Median Reward: 36.65 | Max Reward: 49.33
Iteration: 3399 | Episodes: 207900 | Median Reward: 15.41 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -92.7     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3400      |
|    time_elapsed         | 61785     |
|    total_timesteps      | 20889600  |
| train/                  |           |
|    approx_kl            | 1.7304757 |
|    clip_fraction        | 0.363     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 71        |
|    n_updates            | 33990     |
|    policy_gradient_loss | -0.0739   |
|    std                  | 6.84      |
|    value_loss           | 219       |
---------------------------------------
Iteration: 3401 | Episodes: 208000 | Median Reward: -11.82 | Max Reward: 49.33
Iteration: 3403 | Episodes: 208100 | Median Reward: -16.20 | Max Reward: 49.33
Iteration: 3404 | Episodes: 208200 | Median Reward: 1.22 | Max Reward: 49.33
Iteration: 3406 | Episodes: 208300 | Median Reward: -9.78 | Max Reward: 49.33
Iteration: 3408 | Episodes: 208400 | Median Reward: -12.77 | Max Reward: 49.33
Iteration: 3409 | Episodes: 208500 | Median Reward: 38.76 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -71.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3410      |
|    time_elapsed         | 61961     |
|    total_timesteps      | 20951040  |
| train/                  |           |
|    approx_kl            | 10.536141 |
|    clip_fraction        | 0.451     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 123       |
|    n_updates            | 34090     |
|    policy_gradient_loss | 0.0692    |
|    std                  | 6.85      |
|    value_loss           | 258       |
---------------------------------------
Iteration: 3411 | Episodes: 208600 | Median Reward: 36.88 | Max Reward: 49.33
Iteration: 3413 | Episodes: 208700 | Median Reward: 33.88 | Max Reward: 49.33
Iteration: 3414 | Episodes: 208800 | Median Reward: 31.20 | Max Reward: 49.33
Iteration: 3416 | Episodes: 208900 | Median Reward: -8.40 | Max Reward: 49.33
Iteration: 3417 | Episodes: 209000 | Median Reward: 31.27 | Max Reward: 49.33
Iteration: 3419 | Episodes: 209100 | Median Reward: 21.17 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -87.4     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3420      |
|    time_elapsed         | 62151     |
|    total_timesteps      | 21012480  |
| train/                  |           |
|    approx_kl            | 2.9322605 |
|    clip_fraction        | 0.394     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 136       |
|    n_updates            | 34190     |
|    policy_gradient_loss | 0.112     |
|    std                  | 6.87      |
|    value_loss           | 267       |
---------------------------------------
Iteration: 3421 | Episodes: 209200 | Median Reward: 40.25 | Max Reward: 49.33
Iteration: 3422 | Episodes: 209300 | Median Reward: 43.55 | Max Reward: 49.33
Iteration: 3424 | Episodes: 209400 | Median Reward: 35.46 | Max Reward: 49.33
Iteration: 3426 | Episodes: 209500 | Median Reward: 42.71 | Max Reward: 49.33
Iteration: 3427 | Episodes: 209600 | Median Reward: 41.21 | Max Reward: 49.33
Iteration: 3429 | Episodes: 209700 | Median Reward: 42.24 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -63        |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3430       |
|    time_elapsed         | 62330      |
|    total_timesteps      | 21073920   |
| train/                  |            |
|    approx_kl            | 0.17076696 |
|    clip_fraction        | 0.0888     |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 220        |
|    n_updates            | 34290      |
|    policy_gradient_loss | -0.0329    |
|    std                  | 6.89       |
|    value_loss           | 426        |
----------------------------------------
Iteration: 3431 | Episodes: 209800 | Median Reward: 44.42 | Max Reward: 49.33
Iteration: 3432 | Episodes: 209900 | Median Reward: 11.39 | Max Reward: 49.33
Iteration: 3434 | Episodes: 210000 | Median Reward: -2.00 | Max Reward: 49.33
Iteration: 3436 | Episodes: 210100 | Median Reward: 23.83 | Max Reward: 49.33
Iteration: 3437 | Episodes: 210200 | Median Reward: 19.78 | Max Reward: 49.33
Iteration: 3439 | Episodes: 210300 | Median Reward: 33.12 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.1     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3440      |
|    time_elapsed         | 62510     |
|    total_timesteps      | 21135360  |
| train/                  |           |
|    approx_kl            | 2.1036625 |
|    clip_fraction        | 0.255     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 190       |
|    n_updates            | 34390     |
|    policy_gradient_loss | -0.036    |
|    std                  | 6.92      |
|    value_loss           | 380       |
---------------------------------------
Iteration: 3440 | Episodes: 210400 | Median Reward: 33.03 | Max Reward: 49.33
Iteration: 3442 | Episodes: 210500 | Median Reward: 35.33 | Max Reward: 49.33
Iteration: 3444 | Episodes: 210600 | Median Reward: 31.24 | Max Reward: 49.33
Iteration: 3445 | Episodes: 210700 | Median Reward: 35.89 | Max Reward: 49.33
Iteration: 3447 | Episodes: 210800 | Median Reward: 43.20 | Max Reward: 49.33
Iteration: 3449 | Episodes: 210900 | Median Reward: 40.10 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -69.3    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 3450     |
|    time_elapsed         | 62696    |
|    total_timesteps      | 21196800 |
| train/                  |          |
|    approx_kl            | 0.847623 |
|    clip_fraction        | 0.171    |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 206      |
|    n_updates            | 34490    |
|    policy_gradient_loss | -0.00995 |
|    std                  | 6.95     |
|    value_loss           | 414      |
--------------------------------------
Iteration: 3450 | Episodes: 211000 | Median Reward: 34.67 | Max Reward: 49.33
Iteration: 3452 | Episodes: 211100 | Median Reward: 31.61 | Max Reward: 49.33
Iteration: 3454 | Episodes: 211200 | Median Reward: 12.73 | Max Reward: 49.33
Iteration: 3455 | Episodes: 211300 | Median Reward: 11.26 | Max Reward: 49.33
Iteration: 3457 | Episodes: 211400 | Median Reward: 17.66 | Max Reward: 49.33
Iteration: 3459 | Episodes: 211500 | Median Reward: 29.39 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -73.3     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3460      |
|    time_elapsed         | 62877     |
|    total_timesteps      | 21258240  |
| train/                  |           |
|    approx_kl            | 2.0139592 |
|    clip_fraction        | 0.404     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 170       |
|    n_updates            | 34590     |
|    policy_gradient_loss | -0.0498   |
|    std                  | 6.98      |
|    value_loss           | 318       |
---------------------------------------
Iteration: 3460 | Episodes: 211600 | Median Reward: 30.22 | Max Reward: 49.33
Iteration: 3462 | Episodes: 211700 | Median Reward: 30.33 | Max Reward: 49.33
Iteration: 3463 | Episodes: 211800 | Median Reward: 31.49 | Max Reward: 49.33
Iteration: 3465 | Episodes: 211900 | Median Reward: 31.37 | Max Reward: 49.33
Iteration: 3467 | Episodes: 212000 | Median Reward: -6.86 | Max Reward: 49.33
Iteration: 3468 | Episodes: 212100 | Median Reward: -12.14 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -110      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3470      |
|    time_elapsed         | 63057     |
|    total_timesteps      | 21319680  |
| train/                  |           |
|    approx_kl            | 1.1080469 |
|    clip_fraction        | 0.323     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 43.6      |
|    n_updates            | 34690     |
|    policy_gradient_loss | -0.0478   |
|    std                  | 7         |
|    value_loss           | 97.1      |
---------------------------------------
Iteration: 3470 | Episodes: 212200 | Median Reward: -9.43 | Max Reward: 49.33
Iteration: 3472 | Episodes: 212300 | Median Reward: -13.00 | Max Reward: 49.33
Iteration: 3473 | Episodes: 212400 | Median Reward: -2.74 | Max Reward: 49.33
Iteration: 3475 | Episodes: 212500 | Median Reward: -2.01 | Max Reward: 49.33
Iteration: 3477 | Episodes: 212600 | Median Reward: -10.08 | Max Reward: 49.33
Iteration: 3478 | Episodes: 212700 | Median Reward: -11.84 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -104      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3480      |
|    time_elapsed         | 63245     |
|    total_timesteps      | 21381120  |
| train/                  |           |
|    approx_kl            | 4.7953734 |
|    clip_fraction        | 0.437     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 48.6      |
|    n_updates            | 34790     |
|    policy_gradient_loss | -0.0546   |
|    std                  | 7.02      |
|    value_loss           | 118       |
---------------------------------------
Iteration: 3480 | Episodes: 212800 | Median Reward: -9.12 | Max Reward: 49.33
Iteration: 3482 | Episodes: 212900 | Median Reward: -17.08 | Max Reward: 49.33
Iteration: 3483 | Episodes: 213000 | Median Reward: 13.22 | Max Reward: 49.33
Iteration: 3485 | Episodes: 213100 | Median Reward: -3.64 | Max Reward: 49.33
Iteration: 3487 | Episodes: 213200 | Median Reward: -8.59 | Max Reward: 49.33
Iteration: 3488 | Episodes: 213300 | Median Reward: -9.88 | Max Reward: 49.33
a--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -95.8    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 3490     |
|    time_elapsed         | 63425    |
|    total_timesteps      | 21442560 |
| train/                  |          |
|    approx_kl            | 0.344289 |
|    clip_fraction        | 0.18     |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 91.8     |
|    n_updates            | 34890    |
|    policy_gradient_loss | -0.0465  |
|    std                  | 7.04     |
|    value_loss           | 135      |
--------------------------------------
Iteration: 3490 | Episodes: 213400 | Median Reward: 3.97 | Max Reward: 49.33
Iteration: 3491 | Episodes: 213500 | Median Reward: -8.95 | Max Reward: 49.33
Iteration: 3493 | Episodes: 213600 | Median Reward: -9.57 | Max Reward: 49.33
Iteration: 3495 | Episodes: 213700 | Median Reward: -13.05 | Max Reward: 49.33
Iteration: 3496 | Episodes: 213800 | Median Reward: -3.17 | Max Reward: 49.33
Iteration: 3498 | Episodes: 213900 | Median Reward: 27.51 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3500      |
|    time_elapsed         | 63603     |
|    total_timesteps      | 21504000  |
| train/                  |           |
|    approx_kl            | 1.4950407 |
|    clip_fraction        | 0.33      |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 145       |
|    n_updates            | 34990     |
|    policy_gradient_loss | -0.0707   |
|    std                  | 7.08      |
|    value_loss           | 323       |
---------------------------------------
Iteration: 3500 | Episodes: 214000 | Median Reward: 32.87 | Max Reward: 49.33
Iteration: 3501 | Episodes: 214100 | Median Reward: 38.49 | Max Reward: 49.33
Iteration: 3503 | Episodes: 214200 | Median Reward: 33.59 | Max Reward: 49.33
Iteration: 3505 | Episodes: 214300 | Median Reward: 21.75 | Max Reward: 49.33
Iteration: 3506 | Episodes: 214400 | Median Reward: 29.62 | Max Reward: 49.33
Iteration: 3508 | Episodes: 214500 | Median Reward: 32.83 | Max Reward: 49.33
Iteration: 3508 | Episodes: 214600 | Median Reward: 49.10 | Max Reward: 49.33
Iteration: 3509 | Episodes: 214700 | Median Reward: 49.10 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 72.5      |
|    ep_rew_mean          | -48.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3510      |
|    time_elapsed         | 63798     |
|    total_timesteps      | 21565440  |
| train/                  |           |
|    approx_kl            | 4.2124825 |
|    clip_fraction        | 0.452     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 547       |
|    n_updates            | 35090     |
|    policy_gradient_loss | -0.0401   |
|    std                  | 7.1       |
|    value_loss           | 1.02e+03  |
---------------------------------------
Iteration: 3510 | Episodes: 214800 | Median Reward: 17.74 | Max Reward: 49.33
Iteration: 3512 | Episodes: 214900 | Median Reward: 35.46 | Max Reward: 49.33
Iteration: 3514 | Episodes: 215000 | Median Reward: 31.67 | Max Reward: 49.33
Iteration: 3515 | Episodes: 215100 | Median Reward: 29.84 | Max Reward: 49.33
Iteration: 3517 | Episodes: 215200 | Median Reward: 34.16 | Max Reward: 49.33
Iteration: 3518 | Episodes: 215300 | Median Reward: 31.99 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -72.3     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3520      |
|    time_elapsed         | 63978     |
|    total_timesteps      | 21626880  |
| train/                  |           |
|    approx_kl            | 3.0076551 |
|    clip_fraction        | 0.379     |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 142       |
|    n_updates            | 35190     |
|    policy_gradient_loss | 0.0233    |
|    std                  | 7.12      |
|    value_loss           | 322       |
---------------------------------------
Iteration: 3520 | Episodes: 215400 | Median Reward: 32.56 | Max Reward: 49.33
Iteration: 3522 | Episodes: 215500 | Median Reward: 34.88 | Max Reward: 49.33
Iteration: 3523 | Episodes: 215600 | Median Reward: 37.30 | Max Reward: 49.33
Iteration: 3525 | Episodes: 215700 | Median Reward: 35.20 | Max Reward: 49.33
Iteration: 3527 | Episodes: 215800 | Median Reward: 33.04 | Max Reward: 49.33
Iteration: 3528 | Episodes: 215900 | Median Reward: 34.28 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -74       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3530      |
|    time_elapsed         | 64167     |
|    total_timesteps      | 21688320  |
| train/                  |           |
|    approx_kl            | 6.8709626 |
|    clip_fraction        | 0.469     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 136       |
|    n_updates            | 35290     |
|    policy_gradient_loss | -0.0642   |
|    std                  | 7.15      |
|    value_loss           | 355       |
---------------------------------------
Iteration: 3530 | Episodes: 216000 | Median Reward: 31.87 | Max Reward: 49.33
Iteration: 3532 | Episodes: 216100 | Median Reward: 34.22 | Max Reward: 49.33
Iteration: 3533 | Episodes: 216200 | Median Reward: 36.57 | Max Reward: 49.33
Iteration: 3535 | Episodes: 216300 | Median Reward: 28.95 | Max Reward: 49.33
Iteration: 3537 | Episodes: 216400 | Median Reward: 27.13 | Max Reward: 49.33
Iteration: 3538 | Episodes: 216500 | Median Reward: 21.50 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -70.8      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3540       |
|    time_elapsed         | 64348      |
|    total_timesteps      | 21749760   |
| train/                  |            |
|    approx_kl            | 0.90287185 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 108        |
|    n_updates            | 35390      |
|    policy_gradient_loss | 0.0512     |
|    std                  | 7.19       |
|    value_loss           | 265        |
----------------------------------------
Iteration: 3540 | Episodes: 216600 | Median Reward: 34.17 | Max Reward: 49.33
Iteration: 3542 | Episodes: 216700 | Median Reward: 31.61 | Max Reward: 49.33
Iteration: 3543 | Episodes: 216800 | Median Reward: 22.83 | Max Reward: 49.33
Iteration: 3545 | Episodes: 216900 | Median Reward: 22.50 | Max Reward: 49.33
Iteration: 3546 | Episodes: 217000 | Median Reward: 26.25 | Max Reward: 49.33
Iteration: 3548 | Episodes: 217100 | Median Reward: 23.32 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -76.9    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 3550     |
|    time_elapsed         | 64525    |
|    total_timesteps      | 21811200 |
| train/                  |          |
|    approx_kl            | 4.183312 |
|    clip_fraction        | 0.474    |
|    clip_range           | 0.4      |
|    entropy_loss         | -128     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 134      |
|    n_updates            | 35490    |
|    policy_gradient_loss | 0.0263   |
|    std                  | 7.22     |
|    value_loss           | 216      |
--------------------------------------
Iteration: 3550 | Episodes: 217200 | Median Reward: 29.74 | Max Reward: 49.33
Iteration: 3551 | Episodes: 217300 | Median Reward: -1.81 | Max Reward: 49.33
Iteration: 3553 | Episodes: 217400 | Median Reward: 35.06 | Max Reward: 49.33
Iteration: 3555 | Episodes: 217500 | Median Reward: -3.31 | Max Reward: 49.33
Iteration: 3556 | Episodes: 217600 | Median Reward: -8.40 | Max Reward: 49.33
Iteration: 3558 | Episodes: 217700 | Median Reward: -8.67 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -106      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3560      |
|    time_elapsed         | 64710     |
|    total_timesteps      | 21872640  |
| train/                  |           |
|    approx_kl            | 1.8603088 |
|    clip_fraction        | 0.201     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 46.8      |
|    n_updates            | 35590     |
|    policy_gradient_loss | -0.0246   |
|    std                  | 7.24      |
|    value_loss           | 184       |
---------------------------------------
Iteration: 3560 | Episodes: 217800 | Median Reward: -8.20 | Max Reward: 49.33
Iteration: 3561 | Episodes: 217900 | Median Reward: 17.83 | Max Reward: 49.33
Iteration: 3563 | Episodes: 218000 | Median Reward: -6.68 | Max Reward: 49.33
Iteration: 3565 | Episodes: 218100 | Median Reward: -7.83 | Max Reward: 49.33
Iteration: 3566 | Episodes: 218200 | Median Reward: -14.36 | Max Reward: 49.33
Iteration: 3568 | Episodes: 218300 | Median Reward: 29.89 | Max Reward: 49.33
Iteration: 3569 | Episodes: 218400 | Median Reward: -2.58 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -99.2      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 3570       |
|    time_elapsed         | 64889      |
|    total_timesteps      | 21934080   |
| train/                  |            |
|    approx_kl            | 0.49946696 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.4        |
|    entropy_loss         | -129       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 51.4       |
|    n_updates            | 35690      |
|    policy_gradient_loss | -0.0343    |
|    std                  | 7.25       |
|    value_loss           | 124        |
----------------------------------------
Iteration: 3571 | Episodes: 218500 | Median Reward: -6.57 | Max Reward: 49.33
Iteration: 3573 | Episodes: 218600 | Median Reward: 28.41 | Max Reward: 49.33
Iteration: 3574 | Episodes: 218700 | Median Reward: 28.96 | Max Reward: 49.33
Iteration: 3576 | Episodes: 218800 | Median Reward: 39.71 | Max Reward: 49.33
Iteration: 3578 | Episodes: 218900 | Median Reward: 36.31 | Max Reward: 49.33
Iteration: 3579 | Episodes: 219000 | Median Reward: 4.77 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -90.3     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3580      |
|    time_elapsed         | 65068     |
|    total_timesteps      | 21995520  |
| train/                  |           |
|    approx_kl            | 2.3160892 |
|    clip_fraction        | 0.372     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 119       |
|    n_updates            | 35790     |
|    policy_gradient_loss | -0.0406   |
|    std                  | 7.27      |
|    value_loss           | 252       |
---------------------------------------
Iteration: 3581 | Episodes: 219100 | Median Reward: 31.97 | Max Reward: 49.33
Iteration: 3583 | Episodes: 219200 | Median Reward: 31.55 | Max Reward: 49.33
Iteration: 3584 | Episodes: 219300 | Median Reward: 36.64 | Max Reward: 49.33
Iteration: 3586 | Episodes: 219400 | Median Reward: 34.53 | Max Reward: 49.33
Iteration: 3588 | Episodes: 219500 | Median Reward: 36.65 | Max Reward: 49.33
Iteration: 3589 | Episodes: 219600 | Median Reward: 31.41 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -67.7      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3590       |
|    time_elapsed         | 65258      |
|    total_timesteps      | 22056960   |
| train/                  |            |
|    approx_kl            | 0.44271636 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 147        |
|    n_updates            | 35890      |
|    policy_gradient_loss | -0.0389    |
|    std                  | 7.29       |
|    value_loss           | 375        |
----------------------------------------
Iteration: 3591 | Episodes: 219700 | Median Reward: 23.61 | Max Reward: 49.33
Iteration: 3592 | Episodes: 219800 | Median Reward: 29.39 | Max Reward: 49.33
Iteration: 3594 | Episodes: 219900 | Median Reward: 31.09 | Max Reward: 49.33
Iteration: 3596 | Episodes: 220000 | Median Reward: 36.33 | Max Reward: 49.33
Iteration: 3597 | Episodes: 220100 | Median Reward: 31.75 | Max Reward: 49.33
Iteration: 3599 | Episodes: 220200 | Median Reward: -0.09 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -89.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3600      |
|    time_elapsed         | 65440     |
|    total_timesteps      | 22118400  |
| train/                  |           |
|    approx_kl            | 0.755702  |
|    clip_fraction        | 0.195     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 83.7      |
|    n_updates            | 35990     |
|    policy_gradient_loss | -0.0141   |
|    std                  | 7.3       |
|    value_loss           | 164       |
---------------------------------------
Iteration: 3601 | Episodes: 220300 | Median Reward: 27.70 | Max Reward: 49.33
Iteration: 3602 | Episodes: 220400 | Median Reward: 17.66 | Max Reward: 49.33
Iteration: 3604 | Episodes: 220500 | Median Reward: 0.96 | Max Reward: 49.33
Iteration: 3606 | Episodes: 220600 | Median Reward: 0.84 | Max Reward: 49.33
Iteration: 3607 | Episodes: 220700 | Median Reward: 27.58 | Max Reward: 49.33
Iteration: 3609 | Episodes: 220800 | Median Reward: -4.00 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -96.8     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3610      |
|    time_elapsed         | 65616     |
|    total_timesteps      | 22179840  |
| train/                  |           |
|    approx_kl            | 1.6064498 |
|    clip_fraction        | 0.289     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 47        |
|    n_updates            | 36090     |
|    policy_gradient_loss | -0.0195   |
|    std                  | 7.32      |
|    value_loss           | 117       |
---------------------------------------
Iteration: 3611 | Episodes: 220900 | Median Reward: 32.96 | Max Reward: 49.33
Iteration: 3612 | Episodes: 221000 | Median Reward: 27.85 | Max Reward: 49.33
Iteration: 3614 | Episodes: 221100 | Median Reward: 35.86 | Max Reward: 49.33
Iteration: 3615 | Episodes: 221200 | Median Reward: 33.93 | Max Reward: 49.33
Iteration: 3617 | Episodes: 221300 | Median Reward: 25.50 | Max Reward: 49.33
Iteration: 3619 | Episodes: 221400 | Median Reward: 32.00 | Max Reward: 49.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.2       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 3620        |
|    time_elapsed         | 65809       |
|    total_timesteps      | 22241280    |
| train/                  |             |
|    approx_kl            | 0.047528517 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.4         |
|    entropy_loss         | -131        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 174         |
|    n_updates            | 36190       |
|    policy_gradient_loss | -0.043      |
|    std                  | 7.33        |
|    value_loss           | 333         |
-----------------------------------------
Iteration: 3620 | Episodes: 221500 | Median Reward: 32.43 | Max Reward: 49.33
Iteration: 3622 | Episodes: 221600 | Median Reward: 29.20 | Max Reward: 49.33
Iteration: 3624 | Episodes: 221700 | Median Reward: 30.30 | Max Reward: 49.33
Iteration: 3625 | Episodes: 221800 | Median Reward: 12.32 | Max Reward: 49.33
Iteration: 3627 | Episodes: 221900 | Median Reward: 29.35 | Max Reward: 49.33
Iteration: 3629 | Episodes: 222000 | Median Reward: 34.52 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -74.1    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3630     |
|    time_elapsed         | 65989    |
|    total_timesteps      | 22302720 |
| train/                  |          |
|    approx_kl            | 8.263964 |
|    clip_fraction        | 0.489    |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 129      |
|    n_updates            | 36290    |
|    policy_gradient_loss | -0.0951  |
|    std                  | 7.35     |
|    value_loss           | 357      |
--------------------------------------
Iteration: 3630 | Episodes: 222100 | Median Reward: 29.68 | Max Reward: 49.33
Iteration: 3632 | Episodes: 222200 | Median Reward: 35.65 | Max Reward: 49.33
Iteration: 3634 | Episodes: 222300 | Median Reward: 37.56 | Max Reward: 49.33
Iteration: 3635 | Episodes: 222400 | Median Reward: 34.04 | Max Reward: 49.33
Iteration: 3637 | Episodes: 222500 | Median Reward: 34.04 | Max Reward: 49.33
Iteration: 3638 | Episodes: 222600 | Median Reward: 34.87 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -66.4    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3640     |
|    time_elapsed         | 66178    |
|    total_timesteps      | 22364160 |
| train/                  |          |
|    approx_kl            | 0.953979 |
|    clip_fraction        | 0.287    |
|    clip_range           | 0.4      |
|    entropy_loss         | -130     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 164      |
|    n_updates            | 36390    |
|    policy_gradient_loss | -0.0156  |
|    std                  | 7.37     |
|    value_loss           | 370      |
--------------------------------------
Iteration: 3640 | Episodes: 222700 | Median Reward: 30.72 | Max Reward: 49.33
Iteration: 3642 | Episodes: 222800 | Median Reward: 37.66 | Max Reward: 49.33
Iteration: 3643 | Episodes: 222900 | Median Reward: 40.27 | Max Reward: 49.33
Iteration: 3645 | Episodes: 223000 | Median Reward: 37.99 | Max Reward: 49.33
Iteration: 3647 | Episodes: 223100 | Median Reward: 33.27 | Max Reward: 49.33
Iteration: 3648 | Episodes: 223200 | Median Reward: 31.30 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3650      |
|    time_elapsed         | 66360     |
|    total_timesteps      | 22425600  |
| train/                  |           |
|    approx_kl            | 1.7479932 |
|    clip_fraction        | 0.367     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 180       |
|    n_updates            | 36490     |
|    policy_gradient_loss | -0.0683   |
|    std                  | 7.39      |
|    value_loss           | 298       |
---------------------------------------
Iteration: 3650 | Episodes: 223300 | Median Reward: 33.68 | Max Reward: 49.33
Iteration: 3652 | Episodes: 223400 | Median Reward: 16.61 | Max Reward: 49.33
Iteration: 3653 | Episodes: 223500 | Median Reward: 33.69 | Max Reward: 49.33
Iteration: 3655 | Episodes: 223600 | Median Reward: 34.73 | Max Reward: 49.33
Iteration: 3657 | Episodes: 223700 | Median Reward: 39.43 | Max Reward: 49.33
Iteration: 3658 | Episodes: 223800 | Median Reward: 37.36 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3660      |
|    time_elapsed         | 66539     |
|    total_timesteps      | 22487040  |
| train/                  |           |
|    approx_kl            | 1.6727328 |
|    clip_fraction        | 0.334     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.0001    |
|    loss                 | 175       |
|    n_updates            | 36590     |
|    policy_gradient_loss | -0.0471   |
|    std                  | 7.41      |
|    value_loss           | 361       |
---------------------------------------
Iteration: 3660 | Episodes: 223900 | Median Reward: 32.61 | Max Reward: 49.33
Iteration: 3662 | Episodes: 224000 | Median Reward: 23.65 | Max Reward: 49.33
Iteration: 3663 | Episodes: 224100 | Median Reward: 38.10 | Max Reward: 49.33
Iteration: 3665 | Episodes: 224200 | Median Reward: 36.08 | Max Reward: 49.33
Iteration: 3666 | Episodes: 224300 | Median Reward: 34.63 | Max Reward: 49.33
Iteration: 3668 | Episodes: 224400 | Median Reward: 27.03 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3670      |
|    time_elapsed         | 66728     |
|    total_timesteps      | 22548480  |
| train/                  |           |
|    approx_kl            | 1.3645363 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 90.6      |
|    n_updates            | 36690     |
|    policy_gradient_loss | -0.037    |
|    std                  | 7.43      |
|    value_loss           | 281       |
---------------------------------------
Iteration: 3670 | Episodes: 224500 | Median Reward: 39.25 | Max Reward: 49.33
Iteration: 3671 | Episodes: 224600 | Median Reward: 32.70 | Max Reward: 49.33
Iteration: 3673 | Episodes: 224700 | Median Reward: 21.83 | Max Reward: 49.33
Iteration: 3675 | Episodes: 224800 | Median Reward: 24.69 | Max Reward: 49.33
Iteration: 3676 | Episodes: 224900 | Median Reward: 40.10 | Max Reward: 49.33
Iteration: 3678 | Episodes: 225000 | Median Reward: 36.21 | Max Reward: 49.33
Iteration: 3679 | Episodes: 225100 | Median Reward: 40.00 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -64.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3680      |
|    time_elapsed         | 66910     |
|    total_timesteps      | 22609920  |
| train/                  |           |
|    approx_kl            | 1.0442107 |
|    clip_fraction        | 0.0974    |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 137       |
|    n_updates            | 36790     |
|    policy_gradient_loss | -0.0184   |
|    std                  | 7.45      |
|    value_loss           | 325       |
---------------------------------------
Iteration: 3681 | Episodes: 225200 | Median Reward: 37.86 | Max Reward: 49.33
Iteration: 3682 | Episodes: 225300 | Median Reward: 38.04 | Max Reward: 49.33
Iteration: 3684 | Episodes: 225400 | Median Reward: 32.27 | Max Reward: 49.33
Iteration: 3686 | Episodes: 225500 | Median Reward: 29.90 | Max Reward: 49.33
Iteration: 3687 | Episodes: 225600 | Median Reward: 23.41 | Max Reward: 49.33
Iteration: 3689 | Episodes: 225700 | Median Reward: 23.84 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3690      |
|    time_elapsed         | 67090     |
|    total_timesteps      | 22671360  |
| train/                  |           |
|    approx_kl            | 2.3034232 |
|    clip_fraction        | 0.208     |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 85.5      |
|    n_updates            | 36890     |
|    policy_gradient_loss | -0.0332   |
|    std                  | 7.49      |
|    value_loss           | 263       |
---------------------------------------
Iteration: 3691 | Episodes: 225800 | Median Reward: 22.57 | Max Reward: 49.33
Iteration: 3692 | Episodes: 225900 | Median Reward: 33.65 | Max Reward: 49.33
Iteration: 3694 | Episodes: 226000 | Median Reward: 17.27 | Max Reward: 49.33
Iteration: 3696 | Episodes: 226100 | Median Reward: 3.65 | Max Reward: 49.33
Iteration: 3697 | Episodes: 226200 | Median Reward: 1.94 | Max Reward: 49.33
Iteration: 3699 | Episodes: 226300 | Median Reward: 4.08 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -89.8      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3700       |
|    time_elapsed         | 67280      |
|    total_timesteps      | 22732800   |
| train/                  |            |
|    approx_kl            | 0.93565184 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 124        |
|    n_updates            | 36990      |
|    policy_gradient_loss | -0.0414    |
|    std                  | 7.49       |
|    value_loss           | 246        |
----------------------------------------
Iteration: 3701 | Episodes: 226400 | Median Reward: 8.07 | Max Reward: 49.33
Iteration: 3702 | Episodes: 226500 | Median Reward: 17.45 | Max Reward: 49.33
Iteration: 3704 | Episodes: 226600 | Median Reward: -2.91 | Max Reward: 49.33
Iteration: 3705 | Episodes: 226700 | Median Reward: -4.92 | Max Reward: 49.33
Iteration: 3707 | Episodes: 226800 | Median Reward: -6.27 | Max Reward: 49.33
Iteration: 3709 | Episodes: 226900 | Median Reward: -6.27 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -108      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3710      |
|    time_elapsed         | 67462     |
|    total_timesteps      | 22794240  |
| train/                  |           |
|    approx_kl            | 0.2973629 |
|    clip_fraction        | 0.107     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 60.1      |
|    n_updates            | 37090     |
|    policy_gradient_loss | -0.00819  |
|    std                  | 7.5       |
|    value_loss           | 123       |
---------------------------------------
Iteration: 3710 | Episodes: 227000 | Median Reward: -8.14 | Max Reward: 49.33
Iteration: 3712 | Episodes: 227100 | Median Reward: -3.51 | Max Reward: 49.33
Iteration: 3714 | Episodes: 227200 | Median Reward: -2.82 | Max Reward: 49.33
Iteration: 3715 | Episodes: 227300 | Median Reward: -2.13 | Max Reward: 49.33
Iteration: 3717 | Episodes: 227400 | Median Reward: 3.04 | Max Reward: 49.33
Iteration: 3719 | Episodes: 227500 | Median Reward: 0.95 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -87.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3720      |
|    time_elapsed         | 67649     |
|    total_timesteps      | 22855680  |
| train/                  |           |
|    approx_kl            | 1.8336986 |
|    clip_fraction        | 0.278     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 79.6      |
|    n_updates            | 37190     |
|    policy_gradient_loss | -0.0438   |
|    std                  | 7.51      |
|    value_loss           | 147       |
---------------------------------------
Iteration: 3720 | Episodes: 227600 | Median Reward: 23.89 | Max Reward: 49.33
Iteration: 3722 | Episodes: 227700 | Median Reward: 2.44 | Max Reward: 49.33
Iteration: 3724 | Episodes: 227800 | Median Reward: 0.46 | Max Reward: 49.33
Iteration: 3725 | Episodes: 227900 | Median Reward: 28.12 | Max Reward: 49.33
Iteration: 3727 | Episodes: 228000 | Median Reward: 14.38 | Max Reward: 49.33
Iteration: 3729 | Episodes: 228100 | Median Reward: 30.50 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3730      |
|    time_elapsed         | 67830     |
|    total_timesteps      | 22917120  |
| train/                  |           |
|    approx_kl            | 1.7922645 |
|    clip_fraction        | 0.134     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 179       |
|    n_updates            | 37290     |
|    policy_gradient_loss | -0.00532  |
|    std                  | 7.54      |
|    value_loss           | 351       |
---------------------------------------
Iteration: 3730 | Episodes: 228200 | Median Reward: 38.49 | Max Reward: 49.33
Iteration: 3732 | Episodes: 228300 | Median Reward: 39.97 | Max Reward: 49.33
Iteration: 3733 | Episodes: 228400 | Median Reward: 39.43 | Max Reward: 49.33
Iteration: 3735 | Episodes: 228500 | Median Reward: 37.04 | Max Reward: 49.33
Iteration: 3737 | Episodes: 228600 | Median Reward: 35.16 | Max Reward: 49.33
Iteration: 3738 | Episodes: 228700 | Median Reward: 34.72 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -67.8      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3740       |
|    time_elapsed         | 68008      |
|    total_timesteps      | 22978560   |
| train/                  |            |
|    approx_kl            | 0.64950573 |
|    clip_fraction        | 0.0535     |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 191        |
|    n_updates            | 37390      |
|    policy_gradient_loss | -0.0069    |
|    std                  | 7.55       |
|    value_loss           | 367        |
----------------------------------------
Iteration: 3740 | Episodes: 228800 | Median Reward: 32.14 | Max Reward: 49.33
Iteration: 3742 | Episodes: 228900 | Median Reward: 34.10 | Max Reward: 49.33
Iteration: 3743 | Episodes: 229000 | Median Reward: 36.65 | Max Reward: 49.33
Iteration: 3745 | Episodes: 229100 | Median Reward: 36.07 | Max Reward: 49.33
Iteration: 3747 | Episodes: 229200 | Median Reward: 32.63 | Max Reward: 49.33
Iteration: 3748 | Episodes: 229300 | Median Reward: 38.29 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -71.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3750      |
|    time_elapsed         | 68198     |
|    total_timesteps      | 23040000  |
| train/                  |           |
|    approx_kl            | 1.1151571 |
|    clip_fraction        | 0.0902    |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 202       |
|    n_updates            | 37490     |
|    policy_gradient_loss | -0.003    |
|    std                  | 7.57      |
|    value_loss           | 381       |
---------------------------------------
Iteration: 3750 | Episodes: 229400 | Median Reward: 35.28 | Max Reward: 49.33
Iteration: 3751 | Episodes: 229500 | Median Reward: 44.23 | Max Reward: 49.33
Iteration: 3751 | Episodes: 229600 | Median Reward: 49.00 | Max Reward: 49.33
Iteration: 3752 | Episodes: 229700 | Median Reward: 42.71 | Max Reward: 49.33
Iteration: 3754 | Episodes: 229800 | Median Reward: 36.16 | Max Reward: 49.33
Iteration: 3756 | Episodes: 229900 | Median Reward: 36.16 | Max Reward: 49.33
Iteration: 3757 | Episodes: 230000 | Median Reward: 36.10 | Max Reward: 49.33
Iteration: 3759 | Episodes: 230100 | Median Reward: 33.12 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -69.5      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3760       |
|    time_elapsed         | 68383      |
|    total_timesteps      | 23101440   |
| train/                  |            |
|    approx_kl            | 0.68375003 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 128        |
|    n_updates            | 37590      |
|    policy_gradient_loss | -0.0172    |
|    std                  | 7.6        |
|    value_loss           | 284        |
----------------------------------------
Iteration: 3761 | Episodes: 230200 | Median Reward: 28.30 | Max Reward: 49.33
Iteration: 3762 | Episodes: 230300 | Median Reward: 10.96 | Max Reward: 49.33
Iteration: 3764 | Episodes: 230400 | Median Reward: 6.45 | Max Reward: 49.33
Iteration: 3765 | Episodes: 230500 | Median Reward: 24.30 | Max Reward: 49.33
Iteration: 3767 | Episodes: 230600 | Median Reward: 18.13 | Max Reward: 49.33
Iteration: 3769 | Episodes: 230700 | Median Reward: 35.49 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3770      |
|    time_elapsed         | 68561     |
|    total_timesteps      | 23162880  |
| train/                  |           |
|    approx_kl            | 1.0358999 |
|    clip_fraction        | 0.123     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 194       |
|    n_updates            | 37690     |
|    policy_gradient_loss | -0.00988  |
|    std                  | 7.62      |
|    value_loss           | 359       |
---------------------------------------
Iteration: 3770 | Episodes: 230800 | Median Reward: 26.92 | Max Reward: 49.33
Iteration: 3772 | Episodes: 230900 | Median Reward: 36.71 | Max Reward: 49.33
Iteration: 3773 | Episodes: 231000 | Median Reward: 35.03 | Max Reward: 49.33
Iteration: 3775 | Episodes: 231100 | Median Reward: 37.04 | Max Reward: 49.33
Iteration: 3777 | Episodes: 231200 | Median Reward: 36.53 | Max Reward: 49.33
Iteration: 3778 | Episodes: 231300 | Median Reward: 33.98 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3780      |
|    time_elapsed         | 68751     |
|    total_timesteps      | 23224320  |
| train/                  |           |
|    approx_kl            | 1.6658738 |
|    clip_fraction        | 0.12      |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 157       |
|    n_updates            | 37790     |
|    policy_gradient_loss | -0.0264   |
|    std                  | 7.64      |
|    value_loss           | 305       |
---------------------------------------
Iteration: 3780 | Episodes: 231400 | Median Reward: 33.69 | Max Reward: 49.33
Iteration: 3782 | Episodes: 231500 | Median Reward: 36.29 | Max Reward: 49.33
Iteration: 3783 | Episodes: 231600 | Median Reward: 36.66 | Max Reward: 49.33
Iteration: 3785 | Episodes: 231700 | Median Reward: 28.75 | Max Reward: 49.33
Iteration: 3787 | Episodes: 231800 | Median Reward: 36.75 | Max Reward: 49.33
Iteration: 3788 | Episodes: 231900 | Median Reward: 29.48 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -89.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3790      |
|    time_elapsed         | 68931     |
|    total_timesteps      | 23285760  |
| train/                  |           |
|    approx_kl            | 1.0644642 |
|    clip_fraction        | 0.23      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 71.4      |
|    n_updates            | 37890     |
|    policy_gradient_loss | -0.013    |
|    std                  | 7.67      |
|    value_loss           | 200       |
---------------------------------------
Iteration: 3790 | Episodes: 232000 | Median Reward: 14.85 | Max Reward: 49.33
Iteration: 3792 | Episodes: 232100 | Median Reward: 37.84 | Max Reward: 49.33
Iteration: 3793 | Episodes: 232200 | Median Reward: 35.67 | Max Reward: 49.33
Iteration: 3795 | Episodes: 232300 | Median Reward: 37.50 | Max Reward: 49.33
Iteration: 3796 | Episodes: 232400 | Median Reward: 24.74 | Max Reward: 49.33
Iteration: 3798 | Episodes: 232500 | Median Reward: 39.96 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -64.1    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3800     |
|    time_elapsed         | 69118    |
|    total_timesteps      | 23347200 |
| train/                  |          |
|    approx_kl            | 1.332503 |
|    clip_fraction        | 0.122    |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 1.79e-07 |
|    learning_rate        | 0.0001   |
|    loss                 | 186      |
|    n_updates            | 37990    |
|    policy_gradient_loss | -0.0237  |
|    std                  | 7.68     |
|    value_loss           | 386      |
--------------------------------------
Iteration: 3800 | Episodes: 232600 | Median Reward: 38.11 | Max Reward: 49.33
Iteration: 3801 | Episodes: 232700 | Median Reward: 37.29 | Max Reward: 49.33
Iteration: 3803 | Episodes: 232800 | Median Reward: 35.58 | Max Reward: 49.33
Iteration: 3805 | Episodes: 232900 | Median Reward: 33.08 | Max Reward: 49.33
Iteration: 3806 | Episodes: 233000 | Median Reward: 19.08 | Max Reward: 49.33
Iteration: 3808 | Episodes: 233100 | Median Reward: 36.63 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -86       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3810      |
|    time_elapsed         | 69300     |
|    total_timesteps      | 23408640  |
| train/                  |           |
|    approx_kl            | 0.2631876 |
|    clip_fraction        | 0.0699    |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 53.3      |
|    n_updates            | 38090     |
|    policy_gradient_loss | -0.0117   |
|    std                  | 7.72      |
|    value_loss           | 256       |
---------------------------------------
Iteration: 3810 | Episodes: 233200 | Median Reward: 10.06 | Max Reward: 49.33
Iteration: 3811 | Episodes: 233300 | Median Reward: -4.62 | Max Reward: 49.33
Iteration: 3813 | Episodes: 233400 | Median Reward: 25.77 | Max Reward: 49.33
Iteration: 3815 | Episodes: 233500 | Median Reward: 32.88 | Max Reward: 49.33
Iteration: 3816 | Episodes: 233600 | Median Reward: 27.21 | Max Reward: 49.33
Iteration: 3818 | Episodes: 233700 | Median Reward: 36.67 | Max Reward: 49.33
Iteration: 3819 | Episodes: 233800 | Median Reward: 39.38 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 42.5       |
|    ep_rew_mean          | 2.57       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3820       |
|    time_elapsed         | 69480      |
|    total_timesteps      | 23470080   |
| train/                  |            |
|    approx_kl            | 0.15280178 |
|    clip_fraction        | 0.0308     |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 181        |
|    n_updates            | 38190      |
|    policy_gradient_loss | -0.0155    |
|    std                  | 7.72       |
|    value_loss           | 377        |
----------------------------------------
Iteration: 3820 | Episodes: 233900 | Median Reward: 49.04 | Max Reward: 49.33
Iteration: 3821 | Episodes: 234000 | Median Reward: 32.89 | Max Reward: 49.33
Iteration: 3823 | Episodes: 234100 | Median Reward: 28.91 | Max Reward: 49.33
Iteration: 3825 | Episodes: 234200 | Median Reward: 32.54 | Max Reward: 49.33
Iteration: 3826 | Episodes: 234300 | Median Reward: 30.72 | Max Reward: 49.33
Iteration: 3828 | Episodes: 234400 | Median Reward: 2.60 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -75.8      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3830       |
|    time_elapsed         | 69670      |
|    total_timesteps      | 23531520   |
| train/                  |            |
|    approx_kl            | 0.30501264 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 55.4       |
|    n_updates            | 38290      |
|    policy_gradient_loss | -0.0571    |
|    std                  | 7.74       |
|    value_loss           | 229        |
----------------------------------------
Iteration: 3830 | Episodes: 234500 | Median Reward: 35.95 | Max Reward: 49.33
Iteration: 3831 | Episodes: 234600 | Median Reward: 36.15 | Max Reward: 49.33
Iteration: 3833 | Episodes: 234700 | Median Reward: 32.09 | Max Reward: 49.33
Iteration: 3835 | Episodes: 234800 | Median Reward: 14.24 | Max Reward: 49.33
Iteration: 3836 | Episodes: 234900 | Median Reward: 27.56 | Max Reward: 49.33
Iteration: 3838 | Episodes: 235000 | Median Reward: 39.17 | Max Reward: 49.33
Iteration: 3839 | Episodes: 235100 | Median Reward: 9.89 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3840      |
|    time_elapsed         | 69849     |
|    total_timesteps      | 23592960  |
| train/                  |           |
|    approx_kl            | 2.0250149 |
|    clip_fraction        | 0.257     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 193       |
|    n_updates            | 38390     |
|    policy_gradient_loss | -0.0416   |
|    std                  | 7.76      |
|    value_loss           | 371       |
---------------------------------------
Iteration: 3841 | Episodes: 235200 | Median Reward: -2.68 | Max Reward: 49.33
Iteration: 3843 | Episodes: 235300 | Median Reward: 28.66 | Max Reward: 49.33
Iteration: 3844 | Episodes: 235400 | Median Reward: 28.27 | Max Reward: 49.33
Iteration: 3846 | Episodes: 235500 | Median Reward: 15.65 | Max Reward: 49.33
Iteration: 3848 | Episodes: 235600 | Median Reward: 31.93 | Max Reward: 49.33
Iteration: 3849 | Episodes: 235700 | Median Reward: 4.87 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -91        |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3850       |
|    time_elapsed         | 70029      |
|    total_timesteps      | 23654400   |
| train/                  |            |
|    approx_kl            | 0.40961695 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -131       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 89.4       |
|    n_updates            | 38490      |
|    policy_gradient_loss | -0.0572    |
|    std                  | 7.77       |
|    value_loss           | 253        |
----------------------------------------
Iteration: 3851 | Episodes: 235800 | Median Reward: 16.01 | Max Reward: 49.33
Iteration: 3853 | Episodes: 235900 | Median Reward: 36.32 | Max Reward: 49.33
Iteration: 3854 | Episodes: 236000 | Median Reward: 38.75 | Max Reward: 49.33
Iteration: 3856 | Episodes: 236100 | Median Reward: 33.18 | Max Reward: 49.33
Iteration: 3858 | Episodes: 236200 | Median Reward: 33.11 | Max Reward: 49.33
Iteration: 3859 | Episodes: 236300 | Median Reward: 30.56 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -74.9      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3860       |
|    time_elapsed         | 70222      |
|    total_timesteps      | 23715840   |
| train/                  |            |
|    approx_kl            | 0.77038825 |
|    clip_fraction        | 0.094      |
|    clip_range           | 0.4        |
|    entropy_loss         | -132       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 169        |
|    n_updates            | 38590      |
|    policy_gradient_loss | -0.00106   |
|    std                  | 7.78       |
|    value_loss           | 329        |
----------------------------------------
Iteration: 3861 | Episodes: 236400 | Median Reward: 38.77 | Max Reward: 49.33
Iteration: 3862 | Episodes: 236500 | Median Reward: 28.87 | Max Reward: 49.33
Iteration: 3864 | Episodes: 236600 | Median Reward: 38.27 | Max Reward: 49.33
Iteration: 3866 | Episodes: 236700 | Median Reward: 36.71 | Max Reward: 49.33
Iteration: 3867 | Episodes: 236800 | Median Reward: 31.54 | Max Reward: 49.33
Iteration: 3869 | Episodes: 236900 | Median Reward: 30.16 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3870      |
|    time_elapsed         | 70401     |
|    total_timesteps      | 23777280  |
| train/                  |           |
|    approx_kl            | 1.6976001 |
|    clip_fraction        | 0.123     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 171       |
|    n_updates            | 38690     |
|    policy_gradient_loss | -0.0376   |
|    std                  | 7.81      |
|    value_loss           | 317       |
---------------------------------------
Iteration: 3871 | Episodes: 237000 | Median Reward: 38.04 | Max Reward: 49.33
Iteration: 3872 | Episodes: 237100 | Median Reward: 30.99 | Max Reward: 49.33
Iteration: 3874 | Episodes: 237200 | Median Reward: 34.33 | Max Reward: 49.33
Iteration: 3876 | Episodes: 237300 | Median Reward: 36.37 | Max Reward: 49.33
Iteration: 3877 | Episodes: 237400 | Median Reward: 35.18 | Max Reward: 49.33
Iteration: 3879 | Episodes: 237500 | Median Reward: 28.30 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -72.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3880      |
|    time_elapsed         | 70592     |
|    total_timesteps      | 23838720  |
| train/                  |           |
|    approx_kl            | 0.3738613 |
|    clip_fraction        | 0.204     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 130       |
|    n_updates            | 38790     |
|    policy_gradient_loss | -0.0541   |
|    std                  | 7.83      |
|    value_loss           | 269       |
---------------------------------------
Iteration: 3881 | Episodes: 237600 | Median Reward: 19.93 | Max Reward: 49.33
Iteration: 3882 | Episodes: 237700 | Median Reward: 25.59 | Max Reward: 49.33
Iteration: 3884 | Episodes: 237800 | Median Reward: -7.46 | Max Reward: 49.33
Iteration: 3885 | Episodes: 237900 | Median Reward: -6.07 | Max Reward: 49.33
Iteration: 3887 | Episodes: 238000 | Median Reward: -7.97 | Max Reward: 49.33
Iteration: 3889 | Episodes: 238100 | Median Reward: 3.48 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -99       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3890      |
|    time_elapsed         | 70773     |
|    total_timesteps      | 23900160  |
| train/                  |           |
|    approx_kl            | 0.6727853 |
|    clip_fraction        | 0.176     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 88.1      |
|    n_updates            | 38890     |
|    policy_gradient_loss | -0.0425   |
|    std                  | 7.84      |
|    value_loss           | 185       |
---------------------------------------
Iteration: 3890 | Episodes: 238200 | Median Reward: -7.95 | Max Reward: 49.33
Iteration: 3892 | Episodes: 238300 | Median Reward: -23.66 | Max Reward: 49.33
Iteration: 3894 | Episodes: 238400 | Median Reward: -15.96 | Max Reward: 49.33
Iteration: 3895 | Episodes: 238500 | Median Reward: 13.77 | Max Reward: 49.33
Iteration: 3897 | Episodes: 238600 | Median Reward: -14.40 | Max Reward: 49.33
Iteration: 3899 | Episodes: 238700 | Median Reward: -1.90 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -97.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3900      |
|    time_elapsed         | 70951     |
|    total_timesteps      | 23961600  |
| train/                  |           |
|    approx_kl            | 0.8183611 |
|    clip_fraction        | 0.23      |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0001    |
|    loss                 | 51.9      |
|    n_updates            | 38990     |
|    policy_gradient_loss | -0.0599   |
|    std                  | 7.84      |
|    value_loss           | 143       |
---------------------------------------
Iteration: 3900 | Episodes: 238800 | Median Reward: -2.60 | Max Reward: 49.33
Iteration: 3902 | Episodes: 238900 | Median Reward: -16.10 | Max Reward: 49.33
Iteration: 3904 | Episodes: 239000 | Median Reward: -11.23 | Max Reward: 49.33
Iteration: 3905 | Episodes: 239100 | Median Reward: -13.53 | Max Reward: 49.33
Iteration: 3907 | Episodes: 239200 | Median Reward: 3.18 | Max Reward: 49.33
Iteration: 3908 | Episodes: 239300 | Median Reward: -4.41 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -88.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3910      |
|    time_elapsed         | 71144     |
|    total_timesteps      | 24023040  |
| train/                  |           |
|    approx_kl            | 0.8181976 |
|    clip_fraction        | 0.259     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 77.5      |
|    n_updates            | 39090     |
|    policy_gradient_loss | -0.0444   |
|    std                  | 7.84      |
|    value_loss           | 242       |
---------------------------------------
Iteration: 3910 | Episodes: 239400 | Median Reward: 29.93 | Max Reward: 49.33
Iteration: 3912 | Episodes: 239500 | Median Reward: 17.78 | Max Reward: 49.33
Iteration: 3913 | Episodes: 239600 | Median Reward: 24.17 | Max Reward: 49.33
Iteration: 3915 | Episodes: 239700 | Median Reward: 30.52 | Max Reward: 49.33
Iteration: 3917 | Episodes: 239800 | Median Reward: 31.01 | Max Reward: 49.33
Iteration: 3918 | Episodes: 239900 | Median Reward: 31.24 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -79.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3920      |
|    time_elapsed         | 71327     |
|    total_timesteps      | 24084480  |
| train/                  |           |
|    approx_kl            | 2.5153635 |
|    clip_fraction        | 0.359     |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 147       |
|    n_updates            | 39190     |
|    policy_gradient_loss | -0.00466  |
|    std                  | 7.85      |
|    value_loss           | 244       |
---------------------------------------
Iteration: 3920 | Episodes: 240000 | Median Reward: 30.81 | Max Reward: 49.33
Iteration: 3922 | Episodes: 240100 | Median Reward: 28.21 | Max Reward: 49.33
Iteration: 3923 | Episodes: 240200 | Median Reward: 18.47 | Max Reward: 49.33
Iteration: 3925 | Episodes: 240300 | Median Reward: 35.01 | Max Reward: 49.33
Iteration: 3927 | Episodes: 240400 | Median Reward: 38.33 | Max Reward: 49.33
Iteration: 3928 | Episodes: 240500 | Median Reward: 36.68 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3930      |
|    time_elapsed         | 71504     |
|    total_timesteps      | 24145920  |
| train/                  |           |
|    approx_kl            | 0.3930022 |
|    clip_fraction        | 0.116     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 157       |
|    n_updates            | 39290     |
|    policy_gradient_loss | -0.0462   |
|    std                  | 7.87      |
|    value_loss           | 324       |
---------------------------------------
Iteration: 3930 | Episodes: 240600 | Median Reward: 39.93 | Max Reward: 49.33
Iteration: 3932 | Episodes: 240700 | Median Reward: 39.27 | Max Reward: 49.33
Iteration: 3933 | Episodes: 240800 | Median Reward: 37.64 | Max Reward: 49.33
Iteration: 3935 | Episodes: 240900 | Median Reward: 35.19 | Max Reward: 49.33
Iteration: 3936 | Episodes: 241000 | Median Reward: 26.62 | Max Reward: 49.33
Iteration: 3938 | Episodes: 241100 | Median Reward: 40.75 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3940      |
|    time_elapsed         | 71695     |
|    total_timesteps      | 24207360  |
| train/                  |           |
|    approx_kl            | 3.0883927 |
|    clip_fraction        | 0.308     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 171       |
|    n_updates            | 39390     |
|    policy_gradient_loss | 0.0315    |
|    std                  | 7.89      |
|    value_loss           | 360       |
---------------------------------------
Iteration: 3940 | Episodes: 241200 | Median Reward: 37.87 | Max Reward: 49.33
Iteration: 3941 | Episodes: 241300 | Median Reward: 38.21 | Max Reward: 49.33
Iteration: 3943 | Episodes: 241400 | Median Reward: 32.09 | Max Reward: 49.33
Iteration: 3945 | Episodes: 241500 | Median Reward: 33.28 | Max Reward: 49.33
Iteration: 3946 | Episodes: 241600 | Median Reward: 27.95 | Max Reward: 49.33
Iteration: 3948 | Episodes: 241700 | Median Reward: 30.02 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -74.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3950      |
|    time_elapsed         | 71873     |
|    total_timesteps      | 24268800  |
| train/                  |           |
|    approx_kl            | 1.4375128 |
|    clip_fraction        | 0.158     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 154       |
|    n_updates            | 39490     |
|    policy_gradient_loss | -0.0127   |
|    std                  | 7.9       |
|    value_loss           | 310       |
---------------------------------------
Iteration: 3950 | Episodes: 241800 | Median Reward: 31.19 | Max Reward: 49.33
Iteration: 3951 | Episodes: 241900 | Median Reward: 36.22 | Max Reward: 49.33
Iteration: 3953 | Episodes: 242000 | Median Reward: 31.71 | Max Reward: 49.33
Iteration: 3955 | Episodes: 242100 | Median Reward: 32.18 | Max Reward: 49.33
Iteration: 3956 | Episodes: 242200 | Median Reward: 36.26 | Max Reward: 49.33
Iteration: 3958 | Episodes: 242300 | Median Reward: 32.21 | Max Reward: 49.33
Iteration: 3959 | Episodes: 242400 | Median Reward: 37.44 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -75.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3960      |
|    time_elapsed         | 72063     |
|    total_timesteps      | 24330240  |
| train/                  |           |
|    approx_kl            | 3.8079498 |
|    clip_fraction        | 0.292     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 143       |
|    n_updates            | 39590     |
|    policy_gradient_loss | -0.0828   |
|    std                  | 7.91      |
|    value_loss           | 261       |
---------------------------------------
Iteration: 3961 | Episodes: 242500 | Median Reward: 26.24 | Max Reward: 49.33
Iteration: 3963 | Episodes: 242600 | Median Reward: 36.81 | Max Reward: 49.33
Iteration: 3964 | Episodes: 242700 | Median Reward: 37.52 | Max Reward: 49.33
Iteration: 3966 | Episodes: 242800 | Median Reward: 38.33 | Max Reward: 49.33
Iteration: 3968 | Episodes: 242900 | Median Reward: 27.22 | Max Reward: 49.33
Iteration: 3969 | Episodes: 243000 | Median Reward: 35.22 | Max Reward: 49.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.5       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 3970        |
|    time_elapsed         | 72243       |
|    total_timesteps      | 24391680    |
| train/                  |             |
|    approx_kl            | 0.015900083 |
|    clip_fraction        | 0.0238      |
|    clip_range           | 0.4         |
|    entropy_loss         | -133        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 153         |
|    n_updates            | 39690       |
|    policy_gradient_loss | -0.0235     |
|    std                  | 7.93        |
|    value_loss           | 303         |
-----------------------------------------
Iteration: 3971 | Episodes: 243100 | Median Reward: 35.12 | Max Reward: 49.33
Iteration: 3973 | Episodes: 243200 | Median Reward: 38.01 | Max Reward: 49.33
Iteration: 3974 | Episodes: 243300 | Median Reward: 40.62 | Max Reward: 49.33
Iteration: 3976 | Episodes: 243400 | Median Reward: 38.46 | Max Reward: 49.33
Iteration: 3978 | Episodes: 243500 | Median Reward: 36.06 | Max Reward: 49.33
Iteration: 3979 | Episodes: 243600 | Median Reward: 37.90 | Max Reward: 49.33
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.2       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 3980        |
|    time_elapsed         | 72423       |
|    total_timesteps      | 24453120    |
| train/                  |             |
|    approx_kl            | 0.057729237 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.4         |
|    entropy_loss         | -133        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0001      |
|    loss                 | 184         |
|    n_updates            | 39790       |
|    policy_gradient_loss | -0.0114     |
|    std                  | 7.94        |
|    value_loss           | 348         |
-----------------------------------------
Iteration: 3981 | Episodes: 243700 | Median Reward: 40.28 | Max Reward: 49.33
Iteration: 3982 | Episodes: 243800 | Median Reward: 38.22 | Max Reward: 49.33
Iteration: 3984 | Episodes: 243900 | Median Reward: 34.51 | Max Reward: 49.33
Iteration: 3986 | Episodes: 244000 | Median Reward: 37.60 | Max Reward: 49.33
Iteration: 3987 | Episodes: 244100 | Median Reward: 32.18 | Max Reward: 49.33
Iteration: 3989 | Episodes: 244200 | Median Reward: 39.68 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -67.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3990      |
|    time_elapsed         | 72614     |
|    total_timesteps      | 24514560  |
| train/                  |           |
|    approx_kl            | 0.6687066 |
|    clip_fraction        | 0.0412    |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 198       |
|    n_updates            | 39890     |
|    policy_gradient_loss | -0.00219  |
|    std                  | 7.97      |
|    value_loss           | 313       |
---------------------------------------
Iteration: 3991 | Episodes: 244300 | Median Reward: 25.61 | Max Reward: 49.33
Iteration: 3992 | Episodes: 244400 | Median Reward: 38.71 | Max Reward: 49.33
Iteration: 3994 | Episodes: 244500 | Median Reward: 41.45 | Max Reward: 49.33
Iteration: 3996 | Episodes: 244600 | Median Reward: 41.45 | Max Reward: 49.33
Iteration: 3997 | Episodes: 244700 | Median Reward: 34.82 | Max Reward: 49.33
Iteration: 3998 | Episodes: 244800 | Median Reward: 49.00 | Max Reward: 49.33
Iteration: 3999 | Episodes: 244900 | Median Reward: 37.27 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4000      |
|    time_elapsed         | 72796     |
|    total_timesteps      | 24576000  |
| train/                  |           |
|    approx_kl            | 6.9632874 |
|    clip_fraction        | 0.217     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 173       |
|    n_updates            | 39990     |
|    policy_gradient_loss | -0.0519   |
|    std                  | 7.99      |
|    value_loss           | 358       |
---------------------------------------
Iteration: 4001 | Episodes: 245000 | Median Reward: 37.18 | Max Reward: 49.33
Iteration: 4003 | Episodes: 245100 | Median Reward: 38.37 | Max Reward: 49.33
Iteration: 4004 | Episodes: 245200 | Median Reward: 38.80 | Max Reward: 49.33
Iteration: 4006 | Episodes: 245300 | Median Reward: 40.15 | Max Reward: 49.33
Iteration: 4008 | Episodes: 245400 | Median Reward: 38.93 | Max Reward: 49.33
Iteration: 4009 | Episodes: 245500 | Median Reward: 37.75 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -67.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4010      |
|    time_elapsed         | 72976     |
|    total_timesteps      | 24637440  |
| train/                  |           |
|    approx_kl            | 7.0691566 |
|    clip_fraction        | 0.21      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 196       |
|    n_updates            | 40090     |
|    policy_gradient_loss | -0.0505   |
|    std                  | 8.01      |
|    value_loss           | 352       |
---------------------------------------
Iteration: 4011 | Episodes: 245600 | Median Reward: 37.12 | Max Reward: 49.33
Iteration: 4013 | Episodes: 245700 | Median Reward: 29.64 | Max Reward: 49.33
Iteration: 4014 | Episodes: 245800 | Median Reward: 34.24 | Max Reward: 49.33
Iteration: 4016 | Episodes: 245900 | Median Reward: 33.61 | Max Reward: 49.33
Iteration: 4017 | Episodes: 246000 | Median Reward: 41.00 | Max Reward: 49.33
Iteration: 4019 | Episodes: 246100 | Median Reward: 35.14 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4020      |
|    time_elapsed         | 73169     |
|    total_timesteps      | 24698880  |
| train/                  |           |
|    approx_kl            | 4.5643363 |
|    clip_fraction        | 0.397     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 135       |
|    n_updates            | 40190     |
|    policy_gradient_loss | -0.0659   |
|    std                  | 8.03      |
|    value_loss           | 325       |
---------------------------------------
Iteration: 4021 | Episodes: 246200 | Median Reward: -14.27 | Max Reward: 49.33
Iteration: 4022 | Episodes: 246300 | Median Reward: -9.64 | Max Reward: 49.33
Iteration: 4024 | Episodes: 246400 | Median Reward: -13.56 | Max Reward: 49.33
Iteration: 4026 | Episodes: 246500 | Median Reward: 1.94 | Max Reward: 49.33
Iteration: 4027 | Episodes: 246600 | Median Reward: -15.49 | Max Reward: 49.33
Iteration: 4029 | Episodes: 246700 | Median Reward: -7.49 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -109       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4030       |
|    time_elapsed         | 73352      |
|    total_timesteps      | 24760320   |
| train/                  |            |
|    approx_kl            | 0.43872803 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 42.6       |
|    n_updates            | 40290      |
|    policy_gradient_loss | -0.00489   |
|    std                  | 8.03       |
|    value_loss           | 117        |
----------------------------------------
Iteration: 4031 | Episodes: 246800 | Median Reward: -3.71 | Max Reward: 49.33
Iteration: 4032 | Episodes: 246900 | Median Reward: -7.63 | Max Reward: 49.33
Iteration: 4034 | Episodes: 247000 | Median Reward: 23.82 | Max Reward: 49.33
Iteration: 4036 | Episodes: 247100 | Median Reward: -13.28 | Max Reward: 49.33
Iteration: 4037 | Episodes: 247200 | Median Reward: -5.45 | Max Reward: 49.33
Iteration: 4039 | Episodes: 247300 | Median Reward: 0.19 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4040      |
|    time_elapsed         | 73538     |
|    total_timesteps      | 24821760  |
| train/                  |           |
|    approx_kl            | 0.3475283 |
|    clip_fraction        | 0.155     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 161       |
|    n_updates            | 40390     |
|    policy_gradient_loss | -0.0139   |
|    std                  | 8.04      |
|    value_loss           | 186       |
---------------------------------------
Iteration: 4040 | Episodes: 247400 | Median Reward: 20.38 | Max Reward: 49.33
Iteration: 4042 | Episodes: 247500 | Median Reward: 15.35 | Max Reward: 49.33
Iteration: 4044 | Episodes: 247600 | Median Reward: 40.58 | Max Reward: 49.33
Iteration: 4045 | Episodes: 247700 | Median Reward: 24.91 | Max Reward: 49.33
Iteration: 4047 | Episodes: 247800 | Median Reward: 31.33 | Max Reward: 49.33
Iteration: 4049 | Episodes: 247900 | Median Reward: 24.82 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4050      |
|    time_elapsed         | 73720     |
|    total_timesteps      | 24883200  |
| train/                  |           |
|    approx_kl            | 3.8449807 |
|    clip_fraction        | 0.197     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 186       |
|    n_updates            | 40490     |
|    policy_gradient_loss | -0.0483   |
|    std                  | 8.06      |
|    value_loss           | 263       |
---------------------------------------
Iteration: 4050 | Episodes: 248000 | Median Reward: 35.65 | Max Reward: 49.33
Iteration: 4052 | Episodes: 248100 | Median Reward: 36.46 | Max Reward: 49.33
Iteration: 4054 | Episodes: 248200 | Median Reward: 37.95 | Max Reward: 49.33
Iteration: 4055 | Episodes: 248300 | Median Reward: 37.53 | Max Reward: 49.33
Iteration: 4057 | Episodes: 248400 | Median Reward: 37.55 | Max Reward: 49.33
Iteration: 4059 | Episodes: 248500 | Median Reward: 36.87 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -62.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4060      |
|    time_elapsed         | 73903     |
|    total_timesteps      | 24944640  |
| train/                  |           |
|    approx_kl            | 7.1201153 |
|    clip_fraction        | 0.224     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 178       |
|    n_updates            | 40590     |
|    policy_gradient_loss | -0.0104   |
|    std                  | 8.08      |
|    value_loss           | 379       |
---------------------------------------
Iteration: 4060 | Episodes: 248600 | Median Reward: 41.05 | Max Reward: 49.33
Iteration: 4061 | Episodes: 248700 | Median Reward: 34.03 | Max Reward: 49.33
Iteration: 4063 | Episodes: 248800 | Median Reward: 35.03 | Max Reward: 49.33
Iteration: 4065 | Episodes: 248900 | Median Reward: 34.38 | Max Reward: 49.33
Iteration: 4066 | Episodes: 249000 | Median Reward: 40.36 | Max Reward: 49.33
Iteration: 4068 | Episodes: 249100 | Median Reward: 13.34 | Max Reward: 49.33
Iteration: 4069 | Episodes: 249200 | Median Reward: 33.81 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -72.9    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4070     |
|    time_elapsed         | 74090    |
|    total_timesteps      | 25006080 |
| train/                  |          |
|    approx_kl            | 8.787727 |
|    clip_fraction        | 0.322    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 105      |
|    n_updates            | 40690    |
|    policy_gradient_loss | -0.038   |
|    std                  | 8.1      |
|    value_loss           | 223      |
--------------------------------------
Iteration: 4071 | Episodes: 249300 | Median Reward: 34.70 | Max Reward: 49.33
Iteration: 4073 | Episodes: 249400 | Median Reward: 37.01 | Max Reward: 49.33
Iteration: 4074 | Episodes: 249500 | Median Reward: 29.43 | Max Reward: 49.33
Iteration: 4076 | Episodes: 249600 | Median Reward: 8.22 | Max Reward: 49.33
Iteration: 4078 | Episodes: 249700 | Median Reward: 8.35 | Max Reward: 49.33
Iteration: 4079 | Episodes: 249800 | Median Reward: 30.70 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -82.2    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4080     |
|    time_elapsed         | 74273    |
|    total_timesteps      | 25067520 |
| train/                  |          |
|    approx_kl            | 6.220108 |
|    clip_fraction        | 0.355    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 148      |
|    n_updates            | 40790    |
|    policy_gradient_loss | -0.0903  |
|    std                  | 8.12     |
|    value_loss           | 247      |
--------------------------------------
Iteration: 4081 | Episodes: 249900 | Median Reward: -4.67 | Max Reward: 49.33
Iteration: 4083 | Episodes: 250000 | Median Reward: -20.03 | Max Reward: 49.33
Iteration: 4084 | Episodes: 250100 | Median Reward: -11.84 | Max Reward: 49.33
Iteration: 4086 | Episodes: 250200 | Median Reward: -4.01 | Max Reward: 49.33
Iteration: 4088 | Episodes: 250300 | Median Reward: 22.51 | Max Reward: 49.33
Iteration: 4089 | Episodes: 250400 | Median Reward: 23.38 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -81.9    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4090     |
|    time_elapsed         | 74461    |
|    total_timesteps      | 25128960 |
| train/                  |          |
|    approx_kl            | 4.636996 |
|    clip_fraction        | 0.244    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 120      |
|    n_updates            | 40890    |
|    policy_gradient_loss | 1.7e-06  |
|    std                  | 8.13     |
|    value_loss           | 231      |
--------------------------------------
Iteration: 4091 | Episodes: 250500 | Median Reward: 27.47 | Max Reward: 49.33
Iteration: 4092 | Episodes: 250600 | Median Reward: 19.46 | Max Reward: 49.33
Iteration: 4094 | Episodes: 250700 | Median Reward: 23.63 | Max Reward: 49.33
Iteration: 4096 | Episodes: 250800 | Median Reward: 24.89 | Max Reward: 49.33
Iteration: 4097 | Episodes: 250900 | Median Reward: 24.89 | Max Reward: 49.33
Iteration: 4099 | Episodes: 251000 | Median Reward: 31.79 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4100      |
|    time_elapsed         | 74640     |
|    total_timesteps      | 25190400  |
| train/                  |           |
|    approx_kl            | 5.1578755 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 192       |
|    n_updates            | 40990     |
|    policy_gradient_loss | -0.00875  |
|    std                  | 8.15      |
|    value_loss           | 329       |
---------------------------------------
Iteration: 4101 | Episodes: 251100 | Median Reward: 38.69 | Max Reward: 49.33
Iteration: 4102 | Episodes: 251200 | Median Reward: 35.19 | Max Reward: 49.33
Iteration: 4104 | Episodes: 251300 | Median Reward: 32.71 | Max Reward: 49.33
Iteration: 4106 | Episodes: 251400 | Median Reward: 38.61 | Max Reward: 49.33
Iteration: 4107 | Episodes: 251500 | Median Reward: 36.06 | Max Reward: 49.33
Iteration: 4109 | Episodes: 251600 | Median Reward: 38.58 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -68      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4110     |
|    time_elapsed         | 74823    |
|    total_timesteps      | 25251840 |
| train/                  |          |
|    approx_kl            | 4.506751 |
|    clip_fraction        | 0.27     |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 211      |
|    n_updates            | 41090    |
|    policy_gradient_loss | -0.0187  |
|    std                  | 8.17     |
|    value_loss           | 315      |
--------------------------------------
Iteration: 4111 | Episodes: 251700 | Median Reward: 38.10 | Max Reward: 49.33
Iteration: 4112 | Episodes: 251800 | Median Reward: 36.72 | Max Reward: 49.33
Iteration: 4114 | Episodes: 251900 | Median Reward: 33.44 | Max Reward: 49.33
Iteration: 4115 | Episodes: 252000 | Median Reward: 35.65 | Max Reward: 49.33
Iteration: 4117 | Episodes: 252100 | Median Reward: 34.17 | Max Reward: 49.33
Iteration: 4119 | Episodes: 252200 | Median Reward: 26.90 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4120      |
|    time_elapsed         | 75016     |
|    total_timesteps      | 25313280  |
| train/                  |           |
|    approx_kl            | 2.2655544 |
|    clip_fraction        | 0.23      |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 59.9      |
|    n_updates            | 41190     |
|    policy_gradient_loss | -0.0474   |
|    std                  | 8.19      |
|    value_loss           | 212       |
---------------------------------------
Iteration: 4120 | Episodes: 252300 | Median Reward: 38.57 | Max Reward: 49.33
Iteration: 4122 | Episodes: 252400 | Median Reward: 27.90 | Max Reward: 49.33
Iteration: 4124 | Episodes: 252500 | Median Reward: 22.61 | Max Reward: 49.33
Iteration: 4125 | Episodes: 252600 | Median Reward: 15.91 | Max Reward: 49.33
Iteration: 4127 | Episodes: 252700 | Median Reward: 6.14 | Max Reward: 49.33
Iteration: 4129 | Episodes: 252800 | Median Reward: 39.80 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -64.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4130      |
|    time_elapsed         | 75200     |
|    total_timesteps      | 25374720  |
| train/                  |           |
|    approx_kl            | 1.6013405 |
|    clip_fraction        | 0.177     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 204       |
|    n_updates            | 41290     |
|    policy_gradient_loss | -0.0126   |
|    std                  | 8.2       |
|    value_loss           | 370       |
---------------------------------------
Iteration: 4130 | Episodes: 252900 | Median Reward: 40.76 | Max Reward: 49.33
Iteration: 4132 | Episodes: 253000 | Median Reward: 40.47 | Max Reward: 49.33
Iteration: 4134 | Episodes: 253100 | Median Reward: 36.97 | Max Reward: 49.33
Iteration: 4135 | Episodes: 253200 | Median Reward: 23.70 | Max Reward: 49.33
Iteration: 4137 | Episodes: 253300 | Median Reward: 27.73 | Max Reward: 49.33
Iteration: 4139 | Episodes: 253400 | Median Reward: 24.75 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -74.5    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4140     |
|    time_elapsed         | 75381    |
|    total_timesteps      | 25436160 |
| train/                  |          |
|    approx_kl            | 5.037441 |
|    clip_fraction        | 0.267    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 147      |
|    n_updates            | 41390    |
|    policy_gradient_loss | -0.0337  |
|    std                  | 8.22     |
|    value_loss           | 251      |
--------------------------------------
Iteration: 4140 | Episodes: 253500 | Median Reward: 38.29 | Max Reward: 49.33
Iteration: 4142 | Episodes: 253600 | Median Reward: 28.61 | Max Reward: 49.33
Iteration: 4143 | Episodes: 253700 | Median Reward: 34.40 | Max Reward: 49.33
Iteration: 4145 | Episodes: 253800 | Median Reward: 27.35 | Max Reward: 49.33
Iteration: 4147 | Episodes: 253900 | Median Reward: 34.43 | Max Reward: 49.33
Iteration: 4148 | Episodes: 254000 | Median Reward: 30.59 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4150      |
|    time_elapsed         | 75570     |
|    total_timesteps      | 25497600  |
| train/                  |           |
|    approx_kl            | 0.8727255 |
|    clip_fraction        | 0.145     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 121       |
|    n_updates            | 41490     |
|    policy_gradient_loss | -0.0248   |
|    std                  | 8.23      |
|    value_loss           | 326       |
---------------------------------------
Iteration: 4150 | Episodes: 254100 | Median Reward: 34.77 | Max Reward: 49.33
Iteration: 4152 | Episodes: 254200 | Median Reward: 34.02 | Max Reward: 49.33
Iteration: 4153 | Episodes: 254300 | Median Reward: 2.87 | Max Reward: 49.33
Iteration: 4155 | Episodes: 254400 | Median Reward: 2.15 | Max Reward: 49.33
Iteration: 4157 | Episodes: 254500 | Median Reward: 20.83 | Max Reward: 49.33
Iteration: 4158 | Episodes: 254600 | Median Reward: 28.94 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -79.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4160      |
|    time_elapsed         | 75755     |
|    total_timesteps      | 25559040  |
| train/                  |           |
|    approx_kl            | 10.044367 |
|    clip_fraction        | 0.468     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 163       |
|    n_updates            | 41590     |
|    policy_gradient_loss | -0.0427   |
|    std                  | 8.25      |
|    value_loss           | 252       |
---------------------------------------
Iteration: 4160 | Episodes: 254700 | Median Reward: 26.20 | Max Reward: 49.33
Iteration: 4162 | Episodes: 254800 | Median Reward: 30.36 | Max Reward: 49.33
Iteration: 4163 | Episodes: 254900 | Median Reward: 28.79 | Max Reward: 49.33
Iteration: 4165 | Episodes: 255000 | Median Reward: 33.12 | Max Reward: 49.33
Iteration: 4166 | Episodes: 255100 | Median Reward: -14.98 | Max Reward: 49.33
Iteration: 4168 | Episodes: 255200 | Median Reward: -11.46 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -109      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4170      |
|    time_elapsed         | 75945     |
|    total_timesteps      | 25620480  |
| train/                  |           |
|    approx_kl            | 0.349861  |
|    clip_fraction        | 0.0922    |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 88.1      |
|    n_updates            | 41690     |
|    policy_gradient_loss | -0.0317   |
|    std                  | 8.26      |
|    value_loss           | 156       |
---------------------------------------
Iteration: 4170 | Episodes: 255300 | Median Reward: -14.34 | Max Reward: 49.33
Iteration: 4171 | Episodes: 255400 | Median Reward: -7.57 | Max Reward: 49.33
Iteration: 4173 | Episodes: 255500 | Median Reward: -7.64 | Max Reward: 49.33
Iteration: 4175 | Episodes: 255600 | Median Reward: -7.64 | Max Reward: 49.33
Iteration: 4176 | Episodes: 255700 | Median Reward: 10.58 | Max Reward: 49.33
Iteration: 4178 | Episodes: 255800 | Median Reward: -9.84 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -108       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4180       |
|    time_elapsed         | 76125      |
|    total_timesteps      | 25681920   |
| train/                  |            |
|    approx_kl            | 0.94860536 |
|    clip_fraction        | 0.0776     |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 134        |
|    n_updates            | 41790      |
|    policy_gradient_loss | -0.00537   |
|    std                  | 8.27       |
|    value_loss           | 171        |
----------------------------------------
Iteration: 4180 | Episodes: 255900 | Median Reward: -10.22 | Max Reward: 49.33
Iteration: 4181 | Episodes: 256000 | Median Reward: -11.31 | Max Reward: 49.33
Iteration: 4183 | Episodes: 256100 | Median Reward: -21.57 | Max Reward: 49.33
Iteration: 4185 | Episodes: 256200 | Median Reward: -18.57 | Max Reward: 49.33
Iteration: 4186 | Episodes: 256300 | Median Reward: -10.41 | Max Reward: 49.33
Iteration: 4188 | Episodes: 256400 | Median Reward: -5.68 | Max Reward: 49.33
Iteration: 4189 | Episodes: 256500 | Median Reward: 9.72 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -86       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4190      |
|    time_elapsed         | 76308     |
|    total_timesteps      | 25743360  |
| train/                  |           |
|    approx_kl            | 2.0134447 |
|    clip_fraction        | 0.168     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 131       |
|    n_updates            | 41890     |
|    policy_gradient_loss | -0.0189   |
|    std                  | 8.28      |
|    value_loss           | 230       |
---------------------------------------
Iteration: 4191 | Episodes: 256600 | Median Reward: -3.29 | Max Reward: 49.33
Iteration: 4193 | Episodes: 256700 | Median Reward: 38.26 | Max Reward: 49.33
Iteration: 4194 | Episodes: 256800 | Median Reward: 20.08 | Max Reward: 49.33
Iteration: 4196 | Episodes: 256900 | Median Reward: 7.53 | Max Reward: 49.33
Iteration: 4198 | Episodes: 257000 | Median Reward: 29.59 | Max Reward: 49.33
Iteration: 4199 | Episodes: 257100 | Median Reward: 33.06 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -71.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4200      |
|    time_elapsed         | 76496     |
|    total_timesteps      | 25804800  |
| train/                  |           |
|    approx_kl            | 3.1588411 |
|    clip_fraction        | 0.278     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 172       |
|    n_updates            | 41990     |
|    policy_gradient_loss | -0.0846   |
|    std                  | 8.3       |
|    value_loss           | 317       |
---------------------------------------
Iteration: 4201 | Episodes: 257200 | Median Reward: 38.14 | Max Reward: 49.33
Iteration: 4203 | Episodes: 257300 | Median Reward: 39.73 | Max Reward: 49.33
Iteration: 4204 | Episodes: 257400 | Median Reward: 29.47 | Max Reward: 49.33
Iteration: 4206 | Episodes: 257500 | Median Reward: 34.04 | Max Reward: 49.33
Iteration: 4208 | Episodes: 257600 | Median Reward: 39.35 | Max Reward: 49.33
Iteration: 4209 | Episodes: 257700 | Median Reward: 38.32 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4210      |
|    time_elapsed         | 76675     |
|    total_timesteps      | 25866240  |
| train/                  |           |
|    approx_kl            | 3.293446  |
|    clip_fraction        | 0.237     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 57.3      |
|    n_updates            | 42090     |
|    policy_gradient_loss | -0.0315   |
|    std                  | 8.3       |
|    value_loss           | 257       |
---------------------------------------
Iteration: 4211 | Episodes: 257800 | Median Reward: 39.23 | Max Reward: 49.33
Iteration: 4212 | Episodes: 257900 | Median Reward: 39.73 | Max Reward: 49.33
Iteration: 4214 | Episodes: 258000 | Median Reward: 37.13 | Max Reward: 49.33
Iteration: 4216 | Episodes: 258100 | Median Reward: 38.23 | Max Reward: 49.33
Iteration: 4217 | Episodes: 258200 | Median Reward: 27.29 | Max Reward: 49.33
Iteration: 4219 | Episodes: 258300 | Median Reward: 24.38 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -87.1    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4220     |
|    time_elapsed         | 76855    |
|    total_timesteps      | 25927680 |
| train/                  |          |
|    approx_kl            | 4.808255 |
|    clip_fraction        | 0.314    |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 205      |
|    n_updates            | 42190    |
|    policy_gradient_loss | -0.0706  |
|    std                  | 8.32     |
|    value_loss           | 350      |
--------------------------------------
Iteration: 4221 | Episodes: 258400 | Median Reward: 32.27 | Max Reward: 49.33
Iteration: 4222 | Episodes: 258500 | Median Reward: 35.49 | Max Reward: 49.33
Iteration: 4224 | Episodes: 258600 | Median Reward: 38.50 | Max Reward: 49.33
Iteration: 4226 | Episodes: 258700 | Median Reward: 33.16 | Max Reward: 49.33
Iteration: 4227 | Episodes: 258800 | Median Reward: 37.57 | Max Reward: 49.33
Iteration: 4229 | Episodes: 258900 | Median Reward: 23.07 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -85.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4230      |
|    time_elapsed         | 77034     |
|    total_timesteps      | 25989120  |
| train/                  |           |
|    approx_kl            | 0.3851452 |
|    clip_fraction        | 0.166     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 66.3      |
|    n_updates            | 42290     |
|    policy_gradient_loss | -0.0328   |
|    std                  | 8.34      |
|    value_loss           | 234       |
---------------------------------------
Iteration: 4231 | Episodes: 259000 | Median Reward: 18.78 | Max Reward: 49.33
Iteration: 4232 | Episodes: 259100 | Median Reward: 5.95 | Max Reward: 49.33
Iteration: 4234 | Episodes: 259200 | Median Reward: 32.37 | Max Reward: 49.33
Iteration: 4235 | Episodes: 259300 | Median Reward: 40.95 | Max Reward: 49.33
Iteration: 4237 | Episodes: 259400 | Median Reward: 32.73 | Max Reward: 49.33
Iteration: 4239 | Episodes: 259500 | Median Reward: 41.46 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4240      |
|    time_elapsed         | 77216     |
|    total_timesteps      | 26050560  |
| train/                  |           |
|    approx_kl            | 1.4165522 |
|    clip_fraction        | 0.179     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 179       |
|    n_updates            | 42390     |
|    policy_gradient_loss | -0.0113   |
|    std                  | 8.35      |
|    value_loss           | 404       |
---------------------------------------
Iteration: 4240 | Episodes: 259600 | Median Reward: 38.16 | Max Reward: 49.33
Iteration: 4242 | Episodes: 259700 | Median Reward: 37.75 | Max Reward: 49.33
Iteration: 4244 | Episodes: 259800 | Median Reward: 35.13 | Max Reward: 49.33
Iteration: 4245 | Episodes: 259900 | Median Reward: 26.98 | Max Reward: 49.33
Iteration: 4247 | Episodes: 260000 | Median Reward: 26.20 | Max Reward: 49.33
Iteration: 4249 | Episodes: 260100 | Median Reward: 37.65 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -76.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4250      |
|    time_elapsed         | 77406     |
|    total_timesteps      | 26112000  |
| train/                  |           |
|    approx_kl            | 2.7787254 |
|    clip_fraction        | 0.153     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 207       |
|    n_updates            | 42490     |
|    policy_gradient_loss | 0.0346    |
|    std                  | 8.37      |
|    value_loss           | 400       |
---------------------------------------
Iteration: 4250 | Episodes: 260200 | Median Reward: 23.22 | Max Reward: 49.33
Iteration: 4252 | Episodes: 260300 | Median Reward: 36.87 | Max Reward: 49.33
Iteration: 4254 | Episodes: 260400 | Median Reward: 30.48 | Max Reward: 49.33
Iteration: 4255 | Episodes: 260500 | Median Reward: 25.29 | Max Reward: 49.33
Iteration: 4257 | Episodes: 260600 | Median Reward: 22.42 | Max Reward: 49.33
Iteration: 4259 | Episodes: 260700 | Median Reward: 10.89 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4260      |
|    time_elapsed         | 77583     |
|    total_timesteps      | 26173440  |
| train/                  |           |
|    approx_kl            | 1.4000461 |
|    clip_fraction        | 0.316     |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 158       |
|    n_updates            | 42590     |
|    policy_gradient_loss | -0.00597  |
|    std                  | 8.38      |
|    value_loss           | 227       |
---------------------------------------
Iteration: 4260 | Episodes: 260800 | Median Reward: 34.19 | Max Reward: 49.33
Iteration: 4262 | Episodes: 260900 | Median Reward: 37.86 | Max Reward: 49.33
Iteration: 4263 | Episodes: 261000 | Median Reward: 34.35 | Max Reward: 49.33
Iteration: 4265 | Episodes: 261100 | Median Reward: 35.24 | Max Reward: 49.33
Iteration: 4267 | Episodes: 261200 | Median Reward: 30.96 | Max Reward: 49.33
Iteration: 4268 | Episodes: 261300 | Median Reward: 34.19 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -79       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4270      |
|    time_elapsed         | 77762     |
|    total_timesteps      | 26234880  |
| train/                  |           |
|    approx_kl            | 1.2026563 |
|    clip_fraction        | 0.298     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 77.2      |
|    n_updates            | 42690     |
|    policy_gradient_loss | -0.0441   |
|    std                  | 8.41      |
|    value_loss           | 266       |
---------------------------------------
Iteration: 4270 | Episodes: 261400 | Median Reward: 27.43 | Max Reward: 49.33
Iteration: 4272 | Episodes: 261500 | Median Reward: 29.11 | Max Reward: 49.33
Iteration: 4273 | Episodes: 261600 | Median Reward: 24.92 | Max Reward: 49.33
Iteration: 4275 | Episodes: 261700 | Median Reward: 35.92 | Max Reward: 49.33
Iteration: 4277 | Episodes: 261800 | Median Reward: 39.52 | Max Reward: 49.33
Iteration: 4278 | Episodes: 261900 | Median Reward: 38.71 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -63.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4280      |
|    time_elapsed         | 77950     |
|    total_timesteps      | 26296320  |
| train/                  |           |
|    approx_kl            | 5.9672832 |
|    clip_fraction        | 0.317     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 5.96e-08  |
|    learning_rate        | 0.0001    |
|    loss                 | 154       |
|    n_updates            | 42790     |
|    policy_gradient_loss | -0.0214   |
|    std                  | 8.43      |
|    value_loss           | 345       |
---------------------------------------
Iteration: 4280 | Episodes: 262000 | Median Reward: 41.78 | Max Reward: 49.33
Iteration: 4282 | Episodes: 262100 | Median Reward: 38.31 | Max Reward: 49.33
Iteration: 4283 | Episodes: 262200 | Median Reward: 31.35 | Max Reward: 49.33
Iteration: 4285 | Episodes: 262300 | Median Reward: 31.35 | Max Reward: 49.33
Iteration: 4286 | Episodes: 262400 | Median Reward: 41.45 | Max Reward: 49.33
Iteration: 4288 | Episodes: 262500 | Median Reward: 6.74 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -81.9    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4290     |
|    time_elapsed         | 78130    |
|    total_timesteps      | 26357760 |
| train/                  |          |
|    approx_kl            | 1.337895 |
|    clip_fraction        | 0.307    |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 160      |
|    n_updates            | 42890    |
|    policy_gradient_loss | -0.0694  |
|    std                  | 8.45     |
|    value_loss           | 270      |
--------------------------------------
Iteration: 4290 | Episodes: 262600 | Median Reward: 29.70 | Max Reward: 49.33
Iteration: 4291 | Episodes: 262700 | Median Reward: 16.19 | Max Reward: 49.33
Iteration: 4293 | Episodes: 262800 | Median Reward: 36.84 | Max Reward: 49.33
Iteration: 4295 | Episodes: 262900 | Median Reward: 34.69 | Max Reward: 49.33
Iteration: 4296 | Episodes: 263000 | Median Reward: 26.92 | Max Reward: 49.33
Iteration: 4298 | Episodes: 263100 | Median Reward: 23.60 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4300      |
|    time_elapsed         | 78307     |
|    total_timesteps      | 26419200  |
| train/                  |           |
|    approx_kl            | 7.005597  |
|    clip_fraction        | 0.366     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 131       |
|    n_updates            | 42990     |
|    policy_gradient_loss | -0.0749   |
|    std                  | 8.46      |
|    value_loss           | 271       |
---------------------------------------
Iteration: 4300 | Episodes: 263200 | Median Reward: 37.43 | Max Reward: 49.33
Iteration: 4301 | Episodes: 263300 | Median Reward: 39.10 | Max Reward: 49.33
Iteration: 4303 | Episodes: 263400 | Median Reward: 41.90 | Max Reward: 49.33
Iteration: 4305 | Episodes: 263500 | Median Reward: 37.81 | Max Reward: 49.33
Iteration: 4306 | Episodes: 263600 | Median Reward: 34.97 | Max Reward: 49.33
Iteration: 4308 | Episodes: 263700 | Median Reward: 34.60 | Max Reward: 49.33
Iteration: 4309 | Episodes: 263800 | Median Reward: 26.40 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -71.5      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4310       |
|    time_elapsed         | 78493      |
|    total_timesteps      | 26480640   |
| train/                  |            |
|    approx_kl            | 0.21166208 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.4        |
|    entropy_loss         | -134       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 96.4       |
|    n_updates            | 43090      |
|    policy_gradient_loss | -0.0886    |
|    std                  | 8.48       |
|    value_loss           | 239        |
----------------------------------------
Iteration: 4311 | Episodes: 263900 | Median Reward: 31.40 | Max Reward: 49.33
Iteration: 4313 | Episodes: 264000 | Median Reward: 34.16 | Max Reward: 49.33
Iteration: 4314 | Episodes: 264100 | Median Reward: 34.51 | Max Reward: 49.33
Iteration: 4316 | Episodes: 264200 | Median Reward: 28.39 | Max Reward: 49.33
Iteration: 4318 | Episodes: 264300 | Median Reward: 37.87 | Max Reward: 49.33
Iteration: 4319 | Episodes: 264400 | Median Reward: 32.05 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -72.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4320      |
|    time_elapsed         | 78673     |
|    total_timesteps      | 26542080  |
| train/                  |           |
|    approx_kl            | 4.1141763 |
|    clip_fraction        | 0.312     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 85.7      |
|    n_updates            | 43190     |
|    policy_gradient_loss | -0.0574   |
|    std                  | 8.51      |
|    value_loss           | 294       |
---------------------------------------
Iteration: 4321 | Episodes: 264500 | Median Reward: 40.11 | Max Reward: 49.33
Iteration: 4323 | Episodes: 264600 | Median Reward: 37.93 | Max Reward: 49.33
Iteration: 4324 | Episodes: 264700 | Median Reward: 39.13 | Max Reward: 49.33
Iteration: 4326 | Episodes: 264800 | Median Reward: 41.79 | Max Reward: 49.33
Iteration: 4328 | Episodes: 264900 | Median Reward: 30.57 | Max Reward: 49.33
Iteration: 4329 | Episodes: 265000 | Median Reward: 41.35 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -62.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4330      |
|    time_elapsed         | 78852     |
|    total_timesteps      | 26603520  |
| train/                  |           |
|    approx_kl            | 1.9105873 |
|    clip_fraction        | 0.271     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 162       |
|    n_updates            | 43290     |
|    policy_gradient_loss | -0.0596   |
|    std                  | 8.53      |
|    value_loss           | 366       |
---------------------------------------
Iteration: 4331 | Episodes: 265100 | Median Reward: 38.75 | Max Reward: 49.33
Iteration: 4332 | Episodes: 265200 | Median Reward: 36.76 | Max Reward: 49.33
Iteration: 4334 | Episodes: 265300 | Median Reward: 23.26 | Max Reward: 49.33
Iteration: 4336 | Episodes: 265400 | Median Reward: 41.49 | Max Reward: 49.33
Iteration: 4337 | Episodes: 265500 | Median Reward: 27.97 | Max Reward: 49.33
Iteration: 4339 | Episodes: 265600 | Median Reward: 30.80 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -79.3    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4340     |
|    time_elapsed         | 79039    |
|    total_timesteps      | 26664960 |
| train/                  |          |
|    approx_kl            | 3.98575  |
|    clip_fraction        | 0.315    |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 180      |
|    n_updates            | 43390    |
|    policy_gradient_loss | -0.0313  |
|    std                  | 8.55     |
|    value_loss           | 317      |
--------------------------------------
Iteration: 4341 | Episodes: 265700 | Median Reward: 28.33 | Max Reward: 49.33
Iteration: 4342 | Episodes: 265800 | Median Reward: 38.23 | Max Reward: 49.33
Iteration: 4344 | Episodes: 265900 | Median Reward: 31.75 | Max Reward: 49.33
Iteration: 4346 | Episodes: 266000 | Median Reward: 32.73 | Max Reward: 49.33
Iteration: 4347 | Episodes: 266100 | Median Reward: 35.67 | Max Reward: 49.33
Iteration: 4349 | Episodes: 266200 | Median Reward: 27.94 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4350      |
|    time_elapsed         | 79217     |
|    total_timesteps      | 26726400  |
| train/                  |           |
|    approx_kl            | 1.4021302 |
|    clip_fraction        | 0.21      |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 117       |
|    n_updates            | 43490     |
|    policy_gradient_loss | -0.0428   |
|    std                  | 8.58      |
|    value_loss           | 252       |
---------------------------------------
Iteration: 4351 | Episodes: 266300 | Median Reward: 31.54 | Max Reward: 49.33
Iteration: 4352 | Episodes: 266400 | Median Reward: 26.14 | Max Reward: 49.33
Iteration: 4354 | Episodes: 266500 | Median Reward: 26.14 | Max Reward: 49.33
Iteration: 4355 | Episodes: 266600 | Median Reward: 26.08 | Max Reward: 49.33
Iteration: 4357 | Episodes: 266700 | Median Reward: 25.90 | Max Reward: 49.33
Iteration: 4359 | Episodes: 266800 | Median Reward: 32.18 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -85.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4360      |
|    time_elapsed         | 79396     |
|    total_timesteps      | 26787840  |
| train/                  |           |
|    approx_kl            | 1.7398555 |
|    clip_fraction        | 0.163     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 187       |
|    n_updates            | 43590     |
|    policy_gradient_loss | -0.00875  |
|    std                  | 8.59      |
|    value_loss           | 298       |
---------------------------------------
Iteration: 4360 | Episodes: 266900 | Median Reward: 9.96 | Max Reward: 49.33
Iteration: 4362 | Episodes: 267000 | Median Reward: 38.37 | Max Reward: 49.33
Iteration: 4364 | Episodes: 267100 | Median Reward: 41.89 | Max Reward: 49.33
Iteration: 4365 | Episodes: 267200 | Median Reward: 41.27 | Max Reward: 49.33
Iteration: 4367 | Episodes: 267300 | Median Reward: 36.56 | Max Reward: 49.33
Iteration: 4369 | Episodes: 267400 | Median Reward: 22.78 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -77      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4370     |
|    time_elapsed         | 79582    |
|    total_timesteps      | 26849280 |
| train/                  |          |
|    approx_kl            | 4.024907 |
|    clip_fraction        | 0.244    |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 126      |
|    n_updates            | 43690    |
|    policy_gradient_loss | -0.0274  |
|    std                  | 8.61     |
|    value_loss           | 289      |
--------------------------------------
Iteration: 4370 | Episodes: 267500 | Median Reward: 13.21 | Max Reward: 49.33
Iteration: 4372 | Episodes: 267600 | Median Reward: 41.47 | Max Reward: 49.33
Iteration: 4374 | Episodes: 267700 | Median Reward: 41.71 | Max Reward: 49.33
Iteration: 4375 | Episodes: 267800 | Median Reward: 29.97 | Max Reward: 49.33
Iteration: 4377 | Episodes: 267900 | Median Reward: 34.93 | Max Reward: 49.33
Iteration: 4379 | Episodes: 268000 | Median Reward: 38.73 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -74.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4380      |
|    time_elapsed         | 79763     |
|    total_timesteps      | 26910720  |
| train/                  |           |
|    approx_kl            | 5.1194744 |
|    clip_fraction        | 0.341     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 119       |
|    n_updates            | 43790     |
|    policy_gradient_loss | -0.077    |
|    std                  | 8.63      |
|    value_loss           | 331       |
---------------------------------------
Iteration: 4380 | Episodes: 268100 | Median Reward: 37.32 | Max Reward: 49.33
Iteration: 4382 | Episodes: 268200 | Median Reward: 38.38 | Max Reward: 49.33
Iteration: 4383 | Episodes: 268300 | Median Reward: 37.40 | Max Reward: 49.33
Iteration: 4385 | Episodes: 268400 | Median Reward: 37.10 | Max Reward: 49.33
Iteration: 4387 | Episodes: 268500 | Median Reward: 39.90 | Max Reward: 49.33
Iteration: 4388 | Episodes: 268600 | Median Reward: 33.23 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4390      |
|    time_elapsed         | 79942     |
|    total_timesteps      | 26972160  |
| train/                  |           |
|    approx_kl            | 1.0931323 |
|    clip_fraction        | 0.202     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 88.1      |
|    n_updates            | 43890     |
|    policy_gradient_loss | -0.00742  |
|    std                  | 8.65      |
|    value_loss           | 242       |
---------------------------------------
Iteration: 4390 | Episodes: 268700 | Median Reward: 37.89 | Max Reward: 49.33
Iteration: 4392 | Episodes: 268800 | Median Reward: 33.49 | Max Reward: 49.33
Iteration: 4393 | Episodes: 268900 | Median Reward: 34.48 | Max Reward: 49.33
Iteration: 4395 | Episodes: 269000 | Median Reward: 39.06 | Max Reward: 49.33
Iteration: 4396 | Episodes: 269100 | Median Reward: 40.37 | Max Reward: 49.33
Iteration: 4398 | Episodes: 269200 | Median Reward: 42.81 | Max Reward: 49.33
Iteration: 4399 | Episodes: 269300 | Median Reward: 36.72 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -65.7    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4400     |
|    time_elapsed         | 80130    |
|    total_timesteps      | 27033600 |
| train/                  |          |
|    approx_kl            | 6.653578 |
|    clip_fraction        | 0.327    |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 150      |
|    n_updates            | 43990    |
|    policy_gradient_loss | -0.0727  |
|    std                  | 8.67     |
|    value_loss           | 329      |
--------------------------------------
Iteration: 4401 | Episodes: 269400 | Median Reward: 39.63 | Max Reward: 49.33
Iteration: 4403 | Episodes: 269500 | Median Reward: 38.56 | Max Reward: 49.33
Iteration: 4404 | Episodes: 269600 | Median Reward: 36.27 | Max Reward: 49.33
Iteration: 4406 | Episodes: 269700 | Median Reward: 36.67 | Max Reward: 49.33
Iteration: 4408 | Episodes: 269800 | Median Reward: 35.35 | Max Reward: 49.33
Iteration: 4409 | Episodes: 269900 | Median Reward: 35.04 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4410      |
|    time_elapsed         | 80308     |
|    total_timesteps      | 27095040  |
| train/                  |           |
|    approx_kl            | 7.1299114 |
|    clip_fraction        | 0.383     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 159       |
|    n_updates            | 44090     |
|    policy_gradient_loss | -0.0538   |
|    std                  | 8.69      |
|    value_loss           | 299       |
---------------------------------------
Iteration: 4411 | Episodes: 270000 | Median Reward: 36.78 | Max Reward: 49.33
Iteration: 4412 | Episodes: 270100 | Median Reward: 38.82 | Max Reward: 49.33
Iteration: 4414 | Episodes: 270200 | Median Reward: 35.33 | Max Reward: 49.33
Iteration: 4415 | Episodes: 270300 | Median Reward: 32.21 | Max Reward: 49.33
Iteration: 4417 | Episodes: 270400 | Median Reward: 37.22 | Max Reward: 49.33
Iteration: 4419 | Episodes: 270500 | Median Reward: 42.37 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4420      |
|    time_elapsed         | 80498     |
|    total_timesteps      | 27156480  |
| train/                  |           |
|    approx_kl            | 7.731518  |
|    clip_fraction        | 0.517     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 171       |
|    n_updates            | 44190     |
|    policy_gradient_loss | -0.0399   |
|    std                  | 8.71      |
|    value_loss           | 354       |
---------------------------------------
Iteration: 4420 | Episodes: 270600 | Median Reward: 33.79 | Max Reward: 49.33
Iteration: 4422 | Episodes: 270700 | Median Reward: 38.67 | Max Reward: 49.33
Iteration: 4423 | Episodes: 270800 | Median Reward: 40.58 | Max Reward: 49.33
Iteration: 4425 | Episodes: 270900 | Median Reward: 39.14 | Max Reward: 49.33
Iteration: 4427 | Episodes: 271000 | Median Reward: 25.04 | Max Reward: 49.33
Iteration: 4428 | Episodes: 271100 | Median Reward: 39.49 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -64.1    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4430     |
|    time_elapsed         | 80676    |
|    total_timesteps      | 27217920 |
| train/                  |          |
|    approx_kl            | 6.464765 |
|    clip_fraction        | 0.336    |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 141      |
|    n_updates            | 44290    |
|    policy_gradient_loss | -0.069   |
|    std                  | 8.73     |
|    value_loss           | 345      |
--------------------------------------
Iteration: 4430 | Episodes: 271200 | Median Reward: 36.47 | Max Reward: 49.33
Iteration: 4432 | Episodes: 271300 | Median Reward: 38.16 | Max Reward: 49.33
Iteration: 4433 | Episodes: 271400 | Median Reward: 33.59 | Max Reward: 49.33
Iteration: 4435 | Episodes: 271500 | Median Reward: 33.86 | Max Reward: 49.33
Iteration: 4437 | Episodes: 271600 | Median Reward: 35.51 | Max Reward: 49.33
Iteration: 4438 | Episodes: 271700 | Median Reward: 38.38 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4440      |
|    time_elapsed         | 80859     |
|    total_timesteps      | 27279360  |
| train/                  |           |
|    approx_kl            | 6.0701914 |
|    clip_fraction        | 0.357     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 140       |
|    n_updates            | 44390     |
|    policy_gradient_loss | -0.0131   |
|    std                  | 8.76      |
|    value_loss           | 332       |
---------------------------------------
Iteration: 4440 | Episodes: 271800 | Median Reward: 37.79 | Max Reward: 49.33
Iteration: 4442 | Episodes: 271900 | Median Reward: 39.34 | Max Reward: 49.33
Iteration: 4443 | Episodes: 272000 | Median Reward: 22.58 | Max Reward: 49.33
Iteration: 4445 | Episodes: 272100 | Median Reward: 32.23 | Max Reward: 49.33
Iteration: 4446 | Episodes: 272200 | Median Reward: 32.09 | Max Reward: 49.33
Iteration: 4448 | Episodes: 272300 | Median Reward: 31.56 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -80.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4450      |
|    time_elapsed         | 81046     |
|    total_timesteps      | 27340800  |
| train/                  |           |
|    approx_kl            | 0.5069081 |
|    clip_fraction        | 0.148     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 67.6      |
|    n_updates            | 44490     |
|    policy_gradient_loss | -0.0256   |
|    std                  | 8.77      |
|    value_loss           | 209       |
---------------------------------------
Iteration: 4450 | Episodes: 272400 | Median Reward: 30.11 | Max Reward: 49.33
Iteration: 4451 | Episodes: 272500 | Median Reward: 28.32 | Max Reward: 49.33
Iteration: 4453 | Episodes: 272600 | Median Reward: 37.02 | Max Reward: 49.33
Iteration: 4455 | Episodes: 272700 | Median Reward: 39.23 | Max Reward: 49.33
Iteration: 4456 | Episodes: 272800 | Median Reward: 39.73 | Max Reward: 49.33
Iteration: 4458 | Episodes: 272900 | Median Reward: 30.71 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -79.5    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4460     |
|    time_elapsed         | 81225    |
|    total_timesteps      | 27402240 |
| train/                  |          |
|    approx_kl            | 4.565242 |
|    clip_fraction        | 0.192    |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 127      |
|    n_updates            | 44590    |
|    policy_gradient_loss | -0.0377  |
|    std                  | 8.8      |
|    value_loss           | 274      |
--------------------------------------
Iteration: 4460 | Episodes: 273000 | Median Reward: 20.76 | Max Reward: 49.33
Iteration: 4461 | Episodes: 273100 | Median Reward: 43.68 | Max Reward: 49.33
Iteration: 4463 | Episodes: 273200 | Median Reward: 32.26 | Max Reward: 49.33
Iteration: 4465 | Episodes: 273300 | Median Reward: 28.13 | Max Reward: 49.33
Iteration: 4466 | Episodes: 273400 | Median Reward: 35.99 | Max Reward: 49.33
Iteration: 4468 | Episodes: 273500 | Median Reward: 16.15 | Max Reward: 49.33
Iteration: 4469 | Episodes: 273600 | Median Reward: 25.58 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -77.9      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4470       |
|    time_elapsed         | 81400      |
|    total_timesteps      | 27463680   |
| train/                  |            |
|    approx_kl            | 0.13541235 |
|    clip_fraction        | 0.0492     |
|    clip_range           | 0.4        |
|    entropy_loss         | -134       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 104        |
|    n_updates            | 44690      |
|    policy_gradient_loss | -0.0281    |
|    std                  | 8.82       |
|    value_loss           | 197        |
----------------------------------------
Iteration: 4471 | Episodes: 273700 | Median Reward: 36.70 | Max Reward: 49.33
Iteration: 4473 | Episodes: 273800 | Median Reward: 40.70 | Max Reward: 49.33
Iteration: 4474 | Episodes: 273900 | Median Reward: 35.53 | Max Reward: 49.33
Iteration: 4476 | Episodes: 274000 | Median Reward: 38.58 | Max Reward: 49.33
Iteration: 4478 | Episodes: 274100 | Median Reward: 39.59 | Max Reward: 49.33
Iteration: 4479 | Episodes: 274200 | Median Reward: 36.08 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -72      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4480     |
|    time_elapsed         | 81585    |
|    total_timesteps      | 27525120 |
| train/                  |          |
|    approx_kl            | 7.339522 |
|    clip_fraction        | 0.337    |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 164      |
|    n_updates            | 44790    |
|    policy_gradient_loss | -0.0586  |
|    std                  | 8.84     |
|    value_loss           | 352      |
--------------------------------------
Iteration: 4481 | Episodes: 274300 | Median Reward: 39.38 | Max Reward: 49.33
Iteration: 4483 | Episodes: 274400 | Median Reward: 37.64 | Max Reward: 49.33
Iteration: 4484 | Episodes: 274500 | Median Reward: 38.54 | Max Reward: 49.33
Iteration: 4486 | Episodes: 274600 | Median Reward: 40.86 | Max Reward: 49.33
Iteration: 4488 | Episodes: 274700 | Median Reward: 38.30 | Max Reward: 49.33
Iteration: 4489 | Episodes: 274800 | Median Reward: 39.75 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -63.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4490      |
|    time_elapsed         | 81765     |
|    total_timesteps      | 27586560  |
| train/                  |           |
|    approx_kl            | 1.4434851 |
|    clip_fraction        | 0.169     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 189       |
|    n_updates            | 44890     |
|    policy_gradient_loss | -0.0316   |
|    std                  | 8.85      |
|    value_loss           | 388       |
---------------------------------------
Iteration: 4491 | Episodes: 274900 | Median Reward: 26.54 | Max Reward: 49.33
Iteration: 4493 | Episodes: 275000 | Median Reward: 40.52 | Max Reward: 49.33
Iteration: 4494 | Episodes: 275100 | Median Reward: 37.18 | Max Reward: 49.33
Iteration: 4496 | Episodes: 275200 | Median Reward: 38.85 | Max Reward: 49.33
Iteration: 4497 | Episodes: 275300 | Median Reward: 41.82 | Max Reward: 49.33
Iteration: 4499 | Episodes: 275400 | Median Reward: 40.84 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -67.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4500      |
|    time_elapsed         | 81943     |
|    total_timesteps      | 27648000  |
| train/                  |           |
|    approx_kl            | 3.641764  |
|    clip_fraction        | 0.263     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 136       |
|    n_updates            | 44990     |
|    policy_gradient_loss | -0.0414   |
|    std                  | 8.86      |
|    value_loss           | 339       |
---------------------------------------
Iteration: 4501 | Episodes: 275500 | Median Reward: 35.64 | Max Reward: 49.33
Iteration: 4502 | Episodes: 275600 | Median Reward: 39.78 | Max Reward: 49.33
Iteration: 4504 | Episodes: 275700 | Median Reward: 33.86 | Max Reward: 49.33
Iteration: 4506 | Episodes: 275800 | Median Reward: 12.60 | Max Reward: 49.33
Iteration: 4507 | Episodes: 275900 | Median Reward: 33.99 | Max Reward: 49.33
Iteration: 4509 | Episodes: 276000 | Median Reward: 30.67 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -74.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4510      |
|    time_elapsed         | 82129     |
|    total_timesteps      | 27709440  |
| train/                  |           |
|    approx_kl            | 1.0586793 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 124       |
|    n_updates            | 45090     |
|    policy_gradient_loss | -0.018    |
|    std                  | 8.87      |
|    value_loss           | 272       |
---------------------------------------
Iteration: 4511 | Episodes: 276100 | Median Reward: 5.22 | Max Reward: 49.33
Iteration: 4512 | Episodes: 276200 | Median Reward: 33.89 | Max Reward: 49.33
Iteration: 4514 | Episodes: 276300 | Median Reward: 39.93 | Max Reward: 49.33
Iteration: 4516 | Episodes: 276400 | Median Reward: 37.20 | Max Reward: 49.33
Iteration: 4517 | Episodes: 276500 | Median Reward: 40.31 | Max Reward: 49.33
Iteration: 4519 | Episodes: 276600 | Median Reward: 40.00 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -59.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4520      |
|    time_elapsed         | 82312     |
|    total_timesteps      | 27770880  |
| train/                  |           |
|    approx_kl            | 3.1587572 |
|    clip_fraction        | 0.314     |
|    clip_range           | 0.4       |
|    entropy_loss         | -133      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 181       |
|    n_updates            | 45190     |
|    policy_gradient_loss | -0.0403   |
|    std                  | 8.88      |
|    value_loss           | 384       |
---------------------------------------
Iteration: 4520 | Episodes: 276700 | Median Reward: 42.62 | Max Reward: 49.33
Iteration: 4522 | Episodes: 276800 | Median Reward: 38.60 | Max Reward: 49.33
Iteration: 4524 | Episodes: 276900 | Median Reward: 37.63 | Max Reward: 49.33
Iteration: 4525 | Episodes: 277000 | Median Reward: 38.14 | Max Reward: 49.33
Iteration: 4527 | Episodes: 277100 | Median Reward: 39.37 | Max Reward: 49.33
Iteration: 4529 | Episodes: 277200 | Median Reward: 38.44 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -64.4    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4530     |
|    time_elapsed         | 82490    |
|    total_timesteps      | 27832320 |
| train/                  |          |
|    approx_kl            | 8.63756  |
|    clip_fraction        | 0.51     |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 182      |
|    n_updates            | 45290    |
|    policy_gradient_loss | -0.11    |
|    std                  | 8.91     |
|    value_loss           | 346      |
--------------------------------------
Iteration: 4530 | Episodes: 277300 | Median Reward: 37.32 | Max Reward: 49.33
Iteration: 4532 | Episodes: 277400 | Median Reward: 40.14 | Max Reward: 49.33
Iteration: 4534 | Episodes: 277500 | Median Reward: 40.11 | Max Reward: 49.33
Iteration: 4535 | Episodes: 277600 | Median Reward: 42.45 | Max Reward: 49.33
Iteration: 4537 | Episodes: 277700 | Median Reward: 29.63 | Max Reward: 49.33
Iteration: 4539 | Episodes: 277800 | Median Reward: 36.47 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -64.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4540      |
|    time_elapsed         | 82678     |
|    total_timesteps      | 27893760  |
| train/                  |           |
|    approx_kl            | 2.2260842 |
|    clip_fraction        | 0.256     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 163       |
|    n_updates            | 45390     |
|    policy_gradient_loss | -0.0356   |
|    std                  | 8.94      |
|    value_loss           | 349       |
---------------------------------------
Iteration: 4540 | Episodes: 277900 | Median Reward: 36.81 | Max Reward: 49.33
Iteration: 4542 | Episodes: 278000 | Median Reward: 32.02 | Max Reward: 49.33
Iteration: 4543 | Episodes: 278100 | Median Reward: 38.20 | Max Reward: 49.33
Iteration: 4545 | Episodes: 278200 | Median Reward: 36.69 | Max Reward: 49.33
Iteration: 4547 | Episodes: 278300 | Median Reward: 35.18 | Max Reward: 49.33
Iteration: 4548 | Episodes: 278400 | Median Reward: 39.89 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -60       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4550      |
|    time_elapsed         | 82857     |
|    total_timesteps      | 27955200  |
| train/                  |           |
|    approx_kl            | 1.0606277 |
|    clip_fraction        | 0.085     |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 178       |
|    n_updates            | 45490     |
|    policy_gradient_loss | -0.0168   |
|    std                  | 8.97      |
|    value_loss           | 380       |
---------------------------------------
Iteration: 4550 | Episodes: 278500 | Median Reward: 39.09 | Max Reward: 49.33
Iteration: 4552 | Episodes: 278600 | Median Reward: 41.67 | Max Reward: 49.33
Iteration: 4553 | Episodes: 278700 | Median Reward: 40.15 | Max Reward: 49.33
Iteration: 4555 | Episodes: 278800 | Median Reward: 39.99 | Max Reward: 49.33
Iteration: 4557 | Episodes: 278900 | Median Reward: 32.60 | Max Reward: 49.33
Iteration: 4558 | Episodes: 279000 | Median Reward: 31.31 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4560      |
|    time_elapsed         | 83035     |
|    total_timesteps      | 28016640  |
| train/                  |           |
|    approx_kl            | 1.2425435 |
|    clip_fraction        | 0.142     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 117       |
|    n_updates            | 45590     |
|    policy_gradient_loss | -0.00964  |
|    std                  | 8.99      |
|    value_loss           | 266       |
---------------------------------------
Iteration: 4560 | Episodes: 279100 | Median Reward: 38.29 | Max Reward: 49.33
Iteration: 4562 | Episodes: 279200 | Median Reward: 38.32 | Max Reward: 49.33
Iteration: 4563 | Episodes: 279300 | Median Reward: 38.93 | Max Reward: 49.33
Iteration: 4565 | Episodes: 279400 | Median Reward: 34.94 | Max Reward: 49.33
Iteration: 4566 | Episodes: 279500 | Median Reward: 39.69 | Max Reward: 49.33
Iteration: 4568 | Episodes: 279600 | Median Reward: 34.67 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -62.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4570      |
|    time_elapsed         | 83226     |
|    total_timesteps      | 28078080  |
| train/                  |           |
|    approx_kl            | 5.7946825 |
|    clip_fraction        | 0.221     |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 130       |
|    n_updates            | 45690     |
|    policy_gradient_loss | -0.0423   |
|    std                  | 9.02      |
|    value_loss           | 321       |
---------------------------------------
Iteration: 4570 | Episodes: 279700 | Median Reward: 37.77 | Max Reward: 49.33
Iteration: 4571 | Episodes: 279800 | Median Reward: 41.32 | Max Reward: 49.33
Iteration: 4573 | Episodes: 279900 | Median Reward: 33.73 | Max Reward: 49.33
Iteration: 4575 | Episodes: 280000 | Median Reward: 35.38 | Max Reward: 49.33
Iteration: 4576 | Episodes: 280100 | Median Reward: 40.77 | Max Reward: 49.33
Iteration: 4578 | Episodes: 280200 | Median Reward: 31.95 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -67.4    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4580     |
|    time_elapsed         | 83404    |
|    total_timesteps      | 28139520 |
| train/                  |          |
|    approx_kl            | 6.091756 |
|    clip_fraction        | 0.23     |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 158      |
|    n_updates            | 45790    |
|    policy_gradient_loss | -0.0423  |
|    std                  | 9.05     |
|    value_loss           | 275      |
--------------------------------------
Iteration: 4580 | Episodes: 280300 | Median Reward: 37.65 | Max Reward: 49.33
Iteration: 4581 | Episodes: 280400 | Median Reward: 40.13 | Max Reward: 49.33
Iteration: 4583 | Episodes: 280500 | Median Reward: 40.45 | Max Reward: 49.33
Iteration: 4585 | Episodes: 280600 | Median Reward: 37.85 | Max Reward: 49.33
Iteration: 4586 | Episodes: 280700 | Median Reward: 35.60 | Max Reward: 49.33
Iteration: 4588 | Episodes: 280800 | Median Reward: 40.60 | Max Reward: 49.33
Iteration: 4589 | Episodes: 280900 | Median Reward: 39.32 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -60.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4590      |
|    time_elapsed         | 83594     |
|    total_timesteps      | 28200960  |
| train/                  |           |
|    approx_kl            | 6.9636064 |
|    clip_fraction        | 0.45      |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 176       |
|    n_updates            | 45890     |
|    policy_gradient_loss | -0.102    |
|    std                  | 9.08      |
|    value_loss           | 375       |
---------------------------------------
Iteration: 4591 | Episodes: 281000 | Median Reward: 41.18 | Max Reward: 49.33
Iteration: 4593 | Episodes: 281100 | Median Reward: 41.86 | Max Reward: 49.33
Iteration: 4594 | Episodes: 281200 | Median Reward: 38.13 | Max Reward: 49.33
Iteration: 4596 | Episodes: 281300 | Median Reward: 36.05 | Max Reward: 49.33
Iteration: 4598 | Episodes: 281400 | Median Reward: 36.96 | Max Reward: 49.33
Iteration: 4599 | Episodes: 281500 | Median Reward: 38.28 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4600      |
|    time_elapsed         | 83771     |
|    total_timesteps      | 28262400  |
| train/                  |           |
|    approx_kl            | 9.021683  |
|    clip_fraction        | 0.401     |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 183       |
|    n_updates            | 45990     |
|    policy_gradient_loss | -0.0917   |
|    std                  | 9.12      |
|    value_loss           | 325       |
---------------------------------------
Iteration: 4601 | Episodes: 281600 | Median Reward: 42.34 | Max Reward: 49.33
Iteration: 4603 | Episodes: 281700 | Median Reward: 37.68 | Max Reward: 49.33
Iteration: 4604 | Episodes: 281800 | Median Reward: 37.32 | Max Reward: 49.33
Iteration: 4606 | Episodes: 281900 | Median Reward: 35.82 | Max Reward: 49.33
Iteration: 4608 | Episodes: 282000 | Median Reward: 39.65 | Max Reward: 49.33
Iteration: 4609 | Episodes: 282100 | Median Reward: 40.12 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -62.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4610      |
|    time_elapsed         | 83951     |
|    total_timesteps      | 28323840  |
| train/                  |           |
|    approx_kl            | 5.1635942 |
|    clip_fraction        | 0.345     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 171       |
|    n_updates            | 46090     |
|    policy_gradient_loss | 0.0243    |
|    std                  | 9.16      |
|    value_loss           | 383       |
---------------------------------------
Iteration: 4611 | Episodes: 282200 | Median Reward: 37.50 | Max Reward: 49.33
Iteration: 4612 | Episodes: 282300 | Median Reward: 32.83 | Max Reward: 49.33
Iteration: 4614 | Episodes: 282400 | Median Reward: 41.12 | Max Reward: 49.33
Iteration: 4616 | Episodes: 282500 | Median Reward: 37.88 | Max Reward: 49.33
Iteration: 4617 | Episodes: 282600 | Median Reward: 39.23 | Max Reward: 49.33
Iteration: 4619 | Episodes: 282700 | Median Reward: 37.57 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4620      |
|    time_elapsed         | 84137     |
|    total_timesteps      | 28385280  |
| train/                  |           |
|    approx_kl            | 2.5720522 |
|    clip_fraction        | 0.237     |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.0001    |
|    loss                 | 158       |
|    n_updates            | 46190     |
|    policy_gradient_loss | -0.0241   |
|    std                  | 9.18      |
|    value_loss           | 332       |
---------------------------------------
Iteration: 4621 | Episodes: 282800 | Median Reward: 38.76 | Max Reward: 49.33
Iteration: 4622 | Episodes: 282900 | Median Reward: 31.87 | Max Reward: 49.33
Iteration: 4624 | Episodes: 283000 | Median Reward: 36.82 | Max Reward: 49.33
Iteration: 4626 | Episodes: 283100 | Median Reward: 38.10 | Max Reward: 49.33
Iteration: 4627 | Episodes: 283200 | Median Reward: 42.19 | Max Reward: 49.33
Iteration: 4629 | Episodes: 283300 | Median Reward: 39.92 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4630      |
|    time_elapsed         | 84315     |
|    total_timesteps      | 28446720  |
| train/                  |           |
|    approx_kl            | 1.4282109 |
|    clip_fraction        | 0.158     |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 122       |
|    n_updates            | 46290     |
|    policy_gradient_loss | -0.0288   |
|    std                  | 9.21      |
|    value_loss           | 301       |
---------------------------------------
Iteration: 4631 | Episodes: 283400 | Median Reward: 42.05 | Max Reward: 49.33
Iteration: 4632 | Episodes: 283500 | Median Reward: 39.52 | Max Reward: 49.33
Iteration: 4634 | Episodes: 283600 | Median Reward: 37.49 | Max Reward: 49.33
Iteration: 4636 | Episodes: 283700 | Median Reward: 40.60 | Max Reward: 49.33
Iteration: 4637 | Episodes: 283800 | Median Reward: 39.24 | Max Reward: 49.33
Iteration: 4639 | Episodes: 283900 | Median Reward: 35.17 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4640      |
|    time_elapsed         | 84493     |
|    total_timesteps      | 28508160  |
| train/                  |           |
|    approx_kl            | 3.9410079 |
|    clip_fraction        | 0.281     |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 148       |
|    n_updates            | 46390     |
|    policy_gradient_loss | -0.0453   |
|    std                  | 9.23      |
|    value_loss           | 292       |
---------------------------------------
Iteration: 4640 | Episodes: 284000 | Median Reward: 37.90 | Max Reward: 49.33
Iteration: 4642 | Episodes: 284100 | Median Reward: 28.28 | Max Reward: 49.33
Iteration: 4644 | Episodes: 284200 | Median Reward: 38.11 | Max Reward: 49.33
Iteration: 4645 | Episodes: 284300 | Median Reward: 40.77 | Max Reward: 49.33
Iteration: 4647 | Episodes: 284400 | Median Reward: 40.89 | Max Reward: 49.33
Iteration: 4649 | Episodes: 284500 | Median Reward: 40.89 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -64.5      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4650       |
|    time_elapsed         | 84679      |
|    total_timesteps      | 28569600   |
| train/                  |            |
|    approx_kl            | 0.48696944 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.4        |
|    entropy_loss         | -135       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 103        |
|    n_updates            | 46490      |
|    policy_gradient_loss | -0.0224    |
|    std                  | 9.26       |
|    value_loss           | 283        |
----------------------------------------
Iteration: 4650 | Episodes: 284600 | Median Reward: 36.31 | Max Reward: 49.33
Iteration: 4652 | Episodes: 284700 | Median Reward: 37.72 | Max Reward: 49.33
Iteration: 4654 | Episodes: 284800 | Median Reward: 39.53 | Max Reward: 49.33
Iteration: 4655 | Episodes: 284900 | Median Reward: 42.37 | Max Reward: 49.33
Iteration: 4657 | Episodes: 285000 | Median Reward: 32.00 | Max Reward: 49.33
Iteration: 4659 | Episodes: 285100 | Median Reward: 34.60 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -62.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4660      |
|    time_elapsed         | 84857     |
|    total_timesteps      | 28631040  |
| train/                  |           |
|    approx_kl            | 1.3996849 |
|    clip_fraction        | 0.138     |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 152       |
|    n_updates            | 46590     |
|    policy_gradient_loss | -0.0491   |
|    std                  | 9.28      |
|    value_loss           | 305       |
---------------------------------------
Iteration: 4660 | Episodes: 285200 | Median Reward: 39.68 | Max Reward: 49.33
Iteration: 4662 | Episodes: 285300 | Median Reward: 39.76 | Max Reward: 49.33
Iteration: 4663 | Episodes: 285400 | Median Reward: 41.43 | Max Reward: 49.33
Iteration: 4665 | Episodes: 285500 | Median Reward: 37.40 | Max Reward: 49.33
Iteration: 4666 | Episodes: 285600 | Median Reward: 32.57 | Max Reward: 49.33
Iteration: 4668 | Episodes: 285700 | Median Reward: 38.79 | Max Reward: 49.33
Iteration: 4669 | Episodes: 285800 | Median Reward: 41.20 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 53.4     |
|    ep_rew_mean          | -8.55    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4670     |
|    time_elapsed         | 85036    |
|    total_timesteps      | 28692480 |
| train/                  |          |
|    approx_kl            | 8.330423 |
|    clip_fraction        | 0.319    |
|    clip_range           | 0.4      |
|    entropy_loss         | -136     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 159      |
|    n_updates            | 46690    |
|    policy_gradient_loss | -0.0239  |
|    std                  | 9.31     |
|    value_loss           | 343      |
--------------------------------------
Iteration: 4670 | Episodes: 285900 | Median Reward: 38.90 | Max Reward: 49.33
Iteration: 4672 | Episodes: 286000 | Median Reward: 35.97 | Max Reward: 49.33
Iteration: 4674 | Episodes: 286100 | Median Reward: 36.46 | Max Reward: 49.33
Iteration: 4675 | Episodes: 286200 | Median Reward: 39.94 | Max Reward: 49.33
Iteration: 4676 | Episodes: 286300 | Median Reward: 45.21 | Max Reward: 49.33
Iteration: 4678 | Episodes: 286400 | Median Reward: 35.80 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4680      |
|    time_elapsed         | 85224     |
|    total_timesteps      | 28753920  |
| train/                  |           |
|    approx_kl            | 5.7209363 |
|    clip_fraction        | 0.256     |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 140       |
|    n_updates            | 46790     |
|    policy_gradient_loss | -0.0106   |
|    std                  | 9.34      |
|    value_loss           | 282       |
---------------------------------------
Iteration: 4680 | Episodes: 286500 | Median Reward: 38.72 | Max Reward: 49.33
Iteration: 4681 | Episodes: 286600 | Median Reward: 41.53 | Max Reward: 49.33
Iteration: 4683 | Episodes: 286700 | Median Reward: 36.49 | Max Reward: 49.33
Iteration: 4684 | Episodes: 286800 | Median Reward: 35.75 | Max Reward: 49.33
Iteration: 4686 | Episodes: 286900 | Median Reward: 38.07 | Max Reward: 49.33
Iteration: 4688 | Episodes: 287000 | Median Reward: 38.78 | Max Reward: 49.33
Iteration: 4689 | Episodes: 287100 | Median Reward: 31.96 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -76.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4690      |
|    time_elapsed         | 85407     |
|    total_timesteps      | 28815360  |
| train/                  |           |
|    approx_kl            | 7.4288397 |
|    clip_fraction        | 0.31      |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 196       |
|    n_updates            | 46890     |
|    policy_gradient_loss | -0.0786   |
|    std                  | 9.36      |
|    value_loss           | 351       |
---------------------------------------
Iteration: 4691 | Episodes: 287200 | Median Reward: 34.84 | Max Reward: 49.33
Iteration: 4693 | Episodes: 287300 | Median Reward: 38.05 | Max Reward: 49.33
Iteration: 4694 | Episodes: 287400 | Median Reward: 38.56 | Max Reward: 49.33
Iteration: 4696 | Episodes: 287500 | Median Reward: 38.20 | Max Reward: 49.33
Iteration: 4698 | Episodes: 287600 | Median Reward: 40.57 | Max Reward: 49.33
Iteration: 4699 | Episodes: 287700 | Median Reward: 38.97 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -62.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4700      |
|    time_elapsed         | 85589     |
|    total_timesteps      | 28876800  |
| train/                  |           |
|    approx_kl            | 4.3255877 |
|    clip_fraction        | 0.279     |
|    clip_range           | 0.4       |
|    entropy_loss         | -136      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 186       |
|    n_updates            | 46990     |
|    policy_gradient_loss | -0.0146   |
|    std                  | 9.39      |
|    value_loss           | 361       |
---------------------------------------
Iteration: 4701 | Episodes: 287800 | Median Reward: 41.36 | Max Reward: 49.33
Iteration: 4703 | Episodes: 287900 | Median Reward: 46.59 | Max Reward: 49.33
Iteration: 4704 | Episodes: 288000 | Median Reward: 46.73 | Max Reward: 49.33
Iteration: 4706 | Episodes: 288100 | Median Reward: 46.17 | Max Reward: 49.33
Iteration: 4707 | Episodes: 288200 | Median Reward: 46.73 | Max Reward: 49.33
Iteration: 4709 | Episodes: 288300 | Median Reward: 47.38 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -53       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4710      |
|    time_elapsed         | 85777     |
|    total_timesteps      | 28938240  |
| train/                  |           |
|    approx_kl            | 1.1276462 |
|    clip_fraction        | 0.122     |
|    clip_range           | 0.4       |
|    entropy_loss         | -137      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 204       |
|    n_updates            | 47090     |
|    policy_gradient_loss | 0.0589    |
|    std                  | 9.41      |
|    value_loss           | 423       |
---------------------------------------
Iteration: 4711 | Episodes: 288400 | Median Reward: 47.08 | Max Reward: 49.33
Iteration: 4712 | Episodes: 288500 | Median Reward: 46.13 | Max Reward: 49.33
Iteration: 4714 | Episodes: 288600 | Median Reward: 46.64 | Max Reward: 49.33
Iteration: 4716 | Episodes: 288700 | Median Reward: 46.92 | Max Reward: 49.33
Iteration: 4717 | Episodes: 288800 | Median Reward: 49.04 | Max Reward: 49.33
Iteration: 4718 | Episodes: 288900 | Median Reward: 46.13 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -53.8      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4720       |
|    time_elapsed         | 85959      |
|    total_timesteps      | 28999680   |
| train/                  |            |
|    approx_kl            | 0.04632657 |
|    clip_fraction        | 0.0185     |
|    clip_range           | 0.4        |
|    entropy_loss         | -137       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 199        |
|    n_updates            | 47190      |
|    policy_gradient_loss | 0.0001     |
|    std                  | 9.43       |
|    value_loss           | 407        |
----------------------------------------
Iteration: 4720 | Episodes: 289000 | Median Reward: 46.26 | Max Reward: 49.33
Iteration: 4721 | Episodes: 289100 | Median Reward: 46.67 | Max Reward: 49.33
Iteration: 4723 | Episodes: 289200 | Median Reward: 47.70 | Max Reward: 49.33
Iteration: 4725 | Episodes: 289300 | Median Reward: 47.63 | Max Reward: 49.33
Iteration: 4726 | Episodes: 289400 | Median Reward: 46.89 | Max Reward: 49.33
Iteration: 4728 | Episodes: 289500 | Median Reward: 47.03 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -53.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4730      |
|    time_elapsed         | 86137     |
|    total_timesteps      | 29061120  |
| train/                  |           |
|    approx_kl            | 1.5869858 |
|    clip_fraction        | 0.177     |
|    clip_range           | 0.4       |
|    entropy_loss         | -137      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 188       |
|    n_updates            | 47290     |
|    policy_gradient_loss | 0.0627    |
|    std                  | 9.45      |
|    value_loss           | 392       |
---------------------------------------
Iteration: 4730 | Episodes: 289600 | Median Reward: 46.75 | Max Reward: 49.33
Iteration: 4731 | Episodes: 289700 | Median Reward: 46.18 | Max Reward: 49.33
Iteration: 4732 | Episodes: 289800 | Median Reward: 46.58 | Max Reward: 49.33
Iteration: 4734 | Episodes: 289900 | Median Reward: 47.08 | Max Reward: 49.33
Iteration: 4736 | Episodes: 290000 | Median Reward: 47.34 | Max Reward: 49.33
Iteration: 4737 | Episodes: 290100 | Median Reward: 47.14 | Max Reward: 49.33
Iteration: 4739 | Episodes: 290200 | Median Reward: 47.04 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -53.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4740      |
|    time_elapsed         | 86331     |
|    total_timesteps      | 29122560  |
| train/                  |           |
|    approx_kl            | 12.345036 |
|    clip_fraction        | 0.286     |
|    clip_range           | 0.4       |
|    entropy_loss         | -137      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 186       |
|    n_updates            | 47390     |
|    policy_gradient_loss | 0.139     |
|    std                  | 9.49      |
|    value_loss           | 391       |
---------------------------------------
Iteration: 4740 | Episodes: 290300 | Median Reward: 47.40 | Max Reward: 49.33
Iteration: 4742 | Episodes: 290400 | Median Reward: 47.61 | Max Reward: 49.33
Iteration: 4743 | Episodes: 290500 | Median Reward: 47.47 | Max Reward: 49.33
Iteration: 4745 | Episodes: 290600 | Median Reward: 46.97 | Max Reward: 49.33
Iteration: 4747 | Episodes: 290700 | Median Reward: 47.00 | Max Reward: 49.33
Iteration: 4748 | Episodes: 290800 | Median Reward: 47.52 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -52.9      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4750       |
|    time_elapsed         | 86516      |
|    total_timesteps      | 29184000   |
| train/                  |            |
|    approx_kl            | 0.41225445 |
|    clip_fraction        | 0.0658     |
|    clip_range           | 0.4        |
|    entropy_loss         | -137       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 180        |
|    n_updates            | 47490      |
|    policy_gradient_loss | 0.0328     |
|    std                  | 9.58       |
|    value_loss           | 384        |
----------------------------------------
Iteration: 4750 | Episodes: 290900 | Median Reward: 47.31 | Max Reward: 49.33
Iteration: 4752 | Episodes: 291000 | Median Reward: 47.30 | Max Reward: 49.33
Iteration: 4753 | Episodes: 291100 | Median Reward: 47.45 | Max Reward: 49.33
Iteration: 4755 | Episodes: 291200 | Median Reward: 47.48 | Max Reward: 49.33
Iteration: 4756 | Episodes: 291300 | Median Reward: 47.43 | Max Reward: 49.33
Iteration: 4758 | Episodes: 291400 | Median Reward: 46.72 | Max Reward: 49.33
Iteration: 4759 | Episodes: 291500 | Median Reward: 46.75 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -53.5    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4760     |
|    time_elapsed         | 86698    |
|    total_timesteps      | 29245440 |
| train/                  |          |
|    approx_kl            | 7.288756 |
|    clip_fraction        | 0.282    |
|    clip_range           | 0.4      |
|    entropy_loss         | -137     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 181      |
|    n_updates            | 47590    |
|    policy_gradient_loss | 0.141    |
|    std                  | 9.69     |
|    value_loss           | 373      |
--------------------------------------
Iteration: 4761 | Episodes: 291600 | Median Reward: 47.29 | Max Reward: 49.33
Iteration: 4763 | Episodes: 291700 | Median Reward: 47.12 | Max Reward: 49.33
Iteration: 4764 | Episodes: 291800 | Median Reward: 47.33 | Max Reward: 49.33
Iteration: 4766 | Episodes: 291900 | Median Reward: 47.08 | Max Reward: 49.33
Iteration: 4768 | Episodes: 292000 | Median Reward: 47.06 | Max Reward: 49.33
Iteration: 4769 | Episodes: 292100 | Median Reward: 47.64 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -52.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4770      |
|    time_elapsed         | 86895     |
|    total_timesteps      | 29306880  |
| train/                  |           |
|    approx_kl            | 1.6287103 |
|    clip_fraction        | 0.0862    |
|    clip_range           | 0.4       |
|    entropy_loss         | -137      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 180       |
|    n_updates            | 47690     |
|    policy_gradient_loss | 0.0235    |
|    std                  | 9.83      |
|    value_loss           | 377       |
---------------------------------------
Iteration: 4771 | Episodes: 292200 | Median Reward: 47.80 | Max Reward: 49.33
Iteration: 4773 | Episodes: 292300 | Median Reward: 47.43 | Max Reward: 49.33
Iteration: 4774 | Episodes: 292400 | Median Reward: 47.50 | Max Reward: 49.33
Iteration: 4776 | Episodes: 292500 | Median Reward: 47.45 | Max Reward: 49.33
Iteration: 4777 | Episodes: 292600 | Median Reward: 47.46 | Max Reward: 49.33
Iteration: 4779 | Episodes: 292700 | Median Reward: 47.10 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -53.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4780      |
|    time_elapsed         | 87080     |
|    total_timesteps      | 29368320  |
| train/                  |           |
|    approx_kl            | 1.5725235 |
|    clip_fraction        | 0.119     |
|    clip_range           | 0.4       |
|    entropy_loss         | -138      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 181       |
|    n_updates            | 47790     |
|    policy_gradient_loss | 0.206     |
|    std                  | 9.95      |
|    value_loss           | 376       |
---------------------------------------
Iteration: 4781 | Episodes: 292800 | Median Reward: 47.35 | Max Reward: 49.33
Iteration: 4782 | Episodes: 292900 | Median Reward: 47.40 | Max Reward: 49.33
Iteration: 4784 | Episodes: 293000 | Median Reward: 47.42 | Max Reward: 49.33
Iteration: 4785 | Episodes: 293100 | Median Reward: 47.31 | Max Reward: 49.33
Iteration: 4787 | Episodes: 293200 | Median Reward: 47.21 | Max Reward: 49.33
Iteration: 4789 | Episodes: 293300 | Median Reward: 45.05 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -55.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4790      |
|    time_elapsed         | 87263     |
|    total_timesteps      | 29429760  |
| train/                  |           |
|    approx_kl            | 0.3216222 |
|    clip_fraction        | 0.037     |
|    clip_range           | 0.4       |
|    entropy_loss         | -138      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 173       |
|    n_updates            | 47890     |
|    policy_gradient_loss | 0.0274    |
|    std                  | 10.1      |
|    value_loss           | 359       |
---------------------------------------
Iteration: 4790 | Episodes: 293400 | Median Reward: 44.18 | Max Reward: 49.33
Iteration: 4792 | Episodes: 293500 | Median Reward: 44.21 | Max Reward: 49.33
Iteration: 4794 | Episodes: 293600 | Median Reward: 46.71 | Max Reward: 49.33
Iteration: 4795 | Episodes: 293700 | Median Reward: 46.64 | Max Reward: 49.33
Iteration: 4797 | Episodes: 293800 | Median Reward: 46.25 | Max Reward: 49.33
Iteration: 4799 | Episodes: 293900 | Median Reward: 45.55 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -54.5      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4800       |
|    time_elapsed         | 87459      |
|    total_timesteps      | 29491200   |
| train/                  |            |
|    approx_kl            | 0.62054706 |
|    clip_fraction        | 0.0265     |
|    clip_range           | 0.4        |
|    entropy_loss         | -139       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 174        |
|    n_updates            | 47990      |
|    policy_gradient_loss | -0.0016    |
|    std                  | 10.2       |
|    value_loss           | 361        |
----------------------------------------
Iteration: 4800 | Episodes: 294000 | Median Reward: 45.51 | Max Reward: 49.33
Iteration: 4802 | Episodes: 294100 | Median Reward: 44.76 | Max Reward: 49.33
Iteration: 4804 | Episodes: 294200 | Median Reward: 45.05 | Max Reward: 49.33
Iteration: 4805 | Episodes: 294300 | Median Reward: 45.44 | Max Reward: 49.33
Iteration: 4807 | Episodes: 294400 | Median Reward: 45.89 | Max Reward: 49.33
Iteration: 4808 | Episodes: 294500 | Median Reward: 45.26 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -54.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4810      |
|    time_elapsed         | 87640     |
|    total_timesteps      | 29552640  |
| train/                  |           |
|    approx_kl            | 2.1599135 |
|    clip_fraction        | 0.0684    |
|    clip_range           | 0.4       |
|    entropy_loss         | -139      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 181       |
|    n_updates            | 48090     |
|    policy_gradient_loss | 0.0173    |
|    std                  | 10.3      |
|    value_loss           | 367       |
---------------------------------------
Iteration: 4810 | Episodes: 294600 | Median Reward: 44.42 | Max Reward: 49.33
Iteration: 4812 | Episodes: 294700 | Median Reward: 44.66 | Max Reward: 49.33
Iteration: 4813 | Episodes: 294800 | Median Reward: 43.95 | Max Reward: 49.33
Iteration: 4815 | Episodes: 294900 | Median Reward: 42.98 | Max Reward: 49.33
Iteration: 4817 | Episodes: 295000 | Median Reward: 43.97 | Max Reward: 49.33
Iteration: 4818 | Episodes: 295100 | Median Reward: 44.53 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -55.8      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4820       |
|    time_elapsed         | 87823      |
|    total_timesteps      | 29614080   |
| train/                  |            |
|    approx_kl            | 0.34675732 |
|    clip_fraction        | 0.0177     |
|    clip_range           | 0.4        |
|    entropy_loss         | -139       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 171        |
|    n_updates            | 48190      |
|    policy_gradient_loss | 0.00901    |
|    std                  | 10.5       |
|    value_loss           | 361        |
----------------------------------------
Iteration: 4820 | Episodes: 295200 | Median Reward: 44.22 | Max Reward: 49.33
Iteration: 4822 | Episodes: 295300 | Median Reward: 45.52 | Max Reward: 49.33
Iteration: 4823 | Episodes: 295400 | Median Reward: 45.54 | Max Reward: 49.33
Iteration: 4825 | Episodes: 295500 | Median Reward: 43.30 | Max Reward: 49.33
Iteration: 4827 | Episodes: 295600 | Median Reward: 42.54 | Max Reward: 49.33
Iteration: 4828 | Episodes: 295700 | Median Reward: 40.26 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -57.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4830      |
|    time_elapsed         | 88014     |
|    total_timesteps      | 29675520  |
| train/                  |           |
|    approx_kl            | 1.7714362 |
|    clip_fraction        | 0.0795    |
|    clip_range           | 0.4       |
|    entropy_loss         | -139      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 172       |
|    n_updates            | 48290     |
|    policy_gradient_loss | 0.0186    |
|    std                  | 10.6      |
|    value_loss           | 349       |
---------------------------------------
Iteration: 4830 | Episodes: 295800 | Median Reward: 43.87 | Max Reward: 49.33
Iteration: 4831 | Episodes: 295900 | Median Reward: 41.90 | Max Reward: 49.33
Iteration: 4833 | Episodes: 296000 | Median Reward: 45.96 | Max Reward: 49.33
Iteration: 4835 | Episodes: 296100 | Median Reward: 46.09 | Max Reward: 49.33
Iteration: 4836 | Episodes: 296200 | Median Reward: 46.30 | Max Reward: 49.33
Iteration: 4838 | Episodes: 296300 | Median Reward: 45.92 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -57.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4840      |
|    time_elapsed         | 88194     |
|    total_timesteps      | 29736960  |
| train/                  |           |
|    approx_kl            | 1.0752597 |
|    clip_fraction        | 0.069     |
|    clip_range           | 0.4       |
|    entropy_loss         | -140      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 173       |
|    n_updates            | 48390     |
|    policy_gradient_loss | 0.0251    |
|    std                  | 10.7      |
|    value_loss           | 343       |
---------------------------------------
Iteration: 4840 | Episodes: 296400 | Median Reward: 44.14 | Max Reward: 49.33
Iteration: 4841 | Episodes: 296500 | Median Reward: 38.86 | Max Reward: 49.33
Iteration: 4843 | Episodes: 296600 | Median Reward: 37.29 | Max Reward: 49.33
Iteration: 4845 | Episodes: 296700 | Median Reward: 43.62 | Max Reward: 49.33
Iteration: 4846 | Episodes: 296800 | Median Reward: 45.04 | Max Reward: 49.33
Iteration: 4848 | Episodes: 296900 | Median Reward: 44.00 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -53.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4850      |
|    time_elapsed         | 88378     |
|    total_timesteps      | 29798400  |
| train/                  |           |
|    approx_kl            | 1.8582155 |
|    clip_fraction        | 0.0484    |
|    clip_range           | 0.4       |
|    entropy_loss         | -140      |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.0001    |
|    loss                 | 191       |
|    n_updates            | 48490     |
|    policy_gradient_loss | 0.0157    |
|    std                  | 10.8      |
|    value_loss           | 398       |
---------------------------------------
Iteration: 4850 | Episodes: 297000 | Median Reward: 46.69 | Max Reward: 49.33
Iteration: 4851 | Episodes: 297100 | Median Reward: 47.02 | Max Reward: 49.33
Iteration: 4853 | Episodes: 297200 | Median Reward: 45.41 | Max Reward: 49.33
Iteration: 4854 | Episodes: 297300 | Median Reward: 43.70 | Max Reward: 49.33
Iteration: 4856 | Episodes: 297400 | Median Reward: 44.36 | Max Reward: 49.33
Iteration: 4858 | Episodes: 297500 | Median Reward: 45.31 | Max Reward: 49.33
Iteration: 4859 | Episodes: 297600 | Median Reward: 43.80 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -56.1    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4860     |
|    time_elapsed         | 88569    |
|    total_timesteps      | 29859840 |
| train/                  |          |
|    approx_kl            | 2.514624 |
|    clip_fraction        | 0.0907   |
|    clip_range           | 0.4      |
|    entropy_loss         | -140     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 163      |
|    n_updates            | 48590    |
|    policy_gradient_loss | 0.0291   |
|    std                  | 10.9     |
|    value_loss           | 349      |
--------------------------------------
Iteration: 4861 | Episodes: 297700 | Median Reward: 46.33 | Max Reward: 49.33
Iteration: 4863 | Episodes: 297800 | Median Reward: 47.11 | Max Reward: 49.33
Iteration: 4864 | Episodes: 297900 | Median Reward: 46.63 | Max Reward: 49.33
Iteration: 4866 | Episodes: 298000 | Median Reward: 43.64 | Max Reward: 49.33
Iteration: 4868 | Episodes: 298100 | Median Reward: 43.71 | Max Reward: 49.33
Iteration: 4869 | Episodes: 298200 | Median Reward: 43.63 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -56.5      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4870       |
|    time_elapsed         | 88753      |
|    total_timesteps      | 29921280   |
| train/                  |            |
|    approx_kl            | 0.11563457 |
|    clip_fraction        | 0.00728    |
|    clip_range           | 0.4        |
|    entropy_loss         | -141       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 174        |
|    n_updates            | 48690      |
|    policy_gradient_loss | 0.00192    |
|    std                  | 11.1       |
|    value_loss           | 352        |
----------------------------------------
Iteration: 4871 | Episodes: 298300 | Median Reward: 45.17 | Max Reward: 49.33
Iteration: 4873 | Episodes: 298400 | Median Reward: 46.76 | Max Reward: 49.33
Iteration: 4874 | Episodes: 298500 | Median Reward: 46.64 | Max Reward: 49.33
Iteration: 4876 | Episodes: 298600 | Median Reward: 46.90 | Max Reward: 49.33
Iteration: 4877 | Episodes: 298700 | Median Reward: 47.02 | Max Reward: 49.33
Iteration: 4879 | Episodes: 298800 | Median Reward: 46.90 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -53.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4880      |
|    time_elapsed         | 88933     |
|    total_timesteps      | 29982720  |
| train/                  |           |
|    approx_kl            | 7.9447994 |
|    clip_fraction        | 0.168     |
|    clip_range           | 0.4       |
|    entropy_loss         | -141      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 181       |
|    n_updates            | 48790     |
|    policy_gradient_loss | 0.0905    |
|    std                  | 11.2      |
|    value_loss           | 388       |
---------------------------------------
Iteration: 4881 | Episodes: 298900 | Median Reward: 46.23 | Max Reward: 49.33
Iteration: 4882 | Episodes: 299000 | Median Reward: 45.13 | Max Reward: 49.33
Iteration: 4884 | Episodes: 299100 | Median Reward: 41.68 | Max Reward: 49.33
Iteration: 4886 | Episodes: 299200 | Median Reward: 42.90 | Max Reward: 49.33
Iteration: 4887 | Episodes: 299300 | Median Reward: 41.97 | Max Reward: 49.33
Iteration: 4889 | Episodes: 299400 | Median Reward: 42.95 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -57        |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4890       |
|    time_elapsed         | 89119      |
|    total_timesteps      | 30044160   |
| train/                  |            |
|    approx_kl            | 0.80036324 |
|    clip_fraction        | 0.0319     |
|    clip_range           | 0.4        |
|    entropy_loss         | -141       |
|    explained_variance   | -2.38e-07  |
|    learning_rate        | 0.0001     |
|    loss                 | 170        |
|    n_updates            | 48890      |
|    policy_gradient_loss | 0.0169     |
|    std                  | 11.3       |
|    value_loss           | 358        |
----------------------------------------
Iteration: 4891 | Episodes: 299500 | Median Reward: 41.91 | Max Reward: 49.33
Iteration: 4892 | Episodes: 299600 | Median Reward: 42.63 | Max Reward: 49.33
Iteration: 4894 | Episodes: 299700 | Median Reward: 43.24 | Max Reward: 49.33
Iteration: 4896 | Episodes: 299800 | Median Reward: 47.12 | Max Reward: 49.33
Iteration: 4897 | Episodes: 299900 | Median Reward: 46.01 | Max Reward: 49.33
Iteration: 4899 | Episodes: 300000 | Median Reward: 44.08 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -58.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4900      |
|    time_elapsed         | 89310     |
|    total_timesteps      | 30105600  |
| train/                  |           |
|    approx_kl            | 1.2483864 |
|    clip_fraction        | 0.0509    |
|    clip_range           | 0.4       |
|    entropy_loss         | -141      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 177       |
|    n_updates            | 48990     |
|    policy_gradient_loss | 0.0217    |
|    std                  | 11.5      |
|    value_loss           | 356       |
---------------------------------------
Iteration: 4900 | Episodes: 300100 | Median Reward: 43.37 | Max Reward: 49.33
Iteration: 4902 | Episodes: 300200 | Median Reward: 44.39 | Max Reward: 49.33
Iteration: 4904 | Episodes: 300300 | Median Reward: 44.74 | Max Reward: 49.33
Iteration: 4905 | Episodes: 300400 | Median Reward: 46.31 | Max Reward: 49.33
Iteration: 4907 | Episodes: 300500 | Median Reward: 46.65 | Max Reward: 49.33
Iteration: 4909 | Episodes: 300600 | Median Reward: 47.54 | Max Reward: 49.33
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -56.4    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 4910     |
|    time_elapsed         | 89488    |
|    total_timesteps      | 30167040 |
| train/                  |          |
|    approx_kl            | 4.470093 |
|    clip_fraction        | 0.0935   |
|    clip_range           | 0.4      |
|    entropy_loss         | -142     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0001   |
|    loss                 | 190      |
|    n_updates            | 49090    |
|    policy_gradient_loss | 0.0161   |
|    std                  | 11.6     |
|    value_loss           | 393      |
--------------------------------------
Iteration: 4910 | Episodes: 300700 | Median Reward: 44.36 | Max Reward: 49.33
Iteration: 4912 | Episodes: 300800 | Median Reward: 46.01 | Max Reward: 49.33
Iteration: 4914 | Episodes: 300900 | Median Reward: 36.11 | Max Reward: 49.33
Iteration: 4915 | Episodes: 301000 | Median Reward: 33.88 | Max Reward: 49.33
Iteration: 4917 | Episodes: 301100 | Median Reward: 42.97 | Max Reward: 49.33
Iteration: 4919 | Episodes: 301200 | Median Reward: 37.92 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 87.4       |
|    ep_rew_mean          | -46.6      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4920       |
|    time_elapsed         | 89673      |
|    total_timesteps      | 30228480   |
| train/                  |            |
|    approx_kl            | 0.39927417 |
|    clip_fraction        | 0.0298     |
|    clip_range           | 0.4        |
|    entropy_loss         | -142       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 147        |
|    n_updates            | 49190      |
|    policy_gradient_loss | 0.0102     |
|    std                  | 11.7       |
|    value_loss           | 307        |
----------------------------------------
Iteration: 4920 | Episodes: 301300 | Median Reward: 43.83 | Max Reward: 49.33
Iteration: 4922 | Episodes: 301400 | Median Reward: 47.04 | Max Reward: 49.33
Iteration: 4923 | Episodes: 301500 | Median Reward: 46.81 | Max Reward: 49.33
Iteration: 4925 | Episodes: 301600 | Median Reward: 45.20 | Max Reward: 49.33
Iteration: 4927 | Episodes: 301700 | Median Reward: 43.35 | Max Reward: 49.33
Iteration: 4928 | Episodes: 301800 | Median Reward: 43.36 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -61.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4930      |
|    time_elapsed         | 89863     |
|    total_timesteps      | 30289920  |
| train/                  |           |
|    approx_kl            | 2.6546469 |
|    clip_fraction        | 0.0631    |
|    clip_range           | 0.4       |
|    entropy_loss         | -142      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 167       |
|    n_updates            | 49290     |
|    policy_gradient_loss | 0.0265    |
|    std                  | 11.8      |
|    value_loss           | 355       |
---------------------------------------
Iteration: 4930 | Episodes: 301900 | Median Reward: 36.78 | Max Reward: 49.33
Iteration: 4932 | Episodes: 302000 | Median Reward: 41.15 | Max Reward: 49.33
Iteration: 4933 | Episodes: 302100 | Median Reward: 38.61 | Max Reward: 49.33
Iteration: 4935 | Episodes: 302200 | Median Reward: 46.90 | Max Reward: 49.33
Iteration: 4936 | Episodes: 302300 | Median Reward: 47.00 | Max Reward: 49.33
Iteration: 4938 | Episodes: 302400 | Median Reward: 47.46 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -52.9      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4940       |
|    time_elapsed         | 90043      |
|    total_timesteps      | 30351360   |
| train/                  |            |
|    approx_kl            | 0.32159817 |
|    clip_fraction        | 0.019      |
|    clip_range           | 0.4        |
|    entropy_loss         | -142       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 193        |
|    n_updates            | 49390      |
|    policy_gradient_loss | 0.00186    |
|    std                  | 11.9       |
|    value_loss           | 387        |
----------------------------------------
Iteration: 4940 | Episodes: 302500 | Median Reward: 47.19 | Max Reward: 49.33
Iteration: 4941 | Episodes: 302600 | Median Reward: 46.83 | Max Reward: 49.33
Iteration: 4943 | Episodes: 302700 | Median Reward: 46.41 | Max Reward: 49.33
Iteration: 4945 | Episodes: 302800 | Median Reward: 44.13 | Max Reward: 49.33
Iteration: 4946 | Episodes: 302900 | Median Reward: 37.76 | Max Reward: 49.33
Iteration: 4948 | Episodes: 303000 | Median Reward: 39.12 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -58.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4950      |
|    time_elapsed         | 90225     |
|    total_timesteps      | 30412800  |
| train/                  |           |
|    approx_kl            | 2.0575335 |
|    clip_fraction        | 0.106     |
|    clip_range           | 0.4       |
|    entropy_loss         | -143      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 144       |
|    n_updates            | 49490     |
|    policy_gradient_loss | 0.0425    |
|    std                  | 12.1      |
|    value_loss           | 279       |
---------------------------------------
Iteration: 4950 | Episodes: 303100 | Median Reward: 45.10 | Max Reward: 49.33
Iteration: 4951 | Episodes: 303200 | Median Reward: 38.38 | Max Reward: 49.33
Iteration: 4953 | Episodes: 303300 | Median Reward: 42.05 | Max Reward: 49.33
Iteration: 4955 | Episodes: 303400 | Median Reward: 42.49 | Max Reward: 49.33
Iteration: 4956 | Episodes: 303500 | Median Reward: 45.33 | Max Reward: 49.33
Iteration: 4958 | Episodes: 303600 | Median Reward: 44.85 | Max Reward: 49.33
Iteration: 4959 | Episodes: 303700 | Median Reward: 41.68 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -61       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4960      |
|    time_elapsed         | 90413     |
|    total_timesteps      | 30474240  |
| train/                  |           |
|    approx_kl            | 1.3218149 |
|    clip_fraction        | 0.0488    |
|    clip_range           | 0.4       |
|    entropy_loss         | -143      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 182       |
|    n_updates            | 49590     |
|    policy_gradient_loss | 0.0133    |
|    std                  | 12.2      |
|    value_loss           | 375       |
---------------------------------------
Iteration: 4961 | Episodes: 303800 | Median Reward: 28.21 | Max Reward: 49.33
Iteration: 4963 | Episodes: 303900 | Median Reward: 33.20 | Max Reward: 49.33
Iteration: 4964 | Episodes: 304000 | Median Reward: 34.78 | Max Reward: 49.33
Iteration: 4966 | Episodes: 304100 | Median Reward: 27.00 | Max Reward: 49.33
Iteration: 4968 | Episodes: 304200 | Median Reward: 31.78 | Max Reward: 49.33
Iteration: 4969 | Episodes: 304300 | Median Reward: 35.51 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -64.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4970      |
|    time_elapsed         | 90600     |
|    total_timesteps      | 30535680  |
| train/                  |           |
|    approx_kl            | 0.5903999 |
|    clip_fraction        | 0.0473    |
|    clip_range           | 0.4       |
|    entropy_loss         | -143      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 145       |
|    n_updates            | 49690     |
|    policy_gradient_loss | 0.0218    |
|    std                  | 12.2      |
|    value_loss           | 310       |
---------------------------------------
Iteration: 4971 | Episodes: 304400 | Median Reward: 36.62 | Max Reward: 49.33
Iteration: 4973 | Episodes: 304500 | Median Reward: 37.05 | Max Reward: 49.33
Iteration: 4974 | Episodes: 304600 | Median Reward: 43.28 | Max Reward: 49.33
Iteration: 4976 | Episodes: 304700 | Median Reward: 44.22 | Max Reward: 49.33
Iteration: 4978 | Episodes: 304800 | Median Reward: 27.21 | Max Reward: 49.33
Iteration: 4979 | Episodes: 304900 | Median Reward: 42.78 | Max Reward: 49.33
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -60.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 4980      |
|    time_elapsed         | 90782     |
|    total_timesteps      | 30597120  |
| train/                  |           |
|    approx_kl            | 1.1353266 |
|    clip_fraction        | 0.0408    |
|    clip_range           | 0.4       |
|    entropy_loss         | -143      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0001    |
|    loss                 | 138       |
|    n_updates            | 49790     |
|    policy_gradient_loss | 0.00848   |
|    std                  | 12.3      |
|    value_loss           | 314       |
---------------------------------------
Iteration: 4981 | Episodes: 305000 | Median Reward: 45.22 | Max Reward: 49.33
Iteration: 4982 | Episodes: 305100 | Median Reward: 44.73 | Max Reward: 49.33
Iteration: 4984 | Episodes: 305200 | Median Reward: 40.03 | Max Reward: 49.33
Iteration: 4986 | Episodes: 305300 | Median Reward: 44.91 | Max Reward: 49.33
Iteration: 4987 | Episodes: 305400 | Median Reward: 45.08 | Max Reward: 49.33
Iteration: 4989 | Episodes: 305500 | Median Reward: 46.34 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -54.5      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 4990       |
|    time_elapsed         | 90965      |
|    total_timesteps      | 30658560   |
| train/                  |            |
|    approx_kl            | 0.07622342 |
|    clip_fraction        | 0.00944    |
|    clip_range           | 0.4        |
|    entropy_loss         | -143       |
|    explained_variance   | 5.96e-08   |
|    learning_rate        | 0.0001     |
|    loss                 | 179        |
|    n_updates            | 49890      |
|    policy_gradient_loss | 0.000549   |
|    std                  | 12.5       |
|    value_loss           | 392        |
----------------------------------------
Iteration: 4991 | Episodes: 305600 | Median Reward: 45.60 | Max Reward: 49.33
Iteration: 4992 | Episodes: 305700 | Median Reward: 44.21 | Max Reward: 49.33
Iteration: 4994 | Episodes: 305800 | Median Reward: 38.33 | Max Reward: 49.33
Iteration: 4996 | Episodes: 305900 | Median Reward: 28.03 | Max Reward: 49.33
Iteration: 4997 | Episodes: 306000 | Median Reward: 26.34 | Max Reward: 49.33
Iteration: 4999 | Episodes: 306100 | Median Reward: 9.10 | Max Reward: 49.33
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -91.8      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 5000       |
|    time_elapsed         | 91155      |
|    total_timesteps      | 30720000   |
| train/                  |            |
|    approx_kl            | 0.11888679 |
|    clip_fraction        | 0.024      |
|    clip_range           | 0.4        |
|    entropy_loss         | -143       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0001     |
|    loss                 | 80.6       |
|    n_updates            | 49990      |
|    policy_gradient_loss | 0.00206    |
|    std                  | 12.5       |
|    value_loss           | 177        |
----------------------------------------
Training End | Episodes: 306135 | Median Reward: -2.27 | Max Reward: 49.33
Plot saved as fig_g3d_weighted.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> aaascr[K[K[K[K[K[Ksc[K[Kexit()
exit

Script done on 2024-10-17 04:21:47-04:00 [COMMAND_EXIT_CODE="0"]
