Script started on 2024-10-16 03:05:53-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/27" COLUMNS="120" LINES="30"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> conda activate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python f[Kenv_setup_f[Kg3_d_50[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kls
env_setup2.py	   env_setup_g3_c.py	    env_setup_g3_d.py  env_setup_mg.py	g3_d_2_logs.txt
env_setup_g2.py    env_setup_g3_d_2.py	    env_setup_g3.py    env_setup.py	g3_d_50int_logs.txt
env_setup_g3_b.py  env_setup_g3_d_50int.py  env_setup_g4.py    g3_c_logs.txt	g3_d_logs.txt
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python env_setup_f[Kg3_d_50int.py
GPU 4: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
Iteration: 0 | Episodes: 50 | Median Reward: 17.48 | Max Reward: 20.14
Iteration: 1 | Episodes: 100 | Median Reward: 18.06 | Max Reward: 33.08
Iteration: 2 | Episodes: 150 | Median Reward: 18.91 | Max Reward: 39.03
Iteration: 3 | Episodes: 200 | Median Reward: 19.75 | Max Reward: 39.03
aIteration: 4 | Episodes: 250 | Median Reward: 20.37 | Max Reward: 39.03
Iteration: 4 | Episodes: 300 | Median Reward: 21.29 | Max Reward: 39.03
Iteration: 5 | Episodes: 350 | Median Reward: 17.78 | Max Reward: 39.03
Iteration: 6 | Episodes: 400 | Median Reward: 22.30 | Max Reward: 39.03
Iteration: 7 | Episodes: 450 | Median Reward: 22.45 | Max Reward: 39.03
Iteration: 8 | Episodes: 500 | Median Reward: 21.09 | Max Reward: 39.03
Iteration: 9 | Episodes: 550 | Median Reward: 14.51 | Max Reward: 39.03
Iteration: 9 | Episodes: 600 | Median Reward: 25.90 | Max Reward: 39.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.5        |
| time/                   |              |
|    fps                  | 324          |
|    iterations           | 10           |
|    time_elapsed         | 189          |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0023371521 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -44.3        |
|    explained_variance   | -0.0466      |
|    learning_rate        | 0.0001       |
|    loss                 | 84.6         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00485     |
|    std                  | 1            |
|    value_loss           | 178          |
------------------------------------------
^[[A^[[A^[[A^[[B^[[B^CTraceback (most recent call last):
  File "env_setup_g3_d_50int.py", line 339, in <module>
    main()
  File "env_setup_g3_d_50int.py", line 318, in main
    callback = train_on_gpu(gpu_id, num_envs, total_timesteps, n_steps, save_interval)
  File "env_setup_g3_d_50int.py", line 299, in train_on_gpu
    model.learn(total_timesteps=total_timesteps, callback=callback, log_interval=10)
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py", line 454, in learn
    return super().learn(
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py", line 287, in collect_rollouts
    rollout_buffer.add(
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/common/recurrent/buffers.py", line 144, in add
    super().add(*args, **kwargs)
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/buffers.py", line 475, in add
    self.log_probs[self.pos] = log_prob.clone().cpu().numpy()
KeyboardInterrupt
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> nano env_setup_g3_d_50int.py
[?2004h[?1049h[22;0;0t[1;30r(B[m[4l[?7h[39;49m[?1h=[?1h=[?25l[39;49m(B[m[H[2J[28;54H(B[0;7m[ Reading... ](B[m[28;52H(B[0;7m[ Read 340 lines ](B[m[H(B[0;7m  GNU nano 4.8                                    env_setup_g3_d_50int.py                                               [1;119H(B[m[29d(B[0;7m^G(B[m Get Help    (B[0;7m^O(B[m Write Out   (B[0;7m^W(B[m Where Is    (B[0;7m^K(B[m Cut Text    (B[0;7m^J(B[m Justify     (B[0;7m^C(B[m Cur Pos     (B[0;7mM-U(B[m Undo[106G(B[0;7mM-A(B[m Mark Text[30d(B[0;7m^X(B[m Exit[30;16H(B[0;7m^R(B[m Read File   (B[0;7m^\(B[m Replace     (B[0;7m^U(B[m Paste Text  (B[0;7m^T(B[m To Spell    (B[0;7m^_(B[m Go To Line  (B[0;7mM-E(B[m Redo[106G(B[0;7mM-6(B[m Copy Text[28d[2d(B[0;1m[31m# Reset the env state back to initial, after each episode.[3d[36mimport[39m(B[m gymnasium (B[0;1m[36mas[39m(B[m gym[4d(B[0;1m[36mfrom[39m(B[m gymnasium (B[0;1m[36mimport[39m(B[m spaces[5d(B[0;1m[36mfrom[39m(B[m statistics (B[0;1m[36mimport[39m(B[m median[6d(B[0;1m[36mimport[39m(B[m numpy (B[0;1m[36mas[39m(B[m np[7d(B[0;1m[36mimport[39m(B[m mujoco_py[8d(B[0;1m[36mimport[39m(B[m os[9d(B[0;1m[36mimport[39m(B[m torch[10d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.vec_env (B[0;1m[36mimport[39m(B[m DummyVecEnv, VecMonitor[11d(B[0;1m[36mfrom[39m(B[m sb3_contrib (B[0;1m[36mimport[39m(B[m RecurrentPPO[12d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.callbacks (B[0;1m[36mimport[39m(B[m BaseCallback[13d(B[0;1m[36mimport[39m(B[m logging[14d(B[0;1m[36mimport[39m(B[m matplotlib.pyplot (B[0;1m[36mas[39m(B[m plt (B[0;1m[31m # Import matplotlib for plotting[16d# Configure logging[17d[39m(B[mlogging.basicConfig(level=logging.INFO)[19d(B[0;1m[36mclass[39m(B[m HandEnv(gym.Env):[20;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self):[21;9Hsuper(HandEnv, self).__init__()[22;9Hxml_path = os.path.expanduser((B[0;1m[32m'/home/easgrad/dgusain/Bot_hand/bot_hand.xml'[39m(B[m)[23;9Hlogging.info(f(B[0;1m[32m"Attempting to load Mujoco model from: {xml_path}"[39m(B[m)[25;9H(B[0;1m[36mif[39m(B[m (B[0;1m[36mnot[39m(B[m os.path.isfile(xml_path):[26;13Hlogging.error(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[27;13H(B[0;1m[36mraise[39m(B[m FileNotFoundError(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[2d[?12l[?25h[?25l[4d[?12l[?25h[?25l[6d[?12l[?25h[?25l[7d[?12l[?25h[?25l[8d[?12l[?25h[?25l[9d[?12l[?25h[?25l[10d[?12l[?25h[?25l[11d[?12l[?25h[?25l[12d[?12l[?25h[?25l[13d[?12l[?25h[?25l[14d[?12l[?25h[?25l[15d[?12l[?25h[?25l[17d[?12l[?25h[?25l[19d[?12l[?25h[?25l[21d[?12l[?25h[?25l[22d[?12l[?25h[?25l[23d[?12l[?25h[?25l[25d[?12l[?25h[?25l[27d[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;1H[K[27;9H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hself.model = mujoco_py.load_model_from_path(xml_path)[27;13Hself.sim = mujoco_py.MjSim(self.model)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hlogging.info((B[0;1m[32m"Mujoco model loaded successfully."[39m(B[m)[27;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hlogging.error(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[27;13H(B[0;1m[36mraise[39m(B[m RuntimeError(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;9Hjoint_weights = [[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H0.5, (B[0;1m[31m # ID 0: WristJoint1[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H0.5, (B[0;1m[31m # ID 1: WristJoint0[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 2: ForeFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H2.5, (B[0;1m[31m # ID 3: ForeFingerJoint2 (MCP proximal)[27;13H[39m(B[m2.0, (B[0;1m[31m # ID 4: ForeFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H1.0, (B[0;1m[31m # ID 5: ForeFingerJoint0 (DIP)[27;13H[39m(B[m3.0, (B[0;1m[31m # ID 6: MiddleFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H2.5, (B[0;1m[31m # ID 7: MiddleFingerJoint2 (MCP proximal)[27;13H[39m(B[m2.0, (B[0;1m[31m # ID 8: MiddleFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.0, (B[0;1m[31m # ID 9: MiddleFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 10: RingFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.5, (B[0;1m[31m # ID 11: RingFingerJoint2 (MCP proximal)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H2.0, (B[0;1m[31m # ID 12: RingFingerJoint1 (PIP)[27;13H[39m(B[m1.0, (B[0;1m[31m # ID 13: RingFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.5, (B[0;1m[31m # ID 14: LittleFingerJoint4 (CMC Joint rotation)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 15: LittleFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H2.5, (B[0;1m[31m # ID 16: LittleFingerJoint2 (MCP proximal)[27;13H[39m(B[m2.0, (B[0;1m[31m # ID 17: LittleFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.0, (B[0;1m[31m # ID 18: LittleFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.5, (B[0;1m[31m # ID 19: ThumbJoint4 (CMC Joint abduction/adduction)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.5, (B[0;1m[31m # ID 20: ThumbJoint3 (CMC Joint flexion/extension)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.5, (B[0;1m[31m # ID 21: ThumbJoint2 (CMC Joint rotation)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H3.0, (B[0;1m[31m # ID 22: ThumbJoint1 (MCP)[27;13H[39m(B[m0.5, (B[0;1m[31m # ID 23: ThumbJoint0 (IP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[3S[1;30r[25;13H0.0  (B[0;1m[31m # ID 24: None[26;9H[39m(B[m][27d(B[0;1m[31m # Convertiing joint weights to a tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.joint_weights = torch.tensor(joint_weights, dtype=torch.float32)[27;8H(B[0;1m[31m # Normalize the weights so that their sum equals 1[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.joint_weights /= self.joint_weights.sum()[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.initial_state = self.sim.get_state()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Define observation and action spaces[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(24,), dtype=np.float32)[27;9Hself.target_threshold = 99[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.max_steps = 100[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.steps_taken = 0[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;8H(B[0;1m[31m # Initialize actuator ranges[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hactuator_ranges = self.sim.model.actuator_ctrlrange[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.actuator_min = torch.tensor(actuator_ranges[:, 0], dtype=torch.float32)[27;9Hself.actuator_max = torch.tensor(actuator_ranges[:, 1], dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m # Create normalized action space between [-1, 1][27;9H[39m(B[mself.action_space = spaces.Box(low=-1, high=1, shape=(self.actuator_min.shape[0],), dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Precompute ground truth quaternions as a tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.ground_truth_quats = torch.tensor(self.get_ground_truth_quaternion(), dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[36mdef[34m reset[39m(B[m(self, seed=(B[0;1m[35mNone[39m(B[m, options=(B[0;1m[35mNone[39m(B[m):[27;9Hsuper().reset(seed=seed)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m #self.sim.reset()[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.sim.set_state(self.initial_state)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.sim.forward()[27;9Hself.steps_taken = 0[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hobs = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[27;9H(B[0;1m[36mreturn[39m(B[m obs, {}[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m step[39m(B[m(self, action: np.ndarray):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Rescale action using tensor operations for efficiency[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Haction_tensor = torch.from_numpy(action).float()[27;8H(B[0;1m[31m # adding some Gaussian noise to the action to avoid getting stuck in local optima[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m #noise = torch.normal(mean=0, std=0.05, size=action_tensor.size())[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m #action_tensor += noise[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hrescaled_action = self.actuator_min + (action_tensor + 1) * (self.actuator_max - self.actuator_min) / 2[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m # rescaled_action[0:2] = 0[27;9H[39m(B[mself.sim.data.ctrl[:] = rescaled_action.numpy()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.sim.step()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Fetch state as a tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hstate = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[42m        [49m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m # Calculate done condition[27;9H[39m(B[mdone = self.calculate_done(state, flag=(B[0;1m[35mFalse[39m(B[m) (B[0;1m[36mor[39m(B[m self.steps_taken >= self.max_steps[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m done:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hreward = self.calculate_reward(state, flag=(B[0;1m[35mTrue[39m(B[m).item()[27;9H(B[0;1m[36melse[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hreward = -1.0 (B[0;1m[31m # Penalize extra steps[27d[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.steps_taken += 1[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Htruncated = (B[0;1m[35mFalse[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hinfo = {}[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m state.numpy(), reward, done, truncated, info[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;5H(B[0;1m[36mdef[34m calculate_done[39m(B[m(self, state: torch.Tensor, flag: bool) -> bool:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m flag:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mFalse[27;9H[39m(B[mconfidence = self.calculate_confidence(state)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m confidence >= self.target_threshold[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m calculate_reward[39m(B[m(self, state: torch.Tensor, flag: bool) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m flag:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hconfidence = self.calculate_confidence(state)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H(B[0;1m[36mreturn[39m(B[m confidence - 50 (B[0;1m[31m # Reward calculation[27;9H[36melse[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mif[39m(B[m confidence > 80:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;17H(B[0;1m[36mreturn[39m(B[m (confidence-50)/2.0(B[0;1m[31m # encouraging the model in the right direction[27;13H[36melse[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;17H(B[0;1m[36mreturn[39m(B[m (confidence-50)/4.0(B[0;1m[31m # lesser reward[27d[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m calculate_confidence[39m(B[m(self, state: torch.Tensor) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hrendered_quat = self.get_rendered_pose_quaternion()[27;9Hrendered_quat = self.normalize_quaternion(rendered_quat)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hgt_quat = self.normalize_quaternion(self.ground_truth_quats)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hsimilarity = torch.abs(torch.sum(rendered_quat * gt_quat, dim=1)) * 100[27;9Hweighted_similarity = similarity * self.joint_weights[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Havg_confidence = weighted_similarity.sum()[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mreturn[39m(B[m avg_confidence[27d[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[36mdef[34m get_rendered_pose_quaternion[39m(B[m(self) -> torch.Tensor:[27;9Hquaternions = [][?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mfor[39m(B[m joint (B[0;1m[36min[39m(B[m range(self.model.njnt):[27;13Hq = self.sim.data.qpos[self.model.jnt_qposadr[joint]:self.model.jnt_qposadr[joint] + 4][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hquaternions.append(q)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Convert list of arrays to a single NumPy array[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hquaternions_np = np.array(quaternions, dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Convert NumPy array to PyTorch tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m torch.from_numpy(quaternions_np)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m normalize_quaternion[39m(B[m(self, q: torch.Tensor) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hnorm = torch.norm(q, dim=1, keepdim=(B[0;1m[35mTrue[39m(B[m)[27;9H(B[0;1m[36mreturn[39m(B[m q / norm.clamp(min=1e-8) (B[0;1m[31m # Prevent division by zero[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m get_ground_truth_quaternion[39m(B[m(self) -> np.ndarray:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hground_truth_quats = [[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[6.32658406e-04,  1.25473697e-03, -9.99718591e-03,  1.56702220e+00],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[1.25473697e-03, -9.99718591e-03,  1.56702220e+00,  1.56730258e+00],[27;13H[-0.00999719,  1.5670222,   1.56730258,  1.5674143],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[1.5670222,   1.56730258,  1.5674143,  -0.00999801],[27;13H[1.56730258,  1.5674143,  -0.00999801,  1.56701926],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.5674143,  -0.00999801,  1.56701926,  1.56730194],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[-0.00999801,  1.56701926,  1.56730194,  1.5674143],[27;13H[1.56701926,  1.56730194,  1.5674143,  -0.00999756],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56730194,  1.5674143,  -0.00999756,  1.56701717],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.5674143,  -0.00999756,  1.56701717,  1.56730177],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[-0.00999756,  1.56701717,  1.56730177,  1.56741429],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56701717, 1.56730177, 1.56741429, 0.00868252],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56730177,  1.56741429,  0.00868252, -0.01000805],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56741429,  0.00868252, -0.01000805,  1.56708349],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[0.00868252, -0.01000805,  1.56708349,  1.56730911],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[-0.01000805,  1.56708349,  1.56730911,  1.56741508],[27;13H[1.56708349, 1.56730911, 1.56741508, 0.49916711],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56730911, 1.56741508, 0.49916711, 0.50022545],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56741508, 0.49916711, 0.50022545, 0.25330455],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[4.99167109e-01, 5.00225452e-01, 2.53304550e-01, 4.08440608e-04],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[5.00225452e-01,  2.53304550e-01,  4.08440608e-04, -9.00000689e-01],[27;13H[2.53304550e-01,  4.08440608e-04, -9.00000689e-01,  1.00000000e-01],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[4.08440608e-04, -9.00000689e-01,  1.00000000e-01, -1.00000000e-01],[27;13H[-0.90000069,  0.1,[27;40H-0.1,[27;54H0.01463168],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[0.1,[27;26H-0.1,[27;40H0.01463168,  1.0][?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H][27d(B[0;1m[36mreturn[39m(B[m np.array(ground_truth_quats, dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mclass[39m(B[m RewardCallback(BaseCallback):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m    Custom callback for logging average rewards over every 100 episodes, episode numbers, and iteration numbers.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;1H(B[0;1m[32m    Also stores average rewards for plotting.[27d    """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self, avg_interval=50):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hsuper(RewardCallback, self).__init__()[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.episode_num = 0[27;9Hself.max_reward = -np.inf[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.avg_interval = avg_interval (B[0;1m[31m # Number of episodes per average[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.sum_rewards = 0.0[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.count_rewards = 0[27;9Hself.avg_rewards = {} (B[0;1m[31m # Dictionary to store average rewards, key: block_num[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.iteration_num = 0    (B[0;1m[31m # Counter for iterations[27;9H[39m(B[mself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.median_rewards = {}[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _on_rollout_end[39m(B[m(self) -> bool:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Called at the end of a rollout.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Used to track the number of iterations.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;1H(B[0;1m[32m        """[27;9H[39m(B[mself.iteration_num += 1[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;5H(B[0;1m[36mdef[34m _on_step[39m(B[m(self) -> bool:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[32m"""[27d        Called at every step. Checks if any episode has finished and logs the reward.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m # Retrieve 'dones' and 'rewards' from the current step[27;9H[39m(B[mdones = self.locals.get((B[0;1m[32m'dones'[39m(B[m, [])[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hrewards = self.locals.get((B[0;1m[32m'rewards'[39m(B[m, [])[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;8H(B[0;1m[31m # Check if any of the environments are done[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mif[39m(B[m np.any(dones):[27;13H(B[0;1m[36mfor[39m(B[m done, reward (B[0;1m[36min[39m(B[m zip(dones, rewards):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17H(B[0;1m[36mif[39m(B[m done:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;21Hself.episode_num += 1[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;21H(B[0;1m[36mif[39m(B[m reward > self.max_reward:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hself.max_reward = reward[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;21Hself.rewards_list.append(reward)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;21H(B[0;1m[36mif[39m(B[m len(self.rewards_list) == self.avg_interval:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hmedian_reward = median(self.rewards_list)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hblock_num = self.episode_num // self.avg_interval[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hself.median_rewards[block_num] = median_reward[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hprint(f(B[0;1m[32m"Iteration: {self.iteration_num} | Episodes: {block_num * self.avg_interval} | Median Re[39m(B[0;7m>[27;1H(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;24H(B[0;1m[31m # Reset sum and count[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _on_training_end[39m(B[m(self) -> (B[0;1m[35mNone[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Called at the end of training. If there are remaining episodes, compute and store the average.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m len(self.rewards_list) > 0:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hmedian_reward = median(self.rewards_list)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hblock_num = self.episode_num // self.avg_interval + 1[27;13Hself.median_rewards[block_num] = median_reward[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hcurrent_max = max(self.rewards_list)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mif[39m(B[m current_max > self.max_reward:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17Hself.max_reward = current_max[42m           [49m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hprint(f(B[0;1m[32m"Training End | Episodes: {self.episode_num} | Median Reward: {median_reward:.2f} | Max Reward: {sel[39m(B[0;7m>[27;1H(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mdef[34m make_env[39m(B[m():[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[32m"""Utility function to create a HandEnv without Monitor."""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[36mdef[34m _init[39m(B[m():[27;9H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Henv = HandEnv()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mreturn[39m(B[m env[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hlogging.error(f(B[0;1m[32m"Failed to initialize HandEnv: {e}"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mraise[39m(B[m e[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mreturn[39m(B[m _init[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;1H(B[0;1m[36mdef[34m train_on_gpu[39m(B[m(gpu_id: int, num_envs: int, total_timesteps: int, num_steps: int, save_interval: int):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hos.environ[(B[0;1m[32m'CUDA_VISIBLE_DEVICES'[39m(B[m] = str(gpu_id)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hdevice = (B[0;1m[32m'cuda'[39m(B[m (B[0;1m[36mif[39m(B[m torch.cuda.is_available() (B[0;1m[36melse[39m(B[m (B[0;1m[32m'cpu'[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hprint(f(B[0;1m[32m"GPU {gpu_id}: Using {device} device"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Choose between SubprocVecEnv and DummyVecEnv based on your needs[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # env = SubprocVecEnv([make_env() for _ in range(num_envs)])[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Henv = DummyVecEnv([make_env() (B[0;1m[36mfor[39m(B[m _ (B[0;1m[36min[39m(B[m range(num_envs)]) (B[0;1m[31m # Use DummyVecEnv for testing[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Henv = VecMonitor(env) (B[0;1m[31m # Use VecMonitor instead of individual Monitor wrappers[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Initialize the RecurrentPPO model with optimized parameters[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hmodel = RecurrentPPO([?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m'MlpLstmPolicy'[39m(B[m,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Henv,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hverbose=1,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hdevice=device,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hent_coef=0.05,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hlearning_rate=0.0001,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hclip_range=0.4,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hn_steps=num_steps,[33G(B[0;1m[31m # Steps per environment per update[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hbatch_size=4096,[27;34H(B[0;1m[31m # Increased batch size[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hgamma=0.99,[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hgae_lambda=0.95,[27;9Hmax_grad_norm=0.5,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hvf_coef=0.5,[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Huse_sde=(B[0;1m[35mTrue[39m(B[m,[26;33H(B[0;1m[31m # Use State Dependent Exploration for better exploration[27;5H[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5Hcallback = RewardCallback(avg_interval=50)[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Train the model with the specified total timesteps[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hmodel.learn(total_timesteps=total_timesteps, callback=callback, log_interval=10)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[27;9Hlogging.error(f(B[0;1m[32m"Error during training: {e}"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mraise[39m(B[m e[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Save the final model[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hmodel.save(f(B[0;1m[32m"/home/easgrad/dgusain/Bot_hand/agents/agent_g3d_{gpu_id}_50int_weighted"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mreturn[39m(B[m callback (B[0;1m[31m # Return the callback to access episode_rewards[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;1H(B[0;1m[36mdef[34m main[39m(B[m():[27;5Hgpu_id = 4[27;24H(B[0;1m[31m # Set your GPU ID here[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hnum_envs = 6[24G(B[0;1m[31m # Number of parallel environments (adjust based on your GPU memory)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hn_iter = 100[27;25H(B[0;1m[31m # Number of iterations you want to run[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hn_steps = 1024     (B[0;1m[31m # Steps per environment per update[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Htotal_timesteps = n_iter * n_steps * num_envs (B[0;1m[31m # Total training timesteps[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hsave_interval = 25 (B[0;1m[31m # Not used in this optimized version[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;4H(B[0;1m[31m # Train the model and get the callback with rewards data[27;5H[39m(B[mcallback = train_on_gpu(gpu_id, num_envs, total_timesteps, n_steps, save_interval)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;4H(B[0;1m[31m # Extract block numbers and average rewards from the callback[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hblock_numbers = list(callback.median_rewards.keys())[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hblock_rewards = list(callback.median_rewards.values())[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25lg[?12l[?25h[?25lp[?12l[?25h[?25lu[?12l[?25h[?25l_[?12l[?25h[?25li[?12l[?25h[?25ld[?12l[?25h[?25l [?12l[?25h[?25l=[?12l[?25h[?25l [?12l[?25h[?25l4[?12l[?25h[?25l [?12l[?25h[?25l[?12l[?25h[?25l[1;111H(B[0;7mModified(B[m[15;14H[1P[?12l[?25h[?25l3[15;23H (B[0;1m[31m # Set your GPU ID here[15;15H[39m(B[m[?12l[?25h[?25l[28d(B[0;7mSave modified buffer?                                                                                                   [29;1H Y(B[m Yes[K[30d(B[0;7m N(B[m No  [30;16H (B[0;7m^C(B[m Cancel[K[28;23H[?12l[?25h[?25l[29d(B[0;7m^G(B[m Get Help[29;31H(B[0;7mM-D(B[m DOS Format[29;61H(B[0;7mM-A(B[m Append[29;91H(B[0;7mM-B(B[m Backup File[30d(B[0;7m^C(B[m Cancel[17G[14X[30;31H(B[0;7mM-M(B[m Mac Format[30;61H(B[0;7mM-P(B[m Prepend[30;91H(B[0;7m^T(B[m To Files[28d(B[0;7mFile Name to Write: env_setup_g3_d_50int.py(B[m[28;44H[?12l[?25h[?25l[28;53H[1K (B[0;7m[ Writing... ](B[m[K[1;111H(B[0;7m        (B[m[28;51H(B[0;7m[ Wrote 340 lines ](B[m[J[30d[?12l[?25h[30;1H[?1049l[23;0;0t[?1l>[?2004l(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python env_setup_g3_d_2[K50int.py
GPU 3: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
aIteration: 0 | Episodes: 50 | Median Reward: 24.29 | Max Reward: 29.78
Iteration: 1 | Episodes: 100 | Median Reward: 24.35 | Max Reward: 35.30
Iteration: 2 | Episodes: 150 | Median Reward: 25.60 | Max Reward: 35.64
Iteration: 3 | Episodes: 200 | Median Reward: 25.60 | Max Reward: 35.64
Iteration: 4 | Episodes: 250 | Median Reward: 22.50 | Max Reward: 35.64
Iteration: 4 | Episodes: 300 | Median Reward: 22.77 | Max Reward: 35.64
Iteration: 5 | Episodes: 350 | Median Reward: 24.57 | Max Reward: 41.61
Iteration: 6 | Episodes: 400 | Median Reward: 18.13 | Max Reward: 41.61
Iteration: 7 | Episodes: 450 | Median Reward: 17.42 | Max Reward: 41.61
Iteration: 8 | Episodes: 500 | Median Reward: 17.42 | Max Reward: 41.61
Iteration: 9 | Episodes: 550 | Median Reward: 22.46 | Max Reward: 41.61
Iteration: 9 | Episodes: 600 | Median Reward: 24.16 | Max Reward: 41.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -79.2       |
| time/                   |             |
|    fps                  | 352         |
|    iterations           | 10          |
|    time_elapsed         | 174         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.052140415 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.4         |
|    entropy_loss         | -42.4       |
|    explained_variance   | -0.0383     |
|    learning_rate        | 0.0001      |
|    loss                 | 88.7        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00967    |
|    std                  | 1           |
|    value_loss           | 190         |
-----------------------------------------
Iteration: 10 | Episodes: 650 | Median Reward: 18.82 | Max Reward: 41.61
Iteration: 11 | Episodes: 700 | Median Reward: 20.27 | Max Reward: 41.61
Iteration: 12 | Episodes: 750 | Median Reward: 22.89 | Max Reward: 41.61
Iteration: 13 | Episodes: 800 | Median Reward: 23.98 | Max Reward: 41.61
Iteration: 14 | Episodes: 850 | Median Reward: 27.26 | Max Reward: 41.61
Iteration: 14 | Episodes: 900 | Median Reward: 25.96 | Max Reward: 41.61
Iteration: 15 | Episodes: 950 | Median Reward: 26.96 | Max Reward: 41.61
Iteration: 16 | Episodes: 1000 | Median Reward: 17.24 | Max Reward: 41.61
Iteration: 17 | Episodes: 1050 | Median Reward: 15.19 | Max Reward: 41.61
Iteration: 18 | Episodes: 1100 | Median Reward: 16.11 | Max Reward: 41.61
Iteration: 18 | Episodes: 1150 | Median Reward: 17.63 | Max Reward: 41.61
Iteration: 19 | Episodes: 1200 | Median Reward: 14.26 | Max Reward: 41.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -84.6       |
| time/                   |             |
|    fps                  | 332         |
|    iterations           | 20          |
|    time_elapsed         | 369         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.016427258 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -50.6       |
|    explained_variance   | -0.0248     |
|    learning_rate        | 0.0001      |
|    loss                 | 83.1        |
|    n_updates            | 190         |
|    policy_gradient_loss | 0.000285    |
|    std                  | 1.01        |
|    value_loss           | 179         |
-----------------------------------------
Iteration: 20 | Episodes: 1250 | Median Reward: 16.76 | Max Reward: 41.61
Iteration: 21 | Episodes: 1300 | Median Reward: 18.29 | Max Reward: 41.61
Iteration: 22 | Episodes: 1350 | Median Reward: 18.29 | Max Reward: 41.61
Iteration: 23 | Episodes: 1400 | Median Reward: 12.73 | Max Reward: 41.61
Iteration: 23 | Episodes: 1450 | Median Reward: 21.66 | Max Reward: 41.61
Iteration: 24 | Episodes: 1500 | Median Reward: 13.86 | Max Reward: 41.61
Iteration: 25 | Episodes: 1550 | Median Reward: 13.79 | Max Reward: 41.61
Iteration: 26 | Episodes: 1600 | Median Reward: 16.82 | Max Reward: 41.61
Iteration: 27 | Episodes: 1650 | Median Reward: 19.96 | Max Reward: 41.61
Iteration: 28 | Episodes: 1700 | Median Reward: 17.57 | Max Reward: 41.61
Iteration: 28 | Episodes: 1750 | Median Reward: 19.67 | Max Reward: 41.61
Iteration: 29 | Episodes: 1800 | Median Reward: 21.22 | Max Reward: 41.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.1       |
| time/                   |             |
|    fps                  | 335         |
|    iterations           | 30          |
|    time_elapsed         | 549         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.003335473 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -57.8       |
|    explained_variance   | -0.014      |
|    learning_rate        | 0.0001      |
|    loss                 | 78.1        |
|    n_updates            | 290         |
|    policy_gradient_loss | 0.000518    |
|    std                  | 1.01        |
|    value_loss           | 184         |
-----------------------------------------
Iteration: 30 | Episodes: 1850 | Median Reward: 20.75 | Max Reward: 41.61
Iteration: 31 | Episodes: 1900 | Median Reward: 20.28 | Max Reward: 41.61
Iteration: 32 | Episodes: 1950 | Median Reward: 17.44 | Max Reward: 41.61
Iteration: 32 | Episodes: 2000 | Median Reward: 18.08 | Max Reward: 41.61
Iteration: 33 | Episodes: 2050 | Median Reward: 16.15 | Max Reward: 41.61
Iteration: 34 | Episodes: 2100 | Median Reward: 15.22 | Max Reward: 41.61
Iteration: 35 | Episodes: 2150 | Median Reward: 13.63 | Max Reward: 41.61
Iteration: 36 | Episodes: 2200 | Median Reward: 14.80 | Max Reward: 41.61
Iteration: 36 | Episodes: 2250 | Median Reward: 12.77 | Max Reward: 41.61
Iteration: 37 | Episodes: 2300 | Median Reward: 25.84 | Max Reward: 41.61
Iteration: 38 | Episodes: 2350 | Median Reward: 14.72 | Max Reward: 41.61
Iteration: 39 | Episodes: 2400 | Median Reward: 16.57 | Max Reward: 41.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -83.1       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 40          |
|    time_elapsed         | 727         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.070016086 |
|    clip_fraction        | 0.0215      |
|    clip_range           | 0.4         |
|    entropy_loss         | -63.9       |
|    explained_variance   | -0.0111     |
|    learning_rate        | 0.0001      |
|    loss                 | 88.3        |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.00596     |
|    std                  | 1.01        |
|    value_loss           | 167         |
-----------------------------------------
Iteration: 40 | Episodes: 2450 | Median Reward: 18.03 | Max Reward: 41.61
Iteration: 41 | Episodes: 2500 | Median Reward: 18.55 | Max Reward: 41.61
Iteration: 41 | Episodes: 2550 | Median Reward: 13.75 | Max Reward: 41.61
Iteration: 42 | Episodes: 2600 | Median Reward: 11.99 | Max Reward: 41.61
Iteration: 43 | Episodes: 2650 | Median Reward: 22.97 | Max Reward: 41.61
Iteration: 44 | Episodes: 2700 | Median Reward: 21.08 | Max Reward: 41.61
Iteration: 45 | Episodes: 2750 | Median Reward: 20.23 | Max Reward: 41.61
Iteration: 46 | Episodes: 2800 | Median Reward: 12.22 | Max Reward: 41.61
Iteration: 46 | Episodes: 2850 | Median Reward: 16.76 | Max Reward: 41.61
Iteration: 47 | Episodes: 2900 | Median Reward: 16.38 | Max Reward: 41.61
Iteration: 48 | Episodes: 2950 | Median Reward: 16.78 | Max Reward: 41.61
Iteration: 49 | Episodes: 3000 | Median Reward: 11.23 | Max Reward: 41.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.3        |
| time/                   |              |
|    fps                  | 333          |
|    iterations           | 50           |
|    time_elapsed         | 920          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.0042162305 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -72          |
|    explained_variance   | -0.00683     |
|    learning_rate        | 0.0001       |
|    loss                 | 75.9         |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00186     |
|    std                  | 1.01         |
|    value_loss           | 166          |
------------------------------------------
Iteration: 50 | Episodes: 3050 | Median Reward: 11.09 | Max Reward: 41.61
Iteration: 50 | Episodes: 3100 | Median Reward: 21.05 | Max Reward: 41.61
Iteration: 51 | Episodes: 3150 | Median Reward: 11.03 | Max Reward: 41.61
Iteration: 52 | Episodes: 3200 | Median Reward: 20.66 | Max Reward: 41.61
Iteration: 53 | Episodes: 3250 | Median Reward: 23.54 | Max Reward: 41.61
Iteration: 54 | Episodes: 3300 | Median Reward: 23.49 | Max Reward: 41.61
Iteration: 55 | Episodes: 3350 | Median Reward: 17.96 | Max Reward: 41.61
Iteration: 55 | Episodes: 3400 | Median Reward: 11.86 | Max Reward: 41.61
Iteration: 56 | Episodes: 3450 | Median Reward: 16.26 | Max Reward: 41.61
Iteration: 57 | Episodes: 3500 | Median Reward: 22.93 | Max Reward: 41.61
Iteration: 58 | Episodes: 3550 | Median Reward: 22.93 | Max Reward: 41.61
Iteration: 59 | Episodes: 3600 | Median Reward: 19.82 | Max Reward: 41.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.4        |
| time/                   |              |
|    fps                  | 333          |
|    iterations           | 60           |
|    time_elapsed         | 1104         |
|    total_timesteps      | 368640       |
| train/                  |              |
|    approx_kl            | 0.0023297793 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -75.2        |
|    explained_variance   | -0.00375     |
|    learning_rate        | 0.0001       |
|    loss                 | 72.8         |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00202     |
|    std                  | 1.02         |
|    value_loss           | 176          |
------------------------------------------
Iteration: 60 | Episodes: 3650 | Median Reward: 19.92 | Max Reward: 41.61
Iteration: 60 | Episodes: 3700 | Median Reward: 18.04 | Max Reward: 41.61
Iteration: 61 | Episodes: 3750 | Median Reward: 18.50 | Max Reward: 41.61
Iteration: 62 | Episodes: 3800 | Median Reward: 14.43 | Max Reward: 41.61
Iteration: 63 | Episodes: 3850 | Median Reward: 11.79 | Max Reward: 41.61
Iteration: 64 | Episodes: 3900 | Median Reward: 13.37 | Max Reward: 41.61
Iteration: 64 | Episodes: 3950 | Median Reward: 15.98 | Max Reward: 41.61
Iteration: 65 | Episodes: 4000 | Median Reward: 19.72 | Max Reward: 41.61
Iteration: 66 | Episodes: 4050 | Median Reward: 19.48 | Max Reward: 41.61
Iteration: 67 | Episodes: 4100 | Median Reward: 27.40 | Max Reward: 41.61
Iteration: 68 | Episodes: 4150 | Median Reward: 28.59 | Max Reward: 41.61
Iteration: 69 | Episodes: 4200 | Median Reward: 21.47 | Max Reward: 41.61
Iteration: 69 | Episodes: 4250 | Median Reward: 18.67 | Max Reward: 41.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.3       |
| time/                   |             |
|    fps                  | 334         |
|    iterations           | 70          |
|    time_elapsed         | 1287        |
|    total_timesteps      | 430080      |
| train/                  |             |
|    approx_kl            | 0.010166796 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -78.7       |
|    explained_variance   | -0.00221    |
|    learning_rate        | 0.0001      |
|    loss                 | 102         |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00452    |
|    std                  | 1.02        |
|    value_loss           | 203         |
-----------------------------------------
Iteration: 70 | Episodes: 4300 | Median Reward: 18.70 | Max Reward: 41.61
Iteration: 71 | Episodes: 4350 | Median Reward: 18.71 | Max Reward: 41.61
Iteration: 72 | Episodes: 4400 | Median Reward: 20.92 | Max Reward: 41.61
Iteration: 73 | Episodes: 4450 | Median Reward: 19.20 | Max Reward: 41.61
Iteration: 73 | Episodes: 4500 | Median Reward: 19.38 | Max Reward: 41.61
Iteration: 74 | Episodes: 4550 | Median Reward: 14.49 | Max Reward: 41.61
Iteration: 75 | Episodes: 4600 | Median Reward: 13.68 | Max Reward: 41.61
Iteration: 76 | Episodes: 4650 | Median Reward: 13.68 | Max Reward: 41.61
Iteration: 77 | Episodes: 4700 | Median Reward: 21.55 | Max Reward: 41.61
Iteration: 78 | Episodes: 4750 | Median Reward: 18.97 | Max Reward: 41.61
Iteration: 78 | Episodes: 4800 | Median Reward: 20.46 | Max Reward: 41.61
Iteration: 79 | Episodes: 4850 | Median Reward: 24.05 | Max Reward: 41.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -77.6       |
| time/                   |             |
|    fps                  | 332         |
|    iterations           | 80          |
|    time_elapsed         | 1480        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.004252692 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -80         |
|    explained_variance   | -0.0015     |
|    learning_rate        | 0.0001      |
|    loss                 | 97.4        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00389    |
|    std                  | 1.03        |
|    value_loss           | 194         |
-----------------------------------------
Iteration: 80 | Episodes: 4900 | Median Reward: 22.51 | Max Reward: 41.61
Iteration: 81 | Episodes: 4950 | Median Reward: 22.24 | Max Reward: 41.61
Iteration: 82 | Episodes: 5000 | Median Reward: 18.26 | Max Reward: 41.69
Iteration: 83 | Episodes: 5050 | Median Reward: 20.84 | Max Reward: 41.69
Iteration: 83 | Episodes: 5100 | Median Reward: 15.99 | Max Reward: 41.69
Iteration: 84 | Episodes: 5150 | Median Reward: 19.41 | Max Reward: 41.69
Iteration: 85 | Episodes: 5200 | Median Reward: 19.41 | Max Reward: 41.69
Iteration: 86 | Episodes: 5250 | Median Reward: 19.63 | Max Reward: 41.69
Iteration: 87 | Episodes: 5300 | Median Reward: 19.63 | Max Reward: 41.69
Iteration: 87 | Episodes: 5350 | Median Reward: 21.39 | Max Reward: 41.69
Iteration: 88 | Episodes: 5400 | Median Reward: 18.63 | Max Reward: 41.69
Iteration: 89 | Episodes: 5450 | Median Reward: 16.84 | Max Reward: 41.69
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.8        |
| time/                   |              |
|    fps                  | 332          |
|    iterations           | 90           |
|    time_elapsed         | 1661         |
|    total_timesteps      | 552960       |
| train/                  |              |
|    approx_kl            | 0.0023438912 |
|    clip_fraction        | 0.00222      |
|    clip_range           | 0.4          |
|    entropy_loss         | -81.6        |
|    explained_variance   | -0.00113     |
|    learning_rate        | 0.0001       |
|    loss                 | 91.7         |
|    n_updates            | 890          |
|    policy_gradient_loss | 0.000582     |
|    std                  | 1.04         |
|    value_loss           | 166          |
------------------------------------------
Iteration: 90 | Episodes: 5500 | Median Reward: 24.20 | Max Reward: 41.69
Iteration: 91 | Episodes: 5550 | Median Reward: 25.78 | Max Reward: 42.93
Iteration: 92 | Episodes: 5600 | Median Reward: 17.05 | Max Reward: 42.93
Iteration: 92 | Episodes: 5650 | Median Reward: 17.07 | Max Reward: 42.93
Iteration: 93 | Episodes: 5700 | Median Reward: 32.51 | Max Reward: 42.95
Iteration: 94 | Episodes: 5750 | Median Reward: 22.28 | Max Reward: 42.95
Iteration: 95 | Episodes: 5800 | Median Reward: 22.92 | Max Reward: 42.95
Iteration: 96 | Episodes: 5850 | Median Reward: 24.43 | Max Reward: 42.95
Iteration: 97 | Episodes: 5900 | Median Reward: 25.51 | Max Reward: 42.95
Iteration: 97 | Episodes: 5950 | Median Reward: 21.55 | Max Reward: 42.95
Iteration: 98 | Episodes: 6000 | Median Reward: 22.25 | Max Reward: 42.95
Iteration: 99 | Episodes: 6050 | Median Reward: 22.09 | Max Reward: 42.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.6        |
| time/                   |              |
|    fps                  | 333          |
|    iterations           | 100          |
|    time_elapsed         | 1842         |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0017917216 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -82.2        |
|    explained_variance   | -0.000646    |
|    learning_rate        | 0.0001       |
|    loss                 | 101          |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00345     |
|    std                  | 1.04         |
|    value_loss           | 201          |
------------------------------------------
Training End | Episodes: 6078 | Median Reward: 23.23 | Max Reward: 42.95
Plot saved as fig_g3d_50int_weighted.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> a[Kpytho[K[K[K[K[Kpython env_setup_g3_d_50int.py[43G[2Pnano[71G
[?2004h[?1049h[22;0;0t[1;30r(B[m[4l[?7h[39;49m[?1h=[?1h=[?25l[39;49m(B[m[H[2J[28;54H(B[0;7m[ Reading... ](B[m[28;52H(B[0;7m[ Read 340 lines ](B[m[H(B[0;7m  GNU nano 4.8                                    env_setup_g3_d_50int.py                                               [1;119H(B[m[29d(B[0;7m^G(B[m Get Help    (B[0;7m^O(B[m Write Out   (B[0;7m^W(B[m Where Is    (B[0;7m^K(B[m Cut Text    (B[0;7m^J(B[m Justify     (B[0;7m^C(B[m Cur Pos     (B[0;7mM-U(B[m Undo[106G(B[0;7mM-A(B[m Mark Text[30d(B[0;7m^X(B[m Exit[30;16H(B[0;7m^R(B[m Read File   (B[0;7m^\(B[m Replace     (B[0;7m^U(B[m Paste Text  (B[0;7m^T(B[m To Spell    (B[0;7m^_(B[m Go To Line  (B[0;7mM-E(B[m Redo[106G(B[0;7mM-6(B[m Copy Text[28d[2d(B[0;1m[31m# Reset the env state back to initial, after each episode.[3d[36mimport[39m(B[m gymnasium (B[0;1m[36mas[39m(B[m gym[4d(B[0;1m[36mfrom[39m(B[m gymnasium (B[0;1m[36mimport[39m(B[m spaces[5d(B[0;1m[36mfrom[39m(B[m statistics (B[0;1m[36mimport[39m(B[m median[6d(B[0;1m[36mimport[39m(B[m numpy (B[0;1m[36mas[39m(B[m np[7d(B[0;1m[36mimport[39m(B[m mujoco_py[8d(B[0;1m[36mimport[39m(B[m os[9d(B[0;1m[36mimport[39m(B[m torch[10d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.vec_env (B[0;1m[36mimport[39m(B[m DummyVecEnv, VecMonitor[11d(B[0;1m[36mfrom[39m(B[m sb3_contrib (B[0;1m[36mimport[39m(B[m RecurrentPPO[12d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.callbacks (B[0;1m[36mimport[39m(B[m BaseCallback[13d(B[0;1m[36mimport[39m(B[m logging[14d(B[0;1m[36mimport[39m(B[m matplotlib.pyplot (B[0;1m[36mas[39m(B[m plt (B[0;1m[31m # Import matplotlib for plotting[16d# Configure logging[17d[39m(B[mlogging.basicConfig(level=logging.INFO)[19d(B[0;1m[36mclass[39m(B[m HandEnv(gym.Env):[20;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self):[21;9Hsuper(HandEnv, self).__init__()[22;9Hxml_path = os.path.expanduser((B[0;1m[32m'/home/easgrad/dgusain/Bot_hand/bot_hand.xml'[39m(B[m)[23;9Hlogging.info(f(B[0;1m[32m"Attempting to load Mujoco model from: {xml_path}"[39m(B[m)[25;9H(B[0;1m[36mif[39m(B[m (B[0;1m[36mnot[39m(B[m os.path.isfile(xml_path):[26;13Hlogging.error(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[27;13H(B[0;1m[36mraise[39m(B[m FileNotFoundError(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[2d[?12l[?25h[?25l[3d[?12l[?25h[?25l[4d[?12l[?25h[?25l[5d[?12l[?25h[?25l[6d[?12l[?25h[?25l[7d[?12l[?25h[?25l[8d[?12l[?25h[?25l[9d[?12l[?25h[?25l[10d[?12l[?25h[?25l[11d[?12l[?25h[?25l[12d[?12l[?25h[?25l[13d[?12l[?25h[?25l[14d[?12l[?25h[?25l[15d[?12l[?25h[?25l[16d[?12l[?25h[?25l[17d[?12l[?25h[?25l[18d[?12l[?25h[?25l[19d[?12l[?25h[?25l[20d[?12l[?25h[?25l[21d[?12l[?25h[?25l[22d[?12l[?25h[?25l[23d[?12l[?25h[?25l[24d[?12l[?25h[?25l[25d[?12l[?25h[?25l[26d[?12l[?25h[?25l[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[K[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hself.model = mujoco_py.load_model_from_path(xml_path)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hself.sim = mujoco_py.MjSim(self.model)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hlogging.info((B[0;1m[32m"Mujoco model loaded successfully."[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hlogging.error(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mraise[39m(B[m RuntimeError(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hjoint_weights = [[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H0.5, (B[0;1m[31m # ID 0: WristJoint1[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H0.5, (B[0;1m[31m # ID 1: WristJoint0[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 2: ForeFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.5, (B[0;1m[31m # ID 3: ForeFingerJoint2 (MCP proximal)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.0, (B[0;1m[31m # ID 4: ForeFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.0, (B[0;1m[31m # ID 5: ForeFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 6: MiddleFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.5, (B[0;1m[31m # ID 7: MiddleFingerJoint2 (MCP proximal)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.0, (B[0;1m[31m # ID 8: MiddleFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.0, (B[0;1m[31m # ID 9: MiddleFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 10: RingFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.5, (B[0;1m[31m # ID 11: RingFingerJoint2 (MCP proximal)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.0, (B[0;1m[31m # ID 12: RingFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.0, (B[0;1m[31m # ID 13: RingFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.5, (B[0;1m[31m # ID 14: LittleFingerJoint4 (CMC Joint rotation)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 15: LittleFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.5, (B[0;1m[31m # ID 16: LittleFingerJoint2 (MCP proximal)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.0, (B[0;1m[31m # ID 17: LittleFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.0, (B[0;1m[31m # ID 18: LittleFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.5, (B[0;1m[31m # ID 19: ThumbJoint4 (CMC Joint abduction/adduction)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H1.5, (B[0;1m[31m # ID 20: ThumbJoint3 (CMC Joint flexion/extension)[27;13H[39m(B[m1.5, (B[0;1m[31m # ID 21: ThumbJoint2 (CMC Joint rotation)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 22: ThumbJoint1 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H0.5, (B[0;1m[31m # ID 23: ThumbJoint0 (IP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H0.0  (B[0;1m[31m # ID 24: None[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Convertiing joint weights to a tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.joint_weights = torch.tensor(joint_weights, dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Normalize the weights so that their sum equals 1[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.joint_weights /= self.joint_weights.sum()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.initial_state = self.sim.get_state()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Define observation and action spaces[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(24,), dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.target_threshold = 99[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.max_steps = 100[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.steps_taken = 0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Initialize actuator ranges[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hactuator_ranges = self.sim.model.actuator_ctrlrange[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.actuator_min = torch.tensor(actuator_ranges[:, 0], dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.actuator_max = torch.tensor(actuator_ranges[:, 1], dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Create normalized action space between [-1, 1][39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.action_space = spaces.Box(low=-1, high=1, shape=(self.actuator_min.shape[0],), dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Precompute ground truth quaternions as a tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.ground_truth_quats = torch.tensor(self.get_ground_truth_quaternion(), dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m reset[39m(B[m(self, seed=(B[0;1m[35mNone[39m(B[m, options=(B[0;1m[35mNone[39m(B[m):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hsuper().reset(seed=seed)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m #self.sim.reset()[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.sim.set_state(self.initial_state)[27;9Hself.sim.forward()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.steps_taken = 0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hobs = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m obs, {}[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m step[39m(B[m(self, action: np.ndarray):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Rescale action using tensor operations for efficiency[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Haction_tensor = torch.from_numpy(action).float()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # adding some Gaussian noise to the action to avoid getting stuck in local optima[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m #noise = torch.normal(mean=0, std=0.05, size=action_tensor.size())[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m #action_tensor += noise[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hrescaled_action = self.actuator_min + (action_tensor + 1) * (self.actuator_max - self.actuator_min) / 2[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # rescaled_action[0:2] = 0[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.sim.data.ctrl[:] = rescaled_action.numpy()[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.sim.step()[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Fetch state as a tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hstate = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[42m        [49m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Calculate done condition[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hdone = self.calculate_done(state, flag=(B[0;1m[35mFalse[39m(B[m) (B[0;1m[36mor[39m(B[m self.steps_taken >= self.max_steps[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m done:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hreward = self.calculate_reward(state, flag=(B[0;1m[35mTrue[39m(B[m).item()[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36melse[39m(B[m:[27dreward = -1.0 (B[0;1m[31m # Penalize extra steps[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.steps_taken += 1[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Htruncated = (B[0;1m[35mFalse[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hinfo = {}[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m state.numpy(), reward, done, truncated, info[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m calculate_done[39m(B[m(self, state: torch.Tensor, flag: bool) -> bool:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m flag:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mFalse[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hconfidence = self.calculate_confidence(state)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m confidence >= self.target_threshold[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m calculate_reward[39m(B[m(self, state: torch.Tensor, flag: bool) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m flag:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hconfidence = self.calculate_confidence(state)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mreturn[39m(B[m confidence - 50 (B[0;1m[31m # Reward calculation[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36melse[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mif[39m(B[m confidence > 80:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17H(B[0;1m[36mreturn[39m(B[m (confidence-50)/2.0(B[0;1m[31m # encouraging the model in the right direction[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36melse[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17H(B[0;1m[36mreturn[39m(B[m (confidence-50)/4.0(B[0;1m[31m # lesser reward[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m calculate_confidence[39m(B[m(self, state: torch.Tensor) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hrendered_quat = self.get_rendered_pose_quaternion()[27;9Hrendered_quat = self.normalize_quaternion(rendered_quat)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hgt_quat = self.normalize_quaternion(self.ground_truth_quats)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hsimilarity = torch.abs(torch.sum(rendered_quat * gt_quat, dim=1)) * 100[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hweighted_similarity = similarity * self.joint_weights[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Havg_confidence = weighted_similarity.sum()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m avg_confidence[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m get_rendered_pose_quaternion[39m(B[m(self) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hquaternions = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mfor[39m(B[m joint (B[0;1m[36min[39m(B[m range(self.model.njnt):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hq = self.sim.data.qpos[self.model.jnt_qposadr[joint]:self.model.jnt_qposadr[joint] + 4][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hquaternions.append(q)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Convert list of arrays to a single NumPy array[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hquaternions_np = np.array(quaternions, dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Convert NumPy array to PyTorch tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m torch.from_numpy(quaternions_np)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m normalize_quaternion[39m(B[m(self, q: torch.Tensor) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hnorm = torch.norm(q, dim=1, keepdim=(B[0;1m[35mTrue[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m q / norm.clamp(min=1e-8) (B[0;1m[31m # Prevent division by zero[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m get_ground_truth_quaternion[39m(B[m(self) -> np.ndarray:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hground_truth_quats = [[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[6.32658406e-04,  1.25473697e-03, -9.99718591e-03,  1.56702220e+00],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.25473697e-03, -9.99718591e-03,  1.56702220e+00,  1.56730258e+00],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[-0.00999719,  1.5670222,   1.56730258,  1.5674143],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.5670222,   1.56730258,  1.5674143,  -0.00999801],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56730258,  1.5674143,  -0.00999801,  1.56701926],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.5674143,  -0.00999801,  1.56701926,  1.56730194],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[-0.00999801,  1.56701926,  1.56730194,  1.5674143],[27;13H[1.56701926,  1.56730194,  1.5674143,  -0.00999756],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56730194,  1.5674143,  -0.00999756,  1.56701717],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.5674143,  -0.00999756,  1.56701717,  1.56730177],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[-0.00999756,  1.56701717,  1.56730177,  1.56741429],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56701717, 1.56730177, 1.56741429, 0.00868252],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56730177,  1.56741429,  0.00868252, -0.01000805],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56741429,  0.00868252, -0.01000805,  1.56708349],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[0.00868252, -0.01000805,  1.56708349,  1.56730911],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[-0.01000805,  1.56708349,  1.56730911,  1.56741508],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56708349, 1.56730911, 1.56741508, 0.49916711],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56730911, 1.56741508, 0.49916711, 0.50022545],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56741508, 0.49916711, 0.50022545, 0.25330455],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[4.99167109e-01, 5.00225452e-01, 2.53304550e-01, 4.08440608e-04],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[5.00225452e-01,  2.53304550e-01,  4.08440608e-04, -9.00000689e-01],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[2.53304550e-01,  4.08440608e-04, -9.00000689e-01,  1.00000000e-01],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[4.08440608e-04, -9.00000689e-01,  1.00000000e-01, -1.00000000e-01],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[-0.90000069,  0.1,[27;40H-0.1,[27;54H0.01463168],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[0.1,[27;26H-0.1,[27;40H0.01463168,  1.0][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m np.array(ground_truth_quats, dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mclass[39m(B[m RewardCallback(BaseCallback):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m    Custom callback for logging average rewards over every 100 episodes, episode numbers, and iteration numbers.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m    Also stores average rewards for plotting.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m    """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self, avg_interval=50):[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hsuper(RewardCallback, self).__init__()[27;9Hself.episode_num = 0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.max_reward = -np.inf[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.avg_interval = avg_interval (B[0;1m[31m # Number of episodes per average[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.sum_rewards = 0.0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.count_rewards = 0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.avg_rewards = {} (B[0;1m[31m # Dictionary to store average rewards, key: block_num[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.iteration_num = 0    (B[0;1m[31m # Counter for iterations[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.median_rewards = {}[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _on_rollout_end[39m(B[m(self) -> bool:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Called at the end of a rollout.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Used to track the number of iterations.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.iteration_num += 1[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _on_step[39m(B[m(self) -> bool:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Called at every step. Checks if any episode has finished and logs the reward.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Retrieve 'dones' and 'rewards' from the current step[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hdones = self.locals.get((B[0;1m[32m'dones'[39m(B[m, [])[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hrewards = self.locals.get((B[0;1m[32m'rewards'[39m(B[m, [])[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Check if any of the environments are done[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m np.any(dones):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mfor[39m(B[m done, reward (B[0;1m[36min[39m(B[m zip(dones, rewards):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17H(B[0;1m[36mif[39m(B[m done:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;21Hself.episode_num += 1[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;21H(B[0;1m[36mif[39m(B[m reward > self.max_reward:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hself.max_reward = reward[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;21Hself.rewards_list.append(reward)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;21H(B[0;1m[36mif[39m(B[m len(self.rewards_list) == self.avg_interval:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;25Hmedian_reward = median(self.rewards_list)[27;25Hblock_num = self.episode_num // self.avg_interval[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hself.median_rewards[block_num] = median_reward[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hprint(f(B[0;1m[32m"Iteration: {self.iteration_num} | Episodes: {block_num * self.avg_interval} | Median Re[39m(B[0;7m>[27;1H(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;24H(B[0;1m[31m # Reset sum and count[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _on_training_end[39m(B[m(self) -> (B[0;1m[35mNone[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Called at the end of training. If there are remaining episodes, compute and store the average.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m len(self.rewards_list) > 0:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hmedian_reward = median(self.rewards_list)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hblock_num = self.episode_num // self.avg_interval + 1[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hself.median_rewards[block_num] = median_reward[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hcurrent_max = max(self.rewards_list)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mif[39m(B[m current_max > self.max_reward:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17Hself.max_reward = current_max[42m           [49m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hprint(f(B[0;1m[32m"Training End | Episodes: {self.episode_num} | Median Reward: {median_reward:.2f} | Max Reward: {sel[39m(B[0;7m>[27;1H(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mdef[34m make_env[39m(B[m():[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[32m"""Utility function to create a HandEnv without Monitor."""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _init[39m(B[m():[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Henv = HandEnv()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mreturn[39m(B[m env[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hlogging.error(f(B[0;1m[32m"Failed to initialize HandEnv: {e}"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mraise[39m(B[m e[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mreturn[39m(B[m _init[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mdef[34m train_on_gpu[39m(B[m(gpu_id: int, num_envs: int, total_timesteps: int, num_steps: int, save_interval: int):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hos.environ[(B[0;1m[32m'CUDA_VISIBLE_DEVICES'[39m(B[m] = str(gpu_id)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hdevice = (B[0;1m[32m'cuda'[39m(B[m (B[0;1m[36mif[39m(B[m torch.cuda.is_available() (B[0;1m[36melse[39m(B[m (B[0;1m[32m'cpu'[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hprint(f(B[0;1m[32m"GPU {gpu_id}: Using {device} device"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Choose between SubprocVecEnv and DummyVecEnv based on your needs[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # env = SubprocVecEnv([make_env() for _ in range(num_envs)])[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Henv = DummyVecEnv([make_env() (B[0;1m[36mfor[39m(B[m _ (B[0;1m[36min[39m(B[m range(num_envs)]) (B[0;1m[31m # Use DummyVecEnv for testing[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Henv = VecMonitor(env) (B[0;1m[31m # Use VecMonitor instead of individual Monitor wrappers[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Initialize the RecurrentPPO model with optimized parameters[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hmodel = RecurrentPPO([?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m'MlpLstmPolicy'[39m(B[m,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Henv,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hverbose=1,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hdevice=device,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hent_coef=0.05,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hlearning_rate=0.0001,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hclip_range=0.4,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hn_steps=num_steps,[33G(B[0;1m[31m # Steps per environment per update[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hbatch_size=4096,[27;34H(B[0;1m[31m # Increased batch size[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hgamma=0.99,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hgae_lambda=0.95,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hmax_grad_norm=0.5,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hvf_coef=0.5,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Huse_sde=(B[0;1m[35mTrue[39m(B[m,[27;33H(B[0;1m[31m # Use State Dependent Exploration for better exploration[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5Hcallback = RewardCallback(avg_interval=50)[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Train the model with the specified total timesteps[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hmodel.learn(total_timesteps=total_timesteps, callback=callback, log_interval=10)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hlogging.error(f(B[0;1m[32m"Error during training: {e}"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mraise[39m(B[m e[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Save the final model[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hmodel.save(f(B[0;1m[32m"/home/easgrad/dgusain/Bot_hand/agents/agent_g3d_{gpu_id}_50int_weighted"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mreturn[39m(B[m callback (B[0;1m[31m # Return the callback to access episode_rewards[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mdef[34m main[39m(B[m():[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hgpu_id = 3[27;24H(B[0;1m[31m # Set your GPU ID here[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hnum_envs = 6[24G(B[0;1m[31m # Number of parallel environments (adjust based on your GPU memory)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hn_iter = 100[27;25H(B[0;1m[31m # Number of iterations you want to run[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hn_steps = 1024     (B[0;1m[31m # Steps per environment per update[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Htotal_timesteps = n_iter * n_steps * num_envs (B[0;1m[31m # Total training timesteps[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hsave_interval = 25 (B[0;1m[31m # Not used in this optimized version[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Train the model and get the callback with rewards data[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hcallback = train_on_gpu(gpu_id, num_envs, total_timesteps, n_steps, save_interval)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Extract block numbers and average rewards from the callback[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hblock_numbers = list(callback.median_rewards.keys())[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hblock_rewards = list(callback.median_rewards.values())[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Calculate the corresponding episode numbers (100, 200, 300, ...)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hepisode_numbers = [block_num * 100 (B[0;1m[36mfor[39m(B[m block_num (B[0;1m[36min[39m(B[m block_numbers][?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[13d[?12l[?25h[?25l[14d[?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25ln[?12l[?25h[?25l_[?12l[?25h[?25li[?12l[?25h[?25lt[?12l[?25h[?25le[?12l[?25h[?25lr[?12l[?25h[?25l [?12l[?25h[?25l=[?12l[?25h[?25l [?12l[?25h[?25l1[?12l[?25h[?25l0[?12l[?25h[?25l0[?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[1;111H(B[0;7mModified(B[m[14;15H000[25G (B[0;1m[31m # Number of iterations you want to run[14;18H[39m(B[m[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[28d(B[0;7mSave modified buffer?                                                                                                   [29;1H Y(B[m Yes[K[30d(B[0;7m N(B[m No  [30;16H (B[0;7m^C(B[m Cancel[K[28;23H[?12l[?25h[?25l[29d(B[0;7m^G(B[m Get Help[29;31H(B[0;7mM-D(B[m DOS Format[29;61H(B[0;7mM-A(B[m Append[29;91H(B[0;7mM-B(B[m Backup File[30d(B[0;7m^C(B[m Cancel[17G[14X[30;31H(B[0;7mM-M(B[m Mac Format[30;61H(B[0;7mM-P(B[m Prepend[30;91H(B[0;7m^T(B[m To Files[28d(B[0;7mFile Name to Write: env_setup_g3_d_50int.py(B[m[28;44H[?12l[?25h[?25l[28;53H[1K (B[0;7m[ Writing... ](B[m[K[1;111H(B[0;7m        (B[m[28;51H(B[0;7m[ Wrote 340 lines ](B[m[J[30d[?12l[?25h[30;1H[?1049l[23;0;0t[?1l>[?2004l(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> pytho[K[K[K[K[Knano env_setup_g3_d_50int.py[43G[2@python[73G
GPU 3: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
Iteration: 0 | Episodes: 50 | Median Reward: 20.96 | Max Reward: 30.08
Iteration: 1 | Episodes: 100 | Median Reward: 26.52 | Max Reward: 37.23
Iteration: 2 | Episodes: 150 | Median Reward: 26.34 | Max Reward: 37.23
Iteration: 3 | Episodes: 200 | Median Reward: 23.32 | Max Reward: 37.23
Iteration: 4 | Episodes: 250 | Median Reward: 23.32 | Max Reward: 37.23
Iteration: 4 | Episodes: 300 | Median Reward: 17.95 | Max Reward: 37.23
Iteration: 5 | Episodes: 350 | Median Reward: 8.61 | Max Reward: 37.23
Iteration: 6 | Episodes: 400 | Median Reward: 21.50 | Max Reward: 37.23
Iteration: 7 | Episodes: 450 | Median Reward: 22.91 | Max Reward: 38.17
Iteration: 8 | Episodes: 500 | Median Reward: 22.05 | Max Reward: 38.17
Iteration: 9 | Episodes: 550 | Median Reward: 14.61 | Max Reward: 38.17
Iteration: 9 | Episodes: 600 | Median Reward: 20.91 | Max Reward: 38.17
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -81.8     |
| time/                   |           |
|    fps                  | 346       |
|    iterations           | 10        |
|    time_elapsed         | 177       |
|    total_timesteps      | 61440     |
| train/                  |           |
|    approx_kl            | 0.4632191 |
|    clip_fraction        | 0.268     |
|    clip_range           | 0.4       |
|    entropy_loss         | -45       |
|    explained_variance   | -0.0434   |
|    learning_rate        | 0.0001    |
|    loss                 | 85.4      |
|    n_updates            | 90        |
|    policy_gradient_loss | 0.0962    |
|    std                  | 1         |
|    value_loss           | 179       |
---------------------------------------
Iteration: 10 | Episodes: 650 | Median Reward: 13.80 | Max Reward: 38.17
Iteration: 11 | Episodes: 700 | Median Reward: 15.09 | Max Reward: 38.17
Iteration: 12 | Episodes: 750 | Median Reward: 18.16 | Max Reward: 38.17
Iteration: 13 | Episodes: 800 | Median Reward: 15.29 | Max Reward: 38.17
Iteration: 14 | Episodes: 850 | Median Reward: 16.79 | Max Reward: 38.17
Iteration: 14 | Episodes: 900 | Median Reward: 20.15 | Max Reward: 38.61
Iteration: 15 | Episodes: 950 | Median Reward: 15.42 | Max Reward: 38.61
Iteration: 16 | Episodes: 1000 | Median Reward: 17.13 | Max Reward: 38.61
Iteration: 17 | Episodes: 1050 | Median Reward: 18.64 | Max Reward: 38.61
Iteration: 18 | Episodes: 1100 | Median Reward: 14.22 | Max Reward: 38.61
Iteration: 18 | Episodes: 1150 | Median Reward: 24.27 | Max Reward: 38.61
Iteration: 19 | Episodes: 1200 | Median Reward: 18.64 | Max Reward: 38.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.4       |
| time/                   |             |
|    fps                  | 332         |
|    iterations           | 20          |
|    time_elapsed         | 369         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.043601014 |
|    clip_fraction        | 0.0303      |
|    clip_range           | 0.4         |
|    entropy_loss         | -54.7       |
|    explained_variance   | -0.0238     |
|    learning_rate        | 0.0001      |
|    loss                 | 96.3        |
|    n_updates            | 190         |
|    policy_gradient_loss | 0.000616    |
|    std                  | 1           |
|    value_loss           | 186         |
-----------------------------------------
Iteration: 20 | Episodes: 1250 | Median Reward: 18.53 | Max Reward: 38.61
Iteration: 21 | Episodes: 1300 | Median Reward: 20.29 | Max Reward: 38.61
Iteration: 22 | Episodes: 1350 | Median Reward: 21.89 | Max Reward: 38.61
Iteration: 23 | Episodes: 1400 | Median Reward: 23.84 | Max Reward: 38.61
Iteration: 23 | Episodes: 1450 | Median Reward: 23.20 | Max Reward: 38.61
Iteration: 24 | Episodes: 1500 | Median Reward: 20.10 | Max Reward: 38.61
Iteration: 25 | Episodes: 1550 | Median Reward: 19.44 | Max Reward: 38.61
Iteration: 26 | Episodes: 1600 | Median Reward: 19.44 | Max Reward: 38.61
Iteration: 27 | Episodes: 1650 | Median Reward: 16.32 | Max Reward: 38.61
Iteration: 28 | Episodes: 1700 | Median Reward: 16.68 | Max Reward: 38.61
Iteration: 28 | Episodes: 1750 | Median Reward: 12.48 | Max Reward: 38.61
Iteration: 29 | Episodes: 1800 | Median Reward: 19.51 | Max Reward: 38.61
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -81.2      |
| time/                   |            |
|    fps                  | 335        |
|    iterations           | 30         |
|    time_elapsed         | 549        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.00789043 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -61.9      |
|    explained_variance   | -0.0144    |
|    learning_rate        | 0.0001     |
|    loss                 | 91.6       |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.00448   |
|    std                  | 1.01       |
|    value_loss           | 184        |
----------------------------------------
Iteration: 30 | Episodes: 1850 | Median Reward: 16.20 | Max Reward: 38.61
Iteration: 31 | Episodes: 1900 | Median Reward: 12.56 | Max Reward: 38.61
Iteration: 32 | Episodes: 1950 | Median Reward: 14.79 | Max Reward: 38.61
Iteration: 32 | Episodes: 2000 | Median Reward: 15.01 | Max Reward: 38.61
Iteration: 33 | Episodes: 2050 | Median Reward: 13.70 | Max Reward: 38.61
Iteration: 34 | Episodes: 2100 | Median Reward: 16.53 | Max Reward: 38.61
Iteration: 35 | Episodes: 2150 | Median Reward: 21.30 | Max Reward: 38.69
Iteration: 36 | Episodes: 2200 | Median Reward: 23.10 | Max Reward: 38.89
Iteration: 36 | Episodes: 2250 | Median Reward: 25.17 | Max Reward: 38.89
Iteration: 37 | Episodes: 2300 | Median Reward: 19.37 | Max Reward: 41.12
Iteration: 38 | Episodes: 2350 | Median Reward: 13.99 | Max Reward: 41.12
Iteration: 39 | Episodes: 2400 | Median Reward: 14.39 | Max Reward: 41.12
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -82.5       |
| time/                   |             |
|    fps                  | 336         |
|    iterations           | 40          |
|    time_elapsed         | 730         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.009955734 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -68.4       |
|    explained_variance   | -0.0128     |
|    learning_rate        | 0.0001      |
|    loss                 | 74.2        |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.000284    |
|    std                  | 1.01        |
|    value_loss           | 158         |
-----------------------------------------
Iteration: 40 | Episodes: 2450 | Median Reward: 20.58 | Max Reward: 41.12
Iteration: 41 | Episodes: 2500 | Median Reward: 16.23 | Max Reward: 41.12
Iteration: 41 | Episodes: 2550 | Median Reward: 18.17 | Max Reward: 41.12
Iteration: 42 | Episodes: 2600 | Median Reward: 12.68 | Max Reward: 41.12
Iteration: 43 | Episodes: 2650 | Median Reward: 24.96 | Max Reward: 41.39
Iteration: 44 | Episodes: 2700 | Median Reward: 24.96 | Max Reward: 41.39
Iteration: 45 | Episodes: 2750 | Median Reward: 25.04 | Max Reward: 41.39
Iteration: 46 | Episodes: 2800 | Median Reward: 19.26 | Max Reward: 41.39
Iteration: 46 | Episodes: 2850 | Median Reward: 13.23 | Max Reward: 41.39
Iteration: 47 | Episodes: 2900 | Median Reward: 13.46 | Max Reward: 41.39
Iteration: 48 | Episodes: 2950 | Median Reward: 19.84 | Max Reward: 41.39
Iteration: 49 | Episodes: 3000 | Median Reward: 20.42 | Max Reward: 41.39
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.7        |
| time/                   |              |
|    fps                  | 336          |
|    iterations           | 50           |
|    time_elapsed         | 912          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.0020279726 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -72.4        |
|    explained_variance   | -0.00698     |
|    learning_rate        | 0.0001       |
|    loss                 | 82.5         |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.000717    |
|    std                  | 1.01         |
|    value_loss           | 172          |
------------------------------------------
Iteration: 50 | Episodes: 3050 | Median Reward: 25.66 | Max Reward: 42.68
Iteration: 50 | Episodes: 3100 | Median Reward: 18.62 | Max Reward: 42.68
Iteration: 51 | Episodes: 3150 | Median Reward: 6.84 | Max Reward: 42.68
Iteration: 52 | Episodes: 3200 | Median Reward: 18.76 | Max Reward: 42.68
Iteration: 53 | Episodes: 3250 | Median Reward: 21.23 | Max Reward: 42.68
Iteration: 54 | Episodes: 3300 | Median Reward: 22.69 | Max Reward: 42.68
Iteration: 55 | Episodes: 3350 | Median Reward: 25.73 | Max Reward: 42.68
Iteration: 55 | Episodes: 3400 | Median Reward: 29.00 | Max Reward: 42.68
Iteration: 56 | Episodes: 3450 | Median Reward: 11.70 | Max Reward: 42.68
Iteration: 57 | Episodes: 3500 | Median Reward: 13.73 | Max Reward: 42.68
^X^CTraceback (most recent call last):
  File "env_setup_g3_d_50int.py", line 339, in <module>
    main()
  File "env_setup_g3_d_50int.py", line 318, in main
    callback = train_on_gpu(gpu_id, num_envs, total_timesteps, n_steps, save_interval)
  File "env_setup_g3_d_50int.py", line 299, in train_on_gpu
    model.learn(total_timesteps=total_timesteps, callback=callback, log_interval=10)
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py", line 454, in learn
    return super().learn(
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 313, in learn
    self.train()
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py", line 336, in train
    for rollout_data in self.rollout_buffer.get(self.batch_size):
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/common/recurrent/buffers.py", line 195, in get
    yield self._get_samples(batch_inds, env_change)
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/common/recurrent/buffers.py", line 236, in _get_samples
    advantages=self.pad_and_flatten(self.advantages[batch_inds]),
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/common/recurrent/buffers.py", line 60, in pad_and_flatten
    return pad(seq_start_indices, seq_end_indices, device, tensor, padding_value).flatten()
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/common/recurrent/buffers.py", line 36, in pad
    seq = [th.tensor(tensor[start : end + 1], device=device) for start, end in zip(seq_start_indices, seq_end_indices)]
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/common/recurrent/buffers.py", line 36, in <listcomp>
    seq = [th.tensor(tensor[start : end + 1], device=device) for start, end in zip(seq_start_indices, seq_end_indices)]
KeyboardInterrupt
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python env_setup_g3_d_50int.py[43G[2Pnano[71G
[?2004h[?1049h[22;0;0t[1;30r(B[m[4l[?7h[39;49m[?1h=[?1h=[?25l[39;49m(B[m[H[2J[28;54H(B[0;7m[ Reading... ](B[m[28;52H(B[0;7m[ Read 340 lines ](B[m[H(B[0;7m  GNU nano 4.8                                    env_setup_g3_d_50int.py                                               [1;119H(B[m[29d(B[0;7m^G(B[m Get Help    (B[0;7m^O(B[m Write Out   (B[0;7m^W(B[m Where Is    (B[0;7m^K(B[m Cut Text    (B[0;7m^J(B[m Justify     (B[0;7m^C(B[m Cur Pos     (B[0;7mM-U(B[m Undo[106G(B[0;7mM-A(B[m Mark Text[30d(B[0;7m^X(B[m Exit[30;16H(B[0;7m^R(B[m Read File   (B[0;7m^\(B[m Replace     (B[0;7m^U(B[m Paste Text  (B[0;7m^T(B[m To Spell    (B[0;7m^_(B[m Go To Line  (B[0;7mM-E(B[m Redo[106G(B[0;7mM-6(B[m Copy Text[28d[2d(B[0;1m[31m# Reset the env state back to initial, after each episode.[3d[36mimport[39m(B[m gymnasium (B[0;1m[36mas[39m(B[m gym[4d(B[0;1m[36mfrom[39m(B[m gymnasium (B[0;1m[36mimport[39m(B[m spaces[5d(B[0;1m[36mfrom[39m(B[m statistics (B[0;1m[36mimport[39m(B[m median[6d(B[0;1m[36mimport[39m(B[m numpy (B[0;1m[36mas[39m(B[m np[7d(B[0;1m[36mimport[39m(B[m mujoco_py[8d(B[0;1m[36mimport[39m(B[m os[9d(B[0;1m[36mimport[39m(B[m torch[10d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.vec_env (B[0;1m[36mimport[39m(B[m DummyVecEnv, VecMonitor[11d(B[0;1m[36mfrom[39m(B[m sb3_contrib (B[0;1m[36mimport[39m(B[m RecurrentPPO[12d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.callbacks (B[0;1m[36mimport[39m(B[m BaseCallback[13d(B[0;1m[36mimport[39m(B[m logging[14d(B[0;1m[36mimport[39m(B[m matplotlib.pyplot (B[0;1m[36mas[39m(B[m plt (B[0;1m[31m # Import matplotlib for plotting[16d# Configure logging[17d[39m(B[mlogging.basicConfig(level=logging.INFO)[19d(B[0;1m[36mclass[39m(B[m HandEnv(gym.Env):[20;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self):[21;9Hsuper(HandEnv, self).__init__()[22;9Hxml_path = os.path.expanduser((B[0;1m[32m'/home/easgrad/dgusain/Bot_hand/bot_hand.xml'[39m(B[m)[23;9Hlogging.info(f(B[0;1m[32m"Attempting to load Mujoco model from: {xml_path}"[39m(B[m)[25;9H(B[0;1m[36mif[39m(B[m (B[0;1m[36mnot[39m(B[m os.path.isfile(xml_path):[26;13Hlogging.error(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[27;13H(B[0;1m[36mraise[39m(B[m FileNotFoundError(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[2d[?12l[?25h[?25l[3d[?12l[?25h[?25l[5d[?12l[?25h[?25l[7d[?12l[?25h[?25l[8d[?12l[?25h[?25l[9d[?12l[?25h[?25l[10d[?12l[?25h[?25l[12d[?12l[?25h[?25l[13d[?12l[?25h[?25l[14d[?12l[?25h[?25l[15d[?12l[?25h[?25l[16d[?12l[?25h[?25l[17d[?12l[?25h[?25l[18d[?12l[?25h[?25l[19d[?12l[?25h[?25l[21d[?12l[?25h[?25l[22d[?12l[?25h[?25l[24d[?12l[?25h[?25l[25d[?12l[?25h[?25l[26d[?12l[?25h[?25l[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[K[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hself.model = mujoco_py.load_model_from_path(xml_path)[27;13Hself.sim = mujoco_py.MjSim(self.model)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hlogging.info((B[0;1m[32m"Mujoco model loaded successfully."[39m(B[m)[27;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hlogging.error(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H(B[0;1m[36mraise[39m(B[m RuntimeError(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hjoint_weights = [[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H0.5, (B[0;1m[31m # ID 0: WristJoint1[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H0.5, (B[0;1m[31m # ID 1: WristJoint0[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H3.0, (B[0;1m[31m # ID 2: ForeFingerJoint3 (MCP)[27;13H[39m(B[m2.5, (B[0;1m[31m # ID 3: ForeFingerJoint2 (MCP proximal)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H2.0, (B[0;1m[31m # ID 4: ForeFingerJoint1 (PIP)[27;13H[39m(B[m1.0, (B[0;1m[31m # ID 5: ForeFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 6: MiddleFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.5, (B[0;1m[31m # ID 7: MiddleFingerJoint2 (MCP proximal)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.0, (B[0;1m[31m # ID 8: MiddleFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.0, (B[0;1m[31m # ID 9: MiddleFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 10: RingFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.5, (B[0;1m[31m # ID 11: RingFingerJoint2 (MCP proximal)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.0, (B[0;1m[31m # ID 12: RingFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H1.0, (B[0;1m[31m # ID 13: RingFingerJoint0 (DIP)[27;13H[39m(B[m1.5, (B[0;1m[31m # ID 14: LittleFingerJoint4 (CMC Joint rotation)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 15: LittleFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H2.5, (B[0;1m[31m # ID 16: LittleFingerJoint2 (MCP proximal)[27;13H[39m(B[m2.0, (B[0;1m[31m # ID 17: LittleFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.0, (B[0;1m[31m # ID 18: LittleFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H1.5, (B[0;1m[31m # ID 19: ThumbJoint4 (CMC Joint abduction/adduction)[27;13H[39m(B[m1.5, (B[0;1m[31m # ID 20: ThumbJoint3 (CMC Joint flexion/extension)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H1.5, (B[0;1m[31m # ID 21: ThumbJoint2 (CMC Joint rotation)[27;13H[39m(B[m3.0, (B[0;1m[31m # ID 22: ThumbJoint1 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H0.5, (B[0;1m[31m # ID 23: ThumbJoint0 (IP)[27;13H[39m(B[m0.0  (B[0;1m[31m # ID 24: None[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Convertiing joint weights to a tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.joint_weights = torch.tensor(joint_weights, dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Normalize the weights so that their sum equals 1[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.joint_weights /= self.joint_weights.sum()[27d[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.initial_state = self.sim.get_state()[27;8H(B[0;1m[31m # Define observation and action spaces[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(24,), dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.target_threshold = 99[27;9Hself.max_steps = 100[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.steps_taken = 0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m # Initialize actuator ranges[27;9H[39m(B[mactuator_ranges = self.sim.model.actuator_ctrlrange[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.actuator_min = torch.tensor(actuator_ranges[:, 0], dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.actuator_max = torch.tensor(actuator_ranges[:, 1], dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Create normalized action space between [-1, 1][39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.action_space = spaces.Box(low=-1, high=1, shape=(self.actuator_min.shape[0],), dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m # Precompute ground truth quaternions as a tensor[27;9H[39m(B[mself.ground_truth_quats = torch.tensor(self.get_ground_truth_quaternion(), dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[36mdef[34m reset[39m(B[m(self, seed=(B[0;1m[35mNone[39m(B[m, options=(B[0;1m[35mNone[39m(B[m):[27;9Hsuper().reset(seed=seed)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m #self.sim.reset()[27;9H[39m(B[mself.sim.set_state(self.initial_state)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.sim.forward()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.steps_taken = 0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hobs = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m obs, {}[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m step[39m(B[m(self, action: np.ndarray):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Rescale action using tensor operations for efficiency[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Haction_tensor = torch.from_numpy(action).float()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # adding some Gaussian noise to the action to avoid getting stuck in local optima[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m #noise = torch.normal(mean=0, std=0.05, size=action_tensor.size())[27;8H #action_tensor += noise[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hrescaled_action = self.actuator_min + (action_tensor + 1) * (self.actuator_max - self.actuator_min) / 2[27;8H(B[0;1m[31m # rescaled_action[0:2] = 0[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.sim.data.ctrl[:] = rescaled_action.numpy()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.sim.step()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m # Fetch state as a tensor[27;9H[39m(B[mstate = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;1H[42m        [27d(B[0;1m[31m # Calculate done condition[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hdone = self.calculate_done(state, flag=(B[0;1m[35mFalse[39m(B[m) (B[0;1m[36mor[39m(B[m self.steps_taken >= self.max_steps[27;9H(B[0;1m[36mif[39m(B[m done:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hreward = self.calculate_reward(state, flag=(B[0;1m[35mTrue[39m(B[m).item()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36melse[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hreward = -1.0 (B[0;1m[31m # Penalize extra steps[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.steps_taken += 1[27;9Htruncated = (B[0;1m[35mFalse[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hinfo = {}[27d[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mreturn[39m(B[m state.numpy(), reward, done, truncated, info[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m calculate_done[39m(B[m(self, state: torch.Tensor, flag: bool) -> bool:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mif[39m(B[m flag:[27;13H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mFalse[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hconfidence = self.calculate_confidence(state)[27;9H(B[0;1m[36mreturn[39m(B[m confidence >= self.target_threshold[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[36mdef[34m calculate_reward[39m(B[m(self, state: torch.Tensor, flag: bool) -> torch.Tensor:[27;9H(B[0;1m[36mif[39m(B[m flag:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hconfidence = self.calculate_confidence(state)[27;13H(B[0;1m[36mreturn[39m(B[m confidence - 50 (B[0;1m[31m # Reward calculation[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36melse[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mif[39m(B[m confidence > 80:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17H(B[0;1m[36mreturn[39m(B[m (confidence-50)/2.0(B[0;1m[31m # encouraging the model in the right direction[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36melse[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17H(B[0;1m[36mreturn[39m(B[m (confidence-50)/4.0(B[0;1m[31m # lesser reward[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[36mdef[34m calculate_confidence[39m(B[m(self, state: torch.Tensor) -> torch.Tensor:[27;9Hrendered_quat = self.get_rendered_pose_quaternion()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hrendered_quat = self.normalize_quaternion(rendered_quat)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hgt_quat = self.normalize_quaternion(self.ground_truth_quats)[27;9Hsimilarity = torch.abs(torch.sum(rendered_quat * gt_quat, dim=1)) * 100[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hweighted_similarity = similarity * self.joint_weights[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Havg_confidence = weighted_similarity.sum()[27;9H(B[0;1m[36mreturn[39m(B[m avg_confidence[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[36mdef[34m get_rendered_pose_quaternion[39m(B[m(self) -> torch.Tensor:[27;9Hquaternions = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mfor[39m(B[m joint (B[0;1m[36min[39m(B[m range(self.model.njnt):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hq = self.sim.data.qpos[self.model.jnt_qposadr[joint]:self.model.jnt_qposadr[joint] + 4][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hquaternions.append(q)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Convert list of arrays to a single NumPy array[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hquaternions_np = np.array(quaternions, dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m # Convert NumPy array to PyTorch tensor[27;9H[36mreturn[39m(B[m torch.from_numpy(quaternions_np)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;5H(B[0;1m[36mdef[34m normalize_quaternion[39m(B[m(self, q: torch.Tensor) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hnorm = torch.norm(q, dim=1, keepdim=(B[0;1m[35mTrue[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mreturn[39m(B[m q / norm.clamp(min=1e-8) (B[0;1m[31m # Prevent division by zero[27d[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m get_ground_truth_quaternion[39m(B[m(self) -> np.ndarray:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hground_truth_quats = [[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[6.32658406e-04,  1.25473697e-03, -9.99718591e-03,  1.56702220e+00],[27;13H[1.25473697e-03, -9.99718591e-03,  1.56702220e+00,  1.56730258e+00],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[-0.00999719,  1.5670222,   1.56730258,  1.5674143],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[1.5670222,   1.56730258,  1.5674143,  -0.00999801],[27;13H[1.56730258,  1.5674143,  -0.00999801,  1.56701926],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[1.5674143,  -0.00999801,  1.56701926,  1.56730194],[27;13H[-0.00999801,  1.56701926,  1.56730194,  1.5674143],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56701926,  1.56730194,  1.5674143,  -0.00999756],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56730194,  1.5674143,  -0.00999756,  1.56701717],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.5674143,  -0.00999756,  1.56701717,  1.56730177],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[-0.00999756,  1.56701717,  1.56730177,  1.56741429],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[1.56701717, 1.56730177, 1.56741429, 0.00868252],[27;13H[1.56730177,  1.56741429,  0.00868252, -0.01000805],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[1.56741429,  0.00868252, -0.01000805,  1.56708349],[27;13H[0.00868252, -0.01000805,  1.56708349,  1.56730911],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[-0.01000805,  1.56708349,  1.56730911,  1.56741508],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56708349, 1.56730911, 1.56741508, 0.49916711],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56730911, 1.56741508, 0.49916711, 0.50022545],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[1.56741508, 0.49916711, 0.50022545, 0.25330455],[27;13H[4.99167109e-01, 5.00225452e-01, 2.53304550e-01, 4.08440608e-04],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[5.00225452e-01,  2.53304550e-01,  4.08440608e-04, -9.00000689e-01],[27;13H[2.53304550e-01,  4.08440608e-04, -9.00000689e-01,  1.00000000e-01],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[4.08440608e-04, -9.00000689e-01,  1.00000000e-01, -1.00000000e-01],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[-0.90000069,  0.1,[27;40H-0.1,[27;54H0.01463168],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[0.1,[27;26H-0.1,[27;40H0.01463168,  1.0][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m np.array(ground_truth_quats, dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mclass[39m(B[m RewardCallback(BaseCallback):[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[32m"""[27d    Custom callback for logging average rewards over every 100 episodes, episode numbers, and iteration numbers.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;1H(B[0;1m[32m    Also stores average rewards for plotting.[27d    """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self, avg_interval=50):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hsuper(RewardCallback, self).__init__()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.episode_num = 0[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.max_reward = -np.inf[27;9Hself.avg_interval = avg_interval (B[0;1m[31m # Number of episodes per average[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.sum_rewards = 0.0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.count_rewards = 0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.avg_rewards = {} (B[0;1m[31m # Dictionary to store average rewards, key: block_num[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.iteration_num = 0    (B[0;1m[31m # Counter for iterations[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.median_rewards = {}[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _on_rollout_end[39m(B[m(self) -> bool:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Called at the end of a rollout.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;1H(B[0;1m[32m        Used to track the number of iterations.[27d        """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.iteration_num += 1[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[27d[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[36mdef[34m _on_step[39m(B[m(self) -> bool:[27;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Called at every step. Checks if any episode has finished and logs the reward.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Retrieve 'dones' and 'rewards' from the current step[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hdones = self.locals.get((B[0;1m[32m'dones'[39m(B[m, [])[?12l[?25h[?25l7[2;28r8[28d[3S[1;30r[25;9Hrewards = self.locals.get((B[0;1m[32m'rewards'[39m(B[m, [])[27;8H(B[0;1m[31m # Check if any of the environments are done[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m np.any(dones):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mfor[39m(B[m done, reward (B[0;1m[36min[39m(B[m zip(dones, rewards):[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;17H(B[0;1m[36mif[39m(B[m done:[27;21Hself.episode_num += 1[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;21H(B[0;1m[36mif[39m(B[m reward > self.max_reward:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hself.max_reward = reward[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;21Hself.rewards_list.append(reward)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;21H(B[0;1m[36mif[39m(B[m len(self.rewards_list) == self.avg_interval:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hmedian_reward = median(self.rewards_list)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hblock_num = self.episode_num // self.avg_interval[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;25Hself.median_rewards[block_num] = median_reward[27;25Hprint(f(B[0;1m[32m"Iteration: {self.iteration_num} | Episodes: {block_num * self.avg_interval} | Median Re[39m(B[0;7m>[27;1H(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;24H(B[0;1m[31m # Reset sum and count[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _on_training_end[39m(B[m(self) -> (B[0;1m[35mNone[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Called at the end of training. If there are remaining episodes, compute and store the average.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m len(self.rewards_list) > 0:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hmedian_reward = median(self.rewards_list)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hblock_num = self.episode_num // self.avg_interval + 1[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hself.median_rewards[block_num] = median_reward[27;13Hcurrent_max = max(self.rewards_list)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mif[39m(B[m current_max > self.max_reward:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17Hself.max_reward = current_max[42m           [49m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hprint(f(B[0;1m[32m"Training End | Episodes: {self.episode_num} | Median Reward: {median_reward:.2f} | Max Reward: {sel[39m(B[0;7m>[27;1H(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mdef[34m make_env[39m(B[m():[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[32m"""Utility function to create a HandEnv without Monitor."""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _init[39m(B[m():[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Henv = HandEnv()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mreturn[39m(B[m env[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hlogging.error(f(B[0;1m[32m"Failed to initialize HandEnv: {e}"[39m(B[m)[27;13H(B[0;1m[36mraise[39m(B[m e[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mreturn[39m(B[m _init[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mdef[34m train_on_gpu[39m(B[m(gpu_id: int, num_envs: int, total_timesteps: int, num_steps: int, save_interval: int):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hos.environ[(B[0;1m[32m'CUDA_VISIBLE_DEVICES'[39m(B[m] = str(gpu_id)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hdevice = (B[0;1m[32m'cuda'[39m(B[m (B[0;1m[36mif[39m(B[m torch.cuda.is_available() (B[0;1m[36melse[39m(B[m (B[0;1m[32m'cpu'[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hprint(f(B[0;1m[32m"GPU {gpu_id}: Using {device} device"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Choose between SubprocVecEnv and DummyVecEnv based on your needs[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # env = SubprocVecEnv([make_env() for _ in range(num_envs)])[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[3S[1;30r[25;5Henv = DummyVecEnv([make_env() (B[0;1m[36mfor[39m(B[m _ (B[0;1m[36min[39m(B[m range(num_envs)]) (B[0;1m[31m # Use DummyVecEnv for testing[26;5H[39m(B[menv = VecMonitor(env) (B[0;1m[31m # Use VecMonitor instead of individual Monitor wrappers[27d[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Initialize the RecurrentPPO model with optimized parameters[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hmodel = RecurrentPPO([?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m'MlpLstmPolicy'[39m(B[m,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Henv,[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hverbose=1,[27;9Hdevice=device,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hent_coef=0.05,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hlearning_rate=0.0001,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hclip_range=0.4,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hn_steps=num_steps,[33G(B[0;1m[31m # Steps per environment per update[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hbatch_size=4096,[27;34H(B[0;1m[31m # Increased batch size[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hgamma=0.99,[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hgae_lambda=0.95,[27;9Hmax_grad_norm=0.5,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hvf_coef=0.5,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Huse_sde=(B[0;1m[35mTrue[39m(B[m,[27;33H(B[0;1m[31m # Use State Dependent Exploration for better exploration[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hcallback = RewardCallback(avg_interval=50)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Train the model with the specified total timesteps[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hmodel.learn(total_timesteps=total_timesteps, callback=callback, log_interval=10)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[27;9Hlogging.error(f(B[0;1m[32m"Error during training: {e}"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mraise[39m(B[m e[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;4H(B[0;1m[31m # Save the final model[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hmodel.save(f(B[0;1m[32m"/home/easgrad/dgusain/Bot_hand/agents/agent_g3d_{gpu_id}_50int_weighted"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mreturn[39m(B[m callback (B[0;1m[31m # Return the callback to access episode_rewards[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mdef[34m main[39m(B[m():[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hgpu_id = 3[27;24H(B[0;1m[31m # Set your GPU ID here[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5Hnum_envs = 6[24G(B[0;1m[31m # Number of parallel environments (adjust based on your GPU memory)[27;5H[39m(B[mn_iter = 1000[27;26H(B[0;1m[31m # Number of iterations you want to run[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hn_steps = 1024     (B[0;1m[31m # Steps per environment per update[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5Htotal_timesteps = n_iter * n_steps * num_envs (B[0;1m[31m # Total training timesteps[27;5H[39m(B[msave_interval = 25 (B[0;1m[31m # Not used in this optimized version[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;4H(B[0;1m[31m # Train the model and get the callback with rewards data[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hcallback = train_on_gpu(gpu_id, num_envs, total_timesteps, n_steps, save_interval)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Extract block numbers and average rewards from the callback[39m(B[m[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l [?12l[?25h[?25l  [?12l[?25h[?25l [?12l[?25h[?25ln[?12l[?25h[?25l_[?12l[?25h[?25ls[?12l[?25h[?25lt[?12l[?25h[?25le[?12l[?25h[?25lp[?12l[?25h[?25ls[?12l[?25h[?25l [?12l[?25h[?25l=[?12l[?25h[?25l [?12l[?25h[?25l[A[?12l[?25h[?25l[1;111H(B[0;7mModified(B[m[19;14H[1P[?12l[?25h[?25l5000[25G (B[0;1m[31m # Number of iterations you want to run[19;15H[39m(B[m[?12l[?25h[?25l[28d(B[0;7mSave modified buffer?                                                                                                   [29;1H Y(B[m Yes[K[30d(B[0;7m N(B[m No  [30;16H (B[0;7m^C(B[m Cancel[K[28;23H[?12l[?25h[?25l[29d(B[0;7m^G(B[m Get Help[29;31H(B[0;7mM-D(B[m DOS Format[29;61H(B[0;7mM-A(B[m Append[29;91H(B[0;7mM-B(B[m Backup File[30d(B[0;7m^C(B[m Cancel[17G[14X[30;31H(B[0;7mM-M(B[m Mac Format[30;61H(B[0;7mM-P(B[m Prepend[30;91H(B[0;7m^T(B[m To Files[28d(B[0;7mFile Name to Write: env_setup_g3_d_50int.py(B[m[28;44H[?12l[?25h[?25l[28;53H[1K (B[0;7m[ Writing... ](B[m[K[1;111H(B[0;7m        (B[m[28;51H(B[0;7m[ Wrote 340 lines ](B[m[J[30d[?12l[?25h[30;1H[?1049l[23;0;0t[?1l>[?2004l(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> script d_50int_logs_5k.txt
Script started, file is d_50int_logs_5k.txt
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> conda activa[K[Kvate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python env_setup_g3_d_50int.py
GPU 3: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
Iteration: 0 | Episodes: 50 | Median Reward: 29.06 | Max Reward: 33.27
dIteration: 1 | Episodes: 100 | Median Reward: 20.21 | Max Reward: 33.57
Iteration: 2 | Episodes: 150 | Median Reward: 23.23 | Max Reward: 35.52
Iteration: 3 | Episodes: 200 | Median Reward: 23.23 | Max Reward: 37.80
Iteration: 4 | Episodes: 250 | Median Reward: 22.34 | Max Reward: 37.80
Iteration: 4 | Episodes: 300 | Median Reward: 25.09 | Max Reward: 37.80
Iteration: 5 | Episodes: 350 | Median Reward: 21.84 | Max Reward: 37.80
Iteration: 6 | Episodes: 400 | Median Reward: 20.45 | Max Reward: 37.80
Iteration: 7 | Episodes: 450 | Median Reward: 19.14 | Max Reward: 37.80
Iteration: 8 | Episodes: 500 | Median Reward: 17.44 | Max Reward: 37.80
Iteration: 9 | Episodes: 550 | Median Reward: 21.30 | Max Reward: 37.80
Iteration: 9 | Episodes: 600 | Median Reward: 21.70 | Max Reward: 37.80
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.6       |
| time/                   |             |
|    fps                  | 347         |
|    iterations           | 10          |
|    time_elapsed         | 176         |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.005245072 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -41         |
|    explained_variance   | -0.0385     |
|    learning_rate        | 0.0001      |
|    loss                 | 87.3        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00446    |
|    std                  | 1           |
|    value_loss           | 191         |
-----------------------------------------
Iteration: 10 | Episodes: 650 | Median Reward: 26.56 | Max Reward: 37.80
Iteration: 11 | Episodes: 700 | Median Reward: 15.79 | Max Reward: 37.80
Iteration: 12 | Episodes: 750 | Median Reward: 19.05 | Max Reward: 37.80
Iteration: 13 | Episodes: 800 | Median Reward: 24.53 | Max Reward: 37.80
Iteration: 14 | Episodes: 850 | Median Reward: 22.78 | Max Reward: 37.80
Iteration: 14 | Episodes: 900 | Median Reward: 18.95 | Max Reward: 37.80
Iteration: 15 | Episodes: 950 | Median Reward: 15.03 | Max Reward: 37.80
Iteration: 16 | Episodes: 1000 | Median Reward: 16.16 | Max Reward: 37.80
Iteration: 17 | Episodes: 1050 | Median Reward: 18.14 | Max Reward: 37.80
Iteration: 18 | Episodes: 1100 | Median Reward: 13.76 | Max Reward: 37.80
Iteration: 18 | Episodes: 1150 | Median Reward: 10.85 | Max Reward: 37.80
Iteration: 19 | Episodes: 1200 | Median Reward: 16.20 | Max Reward: 37.80
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -88.3       |
| time/                   |             |
|    fps                  | 331         |
|    iterations           | 20          |
|    time_elapsed         | 371         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.021096148 |
|    clip_fraction        | 0.0176      |
|    clip_range           | 0.4         |
|    entropy_loss         | -46.7       |
|    explained_variance   | -0.0358     |
|    learning_rate        | 0.0001      |
|    loss                 | 80.7        |
|    n_updates            | 190         |
|    policy_gradient_loss | 0.00626     |
|    std                  | 1.01        |
|    value_loss           | 166         |
-----------------------------------------
Iteration: 20 | Episodes: 1250 | Median Reward: 19.03 | Max Reward: 37.80
Iteration: 21 | Episodes: 1300 | Median Reward: 20.48 | Max Reward: 37.80
Iteration: 22 | Episodes: 1350 | Median Reward: 19.87 | Max Reward: 37.80
Iteration: 23 | Episodes: 1400 | Median Reward: 13.60 | Max Reward: 37.80
Iteration: 23 | Episodes: 1450 | Median Reward: 18.06 | Max Reward: 37.80
Iteration: 24 | Episodes: 1500 | Median Reward: 16.76 | Max Reward: 37.80
Iteration: 25 | Episodes: 1550 | Median Reward: 18.34 | Max Reward: 37.80
Iteration: 26 | Episodes: 1600 | Median Reward: 21.20 | Max Reward: 37.80
Iteration: 27 | Episodes: 1650 | Median Reward: 20.98 | Max Reward: 37.80
Iteration: 28 | Episodes: 1700 | Median Reward: 17.08 | Max Reward: 37.80
Iteration: 28 | Episodes: 1750 | Median Reward: 20.70 | Max Reward: 37.80
Iteration: 29 | Episodes: 1800 | Median Reward: 19.35 | Max Reward: 37.80
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.4       |
| time/                   |             |
|    fps                  | 336         |
|    iterations           | 30          |
|    time_elapsed         | 548         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.019373173 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -60.7       |
|    explained_variance   | -0.0147     |
|    learning_rate        | 0.0001      |
|    loss                 | 85          |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00314    |
|    std                  | 1.01        |
|    value_loss           | 185         |
-----------------------------------------
Iteration: 30 | Episodes: 1850 | Median Reward: 20.93 | Max Reward: 37.80
Iteration: 31 | Episodes: 1900 | Median Reward: 21.95 | Max Reward: 37.80
Iteration: 32 | Episodes: 1950 | Median Reward: 18.24 | Max Reward: 39.05
Iteration: 32 | Episodes: 2000 | Median Reward: 17.88 | Max Reward: 39.05
Iteration: 33 | Episodes: 2050 | Median Reward: 13.57 | Max Reward: 39.05
Iteration: 34 | Episodes: 2100 | Median Reward: 19.55 | Max Reward: 39.05
Iteration: 35 | Episodes: 2150 | Median Reward: 19.55 | Max Reward: 39.05
Iteration: 36 | Episodes: 2200 | Median Reward: 15.95 | Max Reward: 39.05
Iteration: 36 | Episodes: 2250 | Median Reward: 21.33 | Max Reward: 39.05
Iteration: 37 | Episodes: 2300 | Median Reward: 14.95 | Max Reward: 39.05
Iteration: 38 | Episodes: 2350 | Median Reward: 22.01 | Max Reward: 39.05
Iteration: 39 | Episodes: 2400 | Median Reward: 14.13 | Max Reward: 39.05
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84.4        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 40           |
|    time_elapsed         | 726          |
|    total_timesteps      | 245760       |
| train/                  |              |
|    approx_kl            | 0.0031620064 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -66.9        |
|    explained_variance   | -0.0106      |
|    learning_rate        | 0.0001       |
|    loss                 | 85.5         |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00354     |
|    std                  | 1.01         |
|    value_loss           | 176          |
------------------------------------------
Iteration: 40 | Episodes: 2450 | Median Reward: 14.13 | Max Reward: 43.72
Iteration: 41 | Episodes: 2500 | Median Reward: 24.94 | Max Reward: 43.72
Iteration: 41 | Episodes: 2550 | Median Reward: 15.01 | Max Reward: 43.72
Iteration: 42 | Episodes: 2600 | Median Reward: 21.24 | Max Reward: 43.72
Iteration: 43 | Episodes: 2650 | Median Reward: 13.92 | Max Reward: 43.72
Iteration: 44 | Episodes: 2700 | Median Reward: 14.54 | Max Reward: 43.72
Iteration: 45 | Episodes: 2750 | Median Reward: 16.90 | Max Reward: 43.72
Iteration: 46 | Episodes: 2800 | Median Reward: 14.13 | Max Reward: 43.72
Iteration: 46 | Episodes: 2850 | Median Reward: 23.69 | Max Reward: 43.72
Iteration: 47 | Episodes: 2900 | Median Reward: 23.44 | Max Reward: 43.72
Iteration: 48 | Episodes: 2950 | Median Reward: 23.68 | Max Reward: 43.72
Iteration: 49 | Episodes: 3000 | Median Reward: 23.24 | Max Reward: 43.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.6        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 50           |
|    time_elapsed         | 905          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.0045295935 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -72.5        |
|    explained_variance   | -0.00596     |
|    learning_rate        | 0.0001       |
|    loss                 | 104          |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00208     |
|    std                  | 1.02         |
|    value_loss           | 194          |
------------------------------------------
Iteration: 50 | Episodes: 3050 | Median Reward: 21.02 | Max Reward: 43.72
Iteration: 50 | Episodes: 3100 | Median Reward: 15.38 | Max Reward: 43.72
Iteration: 51 | Episodes: 3150 | Median Reward: 13.35 | Max Reward: 43.72
Iteration: 52 | Episodes: 3200 | Median Reward: 13.35 | Max Reward: 43.72
Iteration: 53 | Episodes: 3250 | Median Reward: 13.70 | Max Reward: 43.72
Iteration: 54 | Episodes: 3300 | Median Reward: 11.59 | Max Reward: 43.72
Iteration: 55 | Episodes: 3350 | Median Reward: 8.35 | Max Reward: 43.72
Iteration: 55 | Episodes: 3400 | Median Reward: 21.08 | Max Reward: 43.72
Iteration: 56 | Episodes: 3450 | Median Reward: 16.52 | Max Reward: 43.72
Iteration: 57 | Episodes: 3500 | Median Reward: 18.86 | Max Reward: 43.72
Iteration: 58 | Episodes: 3550 | Median Reward: 18.36 | Max Reward: 43.72
Iteration: 59 | Episodes: 3600 | Median Reward: 16.46 | Max Reward: 43.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84.4        |
| time/                   |              |
|    fps                  | 336          |
|    iterations           | 60           |
|    time_elapsed         | 1094         |
|    total_timesteps      | 368640       |
| train/                  |              |
|    approx_kl            | 0.0008973649 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -77.8        |
|    explained_variance   | -0.00425     |
|    learning_rate        | 0.0001       |
|    loss                 | 72.9         |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00107     |
|    std                  | 1.02         |
|    value_loss           | 168          |
------------------------------------------
Iteration: 60 | Episodes: 3650 | Median Reward: 14.70 | Max Reward: 43.72
Iteration: 60 | Episodes: 3700 | Median Reward: 13.36 | Max Reward: 43.72
Iteration: 61 | Episodes: 3750 | Median Reward: 9.78 | Max Reward: 43.72
Iteration: 62 | Episodes: 3800 | Median Reward: 13.34 | Max Reward: 43.72
Iteration: 63 | Episodes: 3850 | Median Reward: 12.54 | Max Reward: 43.72
Iteration: 64 | Episodes: 3900 | Median Reward: 10.81 | Max Reward: 43.72
Iteration: 64 | Episodes: 3950 | Median Reward: 28.14 | Max Reward: 43.72
Iteration: 65 | Episodes: 4000 | Median Reward: 24.10 | Max Reward: 43.72
Iteration: 66 | Episodes: 4050 | Median Reward: 22.36 | Max Reward: 43.72
Iteration: 67 | Episodes: 4100 | Median Reward: 21.05 | Max Reward: 43.72
Iteration: 68 | Episodes: 4150 | Median Reward: 21.97 | Max Reward: 43.72
Iteration: 69 | Episodes: 4200 | Median Reward: 21.59 | Max Reward: 43.72
Iteration: 69 | Episodes: 4250 | Median Reward: 14.63 | Max Reward: 43.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -81.6        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 70           |
|    time_elapsed         | 1275         |
|    total_timesteps      | 430080       |
| train/                  |              |
|    approx_kl            | 0.0053869067 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -80.1        |
|    explained_variance   | -0.00239     |
|    learning_rate        | 0.0001       |
|    loss                 | 86.3         |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00511     |
|    std                  | 1.03         |
|    value_loss           | 202          |
------------------------------------------
Iteration: 70 | Episodes: 4300 | Median Reward: 13.65 | Max Reward: 43.72
Iteration: 71 | Episodes: 4350 | Median Reward: 26.63 | Max Reward: 43.72
Iteration: 72 | Episodes: 4400 | Median Reward: 26.63 | Max Reward: 43.72
Iteration: 73 | Episodes: 4450 | Median Reward: 22.44 | Max Reward: 43.72
Iteration: 73 | Episodes: 4500 | Median Reward: 23.95 | Max Reward: 43.72
Iteration: 74 | Episodes: 4550 | Median Reward: 17.16 | Max Reward: 43.72
Iteration: 75 | Episodes: 4600 | Median Reward: 13.59 | Max Reward: 43.72
Iteration: 76 | Episodes: 4650 | Median Reward: 13.11 | Max Reward: 43.72
Iteration: 77 | Episodes: 4700 | Median Reward: 14.87 | Max Reward: 43.72
Iteration: 78 | Episodes: 4750 | Median Reward: 18.41 | Max Reward: 43.72
Iteration: 78 | Episodes: 4800 | Median Reward: 22.59 | Max Reward: 43.72
Iteration: 79 | Episodes: 4850 | Median Reward: 23.46 | Max Reward: 43.72
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.2        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 80           |
|    time_elapsed         | 1453         |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0042321193 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -80.7        |
|    explained_variance   | -0.00163     |
|    learning_rate        | 0.0001       |
|    loss                 | 99.2         |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.000141    |
|    std                  | 1.03         |
|    value_loss           | 186          |
------------------------------------------
Iteration: 80 | Episodes: 4900 | Median Reward: 24.13 | Max Reward: 43.72
Iteration: 81 | Episodes: 4950 | Median Reward: 14.61 | Max Reward: 43.72
Iteration: 82 | Episodes: 5000 | Median Reward: 14.61 | Max Reward: 43.72
Iteration: 83 | Episodes: 5050 | Median Reward: 23.06 | Max Reward: 43.72
Iteration: 83 | Episodes: 5100 | Median Reward: 18.54 | Max Reward: 43.72
Iteration: 84 | Episodes: 5150 | Median Reward: 21.95 | Max Reward: 43.72
Iteration: 85 | Episodes: 5200 | Median Reward: 25.29 | Max Reward: 44.03
Iteration: 86 | Episodes: 5250 | Median Reward: 25.29 | Max Reward: 44.03
Iteration: 87 | Episodes: 5300 | Median Reward: 25.26 | Max Reward: 44.03
Iteration: 87 | Episodes: 5350 | Median Reward: 21.83 | Max Reward: 44.03
Iteration: 88 | Episodes: 5400 | Median Reward: 25.28 | Max Reward: 44.03
Iteration: 89 | Episodes: 5450 | Median Reward: 16.28 | Max Reward: 44.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83          |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 90           |
|    time_elapsed         | 1635         |
|    total_timesteps      | 552960       |
| train/                  |              |
|    approx_kl            | 0.0019156558 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -81.7        |
|    explained_variance   | -0.000976    |
|    learning_rate        | 0.0001       |
|    loss                 | 106          |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.00166     |
|    std                  | 1.04         |
|    value_loss           | 205          |
------------------------------------------
Iteration: 90 | Episodes: 5500 | Median Reward: 17.80 | Max Reward: 44.03
Iteration: 91 | Episodes: 5550 | Median Reward: 21.51 | Max Reward: 44.03
Iteration: 92 | Episodes: 5600 | Median Reward: 21.51 | Max Reward: 44.03
Iteration: 92 | Episodes: 5650 | Median Reward: 26.12 | Max Reward: 44.03
Iteration: 93 | Episodes: 5700 | Median Reward: 16.93 | Max Reward: 44.03
Iteration: 94 | Episodes: 5750 | Median Reward: 20.03 | Max Reward: 44.03
Iteration: 95 | Episodes: 5800 | Median Reward: 20.38 | Max Reward: 44.03
Iteration: 96 | Episodes: 5850 | Median Reward: 20.55 | Max Reward: 44.03
Iteration: 97 | Episodes: 5900 | Median Reward: 28.73 | Max Reward: 44.03
Iteration: 97 | Episodes: 5950 | Median Reward: 24.64 | Max Reward: 44.03
Iteration: 98 | Episodes: 6000 | Median Reward: 22.48 | Max Reward: 44.03
Iteration: 99 | Episodes: 6050 | Median Reward: 26.45 | Max Reward: 44.18
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72.4        |
| time/                   |              |
|    fps                  | 336          |
|    iterations           | 100          |
|    time_elapsed         | 1823         |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0019420062 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -82.7        |
|    explained_variance   | -0.000669    |
|    learning_rate        | 0.0001       |
|    loss                 | 112          |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.0019      |
|    std                  | 1.05         |
|    value_loss           | 210          |
------------------------------------------
Iteration: 100 | Episodes: 6100 | Median Reward: 24.56 | Max Reward: 44.18
Iteration: 101 | Episodes: 6150 | Median Reward: 22.68 | Max Reward: 44.18
Iteration: 101 | Episodes: 6200 | Median Reward: 22.57 | Max Reward: 44.18
Iteration: 102 | Episodes: 6250 | Median Reward: 29.06 | Max Reward: 44.18
Iteration: 103 | Episodes: 6300 | Median Reward: 22.59 | Max Reward: 44.18
Iteration: 104 | Episodes: 6350 | Median Reward: 21.88 | Max Reward: 44.18
Iteration: 105 | Episodes: 6400 | Median Reward: 25.11 | Max Reward: 44.18
Iteration: 106 | Episodes: 6450 | Median Reward: 28.62 | Max Reward: 44.18
Iteration: 106 | Episodes: 6500 | Median Reward: 17.28 | Max Reward: 44.18
Iteration: 107 | Episodes: 6550 | Median Reward: 33.22 | Max Reward: 44.18
Iteration: 108 | Episodes: 6600 | Median Reward: 25.77 | Max Reward: 44.18
Iteration: 109 | Episodes: 6650 | Median Reward: 21.33 | Max Reward: 44.18
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.3        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 110          |
|    time_elapsed         | 2004         |
|    total_timesteps      | 675840       |
| train/                  |              |
|    approx_kl            | 0.0018233807 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -83.2        |
|    explained_variance   | -0.000479    |
|    learning_rate        | 0.0001       |
|    loss                 | 93.4         |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00219     |
|    std                  | 1.05         |
|    value_loss           | 199          |
------------------------------------------
Iteration: 110 | Episodes: 6700 | Median Reward: 27.87 | Max Reward: 44.18
Iteration: 110 | Episodes: 6750 | Median Reward: 18.00 | Max Reward: 44.18
Iteration: 111 | Episodes: 6800 | Median Reward: 21.31 | Max Reward: 44.18
Iteration: 112 | Episodes: 6850 | Median Reward: 30.42 | Max Reward: 44.18
Iteration: 113 | Episodes: 6900 | Median Reward: 26.80 | Max Reward: 44.18
Iteration: 114 | Episodes: 6950 | Median Reward: 21.28 | Max Reward: 44.18
Iteration: 115 | Episodes: 7000 | Median Reward: 27.57 | Max Reward: 44.18
Iteration: 115 | Episodes: 7050 | Median Reward: 29.14 | Max Reward: 44.18
Iteration: 116 | Episodes: 7100 | Median Reward: 31.65 | Max Reward: 44.18
Iteration: 117 | Episodes: 7150 | Median Reward: 22.39 | Max Reward: 44.18
Iteration: 118 | Episodes: 7200 | Median Reward: 22.65 | Max Reward: 44.18
Iteration: 119 | Episodes: 7250 | Median Reward: 23.36 | Max Reward: 44.18
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.6        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 120          |
|    time_elapsed         | 2182         |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.0066987076 |
|    clip_fraction        | 0.00631      |
|    clip_range           | 0.4          |
|    entropy_loss         | -83.6        |
|    explained_variance   | -0.000307    |
|    learning_rate        | 0.0001       |
|    loss                 | 96.6         |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 1.06         |
|    value_loss           | 210          |
------------------------------------------
Iteration: 120 | Episodes: 7300 | Median Reward: 25.84 | Max Reward: 44.18
Iteration: 120 | Episodes: 7350 | Median Reward: 16.44 | Max Reward: 44.18
Iteration: 121 | Episodes: 7400 | Median Reward: 20.96 | Max Reward: 44.18
Iteration: 122 | Episodes: 7450 | Median Reward: 23.69 | Max Reward: 44.18
Iteration: 123 | Episodes: 7500 | Median Reward: 24.02 | Max Reward: 44.18
Iteration: 124 | Episodes: 7550 | Median Reward: 26.06 | Max Reward: 44.18
Iteration: 124 | Episodes: 7600 | Median Reward: 20.56 | Max Reward: 44.18
Iteration: 125 | Episodes: 7650 | Median Reward: 25.81 | Max Reward: 44.18
Iteration: 126 | Episodes: 7700 | Median Reward: 29.84 | Max Reward: 44.18
Iteration: 127 | Episodes: 7750 | Median Reward: 29.31 | Max Reward: 44.18
Iteration: 128 | Episodes: 7800 | Median Reward: 26.73 | Max Reward: 44.18
Iteration: 129 | Episodes: 7850 | Median Reward: 22.46 | Max Reward: 44.18
Iteration: 129 | Episodes: 7900 | Median Reward: 33.57 | Max Reward: 44.18
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.6        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 130          |
|    time_elapsed         | 2356         |
|    total_timesteps      | 798720       |
| train/                  |              |
|    approx_kl            | 0.0041944324 |
|    clip_fraction        | 0.00145      |
|    clip_range           | 0.4          |
|    entropy_loss         | -84.1        |
|    explained_variance   | -0.000219    |
|    learning_rate        | 0.0001       |
|    loss                 | 96.3         |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00221     |
|    std                  | 1.08         |
|    value_loss           | 202          |
------------------------------------------
Iteration: 130 | Episodes: 7950 | Median Reward: 29.90 | Max Reward: 44.18
Iteration: 131 | Episodes: 8000 | Median Reward: 29.59 | Max Reward: 44.18
Iteration: 132 | Episodes: 8050 | Median Reward: 32.04 | Max Reward: 44.18
Iteration: 133 | Episodes: 8100 | Median Reward: 35.21 | Max Reward: 44.18
Iteration: 134 | Episodes: 8150 | Median Reward: 32.86 | Max Reward: 44.18
Iteration: 134 | Episodes: 8200 | Median Reward: 32.80 | Max Reward: 44.18
Iteration: 135 | Episodes: 8250 | Median Reward: 31.13 | Max Reward: 44.18
Iteration: 136 | Episodes: 8300 | Median Reward: 26.33 | Max Reward: 44.18
Iteration: 137 | Episodes: 8350 | Median Reward: 27.15 | Max Reward: 44.18
Iteration: 138 | Episodes: 8400 | Median Reward: 27.59 | Max Reward: 44.18
Iteration: 138 | Episodes: 8450 | Median Reward: 17.21 | Max Reward: 44.18
Iteration: 139 | Episodes: 8500 | Median Reward: 22.36 | Max Reward: 44.18
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -77.4      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 140        |
|    time_elapsed         | 2544       |
|    total_timesteps      | 860160     |
| train/                  |            |
|    approx_kl            | 0.18969756 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.4        |
|    entropy_loss         | -80        |
|    explained_variance   | -0.000117  |
|    learning_rate        | 0.0001     |
|    loss                 | 89.2       |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.0131    |
|    std                  | 1.09       |
|    value_loss           | 194        |
----------------------------------------
Iteration: 140 | Episodes: 8550 | Median Reward: 31.80 | Max Reward: 44.18
Iteration: 141 | Episodes: 8600 | Median Reward: 29.12 | Max Reward: 44.18
Iteration: 142 | Episodes: 8650 | Median Reward: 25.61 | Max Reward: 44.27
Iteration: 143 | Episodes: 8700 | Median Reward: 0.06 | Max Reward: 44.27
Iteration: 143 | Episodes: 8750 | Median Reward: 2.12 | Max Reward: 44.27
Iteration: 144 | Episodes: 8800 | Median Reward: 7.30 | Max Reward: 44.27
Iteration: 145 | Episodes: 8850 | Median Reward: 7.30 | Max Reward: 44.27
Iteration: 146 | Episodes: 8900 | Median Reward: 3.34 | Max Reward: 44.27
Iteration: 147 | Episodes: 8950 | Median Reward: 5.49 | Max Reward: 44.27
Iteration: 147 | Episodes: 9000 | Median Reward: 9.43 | Max Reward: 44.27
Iteration: 148 | Episodes: 9050 | Median Reward: 10.87 | Max Reward: 44.27
Iteration: 149 | Episodes: 9100 | Median Reward: 17.73 | Max Reward: 44.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -83.9       |
| time/                   |             |
|    fps                  | 339         |
|    iterations           | 150         |
|    time_elapsed         | 2717        |
|    total_timesteps      | 921600      |
| train/                  |             |
|    approx_kl            | 0.050714236 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.4         |
|    entropy_loss         | -50.8       |
|    explained_variance   | -0.000107   |
|    learning_rate        | 0.0001      |
|    loss                 | 78.7        |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.00236    |
|    std                  | 1.1         |
|    value_loss           | 166         |
-----------------------------------------
Iteration: 150 | Episodes: 9150 | Median Reward: 15.66 | Max Reward: 44.27
Iteration: 151 | Episodes: 9200 | Median Reward: 12.57 | Max Reward: 44.27
Iteration: 152 | Episodes: 9250 | Median Reward: 5.04 | Max Reward: 44.27
Iteration: 152 | Episodes: 9300 | Median Reward: 19.06 | Max Reward: 44.27
Iteration: 153 | Episodes: 9350 | Median Reward: 15.02 | Max Reward: 44.27
Iteration: 154 | Episodes: 9400 | Median Reward: 4.48 | Max Reward: 44.27
Iteration: 155 | Episodes: 9450 | Median Reward: 4.84 | Max Reward: 44.27
Iteration: 156 | Episodes: 9500 | Median Reward: 12.23 | Max Reward: 44.27
Iteration: 157 | Episodes: 9550 | Median Reward: 19.04 | Max Reward: 44.27
Iteration: 157 | Episodes: 9600 | Median Reward: 27.18 | Max Reward: 44.27
Iteration: 158 | Episodes: 9650 | Median Reward: 19.59 | Max Reward: 44.27
Iteration: 159 | Episodes: 9700 | Median Reward: 28.90 | Max Reward: 44.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -75.2      |
| time/                   |            |
|    fps                  | 340        |
|    iterations           | 160        |
|    time_elapsed         | 2890       |
|    total_timesteps      | 983040     |
| train/                  |            |
|    approx_kl            | 0.10774122 |
|    clip_fraction        | 0.0791     |
|    clip_range           | 0.4        |
|    entropy_loss         | -55.8      |
|    explained_variance   | -7.69e-05  |
|    learning_rate        | 0.0001     |
|    loss                 | 120        |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.00802   |
|    std                  | 1.1        |
|    value_loss           | 239        |
----------------------------------------
Iteration: 160 | Episodes: 9750 | Median Reward: 31.00 | Max Reward: 44.27
Iteration: 161 | Episodes: 9800 | Median Reward: 29.71 | Max Reward: 44.27
Iteration: 161 | Episodes: 9850 | Median Reward: 3.69 | Max Reward: 44.27
Iteration: 162 | Episodes: 9900 | Median Reward: 16.60 | Max Reward: 44.27
Iteration: 163 | Episodes: 9950 | Median Reward: 5.19 | Max Reward: 44.27
Iteration: 164 | Episodes: 10000 | Median Reward: 10.34 | Max Reward: 44.27
Iteration: 165 | Episodes: 10050 | Median Reward: 11.90 | Max Reward: 44.27
Iteration: 166 | Episodes: 10100 | Median Reward: 7.50 | Max Reward: 44.27
Iteration: 166 | Episodes: 10150 | Median Reward: 4.45 | Max Reward: 44.27
Iteration: 167 | Episodes: 10200 | Median Reward: 12.78 | Max Reward: 44.27
Iteration: 168 | Episodes: 10250 | Median Reward: 16.28 | Max Reward: 44.27
Iteration: 169 | Episodes: 10300 | Median Reward: 22.79 | Max Reward: 44.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -83.8     |
| time/                   |           |
|    fps                  | 340       |
|    iterations           | 170       |
|    time_elapsed         | 3068      |
|    total_timesteps      | 1044480   |
| train/                  |           |
|    approx_kl            | 4.196755  |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.4       |
|    entropy_loss         | -67.6     |
|    explained_variance   | -6.45e-05 |
|    learning_rate        | 0.0001    |
|    loss                 | 134       |
|    n_updates            | 1690      |
|    policy_gradient_loss | 0.159     |
|    std                  | 1.11      |
|    value_loss           | 213       |
---------------------------------------
Iteration: 170 | Episodes: 10350 | Median Reward: 17.80 | Max Reward: 44.27
Iteration: 171 | Episodes: 10400 | Median Reward: 25.31 | Max Reward: 44.27
Iteration: 171 | Episodes: 10450 | Median Reward: 17.88 | Max Reward: 44.27
Iteration: 172 | Episodes: 10500 | Median Reward: 12.40 | Max Reward: 44.27
Iteration: 173 | Episodes: 10550 | Median Reward: 6.51 | Max Reward: 44.27
Iteration: 174 | Episodes: 10600 | Median Reward: 3.18 | Max Reward: 44.27
Iteration: 175 | Episodes: 10650 | Median Reward: -0.48 | Max Reward: 44.27
Iteration: 175 | Episodes: 10700 | Median Reward: -3.17 | Max Reward: 44.27
Iteration: 176 | Episodes: 10750 | Median Reward: 0.75 | Max Reward: 44.27
Iteration: 177 | Episodes: 10800 | Median Reward: 8.94 | Max Reward: 44.27
Iteration: 178 | Episodes: 10850 | Median Reward: 4.85 | Max Reward: 44.27
Iteration: 179 | Episodes: 10900 | Median Reward: -8.20 | Max Reward: 44.27
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -106         |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 180          |
|    time_elapsed         | 3246         |
|    total_timesteps      | 1105920      |
| train/                  |              |
|    approx_kl            | 0.0006506661 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -58          |
|    explained_variance   | -0.000104    |
|    learning_rate        | 0.0001       |
|    loss                 | 50.2         |
|    n_updates            | 1790         |
|    policy_gradient_loss | 0.00064      |
|    std                  | 1.11         |
|    value_loss           | 104          |
------------------------------------------
Iteration: 180 | Episodes: 10950 | Median Reward: -2.98 | Max Reward: 44.27
Iteration: 180 | Episodes: 11000 | Median Reward: 4.25 | Max Reward: 44.27
Iteration: 181 | Episodes: 11050 | Median Reward: 8.03 | Max Reward: 44.27
Iteration: 182 | Episodes: 11100 | Median Reward: 9.26 | Max Reward: 44.27
Iteration: 183 | Episodes: 11150 | Median Reward: 5.14 | Max Reward: 44.27
Iteration: 184 | Episodes: 11200 | Median Reward: 5.29 | Max Reward: 44.27
Iteration: 184 | Episodes: 11250 | Median Reward: 11.05 | Max Reward: 44.27
Iteration: 185 | Episodes: 11300 | Median Reward: 9.95 | Max Reward: 44.27
Iteration: 186 | Episodes: 11350 | Median Reward: 20.57 | Max Reward: 44.27
Iteration: 187 | Episodes: 11400 | Median Reward: 18.59 | Max Reward: 44.27
Iteration: 188 | Episodes: 11450 | Median Reward: 11.36 | Max Reward: 44.27
Iteration: 189 | Episodes: 11500 | Median Reward: 16.00 | Max Reward: 44.27
Iteration: 189 | Episodes: 11550 | Median Reward: 11.72 | Max Reward: 44.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -86.2     |
| time/                   |           |
|    fps                  | 340       |
|    iterations           | 190       |
|    time_elapsed         | 3430      |
|    total_timesteps      | 1167360   |
| train/                  |           |
|    approx_kl            | 3.3938267 |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.4       |
|    entropy_loss         | -66       |
|    explained_variance   | -5.2e-05  |
|    learning_rate        | 0.0001    |
|    loss                 | 103       |
|    n_updates            | 1890      |
|    policy_gradient_loss | 0.11      |
|    std                  | 1.11      |
|    value_loss           | 173       |
---------------------------------------
Iteration: 190 | Episodes: 11600 | Median Reward: -1.81 | Max Reward: 44.27
Iteration: 191 | Episodes: 11650 | Median Reward: 0.90 | Max Reward: 44.27
Iteration: 192 | Episodes: 11700 | Median Reward: 3.70 | Max Reward: 44.27
Iteration: 193 | Episodes: 11750 | Median Reward: 7.02 | Max Reward: 44.27
Iteration: 194 | Episodes: 11800 | Median Reward: 6.57 | Max Reward: 44.27
Iteration: 194 | Episodes: 11850 | Median Reward: -10.27 | Max Reward: 44.27
Iteration: 195 | Episodes: 11900 | Median Reward: 5.29 | Max Reward: 44.27
Iteration: 196 | Episodes: 11950 | Median Reward: 2.86 | Max Reward: 44.27
Iteration: 197 | Episodes: 12000 | Median Reward: 0.43 | Max Reward: 44.27
Iteration: 198 | Episodes: 12050 | Median Reward: 0.01 | Max Reward: 44.27
Iteration: 198 | Episodes: 12100 | Median Reward: 1.56 | Max Reward: 44.27
Iteration: 199 | Episodes: 12150 | Median Reward: 1.98 | Max Reward: 44.27
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -93.6       |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 200         |
|    time_elapsed         | 3607        |
|    total_timesteps      | 1228800     |
| train/                  |             |
|    approx_kl            | 0.083818585 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.4         |
|    entropy_loss         | -67.2       |
|    explained_variance   | -5.4e-05    |
|    learning_rate        | 0.0001      |
|    loss                 | 55.4        |
|    n_updates            | 1990        |
|    policy_gradient_loss | 0.00907     |
|    std                  | 1.12        |
|    value_loss           | 150         |
-----------------------------------------
Iteration: 200 | Episodes: 12200 | Median Reward: -3.39 | Max Reward: 44.27
Iteration: 201 | Episodes: 12250 | Median Reward: -1.10 | Max Reward: 44.27
Iteration: 202 | Episodes: 12300 | Median Reward: 1.65 | Max Reward: 44.27
Iteration: 203 | Episodes: 12350 | Median Reward: 2.26 | Max Reward: 44.27
Iteration: 203 | Episodes: 12400 | Median Reward: -5.58 | Max Reward: 44.27
Iteration: 204 | Episodes: 12450 | Median Reward: 16.93 | Max Reward: 44.27
Iteration: 205 | Episodes: 12500 | Median Reward: 28.77 | Max Reward: 44.27
Iteration: 206 | Episodes: 12550 | Median Reward: 16.04 | Max Reward: 44.27
Iteration: 207 | Episodes: 12600 | Median Reward: 10.13 | Max Reward: 44.27
Iteration: 208 | Episodes: 12650 | Median Reward: 4.13 | Max Reward: 44.27
Iteration: 208 | Episodes: 12700 | Median Reward: 2.64 | Max Reward: 44.27
Iteration: 209 | Episodes: 12750 | Median Reward: 2.31 | Max Reward: 44.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 341        |
|    iterations           | 210        |
|    time_elapsed         | 3782       |
|    total_timesteps      | 1290240    |
| train/                  |            |
|    approx_kl            | 0.30590478 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.4        |
|    entropy_loss         | -68.5      |
|    explained_variance   | -4.32e-05  |
|    learning_rate        | 0.0001     |
|    loss                 | 63.6       |
|    n_updates            | 2090       |
|    policy_gradient_loss | 0.047      |
|    std                  | 1.12       |
|    value_loss           | 149        |
----------------------------------------
Iteration: 210 | Episodes: 12800 | Median Reward: 4.99 | Max Reward: 44.27
Iteration: 211 | Episodes: 12850 | Median Reward: 6.46 | Max Reward: 44.27
Iteration: 212 | Episodes: 12900 | Median Reward: 4.06 | Max Reward: 44.27
Iteration: 212 | Episodes: 12950 | Median Reward: -5.97 | Max Reward: 44.27
Iteration: 213 | Episodes: 13000 | Median Reward: 4.32 | Max Reward: 44.27
Iteration: 214 | Episodes: 13050 | Median Reward: 4.27 | Max Reward: 44.27
Iteration: 215 | Episodes: 13100 | Median Reward: 4.27 | Max Reward: 44.27
Iteration: 216 | Episodes: 13150 | Median Reward: 5.87 | Max Reward: 44.27
Iteration: 216 | Episodes: 13200 | Median Reward: -1.68 | Max Reward: 44.27
Iteration: 217 | Episodes: 13250 | Median Reward: 0.87 | Max Reward: 44.27
Iteration: 218 | Episodes: 13300 | Median Reward: -2.44 | Max Reward: 44.27
Iteration: 219 | Episodes: 13350 | Median Reward: -1.42 | Max Reward: 44.27
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -94.5      |
| time/                   |            |
|    fps                  | 341        |
|    iterations           | 220        |
|    time_elapsed         | 3956       |
|    total_timesteps      | 1351680    |
| train/                  |            |
|    approx_kl            | 0.07725833 |
|    clip_fraction        | 0.0692     |
|    clip_range           | 0.4        |
|    entropy_loss         | -62.4      |
|    explained_variance   | -4.04e-05  |
|    learning_rate        | 0.0001     |
|    loss                 | 95.6       |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.019     |
|    std                  | 1.13       |
|    value_loss           | 137        |
----------------------------------------
Iteration: 220 | Episodes: 13400 | Median Reward: 5.11 | Max Reward: 44.27
Iteration: 221 | Episodes: 13450 | Median Reward: 5.11 | Max Reward: 44.27
Iteration: 221 | Episodes: 13500 | Median Reward: 21.16 | Max Reward: 44.27
Iteration: 222 | Episodes: 13550 | Median Reward: 12.39 | Max Reward: 44.27
Iteration: 223 | Episodes: 13600 | Median Reward: 12.07 | Max Reward: 44.27
Iteration: 224 | Episodes: 13650 | Median Reward: 6.15 | Max Reward: 44.27
Iteration: 225 | Episodes: 13700 | Median Reward: 15.76 | Max Reward: 44.27
Iteration: 226 | Episodes: 13750 | Median Reward: 15.76 | Max Reward: 44.27
Iteration: 226 | Episodes: 13800 | Median Reward: 24.92 | Max Reward: 44.27
Iteration: 227 | Episodes: 13850 | Median Reward: 23.17 | Max Reward: 44.27
Iteration: 228 | Episodes: 13900 | Median Reward: 24.80 | Max Reward: 44.27
Iteration: 229 | Episodes: 13950 | Median Reward: 30.85 | Max Reward: 44.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -72.6     |
| time/                   |           |
|    fps                  | 341       |
|    iterations           | 230       |
|    time_elapsed         | 4133      |
|    total_timesteps      | 1413120   |
| train/                  |           |
|    approx_kl            | 1.1526648 |
|    clip_fraction        | 0.406     |
|    clip_range           | 0.4       |
|    entropy_loss         | -67.4     |
|    explained_variance   | -1.94e-05 |
|    learning_rate        | 0.0001    |
|    loss                 | 148       |
|    n_updates            | 2290      |
|    policy_gradient_loss | 0.0481    |
|    std                  | 1.14      |
|    value_loss           | 300       |
---------------------------------------
Iteration: 230 | Episodes: 14000 | Median Reward: 30.85 | Max Reward: 44.27
Iteration: 230 | Episodes: 14050 | Median Reward: 32.12 | Max Reward: 44.27
Iteration: 231 | Episodes: 14100 | Median Reward: 27.77 | Max Reward: 44.27
Iteration: 232 | Episodes: 14150 | Median Reward: 27.90 | Max Reward: 44.27
Iteration: 233 | Episodes: 14200 | Median Reward: 26.84 | Max Reward: 44.27
Iteration: 234 | Episodes: 14250 | Median Reward: 27.45 | Max Reward: 44.27
Iteration: 235 | Episodes: 14300 | Median Reward: 25.70 | Max Reward: 44.27
Iteration: 235 | Episodes: 14350 | Median Reward: 29.35 | Max Reward: 44.27
Iteration: 236 | Episodes: 14400 | Median Reward: 27.21 | Max Reward: 44.27
Iteration: 237 | Episodes: 14450 | Median Reward: 23.42 | Max Reward: 44.27
Iteration: 238 | Episodes: 14500 | Median Reward: 26.05 | Max Reward: 44.27
Iteration: 239 | Episodes: 14550 | Median Reward: 24.18 | Max Reward: 44.27
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -83.1     |
| time/                   |           |
|    fps                  | 340       |
|    iterations           | 240       |
|    time_elapsed         | 4325      |
|    total_timesteps      | 1474560   |
| train/                  |           |
|    approx_kl            | 1.4854834 |
|    clip_fraction        | 0.388     |
|    clip_range           | 0.4       |
|    entropy_loss         | -73.4     |
|    explained_variance   | -1.54e-05 |
|    learning_rate        | 0.0001    |
|    loss                 | 132       |
|    n_updates            | 2390      |
|    policy_gradient_loss | -0.0358   |
|    std                  | 1.15      |
|    value_loss           | 275       |
---------------------------------------
Iteration: 240 | Episodes: 14600 | Median Reward: 19.52 | Max Reward: 44.27
Iteration: 240 | Episodes: 14650 | Median Reward: 16.05 | Max Reward: 44.27
Iteration: 241 | Episodes: 14700 | Median Reward: -4.60 | Max Reward: 45.65
Iteration: 242 | Episodes: 14750 | Median Reward: 0.74 | Max Reward: 45.65
Iteration: 243 | Episodes: 14800 | Median Reward: 0.74 | Max Reward: 45.65
Iteration: 244 | Episodes: 14850 | Median Reward: -9.78 | Max Reward: 45.65
Iteration: 245 | Episodes: 14900 | Median Reward: -1.59 | Max Reward: 45.65
Iteration: 245 | Episodes: 14950 | Median Reward: 0.89 | Max Reward: 45.65
Iteration: 246 | Episodes: 15000 | Median Reward: -0.80 | Max Reward: 45.65
Iteration: 247 | Episodes: 15050 | Median Reward: -0.20 | Max Reward: 45.65
Iteration: 248 | Episodes: 15100 | Median Reward: -0.04 | Max Reward: 45.65
Iteration: 249 | Episodes: 15150 | Median Reward: 0.23 | Max Reward: 45.65
Iteration: 249 | Episodes: 15200 | Median Reward: -4.74 | Max Reward: 45.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -102         |
| time/                   |              |
|    fps                  | 341          |
|    iterations           | 250          |
|    time_elapsed         | 4499         |
|    total_timesteps      | 1536000      |
| train/                  |              |
|    approx_kl            | 0.0073347283 |
|    clip_fraction        | 0.00493      |
|    clip_range           | 0.4          |
|    entropy_loss         | -83.3        |
|    explained_variance   | -1.92e-05    |
|    learning_rate        | 0.0001       |
|    loss                 | 55.2         |
|    n_updates            | 2490         |
|    policy_gradient_loss | -0.00398     |
|    std                  | 1.15         |
|    value_loss           | 151          |
------------------------------------------
Iteration: 250 | Episodes: 15250 | Median Reward: -8.50 | Max Reward: 45.65
Iteration: 251 | Episodes: 15300 | Median Reward: 3.51 | Max Reward: 45.65
Iteration: 252 | Episodes: 15350 | Median Reward: 3.53 | Max Reward: 45.65
Iteration: 253 | Episodes: 15400 | Median Reward: -1.76 | Max Reward: 45.65
Iteration: 253 | Episodes: 15450 | Median Reward: -0.02 | Max Reward: 45.65
Iteration: 254 | Episodes: 15500 | Median Reward: -1.54 | Max Reward: 45.65
Iteration: 255 | Episodes: 15550 | Median Reward: -0.79 | Max Reward: 45.65
Iteration: 256 | Episodes: 15600 | Median Reward: -6.74 | Max Reward: 45.65
Iteration: 257 | Episodes: 15650 | Median Reward: -8.40 | Max Reward: 45.65
Iteration: 258 | Episodes: 15700 | Median Reward: 0.06 | Max Reward: 46.38
Iteration: 258 | Episodes: 15750 | Median Reward: -6.77 | Max Reward: 46.38
Iteration: 259 | Episodes: 15800 | Median Reward: -1.36 | Max Reward: 46.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -99.2      |
| time/                   |            |
|    fps                  | 341        |
|    iterations           | 260        |
|    time_elapsed         | 4671       |
|    total_timesteps      | 1597440    |
| train/                  |            |
|    approx_kl            | 0.10098801 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.4        |
|    entropy_loss         | -80.4      |
|    explained_variance   | -1.48e-05  |
|    learning_rate        | 0.0001     |
|    loss                 | 56.6       |
|    n_updates            | 2590       |
|    policy_gradient_loss | 0.0117     |
|    std                  | 1.15       |
|    value_loss           | 166        |
----------------------------------------
Iteration: 260 | Episodes: 15850 | Median Reward: -9.82 | Max Reward: 46.38
Iteration: 261 | Episodes: 15900 | Median Reward: -10.27 | Max Reward: 46.38
Iteration: 262 | Episodes: 15950 | Median Reward: -0.98 | Max Reward: 46.38
Iteration: 263 | Episodes: 16000 | Median Reward: -4.36 | Max Reward: 46.38
Iteration: 263 | Episodes: 16050 | Median Reward: -4.44 | Max Reward: 46.38
Iteration: 264 | Episodes: 16100 | Median Reward: -13.96 | Max Reward: 46.38
Iteration: 265 | Episodes: 16150 | Median Reward: -3.61 | Max Reward: 46.38
Iteration: 266 | Episodes: 16200 | Median Reward: 2.45 | Max Reward: 46.38
Iteration: 267 | Episodes: 16250 | Median Reward: 3.63 | Max Reward: 46.38
Iteration: 267 | Episodes: 16300 | Median Reward: -8.80 | Max Reward: 46.38
Iteration: 268 | Episodes: 16350 | Median Reward: 2.72 | Max Reward: 46.38
Iteration: 269 | Episodes: 16400 | Median Reward: 1.36 | Max Reward: 46.38
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -99.8      |
| time/                   |            |
|    fps                  | 342        |
|    iterations           | 270        |
|    time_elapsed         | 4848       |
|    total_timesteps      | 1658880    |
| train/                  |            |
|    approx_kl            | 0.27393353 |
|    clip_fraction        | 0.0669     |
|    clip_range           | 0.4        |
|    entropy_loss         | -78.7      |
|    explained_variance   | -1.65e-05  |
|    learning_rate        | 0.0001     |
|    loss                 | 57.4       |
|    n_updates            | 2690       |
|    policy_gradient_loss | 0.0131     |
|    std                  | 1.15       |
|    value_loss           | 146        |
----------------------------------------
Iteration: 270 | Episodes: 16450 | Median Reward: -6.32 | Max Reward: 46.38
Iteration: 271 | Episodes: 16500 | Median Reward: -5.83 | Max Reward: 46.38
Iteration: 272 | Episodes: 16550 | Median Reward: 1.96 | Max Reward: 46.38
Iteration: 272 | Episodes: 16600 | Median Reward: -6.85 | Max Reward: 46.38
Iteration: 273 | Episodes: 16650 | Median Reward: 7.88 | Max Reward: 46.38
Iteration: 274 | Episodes: 16700 | Median Reward: -3.90 | Max Reward: 46.38
Iteration: 275 | Episodes: 16750 | Median Reward: -0.64 | Max Reward: 46.38
Iteration: 276 | Episodes: 16800 | Median Reward: 4.97 | Max Reward: 46.38
Iteration: 277 | Episodes: 16850 | Median Reward: 6.95 | Max Reward: 46.38
Iteration: 277 | Episodes: 16900 | Median Reward: 5.88 | Max Reward: 46.38
Iteration: 278 | Episodes: 16950 | Median Reward: -2.36 | Max Reward: 46.38
Iteration: 279 | Episodes: 17000 | Median Reward: 2.40 | Max Reward: 46.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -99.3     |
| time/                   |           |
|    fps                  | 341       |
|    iterations           | 280       |
|    time_elapsed         | 5036      |
|    total_timesteps      | 1720320   |
| train/                  |           |
|    approx_kl            | 20.275194 |
|    clip_fraction        | 0.401     |
|    clip_range           | 0.4       |
|    entropy_loss         | -75.1     |
|    explained_variance   | -1.44e-05 |
|    learning_rate        | 0.0001    |
|    loss                 | 54.5      |
|    n_updates            | 2790      |
|    policy_gradient_loss | 0.0999    |
|    std                  | 1.16      |
|    value_loss           | 132       |
---------------------------------------
Iteration: 280 | Episodes: 17050 | Median Reward: 2.40 | Max Reward: 46.38
Iteration: 281 | Episodes: 17100 | Median Reward: 12.64 | Max Reward: 46.38
Iteration: 281 | Episodes: 17150 | Median Reward: 1.33 | Max Reward: 46.38
Iteration: 282 | Episodes: 17200 | Median Reward: 1.05 | Max Reward: 46.38
Iteration: 283 | Episodes: 17250 | Median Reward: 3.21 | Max Reward: 46.38
Iteration: 284 | Episodes: 17300 | Median Reward: 10.16 | Max Reward: 46.38
Iteration: 285 | Episodes: 17350 | Median Reward: 9.52 | Max Reward: 46.38
Iteration: 286 | Episodes: 17400 | Median Reward: -0.31 | Max Reward: 46.38
Iteration: 286 | Episodes: 17450 | Median Reward: -2.21 | Max Reward: 46.38
Iteration: 287 | Episodes: 17500 | Median Reward: -2.01 | Max Reward: 46.38
Iteration: 288 | Episodes: 17550 | Median Reward: 2.38 | Max Reward: 46.38
Iteration: 289 | Episodes: 17600 | Median Reward: 8.95 | Max Reward: 46.38
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -81.8     |
| time/                   |           |
|    fps                  | 341       |
|    iterations           | 290       |
|    time_elapsed         | 5212      |
|    total_timesteps      | 1781760   |
| train/                  |           |
|    approx_kl            | 15.825111 |
|    clip_fraction        | 0.39      |
|    clip_range           | 0.4       |
|    entropy_loss         | -76.1     |
|    explained_variance   | -9.78e-06 |
|    learning_rate        | 0.0001    |
|    loss                 | 94.2      |
|    n_updates            | 2890      |
|    policy_gradient_loss | 0.275     |
|    std                  | 1.16      |
|    value_loss           | 211       |
---------------------------------------
Iteration: 290 | Episodes: 17650 | Median Reward: 27.71 | Max Reward: 46.38
Iteration: 290 | Episodes: 17700 | Median Reward: 18.39 | Max Reward: 46.38
Iteration: 291 | Episodes: 17750 | Median Reward: 28.13 | Max Reward: 46.38
Iteration: 292 | Episodes: 17800 | Median Reward: 26.41 | Max Reward: 46.38
Iteration: 293 | Episodes: 17850 | Median Reward: 28.77 | Max Reward: 46.38
Iteration: 294 | Episodes: 17900 | Median Reward: 29.07 | Max Reward: 46.38
Iteration: 295 | Episodes: 17950 | Median Reward: 13.93 | Max Reward: 46.38
Iteration: 295 | Episodes: 18000 | Median Reward: 14.77 | Max Reward: 46.38
Iteration: 296 | Episodes: 18050 | Median Reward: 29.20 | Max Reward: 46.38
Iteration: 297 | Episodes: 18100 | Median Reward: 32.60 | Max Reward: 46.62
Iteration: 298 | Episodes: 18150 | Median Reward: 27.78 | Max Reward: 46.62
Iteration: 299 | Episodes: 18200 | Median Reward: 27.01 | Max Reward: 46.62
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.4     |
| time/                   |           |
|    fps                  | 342       |
|    iterations           | 300       |
|    time_elapsed         | 5388      |
|    total_timesteps      | 1843200   |
| train/                  |           |
|    approx_kl            | 0.9949995 |
|    clip_fraction        | 0.152     |
|    clip_range           | 0.4       |
|    entropy_loss         | -79       |
|    explained_variance   | -7.39e-06 |
|    learning_rate        | 0.0001    |
|    loss                 | 121       |
|    n_updates            | 2990      |
|    policy_gradient_loss | 0.0362    |
|    std                  | 1.17      |
|    value_loss           | 290       |
---------------------------------------
Iteration: 300 | Episodes: 18250 | Median Reward: 29.80 | Max Reward: 46.62
Iteration: 300 | Episodes: 18300 | Median Reward: 24.17 | Max Reward: 46.62
Iteration: 301 | Episodes: 18350 | Median Reward: 33.63 | Max Reward: 46.62
Iteration: 302 | Episodes: 18400 | Median Reward: 20.17 | Max Reward: 46.62
Iteration: 303 | Episodes: 18450 | Median Reward: 0.98 | Max Reward: 46.62
Iteration: 304 | Episodes: 18500 | Median Reward: -3.61 | Max Reward: 46.62
Iteration: 304 | Episodes: 18550 | Median Reward: 7.47 | Max Reward: 46.62
Iteration: 305 | Episodes: 18600 | Median Reward: 9.77 | Max Reward: 46.62
Iteration: 306 | Episodes: 18650 | Median Reward: 13.77 | Max Reward: 46.62
Iteration: 307 | Episodes: 18700 | Median Reward: 8.16 | Max Reward: 46.62
Iteration: 308 | Episodes: 18750 | Median Reward: 4.32 | Max Reward: 46.62
Iteration: 309 | Episodes: 18800 | Median Reward: 7.41 | Max Reward: 46.62
Iteration: 309 | Episodes: 18850 | Median Reward: -4.25 | Max Reward: 46.62
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -95.7     |
| time/                   |           |
|    fps                  | 342       |
|    iterations           | 310       |
|    time_elapsed         | 5565      |
|    total_timesteps      | 1904640   |
| train/                  |           |
|    approx_kl            | 8.3842325 |
|    clip_fraction        | 0.326     |
|    clip_range           | 0.4       |
|    entropy_loss         | -81.9     |
|    explained_variance   | -6.56e-06 |
|    learning_rate        | 0.0001    |
|    loss                 | 149       |
|    n_updates            | 3090      |
|    policy_gradient_loss | 0.0997    |
|    std                  | 1.17      |
|    value_loss           | 230       |
---------------------------------------
Iteration: 310 | Episodes: 18900 | Median Reward: -0.11 | Max Reward: 46.62
Iteration: 311 | Episodes: 18950 | Median Reward: 11.53 | Max Reward: 46.62
Iteration: 312 | Episodes: 19000 | Median Reward: 11.53 | Max Reward: 46.62
Iteration: 313 | Episodes: 19050 | Median Reward: 15.47 | Max Reward: 46.62
Iteration: 314 | Episodes: 19100 | Median Reward: 15.15 | Max Reward: 46.62
Iteration: 314 | Episodes: 19150 | Median Reward: 9.00 | Max Reward: 46.62
Iteration: 315 | Episodes: 19200 | Median Reward: 17.90 | Max Reward: 46.62
Iteration: 316 | Episodes: 19250 | Median Reward: 25.94 | Max Reward: 46.62
Iteration: 317 | Episodes: 19300 | Median Reward: 8.02 | Max Reward: 46.62
Iteration: 318 | Episodes: 19350 | Median Reward: 8.02 | Max Reward: 46.62
Iteration: 318 | Episodes: 19400 | Median Reward: 6.50 | Max Reward: 46.62
Iteration: 319 | Episodes: 19450 | Median Reward: 3.86 | Max Reward: 46.62
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -88.7      |
| time/                   |            |
|    fps                  | 342        |
|    iterations           | 320        |
|    time_elapsed         | 5742       |
|    total_timesteps      | 1966080    |
| train/                  |            |
|    approx_kl            | 0.38044629 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.4        |
|    entropy_loss         | -83.9      |
|    explained_variance   | -5.48e-06  |
|    learning_rate        | 0.0001     |
|    loss                 | 132        |
|    n_updates            | 3190       |
|    policy_gradient_loss | 0.0928     |
|    std                  | 1.18       |
|    value_loss           | 236        |
----------------------------------------
Iteration: 320 | Episodes: 19500 | Median Reward: 21.08 | Max Reward: 48.26
Iteration: 321 | Episodes: 19550 | Median Reward: 29.27 | Max Reward: 48.26
Iteration: 322 | Episodes: 19600 | Median Reward: 29.48 | Max Reward: 48.26
Iteration: 323 | Episodes: 19650 | Median Reward: 24.45 | Max Reward: 48.26
Iteration: 323 | Episodes: 19700 | Median Reward: 15.85 | Max Reward: 48.26
Iteration: 324 | Episodes: 19750 | Median Reward: 20.85 | Max Reward: 48.26
Iteration: 325 | Episodes: 19800 | Median Reward: 32.84 | Max Reward: 48.26
Iteration: 326 | Episodes: 19850 | Median Reward: 28.49 | Max Reward: 48.26
Iteration: 327 | Episodes: 19900 | Median Reward: 18.97 | Max Reward: 48.26
Iteration: 327 | Episodes: 19950 | Median Reward: 19.08 | Max Reward: 48.26
Iteration: 328 | Episodes: 20000 | Median Reward: 22.78 | Max Reward: 48.26
Iteration: 329 | Episodes: 20050 | Median Reward: 43.48 | Max Reward: 49.14
Iteration: 329 | Episodes: 20100 | Median Reward: 49.14 | Max Reward: 49.14
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 43.1       |
|    ep_rew_mean          | -2.12      |
| time/                   |            |
|    fps                  | 341        |
|    iterations           | 330        |
|    time_elapsed         | 5934       |
|    total_timesteps      | 2027520    |
| train/                  |            |
|    approx_kl            | 0.19332471 |
|    clip_fraction        | 0.0969     |
|    clip_range           | 0.4        |
|    entropy_loss         | -84        |
|    explained_variance   | -4.29e-06  |
|    learning_rate        | 0.0001     |
|    loss                 | 181        |
|    n_updates            | 3290       |
|    policy_gradient_loss | 0.00566    |
|    std                  | 1.19       |
|    value_loss           | 314        |
----------------------------------------
Iteration: 330 | Episodes: 20150 | Median Reward: 49.14 | Max Reward: 49.14
Iteration: 330 | Episodes: 20200 | Median Reward: 31.02 | Max Reward: 49.14
Iteration: 331 | Episodes: 20250 | Median Reward: 37.85 | Max Reward: 49.14
Iteration: 332 | Episodes: 20300 | Median Reward: 30.50 | Max Reward: 49.14
Iteration: 333 | Episodes: 20350 | Median Reward: 30.33 | Max Reward: 49.14
Iteration: 334 | Episodes: 20400 | Median Reward: 30.32 | Max Reward: 49.14
Iteration: 335 | Episodes: 20450 | Median Reward: 26.42 | Max Reward: 49.14
Iteration: 335 | Episodes: 20500 | Median Reward: 29.74 | Max Reward: 49.14
Iteration: 336 | Episodes: 20550 | Median Reward: 24.69 | Max Reward: 49.14
Iteration: 337 | Episodes: 20600 | Median Reward: 25.22 | Max Reward: 49.14
Iteration: 338 | Episodes: 20650 | Median Reward: 25.54 | Max Reward: 49.14
Iteration: 339 | Episodes: 20700 | Median Reward: 24.96 | Max Reward: 49.14
Iteration: 339 | Episodes: 20750 | Median Reward: 24.17 | Max Reward: 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -75.9     |
| time/                   |           |
|    fps                  | 341       |
|    iterations           | 340       |
|    time_elapsed         | 6115      |
|    total_timesteps      | 2088960   |
| train/                  |           |
|    approx_kl            | 5.2454157 |
|    clip_fraction        | 0.213     |
|    clip_range           | 0.4       |
|    entropy_loss         | -85.2     |
|    explained_variance   | -3.81e-06 |
|    learning_rate        | 0.0001    |
|    loss                 | 168       |
|    n_updates            | 3390      |
|    policy_gradient_loss | 0.015     |
|    std                  | 1.19      |
|    value_loss           | 340       |
---------------------------------------
Iteration: 340 | Episodes: 20800 | Median Reward: 34.69 | Max Reward: 49.14
Iteration: 341 | Episodes: 20850 | Median Reward: 30.57 | Max Reward: 49.14
Iteration: 342 | Episodes: 20900 | Median Reward: 30.75 | Max Reward: 49.14
Iteration: 343 | Episodes: 20950 | Median Reward: 32.49 | Max Reward: 49.14
Iteration: 344 | Episodes: 21000 | Median Reward: 29.28 | Max Reward: 49.14
Iteration: 344 | Episodes: 21050 | Median Reward: 31.08 | Max Reward: 49.14
Iteration: 345 | Episodes: 21100 | Median Reward: 33.66 | Max Reward: 49.14
Iteration: 346 | Episodes: 21150 | Median Reward: 29.08 | Max Reward: 49.14
Iteration: 347 | Episodes: 21200 | Median Reward: 30.93 | Max Reward: 49.14
Iteration: 348 | Episodes: 21250 | Median Reward: 33.47 | Max Reward: 49.14
Iteration: 348 | Episodes: 21300 | Median Reward: 23.63 | Max Reward: 49.14
Iteration: 349 | Episodes: 21350 | Median Reward: 33.75 | Max Reward: 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.9     |
| time/                   |           |
|    fps                  | 341       |
|    iterations           | 350       |
|    time_elapsed         | 6296      |
|    total_timesteps      | 2150400   |
| train/                  |           |
|    approx_kl            | 0.3979816 |
|    clip_fraction        | 0.0178    |
|    clip_range           | 0.4       |
|    entropy_loss         | -87.5     |
|    explained_variance   | -3.93e-06 |
|    learning_rate        | 0.0001    |
|    loss                 | 142       |
|    n_updates            | 3490      |
|    policy_gradient_loss | -0.00487  |
|    std                  | 1.19      |
|    value_loss           | 303       |
---------------------------------------
Iteration: 350 | Episodes: 21400 | Median Reward: 33.25 | Max Reward: 49.14
Iteration: 351 | Episodes: 21450 | Median Reward: 21.64 | Max Reward: 49.14
Iteration: 352 | Episodes: 21500 | Median Reward: 20.56 | Max Reward: 49.14
Iteration: 353 | Episodes: 21550 | Median Reward: 23.14 | Max Reward: 49.14
Iteration: 353 | Episodes: 21600 | Median Reward: 30.87 | Max Reward: 49.14
Iteration: 354 | Episodes: 21650 | Median Reward: 21.41 | Max Reward: 49.14
Iteration: 355 | Episodes: 21700 | Median Reward: 33.89 | Max Reward: 49.14
Iteration: 356 | Episodes: 21750 | Median Reward: 34.64 | Max Reward: 49.14
Iteration: 357 | Episodes: 21800 | Median Reward: 33.29 | Max Reward: 49.14
Iteration: 358 | Episodes: 21850 | Median Reward: 28.73 | Max Reward: 49.14
Iteration: 358 | Episodes: 21900 | Median Reward: 37.15 | Max Reward: 49.14
Iteration: 359 | Episodes: 21950 | Median Reward: 28.75 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.9         |
| time/                   |               |
|    fps                  | 341           |
|    iterations           | 360           |
|    time_elapsed         | 6479          |
|    total_timesteps      | 2211840       |
| train/                  |               |
|    approx_kl            | 1.3734825e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -88           |
|    explained_variance   | -2.86e-06     |
|    learning_rate        | 0.0001        |
|    loss                 | 211           |
|    n_updates            | 3590          |
|    policy_gradient_loss | -9.1e-05      |
|    std                  | 1.2           |
|    value_loss           | 371           |
-------------------------------------------
Iteration: 360 | Episodes: 22000 | Median Reward: 26.83 | Max Reward: 49.14
Iteration: 361 | Episodes: 22050 | Median Reward: 30.66 | Max Reward: 49.14
Iteration: 362 | Episodes: 22100 | Median Reward: 32.63 | Max Reward: 49.14
Iteration: 362 | Episodes: 22150 | Median Reward: 28.83 | Max Reward: 49.14
Iteration: 363 | Episodes: 22200 | Median Reward: 26.39 | Max Reward: 49.14
Iteration: 364 | Episodes: 22250 | Median Reward: 36.48 | Max Reward: 49.14
Iteration: 365 | Episodes: 22300 | Median Reward: 31.79 | Max Reward: 49.14
Iteration: 366 | Episodes: 22350 | Median Reward: 31.79 | Max Reward: 49.14
Iteration: 367 | Episodes: 22400 | Median Reward: 31.95 | Max Reward: 49.14
Iteration: 367 | Episodes: 22450 | Median Reward: 35.82 | Max Reward: 49.14
Iteration: 368 | Episodes: 22500 | Median Reward: 35.34 | Max Reward: 49.14
Iteration: 369 | Episodes: 22550 | Median Reward: 31.32 | Max Reward: 49.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.1       |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 370         |
|    time_elapsed         | 6672        |
|    total_timesteps      | 2273280     |
| train/                  |             |
|    approx_kl            | 0.000323005 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -88.1       |
|    explained_variance   | -2.74e-06   |
|    learning_rate        | 0.0001      |
|    loss                 | 125         |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.000489   |
|    std                  | 1.21        |
|    value_loss           | 343         |
-----------------------------------------
Iteration: 370 | Episodes: 22600 | Median Reward: 28.06 | Max Reward: 49.14
Iteration: 371 | Episodes: 22650 | Median Reward: 28.11 | Max Reward: 49.14
Iteration: 372 | Episodes: 22700 | Median Reward: 27.87 | Max Reward: 49.14
Iteration: 372 | Episodes: 22750 | Median Reward: 28.25 | Max Reward: 49.14
Iteration: 373 | Episodes: 22800 | Median Reward: 28.46 | Max Reward: 49.14
Iteration: 374 | Episodes: 22850 | Median Reward: 31.83 | Max Reward: 49.14
Iteration: 375 | Episodes: 22900 | Median Reward: 33.81 | Max Reward: 49.14
Iteration: 376 | Episodes: 22950 | Median Reward: 34.16 | Max Reward: 49.14
Iteration: 376 | Episodes: 23000 | Median Reward: 22.87 | Max Reward: 49.14
Iteration: 377 | Episodes: 23050 | Median Reward: 32.61 | Max Reward: 49.14
Iteration: 378 | Episodes: 23100 | Median Reward: 31.36 | Max Reward: 49.14
Iteration: 379 | Episodes: 23150 | Median Reward: 29.51 | Max Reward: 49.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.9       |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 380         |
|    time_elapsed         | 6852        |
|    total_timesteps      | 2334720     |
| train/                  |             |
|    approx_kl            | 0.004709366 |
|    clip_fraction        | 0.00127     |
|    clip_range           | 0.4         |
|    entropy_loss         | -88.4       |
|    explained_variance   | -2.5e-06    |
|    learning_rate        | 0.0001      |
|    loss                 | 148         |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00288    |
|    std                  | 1.22        |
|    value_loss           | 271         |
-----------------------------------------
Iteration: 380 | Episodes: 23200 | Median Reward: 29.65 | Max Reward: 49.14
Iteration: 381 | Episodes: 23250 | Median Reward: 27.35 | Max Reward: 49.14
Iteration: 381 | Episodes: 23300 | Median Reward: 29.68 | Max Reward: 49.14
Iteration: 382 | Episodes: 23350 | Median Reward: 32.77 | Max Reward: 49.14
Iteration: 383 | Episodes: 23400 | Median Reward: 34.64 | Max Reward: 49.14
Iteration: 384 | Episodes: 23450 | Median Reward: 32.92 | Max Reward: 49.14
Iteration: 385 | Episodes: 23500 | Median Reward: 33.42 | Max Reward: 49.14
Iteration: 385 | Episodes: 23550 | Median Reward: 36.13 | Max Reward: 49.14
Iteration: 386 | Episodes: 23600 | Median Reward: 38.20 | Max Reward: 49.14
Iteration: 387 | Episodes: 23650 | Median Reward: 35.93 | Max Reward: 49.14
Iteration: 388 | Episodes: 23700 | Median Reward: 28.34 | Max Reward: 49.14
Iteration: 389 | Episodes: 23750 | Median Reward: 30.19 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.4        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 390          |
|    time_elapsed         | 7032         |
|    total_timesteps      | 2396160      |
| train/                  |              |
|    approx_kl            | 0.0021904616 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -88.6        |
|    explained_variance   | -2.03e-06    |
|    learning_rate        | 0.0001       |
|    loss                 | 126          |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.00186     |
|    std                  | 1.22         |
|    value_loss           | 317          |
------------------------------------------
Iteration: 390 | Episodes: 23800 | Median Reward: 30.38 | Max Reward: 49.14
Iteration: 390 | Episodes: 23850 | Median Reward: 26.75 | Max Reward: 49.14
Iteration: 391 | Episodes: 23900 | Median Reward: 35.12 | Max Reward: 49.14
Iteration: 392 | Episodes: 23950 | Median Reward: 33.60 | Max Reward: 49.14
Iteration: 393 | Episodes: 24000 | Median Reward: 32.46 | Max Reward: 49.14
Iteration: 394 | Episodes: 24050 | Median Reward: 30.17 | Max Reward: 49.14
Iteration: 395 | Episodes: 24100 | Median Reward: 36.53 | Max Reward: 49.14
Iteration: 395 | Episodes: 24150 | Median Reward: 29.28 | Max Reward: 49.14
Iteration: 396 | Episodes: 24200 | Median Reward: 35.12 | Max Reward: 49.14
Iteration: 397 | Episodes: 24250 | Median Reward: 33.82 | Max Reward: 49.14
Iteration: 398 | Episodes: 24300 | Median Reward: 27.22 | Max Reward: 49.14
Iteration: 399 | Episodes: 24350 | Median Reward: 28.48 | Max Reward: 49.14
Iteration: 399 | Episodes: 24400 | Median Reward: 31.22 | Max Reward: 49.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.7       |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 400         |
|    time_elapsed         | 7217        |
|    total_timesteps      | 2457600     |
| train/                  |             |
|    approx_kl            | 0.004808559 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -88.9       |
|    explained_variance   | -1.79e-06   |
|    learning_rate        | 0.0001      |
|    loss                 | 143         |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.00225    |
|    std                  | 1.24        |
|    value_loss           | 267         |
-----------------------------------------
Iteration: 400 | Episodes: 24450 | Median Reward: 30.43 | Max Reward: 49.14
Iteration: 401 | Episodes: 24500 | Median Reward: 30.92 | Max Reward: 49.14
Iteration: 402 | Episodes: 24550 | Median Reward: 33.38 | Max Reward: 49.14
Iteration: 403 | Episodes: 24600 | Median Reward: 33.63 | Max Reward: 49.14
Iteration: 404 | Episodes: 24650 | Median Reward: 30.76 | Max Reward: 49.14
Iteration: 404 | Episodes: 24700 | Median Reward: 31.99 | Max Reward: 49.14
Iteration: 405 | Episodes: 24750 | Median Reward: 30.07 | Max Reward: 49.14
Iteration: 406 | Episodes: 24800 | Median Reward: 30.07 | Max Reward: 49.14
Iteration: 407 | Episodes: 24850 | Median Reward: 37.38 | Max Reward: 49.14
Iteration: 408 | Episodes: 24900 | Median Reward: 37.54 | Max Reward: 49.14
Iteration: 409 | Episodes: 24950 | Median Reward: 31.33 | Max Reward: 49.14
Iteration: 409 | Episodes: 25000 | Median Reward: 29.56 | Max Reward: 49.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.3       |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 410         |
|    time_elapsed         | 7402        |
|    total_timesteps      | 2519040     |
| train/                  |             |
|    approx_kl            | 0.002790405 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -89.2       |
|    explained_variance   | -1.55e-06   |
|    learning_rate        | 0.0001      |
|    loss                 | 156         |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.00384    |
|    std                  | 1.25        |
|    value_loss           | 283         |
-----------------------------------------
Iteration: 410 | Episodes: 25050 | Median Reward: 31.00 | Max Reward: 49.14
Iteration: 411 | Episodes: 25100 | Median Reward: 33.73 | Max Reward: 49.14
Iteration: 412 | Episodes: 25150 | Median Reward: 33.73 | Max Reward: 49.14
Iteration: 413 | Episodes: 25200 | Median Reward: 34.24 | Max Reward: 49.14
Iteration: 413 | Episodes: 25250 | Median Reward: 30.73 | Max Reward: 49.14
Iteration: 414 | Episodes: 25300 | Median Reward: 33.70 | Max Reward: 49.14
Iteration: 415 | Episodes: 25350 | Median Reward: 33.01 | Max Reward: 49.14
Iteration: 416 | Episodes: 25400 | Median Reward: 33.01 | Max Reward: 49.14
Iteration: 417 | Episodes: 25450 | Median Reward: 29.89 | Max Reward: 49.14
Iteration: 418 | Episodes: 25500 | Median Reward: 35.12 | Max Reward: 49.14
Iteration: 418 | Episodes: 25550 | Median Reward: 35.15 | Max Reward: 49.14
Iteration: 419 | Episodes: 25600 | Median Reward: 33.46 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.4         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 420           |
|    time_elapsed         | 7583          |
|    total_timesteps      | 2580480       |
| train/                  |               |
|    approx_kl            | 5.0538023e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -89.4         |
|    explained_variance   | -1.07e-06     |
|    learning_rate        | 0.0001        |
|    loss                 | 175           |
|    n_updates            | 4190          |
|    policy_gradient_loss | -0.00013      |
|    std                  | 1.27          |
|    value_loss           | 355           |
-------------------------------------------
Iteration: 420 | Episodes: 25650 | Median Reward: 33.60 | Max Reward: 49.14
Iteration: 421 | Episodes: 25700 | Median Reward: 34.05 | Max Reward: 49.14
Iteration: 422 | Episodes: 25750 | Median Reward: 33.63 | Max Reward: 49.14
Iteration: 422 | Episodes: 25800 | Median Reward: 31.90 | Max Reward: 49.14
Iteration: 423 | Episodes: 25850 | Median Reward: 34.95 | Max Reward: 49.14
Iteration: 424 | Episodes: 25900 | Median Reward: 36.18 | Max Reward: 49.14
Iteration: 425 | Episodes: 25950 | Median Reward: 35.34 | Max Reward: 49.14
Iteration: 426 | Episodes: 26000 | Median Reward: 33.98 | Max Reward: 49.14
Iteration: 427 | Episodes: 26050 | Median Reward: 30.62 | Max Reward: 49.14
Iteration: 427 | Episodes: 26100 | Median Reward: 33.18 | Max Reward: 49.14
Iteration: 428 | Episodes: 26150 | Median Reward: 36.96 | Max Reward: 49.14
Iteration: 429 | Episodes: 26200 | Median Reward: 37.13 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.5         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 430           |
|    time_elapsed         | 7762          |
|    total_timesteps      | 2641920       |
| train/                  |               |
|    approx_kl            | 0.00028679555 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -89.7         |
|    explained_variance   | -8.34e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 182           |
|    n_updates            | 4290          |
|    policy_gradient_loss | -0.000377     |
|    std                  | 1.28          |
|    value_loss           | 340           |
-------------------------------------------
Iteration: 430 | Episodes: 26250 | Median Reward: 33.58 | Max Reward: 49.14
Iteration: 431 | Episodes: 26300 | Median Reward: 33.58 | Max Reward: 49.14
Iteration: 432 | Episodes: 26350 | Median Reward: 31.92 | Max Reward: 49.14
Iteration: 432 | Episodes: 26400 | Median Reward: 27.38 | Max Reward: 49.14
Iteration: 433 | Episodes: 26450 | Median Reward: 37.51 | Max Reward: 49.14
Iteration: 434 | Episodes: 26500 | Median Reward: 34.29 | Max Reward: 49.14
Iteration: 435 | Episodes: 26550 | Median Reward: 34.27 | Max Reward: 49.14
Iteration: 436 | Episodes: 26600 | Median Reward: 32.18 | Max Reward: 49.14
Iteration: 436 | Episodes: 26650 | Median Reward: 22.86 | Max Reward: 49.14
Iteration: 437 | Episodes: 26700 | Median Reward: 34.58 | Max Reward: 49.14
Iteration: 438 | Episodes: 26750 | Median Reward: 35.35 | Max Reward: 49.14
Iteration: 439 | Episodes: 26800 | Median Reward: 33.85 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68           |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 440           |
|    time_elapsed         | 7950          |
|    total_timesteps      | 2703360       |
| train/                  |               |
|    approx_kl            | 0.00087437505 |
|    clip_fraction        | 0.000366      |
|    clip_range           | 0.4           |
|    entropy_loss         | -89.9         |
|    explained_variance   | -8.34e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 165           |
|    n_updates            | 4390          |
|    policy_gradient_loss | -0.000818     |
|    std                  | 1.29          |
|    value_loss           | 317           |
-------------------------------------------
Iteration: 440 | Episodes: 26850 | Median Reward: 33.85 | Max Reward: 49.14
Iteration: 441 | Episodes: 26900 | Median Reward: 26.44 | Max Reward: 49.14
Iteration: 441 | Episodes: 26950 | Median Reward: 33.19 | Max Reward: 49.14
Iteration: 442 | Episodes: 27000 | Median Reward: 30.75 | Max Reward: 49.14
Iteration: 443 | Episodes: 27050 | Median Reward: 32.93 | Max Reward: 49.14
Iteration: 444 | Episodes: 27100 | Median Reward: 33.84 | Max Reward: 49.14
Iteration: 445 | Episodes: 27150 | Median Reward: 35.73 | Max Reward: 49.14
Iteration: 446 | Episodes: 27200 | Median Reward: 34.48 | Max Reward: 49.14
Iteration: 446 | Episodes: 27250 | Median Reward: 32.07 | Max Reward: 49.14
Iteration: 447 | Episodes: 27300 | Median Reward: 39.12 | Max Reward: 49.14
Iteration: 448 | Episodes: 27350 | Median Reward: 36.94 | Max Reward: 49.14
Iteration: 449 | Episodes: 27400 | Median Reward: 36.48 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.3        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 450          |
|    time_elapsed         | 8134         |
|    total_timesteps      | 2764800      |
| train/                  |              |
|    approx_kl            | 0.0007241854 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -90.2        |
|    explained_variance   | -7.15e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 139          |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.000771    |
|    std                  | 1.31         |
|    value_loss           | 320          |
------------------------------------------
Iteration: 450 | Episodes: 27450 | Median Reward: 36.74 | Max Reward: 49.14
Iteration: 450 | Episodes: 27500 | Median Reward: 31.30 | Max Reward: 49.14
Iteration: 451 | Episodes: 27550 | Median Reward: 32.08 | Max Reward: 49.14
Iteration: 452 | Episodes: 27600 | Median Reward: 30.41 | Max Reward: 49.14
Iteration: 453 | Episodes: 27650 | Median Reward: 29.69 | Max Reward: 49.14
Iteration: 454 | Episodes: 27700 | Median Reward: 29.85 | Max Reward: 49.14
Iteration: 454 | Episodes: 27750 | Median Reward: 25.12 | Max Reward: 49.14
Iteration: 455 | Episodes: 27800 | Median Reward: 34.19 | Max Reward: 49.14
Iteration: 456 | Episodes: 27850 | Median Reward: 35.07 | Max Reward: 49.14
Iteration: 457 | Episodes: 27900 | Median Reward: 34.90 | Max Reward: 49.14
Iteration: 458 | Episodes: 27950 | Median Reward: 33.22 | Max Reward: 49.14
Iteration: 459 | Episodes: 28000 | Median Reward: 34.00 | Max Reward: 49.14
Iteration: 459 | Episodes: 28050 | Median Reward: 35.50 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66           |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 460           |
|    time_elapsed         | 8316          |
|    total_timesteps      | 2826240       |
| train/                  |               |
|    approx_kl            | 0.00019935815 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -90.5         |
|    explained_variance   | -4.77e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 159           |
|    n_updates            | 4590          |
|    policy_gradient_loss | -0.000651     |
|    std                  | 1.32          |
|    value_loss           | 318           |
-------------------------------------------
Iteration: 460 | Episodes: 28100 | Median Reward: 34.79 | Max Reward: 49.14
Iteration: 461 | Episodes: 28150 | Median Reward: 30.86 | Max Reward: 49.14
Iteration: 462 | Episodes: 28200 | Median Reward: 33.22 | Max Reward: 49.14
Iteration: 463 | Episodes: 28250 | Median Reward: 38.29 | Max Reward: 49.14
Iteration: 464 | Episodes: 28300 | Median Reward: 32.73 | Max Reward: 49.14
Iteration: 464 | Episodes: 28350 | Median Reward: 32.51 | Max Reward: 49.14
Iteration: 465 | Episodes: 28400 | Median Reward: 33.35 | Max Reward: 49.14
Iteration: 466 | Episodes: 28450 | Median Reward: 33.83 | Max Reward: 49.14
Iteration: 467 | Episodes: 28500 | Median Reward: 36.18 | Max Reward: 49.14
Iteration: 468 | Episodes: 28550 | Median Reward: 35.60 | Max Reward: 49.14
Iteration: 468 | Episodes: 28600 | Median Reward: 34.77 | Max Reward: 49.14
Iteration: 469 | Episodes: 28650 | Median Reward: 29.75 | Max Reward: 49.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.9       |
| time/                   |             |
|    fps                  | 339         |
|    iterations           | 470         |
|    time_elapsed         | 8494        |
|    total_timesteps      | 2887680     |
| train/                  |             |
|    approx_kl            | 0.007857042 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -90.9       |
|    explained_variance   | -4.77e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 178         |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.0053     |
|    std                  | 1.34        |
|    value_loss           | 329         |
-----------------------------------------
Iteration: 470 | Episodes: 28700 | Median Reward: 34.90 | Max Reward: 49.14
Iteration: 471 | Episodes: 28750 | Median Reward: 35.77 | Max Reward: 49.14
Iteration: 472 | Episodes: 28800 | Median Reward: 37.04 | Max Reward: 49.14
Iteration: 473 | Episodes: 28850 | Median Reward: 35.17 | Max Reward: 49.14
Iteration: 473 | Episodes: 28900 | Median Reward: 40.16 | Max Reward: 49.14
Iteration: 474 | Episodes: 28950 | Median Reward: 36.03 | Max Reward: 49.14
Iteration: 475 | Episodes: 29000 | Median Reward: 33.87 | Max Reward: 49.14
Iteration: 476 | Episodes: 29050 | Median Reward: 33.02 | Max Reward: 49.14
Iteration: 477 | Episodes: 29100 | Median Reward: 32.73 | Max Reward: 49.14
Iteration: 478 | Episodes: 29150 | Median Reward: 32.34 | Max Reward: 49.14
Iteration: 478 | Episodes: 29200 | Median Reward: 32.04 | Max Reward: 49.14
Iteration: 479 | Episodes: 29250 | Median Reward: 39.28 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.2        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 480          |
|    time_elapsed         | 8680         |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0016010546 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -91.1        |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 134          |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.000684    |
|    std                  | 1.35         |
|    value_loss           | 287          |
------------------------------------------
Iteration: 480 | Episodes: 29300 | Median Reward: 39.02 | Max Reward: 49.14
Iteration: 481 | Episodes: 29350 | Median Reward: 37.80 | Max Reward: 49.14
Iteration: 482 | Episodes: 29400 | Median Reward: 36.12 | Max Reward: 49.14
Iteration: 483 | Episodes: 29450 | Median Reward: 36.52 | Max Reward: 49.14
Iteration: 483 | Episodes: 29500 | Median Reward: 38.85 | Max Reward: 49.14
Iteration: 484 | Episodes: 29550 | Median Reward: 34.37 | Max Reward: 49.14
Iteration: 485 | Episodes: 29600 | Median Reward: 37.72 | Max Reward: 49.14
Iteration: 486 | Episodes: 29650 | Median Reward: 39.83 | Max Reward: 49.14
Iteration: 487 | Episodes: 29700 | Median Reward: 33.96 | Max Reward: 49.14
Iteration: 487 | Episodes: 29750 | Median Reward: 37.98 | Max Reward: 49.14
Iteration: 488 | Episodes: 29800 | Median Reward: 30.43 | Max Reward: 49.14
Iteration: 489 | Episodes: 29850 | Median Reward: 26.30 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.4         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 490           |
|    time_elapsed         | 8864          |
|    total_timesteps      | 3010560       |
| train/                  |               |
|    approx_kl            | 0.00019755613 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -91.3         |
|    explained_variance   | -3.58e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 125           |
|    n_updates            | 4890          |
|    policy_gradient_loss | -0.000163     |
|    std                  | 1.37          |
|    value_loss           | 263           |
-------------------------------------------
Iteration: 490 | Episodes: 29900 | Median Reward: 29.87 | Max Reward: 49.14
Iteration: 491 | Episodes: 29950 | Median Reward: 30.14 | Max Reward: 49.14
Iteration: 491 | Episodes: 30000 | Median Reward: 29.89 | Max Reward: 49.14
Iteration: 492 | Episodes: 30050 | Median Reward: 29.62 | Max Reward: 49.14
Iteration: 493 | Episodes: 30100 | Median Reward: 34.45 | Max Reward: 49.14
Iteration: 494 | Episodes: 30150 | Median Reward: 35.37 | Max Reward: 49.14
Iteration: 495 | Episodes: 30200 | Median Reward: 34.71 | Max Reward: 49.14
Iteration: 496 | Episodes: 30250 | Median Reward: 33.37 | Max Reward: 49.14
Iteration: 496 | Episodes: 30300 | Median Reward: 35.33 | Max Reward: 49.14
Iteration: 497 | Episodes: 30350 | Median Reward: 29.79 | Max Reward: 49.14
Iteration: 498 | Episodes: 30400 | Median Reward: 31.02 | Max Reward: 49.14
Iteration: 499 | Episodes: 30450 | Median Reward: 33.71 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.5        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 500          |
|    time_elapsed         | 9042         |
|    total_timesteps      | 3072000      |
| train/                  |              |
|    approx_kl            | 0.0003622198 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -91.5        |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 137          |
|    n_updates            | 4990         |
|    policy_gradient_loss | -0.000438    |
|    std                  | 1.38         |
|    value_loss           | 287          |
------------------------------------------
Iteration: 500 | Episodes: 30500 | Median Reward: 34.66 | Max Reward: 49.14
Iteration: 501 | Episodes: 30550 | Median Reward: 32.07 | Max Reward: 49.14
Iteration: 501 | Episodes: 30600 | Median Reward: 33.08 | Max Reward: 49.14
Iteration: 502 | Episodes: 30650 | Median Reward: 33.08 | Max Reward: 49.14
Iteration: 503 | Episodes: 30700 | Median Reward: 36.72 | Max Reward: 49.14
Iteration: 504 | Episodes: 30750 | Median Reward: 34.37 | Max Reward: 49.14
Iteration: 505 | Episodes: 30800 | Median Reward: 35.45 | Max Reward: 49.14
Iteration: 505 | Episodes: 30850 | Median Reward: 39.62 | Max Reward: 49.14
Iteration: 506 | Episodes: 30900 | Median Reward: 34.24 | Max Reward: 49.14
Iteration: 507 | Episodes: 30950 | Median Reward: 33.51 | Max Reward: 49.14
Iteration: 508 | Episodes: 31000 | Median Reward: 36.04 | Max Reward: 49.14
Iteration: 509 | Episodes: 31050 | Median Reward: 37.51 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65           |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 510           |
|    time_elapsed         | 9221          |
|    total_timesteps      | 3133440       |
| train/                  |               |
|    approx_kl            | 0.00045615312 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -91.8         |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 158           |
|    n_updates            | 5090          |
|    policy_gradient_loss | -0.000402     |
|    std                  | 1.39          |
|    value_loss           | 340           |
-------------------------------------------
Iteration: 510 | Episodes: 31100 | Median Reward: 34.81 | Max Reward: 49.14
Iteration: 510 | Episodes: 31150 | Median Reward: 31.28 | Max Reward: 49.14
Iteration: 511 | Episodes: 31200 | Median Reward: 28.76 | Max Reward: 49.14
Iteration: 512 | Episodes: 31250 | Median Reward: 31.12 | Max Reward: 49.14
Iteration: 513 | Episodes: 31300 | Median Reward: 36.31 | Max Reward: 49.14
Iteration: 514 | Episodes: 31350 | Median Reward: 37.39 | Max Reward: 49.14
Iteration: 515 | Episodes: 31400 | Median Reward: 29.64 | Max Reward: 49.14
Iteration: 515 | Episodes: 31450 | Median Reward: 35.20 | Max Reward: 49.14
Iteration: 516 | Episodes: 31500 | Median Reward: 35.12 | Max Reward: 49.14
Iteration: 517 | Episodes: 31550 | Median Reward: 34.14 | Max Reward: 49.14
Iteration: 518 | Episodes: 31600 | Median Reward: 29.23 | Max Reward: 49.14
Iteration: 519 | Episodes: 31650 | Median Reward: 35.25 | Max Reward: 49.14
Iteration: 519 | Episodes: 31700 | Median Reward: 37.48 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64          |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 520          |
|    time_elapsed         | 9408         |
|    total_timesteps      | 3194880      |
| train/                  |              |
|    approx_kl            | 0.0006946716 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -92          |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 106          |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.000683    |
|    std                  | 1.4          |
|    value_loss           | 296          |
------------------------------------------
Iteration: 520 | Episodes: 31750 | Median Reward: 36.73 | Max Reward: 49.14
Iteration: 521 | Episodes: 31800 | Median Reward: 34.53 | Max Reward: 49.14
Iteration: 522 | Episodes: 31850 | Median Reward: 34.57 | Max Reward: 49.14
Iteration: 523 | Episodes: 31900 | Median Reward: 35.64 | Max Reward: 49.14
Iteration: 524 | Episodes: 31950 | Median Reward: 40.01 | Max Reward: 49.14
Iteration: 524 | Episodes: 32000 | Median Reward: 36.20 | Max Reward: 49.14
Iteration: 525 | Episodes: 32050 | Median Reward: 33.63 | Max Reward: 49.14
Iteration: 526 | Episodes: 32100 | Median Reward: 36.15 | Max Reward: 49.14
Iteration: 527 | Episodes: 32150 | Median Reward: 35.32 | Max Reward: 49.14
Iteration: 528 | Episodes: 32200 | Median Reward: 34.49 | Max Reward: 49.14
Iteration: 528 | Episodes: 32250 | Median Reward: 31.94 | Max Reward: 49.14
Iteration: 529 | Episodes: 32300 | Median Reward: 42.55 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.7         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 530           |
|    time_elapsed         | 9580          |
|    total_timesteps      | 3256320       |
| train/                  |               |
|    approx_kl            | 0.00026201882 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -92.2         |
|    explained_variance   | -3.58e-07     |
|    learning_rate        | 0.0001        |
|    loss                 | 138           |
|    n_updates            | 5290          |
|    policy_gradient_loss | -0.000264     |
|    std                  | 1.42          |
|    value_loss           | 293           |
-------------------------------------------
Iteration: 530 | Episodes: 32350 | Median Reward: 35.09 | Max Reward: 49.14
Iteration: 531 | Episodes: 32400 | Median Reward: 34.35 | Max Reward: 49.14
Iteration: 532 | Episodes: 32450 | Median Reward: 36.89 | Max Reward: 49.14
Iteration: 533 | Episodes: 32500 | Median Reward: 38.55 | Max Reward: 49.14
Iteration: 533 | Episodes: 32550 | Median Reward: 33.43 | Max Reward: 49.14
Iteration: 534 | Episodes: 32600 | Median Reward: 38.34 | Max Reward: 49.14
Iteration: 535 | Episodes: 32650 | Median Reward: 38.37 | Max Reward: 49.14
Iteration: 536 | Episodes: 32700 | Median Reward: 36.62 | Max Reward: 49.14
Iteration: 537 | Episodes: 32750 | Median Reward: 32.01 | Max Reward: 49.14
Iteration: 538 | Episodes: 32800 | Median Reward: 33.58 | Max Reward: 49.14
Iteration: 538 | Episodes: 32850 | Median Reward: 40.00 | Max Reward: 49.14
Iteration: 539 | Episodes: 32900 | Median Reward: 30.74 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.7        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 540          |
|    time_elapsed         | 9754         |
|    total_timesteps      | 3317760      |
| train/                  |              |
|    approx_kl            | 7.960401e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -92.5        |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 161          |
|    n_updates            | 5390         |
|    policy_gradient_loss | 0.000136     |
|    std                  | 1.43         |
|    value_loss           | 362          |
------------------------------------------
Iteration: 540 | Episodes: 32950 | Median Reward: 35.58 | Max Reward: 49.14
Iteration: 541 | Episodes: 33000 | Median Reward: 36.09 | Max Reward: 49.14
Iteration: 542 | Episodes: 33050 | Median Reward: 35.26 | Max Reward: 49.14
Iteration: 542 | Episodes: 33100 | Median Reward: 40.29 | Max Reward: 49.14
Iteration: 543 | Episodes: 33150 | Median Reward: 33.90 | Max Reward: 49.14
Iteration: 544 | Episodes: 33200 | Median Reward: 32.60 | Max Reward: 49.14
Iteration: 545 | Episodes: 33250 | Median Reward: 35.04 | Max Reward: 49.14
Iteration: 546 | Episodes: 33300 | Median Reward: 36.26 | Max Reward: 49.14
Iteration: 547 | Episodes: 33350 | Median Reward: 35.66 | Max Reward: 49.14
Iteration: 547 | Episodes: 33400 | Median Reward: 36.12 | Max Reward: 49.14
Iteration: 548 | Episodes: 33450 | Median Reward: 38.83 | Max Reward: 49.14
Iteration: 549 | Episodes: 33500 | Median Reward: 36.51 | Max Reward: 49.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65         |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 550         |
|    time_elapsed         | 9927        |
|    total_timesteps      | 3379200     |
| train/                  |             |
|    approx_kl            | 0.001390999 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -92.7       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0001      |
|    loss                 | 149         |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.000901   |
|    std                  | 1.45        |
|    value_loss           | 340         |
-----------------------------------------
Iteration: 550 | Episodes: 33550 | Median Reward: 35.21 | Max Reward: 49.14
Iteration: 551 | Episodes: 33600 | Median Reward: 35.59 | Max Reward: 49.14
Iteration: 552 | Episodes: 33650 | Median Reward: 40.18 | Max Reward: 49.14
Iteration: 552 | Episodes: 33700 | Median Reward: 37.41 | Max Reward: 49.14
Iteration: 553 | Episodes: 33750 | Median Reward: 37.25 | Max Reward: 49.14
Iteration: 554 | Episodes: 33800 | Median Reward: 36.25 | Max Reward: 49.14
Iteration: 555 | Episodes: 33850 | Median Reward: 36.04 | Max Reward: 49.14
Iteration: 556 | Episodes: 33900 | Median Reward: 35.97 | Max Reward: 49.14
Iteration: 556 | Episodes: 33950 | Median Reward: 35.49 | Max Reward: 49.14
Iteration: 557 | Episodes: 34000 | Median Reward: 35.97 | Max Reward: 49.14
Iteration: 558 | Episodes: 34050 | Median Reward: 35.99 | Max Reward: 49.14
Iteration: 559 | Episodes: 34100 | Median Reward: 36.12 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.2        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 560          |
|    time_elapsed         | 10109        |
|    total_timesteps      | 3440640      |
| train/                  |              |
|    approx_kl            | 0.0034279476 |
|    clip_fraction        | 0.00106      |
|    clip_range           | 0.4          |
|    entropy_loss         | -92.9        |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0001       |
|    loss                 | 158          |
|    n_updates            | 5590         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 1.46         |
|    value_loss           | 318          |
------------------------------------------
Iteration: 560 | Episodes: 34150 | Median Reward: 38.89 | Max Reward: 49.14
Iteration: 561 | Episodes: 34200 | Median Reward: 39.68 | Max Reward: 49.14
Iteration: 561 | Episodes: 34250 | Median Reward: 33.57 | Max Reward: 49.14
Iteration: 562 | Episodes: 34300 | Median Reward: 34.44 | Max Reward: 49.14
Iteration: 563 | Episodes: 34350 | Median Reward: 36.57 | Max Reward: 49.14
Iteration: 564 | Episodes: 34400 | Median Reward: 37.63 | Max Reward: 49.14
Iteration: 565 | Episodes: 34450 | Median Reward: 40.35 | Max Reward: 49.14
Iteration: 565 | Episodes: 34500 | Median Reward: 32.25 | Max Reward: 49.14
Iteration: 566 | Episodes: 34550 | Median Reward: 36.28 | Max Reward: 49.14
Iteration: 567 | Episodes: 34600 | Median Reward: 39.26 | Max Reward: 49.14
Iteration: 568 | Episodes: 34650 | Median Reward: 39.26 | Max Reward: 49.14
Iteration: 569 | Episodes: 34700 | Median Reward: 36.16 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.7        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 570          |
|    time_elapsed         | 10284        |
|    total_timesteps      | 3502080      |
| train/                  |              |
|    approx_kl            | 0.0010242679 |
|    clip_fraction        | 0.000647     |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.1        |
|    explained_variance   | 6.56e-07     |
|    learning_rate        | 0.0001       |
|    loss                 | 158          |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.00175     |
|    std                  | 1.48         |
|    value_loss           | 340          |
------------------------------------------
Iteration: 570 | Episodes: 34750 | Median Reward: 35.85 | Max Reward: 49.14
Iteration: 570 | Episodes: 34800 | Median Reward: 32.10 | Max Reward: 49.14
Iteration: 571 | Episodes: 34850 | Median Reward: 34.46 | Max Reward: 49.14
Iteration: 572 | Episodes: 34900 | Median Reward: 38.54 | Max Reward: 49.14
Iteration: 573 | Episodes: 34950 | Median Reward: 38.92 | Max Reward: 49.14
Iteration: 574 | Episodes: 35000 | Median Reward: 35.39 | Max Reward: 49.14
Iteration: 575 | Episodes: 35050 | Median Reward: 30.51 | Max Reward: 49.14
Iteration: 575 | Episodes: 35100 | Median Reward: 34.53 | Max Reward: 49.14
Iteration: 576 | Episodes: 35150 | Median Reward: 40.85 | Max Reward: 49.14
Iteration: 577 | Episodes: 35200 | Median Reward: 38.77 | Max Reward: 49.14
Iteration: 578 | Episodes: 35250 | Median Reward: 37.00 | Max Reward: 49.14
Iteration: 579 | Episodes: 35300 | Median Reward: 36.77 | Max Reward: 49.14
Iteration: 579 | Episodes: 35350 | Median Reward: 38.53 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.7        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 580          |
|    time_elapsed         | 10457        |
|    total_timesteps      | 3563520      |
| train/                  |              |
|    approx_kl            | 1.205755e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.2        |
|    explained_variance   | 0.00663      |
|    learning_rate        | 0.0001       |
|    loss                 | 153          |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.00016     |
|    std                  | 1.48         |
|    value_loss           | 287          |
------------------------------------------
Iteration: 580 | Episodes: 35400 | Median Reward: 38.87 | Max Reward: 49.14
Iteration: 581 | Episodes: 35450 | Median Reward: 37.17 | Max Reward: 49.14
Iteration: 582 | Episodes: 35500 | Median Reward: 38.93 | Max Reward: 49.14
Iteration: 583 | Episodes: 35550 | Median Reward: 37.46 | Max Reward: 49.14
Iteration: 584 | Episodes: 35600 | Median Reward: 37.89 | Max Reward: 49.14
Iteration: 584 | Episodes: 35650 | Median Reward: 38.87 | Max Reward: 49.14
Iteration: 585 | Episodes: 35700 | Median Reward: 38.47 | Max Reward: 49.14
Iteration: 586 | Episodes: 35750 | Median Reward: 40.49 | Max Reward: 49.14
Iteration: 587 | Episodes: 35800 | Median Reward: 36.92 | Max Reward: 49.14
Iteration: 588 | Episodes: 35850 | Median Reward: 34.89 | Max Reward: 49.14
Iteration: 589 | Episodes: 35900 | Median Reward: 38.32 | Max Reward: 49.14
Iteration: 589 | Episodes: 35950 | Median Reward: 38.92 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.5        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 590          |
|    time_elapsed         | 10637        |
|    total_timesteps      | 3624960      |
| train/                  |              |
|    approx_kl            | 8.283605e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.3        |
|    explained_variance   | 0.0384       |
|    learning_rate        | 0.0001       |
|    loss                 | 188          |
|    n_updates            | 5890         |
|    policy_gradient_loss | -8.7e-06     |
|    std                  | 1.48         |
|    value_loss           | 339          |
------------------------------------------
Iteration: 590 | Episodes: 36000 | Median Reward: 38.10 | Max Reward: 49.14
Iteration: 591 | Episodes: 36050 | Median Reward: 38.84 | Max Reward: 49.14
Iteration: 592 | Episodes: 36100 | Median Reward: 38.94 | Max Reward: 49.14
Iteration: 593 | Episodes: 36150 | Median Reward: 41.20 | Max Reward: 49.14
Iteration: 593 | Episodes: 36200 | Median Reward: 39.44 | Max Reward: 49.14
Iteration: 594 | Episodes: 36250 | Median Reward: 33.43 | Max Reward: 49.14
Iteration: 595 | Episodes: 36300 | Median Reward: 34.98 | Max Reward: 49.14
Iteration: 596 | Episodes: 36350 | Median Reward: 37.02 | Max Reward: 49.14
Iteration: 597 | Episodes: 36400 | Median Reward: 37.66 | Max Reward: 49.14
Iteration: 598 | Episodes: 36450 | Median Reward: 35.22 | Max Reward: 49.14
Iteration: 598 | Episodes: 36500 | Median Reward: 38.34 | Max Reward: 49.14
Iteration: 599 | Episodes: 36550 | Median Reward: 35.92 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.9        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 600          |
|    time_elapsed         | 10812        |
|    total_timesteps      | 3686400      |
| train/                  |              |
|    approx_kl            | 4.909314e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.3        |
|    explained_variance   | 0.0709       |
|    learning_rate        | 0.0001       |
|    loss                 | 159          |
|    n_updates            | 5990         |
|    policy_gradient_loss | -3.7e-05     |
|    std                  | 1.48         |
|    value_loss           | 340          |
------------------------------------------
Iteration: 600 | Episodes: 36600 | Median Reward: 37.32 | Max Reward: 49.14
Iteration: 601 | Episodes: 36650 | Median Reward: 37.47 | Max Reward: 49.14
Iteration: 602 | Episodes: 36700 | Median Reward: 30.68 | Max Reward: 49.14
Iteration: 602 | Episodes: 36750 | Median Reward: 32.31 | Max Reward: 49.14
Iteration: 603 | Episodes: 36800 | Median Reward: 34.89 | Max Reward: 49.14
Iteration: 604 | Episodes: 36850 | Median Reward: 36.88 | Max Reward: 49.14
Iteration: 605 | Episodes: 36900 | Median Reward: 36.60 | Max Reward: 49.14
Iteration: 606 | Episodes: 36950 | Median Reward: 32.37 | Max Reward: 49.14
Iteration: 607 | Episodes: 37000 | Median Reward: 31.96 | Max Reward: 49.14
Iteration: 607 | Episodes: 37050 | Median Reward: 32.19 | Max Reward: 49.14
Iteration: 608 | Episodes: 37100 | Median Reward: 33.91 | Max Reward: 49.14
Iteration: 609 | Episodes: 37150 | Median Reward: 33.48 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.8        |
| time/                   |              |
|    fps                  | 341          |
|    iterations           | 610          |
|    time_elapsed         | 10984        |
|    total_timesteps      | 3747840      |
| train/                  |              |
|    approx_kl            | 8.767114e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.3        |
|    explained_variance   | 0.0886       |
|    learning_rate        | 0.0001       |
|    loss                 | 119          |
|    n_updates            | 6090         |
|    policy_gradient_loss | -7.45e-05    |
|    std                  | 1.49         |
|    value_loss           | 281          |
------------------------------------------
Iteration: 610 | Episodes: 37200 | Median Reward: 32.09 | Max Reward: 49.14
Iteration: 611 | Episodes: 37250 | Median Reward: 31.39 | Max Reward: 49.14
Iteration: 612 | Episodes: 37300 | Median Reward: 33.97 | Max Reward: 49.14
Iteration: 612 | Episodes: 37350 | Median Reward: 37.55 | Max Reward: 49.14
Iteration: 613 | Episodes: 37400 | Median Reward: 34.46 | Max Reward: 49.14
Iteration: 614 | Episodes: 37450 | Median Reward: 34.37 | Max Reward: 49.14
Iteration: 615 | Episodes: 37500 | Median Reward: 32.39 | Max Reward: 49.14
Iteration: 616 | Episodes: 37550 | Median Reward: 33.64 | Max Reward: 49.14
Iteration: 616 | Episodes: 37600 | Median Reward: 38.14 | Max Reward: 49.14
Iteration: 617 | Episodes: 37650 | Median Reward: 34.73 | Max Reward: 49.14
Iteration: 618 | Episodes: 37700 | Median Reward: 34.88 | Max Reward: 49.14
Iteration: 619 | Episodes: 37750 | Median Reward: 32.70 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.5        |
| time/                   |              |
|    fps                  | 341          |
|    iterations           | 620          |
|    time_elapsed         | 11162        |
|    total_timesteps      | 3809280      |
| train/                  |              |
|    approx_kl            | 7.202805e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.3        |
|    explained_variance   | 0.0935       |
|    learning_rate        | 0.0001       |
|    loss                 | 146          |
|    n_updates            | 6190         |
|    policy_gradient_loss | -7.76e-05    |
|    std                  | 1.49         |
|    value_loss           | 295          |
------------------------------------------
Iteration: 620 | Episodes: 37800 | Median Reward: 33.45 | Max Reward: 49.14
Iteration: 621 | Episodes: 37850 | Median Reward: 33.48 | Max Reward: 49.14
Iteration: 621 | Episodes: 37900 | Median Reward: 21.99 | Max Reward: 49.14
Iteration: 622 | Episodes: 37950 | Median Reward: 36.42 | Max Reward: 49.14
Iteration: 623 | Episodes: 38000 | Median Reward: 37.24 | Max Reward: 49.14
Iteration: 624 | Episodes: 38050 | Median Reward: 36.76 | Max Reward: 49.14
Iteration: 625 | Episodes: 38100 | Median Reward: 34.32 | Max Reward: 49.14
Iteration: 626 | Episodes: 38150 | Median Reward: 29.78 | Max Reward: 49.14
Iteration: 626 | Episodes: 38200 | Median Reward: 36.99 | Max Reward: 49.14
Iteration: 627 | Episodes: 38250 | Median Reward: 34.88 | Max Reward: 49.14
Iteration: 628 | Episodes: 38300 | Median Reward: 38.07 | Max Reward: 49.14
Iteration: 629 | Episodes: 38350 | Median Reward: 38.60 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.3         |
| time/                   |               |
|    fps                  | 341           |
|    iterations           | 630           |
|    time_elapsed         | 11344         |
|    total_timesteps      | 3870720       |
| train/                  |               |
|    approx_kl            | 1.1034448e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.3         |
|    explained_variance   | 0.0935        |
|    learning_rate        | 0.0001        |
|    loss                 | 132           |
|    n_updates            | 6290          |
|    policy_gradient_loss | -4.03e-06     |
|    std                  | 1.49          |
|    value_loss           | 329           |
-------------------------------------------
Iteration: 630 | Episodes: 38400 | Median Reward: 38.54 | Max Reward: 49.14
Iteration: 630 | Episodes: 38450 | Median Reward: 35.35 | Max Reward: 49.14
Iteration: 631 | Episodes: 38500 | Median Reward: 35.35 | Max Reward: 49.14
Iteration: 632 | Episodes: 38550 | Median Reward: 39.19 | Max Reward: 49.14
Iteration: 633 | Episodes: 38600 | Median Reward: 41.13 | Max Reward: 49.14
Iteration: 634 | Episodes: 38650 | Median Reward: 41.00 | Max Reward: 49.14
Iteration: 634 | Episodes: 38700 | Median Reward: 38.85 | Max Reward: 49.14
Iteration: 635 | Episodes: 38750 | Median Reward: 32.39 | Max Reward: 49.14
Iteration: 636 | Episodes: 38800 | Median Reward: 35.08 | Max Reward: 49.14
Iteration: 637 | Episodes: 38850 | Median Reward: 35.48 | Max Reward: 49.14
Iteration: 638 | Episodes: 38900 | Median Reward: 34.99 | Max Reward: 49.14
Iteration: 639 | Episodes: 38950 | Median Reward: 33.12 | Max Reward: 49.14
Iteration: 639 | Episodes: 39000 | Median Reward: 29.26 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.4         |
| time/                   |               |
|    fps                  | 341           |
|    iterations           | 640           |
|    time_elapsed         | 11524         |
|    total_timesteps      | 3932160       |
| train/                  |               |
|    approx_kl            | 1.5587277e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.4         |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.0001        |
|    loss                 | 132           |
|    n_updates            | 6390          |
|    policy_gradient_loss | -0.000144     |
|    std                  | 1.49          |
|    value_loss           | 274           |
-------------------------------------------
Iteration: 640 | Episodes: 39050 | Median Reward: 38.27 | Max Reward: 49.14
Iteration: 641 | Episodes: 39100 | Median Reward: 35.56 | Max Reward: 49.14
Iteration: 642 | Episodes: 39150 | Median Reward: 35.56 | Max Reward: 49.14
Iteration: 643 | Episodes: 39200 | Median Reward: 33.78 | Max Reward: 49.14
Iteration: 644 | Episodes: 39250 | Median Reward: 36.01 | Max Reward: 49.14
Iteration: 644 | Episodes: 39300 | Median Reward: 36.99 | Max Reward: 49.14
Iteration: 645 | Episodes: 39350 | Median Reward: 34.85 | Max Reward: 49.14
Iteration: 646 | Episodes: 39400 | Median Reward: 34.54 | Max Reward: 49.14
Iteration: 647 | Episodes: 39450 | Median Reward: 35.92 | Max Reward: 49.14
Iteration: 648 | Episodes: 39500 | Median Reward: 35.81 | Max Reward: 49.14
Iteration: 649 | Episodes: 39550 | Median Reward: 40.50 | Max Reward: 49.14
Iteration: 649 | Episodes: 39600 | Median Reward: 35.88 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.8        |
| time/                   |              |
|    fps                  | 341          |
|    iterations           | 650          |
|    time_elapsed         | 11703        |
|    total_timesteps      | 3993600      |
| train/                  |              |
|    approx_kl            | 1.397857e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.4        |
|    explained_variance   | 0.132        |
|    learning_rate        | 0.0001       |
|    loss                 | 153          |
|    n_updates            | 6490         |
|    policy_gradient_loss | -7.94e-06    |
|    std                  | 1.49         |
|    value_loss           | 312          |
------------------------------------------
Iteration: 650 | Episodes: 39650 | Median Reward: 35.84 | Max Reward: 49.14
Iteration: 651 | Episodes: 39700 | Median Reward: 41.13 | Max Reward: 49.14
Iteration: 652 | Episodes: 39750 | Median Reward: 36.00 | Max Reward: 49.14
Iteration: 653 | Episodes: 39800 | Median Reward: 34.59 | Max Reward: 49.14
Iteration: 653 | Episodes: 39850 | Median Reward: 36.52 | Max Reward: 49.14
Iteration: 654 | Episodes: 39900 | Median Reward: 38.53 | Max Reward: 49.14
Iteration: 655 | Episodes: 39950 | Median Reward: 34.54 | Max Reward: 49.14
Iteration: 656 | Episodes: 40000 | Median Reward: 36.27 | Max Reward: 49.14
Iteration: 657 | Episodes: 40050 | Median Reward: 33.61 | Max Reward: 49.14
Iteration: 658 | Episodes: 40100 | Median Reward: 34.06 | Max Reward: 49.14
Iteration: 658 | Episodes: 40150 | Median Reward: 33.18 | Max Reward: 49.14
Iteration: 659 | Episodes: 40200 | Median Reward: 38.02 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.1        |
| time/                   |              |
|    fps                  | 341          |
|    iterations           | 660          |
|    time_elapsed         | 11890        |
|    total_timesteps      | 4055040      |
| train/                  |              |
|    approx_kl            | 6.965387e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.4        |
|    explained_variance   | 0.155        |
|    learning_rate        | 0.0001       |
|    loss                 | 108          |
|    n_updates            | 6590         |
|    policy_gradient_loss | -0.000301    |
|    std                  | 1.5          |
|    value_loss           | 245          |
------------------------------------------
Iteration: 660 | Episodes: 40250 | Median Reward: 33.04 | Max Reward: 49.14
Iteration: 661 | Episodes: 40300 | Median Reward: 32.52 | Max Reward: 49.14
Iteration: 662 | Episodes: 40350 | Median Reward: 32.99 | Max Reward: 49.14
Iteration: 663 | Episodes: 40400 | Median Reward: 32.93 | Max Reward: 49.14
Iteration: 663 | Episodes: 40450 | Median Reward: 38.84 | Max Reward: 49.14
Iteration: 664 | Episodes: 40500 | Median Reward: 29.96 | Max Reward: 49.14
Iteration: 665 | Episodes: 40550 | Median Reward: 36.80 | Max Reward: 49.14
Iteration: 666 | Episodes: 40600 | Median Reward: 35.14 | Max Reward: 49.14
Iteration: 667 | Episodes: 40650 | Median Reward: 28.58 | Max Reward: 49.14
Iteration: 667 | Episodes: 40700 | Median Reward: 38.34 | Max Reward: 49.14
Iteration: 668 | Episodes: 40750 | Median Reward: 34.03 | Max Reward: 49.14
Iteration: 669 | Episodes: 40800 | Median Reward: 34.03 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.4        |
| time/                   |              |
|    fps                  | 341          |
|    iterations           | 670          |
|    time_elapsed         | 12065        |
|    total_timesteps      | 4116480      |
| train/                  |              |
|    approx_kl            | 1.543064e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.5        |
|    explained_variance   | 0.157        |
|    learning_rate        | 0.0001       |
|    loss                 | 149          |
|    n_updates            | 6690         |
|    policy_gradient_loss | -0.000123    |
|    std                  | 1.5          |
|    value_loss           | 253          |
------------------------------------------
Iteration: 670 | Episodes: 40850 | Median Reward: 35.55 | Max Reward: 49.14
Iteration: 671 | Episodes: 40900 | Median Reward: 35.55 | Max Reward: 49.14
Iteration: 671 | Episodes: 40950 | Median Reward: 31.44 | Max Reward: 49.14
Iteration: 672 | Episodes: 41000 | Median Reward: 38.96 | Max Reward: 49.14
Iteration: 673 | Episodes: 41050 | Median Reward: 37.04 | Max Reward: 49.14
Iteration: 674 | Episodes: 41100 | Median Reward: 35.08 | Max Reward: 49.14
Iteration: 675 | Episodes: 41150 | Median Reward: 37.31 | Max Reward: 49.14
Iteration: 676 | Episodes: 41200 | Median Reward: 39.20 | Max Reward: 49.14
Iteration: 676 | Episodes: 41250 | Median Reward: 38.59 | Max Reward: 49.14
Iteration: 677 | Episodes: 41300 | Median Reward: 39.27 | Max Reward: 49.14
Iteration: 678 | Episodes: 41350 | Median Reward: 40.18 | Max Reward: 49.14
Iteration: 679 | Episodes: 41400 | Median Reward: 34.94 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.7         |
| time/                   |               |
|    fps                  | 341           |
|    iterations           | 680           |
|    time_elapsed         | 12251         |
|    total_timesteps      | 4177920       |
| train/                  |               |
|    approx_kl            | 5.4373377e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.5         |
|    explained_variance   | 0.149         |
|    learning_rate        | 0.0001        |
|    loss                 | 151           |
|    n_updates            | 6790          |
|    policy_gradient_loss | -8.02e-05     |
|    std                  | 1.5           |
|    value_loss           | 314           |
-------------------------------------------
Iteration: 680 | Episodes: 41450 | Median Reward: 34.51 | Max Reward: 49.14
Iteration: 681 | Episodes: 41500 | Median Reward: 38.74 | Max Reward: 49.14
Iteration: 681 | Episodes: 41550 | Median Reward: 35.17 | Max Reward: 49.14
Iteration: 682 | Episodes: 41600 | Median Reward: 27.73 | Max Reward: 49.14
Iteration: 683 | Episodes: 41650 | Median Reward: 35.77 | Max Reward: 49.14
Iteration: 684 | Episodes: 41700 | Median Reward: 35.77 | Max Reward: 49.14
Iteration: 685 | Episodes: 41750 | Median Reward: 38.20 | Max Reward: 49.14
Iteration: 685 | Episodes: 41800 | Median Reward: 36.64 | Max Reward: 49.14
Iteration: 686 | Episodes: 41850 | Median Reward: 39.52 | Max Reward: 49.14
Iteration: 687 | Episodes: 41900 | Median Reward: 36.50 | Max Reward: 49.14
Iteration: 688 | Episodes: 41950 | Median Reward: 36.28 | Max Reward: 49.14
Iteration: 689 | Episodes: 42000 | Median Reward: 34.64 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.8        |
| time/                   |              |
|    fps                  | 341          |
|    iterations           | 690          |
|    time_elapsed         | 12431        |
|    total_timesteps      | 4239360      |
| train/                  |              |
|    approx_kl            | 5.487856e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.6        |
|    explained_variance   | 0.161        |
|    learning_rate        | 0.0001       |
|    loss                 | 117          |
|    n_updates            | 6890         |
|    policy_gradient_loss | -0.000126    |
|    std                  | 1.5          |
|    value_loss           | 285          |
------------------------------------------
Iteration: 690 | Episodes: 42050 | Median Reward: 31.87 | Max Reward: 49.14
Iteration: 690 | Episodes: 42100 | Median Reward: 41.07 | Max Reward: 49.14
Iteration: 691 | Episodes: 42150 | Median Reward: 34.31 | Max Reward: 49.14
Iteration: 692 | Episodes: 42200 | Median Reward: 37.43 | Max Reward: 49.14
Iteration: 693 | Episodes: 42250 | Median Reward: 37.43 | Max Reward: 49.14
Iteration: 694 | Episodes: 42300 | Median Reward: 35.76 | Max Reward: 49.14
Iteration: 695 | Episodes: 42350 | Median Reward: 38.91 | Max Reward: 49.14
Iteration: 695 | Episodes: 42400 | Median Reward: 35.95 | Max Reward: 49.14
Iteration: 696 | Episodes: 42450 | Median Reward: 36.99 | Max Reward: 49.14
Iteration: 697 | Episodes: 42500 | Median Reward: 36.38 | Max Reward: 49.14
Iteration: 698 | Episodes: 42550 | Median Reward: 35.04 | Max Reward: 49.14
Iteration: 699 | Episodes: 42600 | Median Reward: 35.06 | Max Reward: 49.14
Iteration: 699 | Episodes: 42650 | Median Reward: 35.44 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.6         |
| time/                   |               |
|    fps                  | 341           |
|    iterations           | 700           |
|    time_elapsed         | 12611         |
|    total_timesteps      | 4300800       |
| train/                  |               |
|    approx_kl            | 7.7862824e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.7         |
|    explained_variance   | 0.165         |
|    learning_rate        | 0.0001        |
|    loss                 | 142           |
|    n_updates            | 6990          |
|    policy_gradient_loss | -0.000287     |
|    std                  | 1.51          |
|    value_loss           | 280           |
-------------------------------------------
Iteration: 700 | Episodes: 42700 | Median Reward: 34.52 | Max Reward: 49.14
Iteration: 701 | Episodes: 42750 | Median Reward: 38.08 | Max Reward: 49.14
Iteration: 702 | Episodes: 42800 | Median Reward: 38.58 | Max Reward: 49.14
Iteration: 703 | Episodes: 42850 | Median Reward: 36.46 | Max Reward: 49.14
Iteration: 704 | Episodes: 42900 | Median Reward: 34.19 | Max Reward: 49.14
Iteration: 704 | Episodes: 42950 | Median Reward: 35.60 | Max Reward: 49.14
Iteration: 705 | Episodes: 43000 | Median Reward: 34.91 | Max Reward: 49.14
Iteration: 706 | Episodes: 43050 | Median Reward: 32.57 | Max Reward: 49.14
Iteration: 707 | Episodes: 43100 | Median Reward: 33.96 | Max Reward: 49.14
Iteration: 708 | Episodes: 43150 | Median Reward: 35.93 | Max Reward: 49.14
Iteration: 708 | Episodes: 43200 | Median Reward: 38.97 | Max Reward: 49.14
Iteration: 709 | Episodes: 43250 | Median Reward: 38.99 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.2        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 710          |
|    time_elapsed         | 12796        |
|    total_timesteps      | 4362240      |
| train/                  |              |
|    approx_kl            | 4.715094e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.7        |
|    explained_variance   | 0.167        |
|    learning_rate        | 0.0001       |
|    loss                 | 162          |
|    n_updates            | 7090         |
|    policy_gradient_loss | -0.000193    |
|    std                  | 1.51         |
|    value_loss           | 298          |
------------------------------------------
Iteration: 710 | Episodes: 43300 | Median Reward: 40.54 | Max Reward: 49.14
Iteration: 711 | Episodes: 43350 | Median Reward: 37.32 | Max Reward: 49.14
Iteration: 712 | Episodes: 43400 | Median Reward: 34.54 | Max Reward: 49.14
Iteration: 713 | Episodes: 43450 | Median Reward: 35.26 | Max Reward: 49.14
Iteration: 713 | Episodes: 43500 | Median Reward: 37.26 | Max Reward: 49.14
Iteration: 714 | Episodes: 43550 | Median Reward: 37.04 | Max Reward: 49.14
Iteration: 715 | Episodes: 43600 | Median Reward: 37.21 | Max Reward: 49.14
Iteration: 716 | Episodes: 43650 | Median Reward: 39.05 | Max Reward: 49.14
Iteration: 717 | Episodes: 43700 | Median Reward: 39.05 | Max Reward: 49.14
Iteration: 718 | Episodes: 43750 | Median Reward: 37.04 | Max Reward: 49.14
Iteration: 718 | Episodes: 43800 | Median Reward: 36.47 | Max Reward: 49.14
Iteration: 719 | Episodes: 43850 | Median Reward: 39.93 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64           |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 720           |
|    time_elapsed         | 12972         |
|    total_timesteps      | 4423680       |
| train/                  |               |
|    approx_kl            | 8.2195635e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.8         |
|    explained_variance   | 0.177         |
|    learning_rate        | 0.0001        |
|    loss                 | 143           |
|    n_updates            | 7190          |
|    policy_gradient_loss | -0.000535     |
|    std                  | 1.52          |
|    value_loss           | 269           |
-------------------------------------------
Iteration: 720 | Episodes: 43900 | Median Reward: 36.12 | Max Reward: 49.14
Iteration: 721 | Episodes: 43950 | Median Reward: 36.90 | Max Reward: 49.14
Iteration: 722 | Episodes: 44000 | Median Reward: 37.53 | Max Reward: 49.14
Iteration: 722 | Episodes: 44050 | Median Reward: 38.79 | Max Reward: 49.14
Iteration: 723 | Episodes: 44100 | Median Reward: 35.72 | Max Reward: 49.14
Iteration: 724 | Episodes: 44150 | Median Reward: 35.51 | Max Reward: 49.14
Iteration: 725 | Episodes: 44200 | Median Reward: 34.10 | Max Reward: 49.14
Iteration: 726 | Episodes: 44250 | Median Reward: 35.24 | Max Reward: 49.14
Iteration: 727 | Episodes: 44300 | Median Reward: 31.56 | Max Reward: 49.14
Iteration: 727 | Episodes: 44350 | Median Reward: 31.71 | Max Reward: 49.14
Iteration: 728 | Episodes: 44400 | Median Reward: 39.88 | Max Reward: 49.14
Iteration: 729 | Episodes: 44450 | Median Reward: 39.11 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.2         |
| time/                   |               |
|    fps                  | 341           |
|    iterations           | 730           |
|    time_elapsed         | 13152         |
|    total_timesteps      | 4485120       |
| train/                  |               |
|    approx_kl            | 1.2540499e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.9         |
|    explained_variance   | 0.17          |
|    learning_rate        | 0.0001        |
|    loss                 | 160           |
|    n_updates            | 7290          |
|    policy_gradient_loss | -2.12e-05     |
|    std                  | 1.52          |
|    value_loss           | 326           |
-------------------------------------------
Iteration: 730 | Episodes: 44500 | Median Reward: 36.75 | Max Reward: 49.14
Iteration: 731 | Episodes: 44550 | Median Reward: 37.48 | Max Reward: 49.14
Iteration: 732 | Episodes: 44600 | Median Reward: 41.87 | Max Reward: 49.14
Iteration: 732 | Episodes: 44650 | Median Reward: 38.39 | Max Reward: 49.14
Iteration: 733 | Episodes: 44700 | Median Reward: 34.82 | Max Reward: 49.14
Iteration: 734 | Episodes: 44750 | Median Reward: 37.44 | Max Reward: 49.14
Iteration: 735 | Episodes: 44800 | Median Reward: 35.83 | Max Reward: 49.14
Iteration: 736 | Episodes: 44850 | Median Reward: 35.83 | Max Reward: 49.14
Iteration: 736 | Episodes: 44900 | Median Reward: 38.82 | Max Reward: 49.14
Iteration: 737 | Episodes: 44950 | Median Reward: 31.14 | Max Reward: 49.14
Iteration: 738 | Episodes: 45000 | Median Reward: 29.69 | Max Reward: 49.14
Iteration: 739 | Episodes: 45050 | Median Reward: 35.01 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.7         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 740           |
|    time_elapsed         | 13340         |
|    total_timesteps      | 4546560       |
| train/                  |               |
|    approx_kl            | 0.00029837657 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94           |
|    explained_variance   | 0.197         |
|    learning_rate        | 0.0001        |
|    loss                 | 107           |
|    n_updates            | 7390          |
|    policy_gradient_loss | -0.000625     |
|    std                  | 1.53          |
|    value_loss           | 229           |
-------------------------------------------
Iteration: 740 | Episodes: 45100 | Median Reward: 37.62 | Max Reward: 49.14
Iteration: 741 | Episodes: 45150 | Median Reward: 43.10 | Max Reward: 49.14
Iteration: 741 | Episodes: 45200 | Median Reward: 40.85 | Max Reward: 49.14
Iteration: 742 | Episodes: 45250 | Median Reward: 37.89 | Max Reward: 49.14
Iteration: 743 | Episodes: 45300 | Median Reward: 37.87 | Max Reward: 49.14
Iteration: 744 | Episodes: 45350 | Median Reward: 37.85 | Max Reward: 49.14
Iteration: 745 | Episodes: 45400 | Median Reward: 37.16 | Max Reward: 49.14
Iteration: 745 | Episodes: 45450 | Median Reward: 36.36 | Max Reward: 49.14
Iteration: 746 | Episodes: 45500 | Median Reward: 39.50 | Max Reward: 49.14
Iteration: 747 | Episodes: 45550 | Median Reward: 38.83 | Max Reward: 49.14
Iteration: 748 | Episodes: 45600 | Median Reward: 37.35 | Max Reward: 49.14
Iteration: 749 | Episodes: 45650 | Median Reward: 35.36 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.9         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 750           |
|    time_elapsed         | 13516         |
|    total_timesteps      | 4608000       |
| train/                  |               |
|    approx_kl            | 6.1294864e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94           |
|    explained_variance   | 0.184         |
|    learning_rate        | 0.0001        |
|    loss                 | 141           |
|    n_updates            | 7490          |
|    policy_gradient_loss | -0.000205     |
|    std                  | 1.53          |
|    value_loss           | 292           |
-------------------------------------------
Iteration: 750 | Episodes: 45700 | Median Reward: 28.58 | Max Reward: 49.14
Iteration: 750 | Episodes: 45750 | Median Reward: 35.80 | Max Reward: 49.14
Iteration: 751 | Episodes: 45800 | Median Reward: 37.43 | Max Reward: 49.14
Iteration: 752 | Episodes: 45850 | Median Reward: 35.73 | Max Reward: 49.14
Iteration: 753 | Episodes: 45900 | Median Reward: 35.69 | Max Reward: 49.14
Iteration: 754 | Episodes: 45950 | Median Reward: 34.75 | Max Reward: 49.14
Iteration: 755 | Episodes: 46000 | Median Reward: 37.82 | Max Reward: 49.14
Iteration: 755 | Episodes: 46050 | Median Reward: 38.62 | Max Reward: 49.14
Iteration: 756 | Episodes: 46100 | Median Reward: 35.15 | Max Reward: 49.14
Iteration: 757 | Episodes: 46150 | Median Reward: 34.45 | Max Reward: 49.14
Iteration: 758 | Episodes: 46200 | Median Reward: 34.45 | Max Reward: 49.14
Iteration: 759 | Episodes: 46250 | Median Reward: 36.10 | Max Reward: 49.14
Iteration: 759 | Episodes: 46300 | Median Reward: 30.87 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.5         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 760           |
|    time_elapsed         | 13694         |
|    total_timesteps      | 4669440       |
| train/                  |               |
|    approx_kl            | 2.2750479e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94.1         |
|    explained_variance   | 0.188         |
|    learning_rate        | 0.0001        |
|    loss                 | 140           |
|    n_updates            | 7590          |
|    policy_gradient_loss | -4.5e-05      |
|    std                  | 1.54          |
|    value_loss           | 290           |
-------------------------------------------
Iteration: 760 | Episodes: 46350 | Median Reward: 33.67 | Max Reward: 49.14
Iteration: 761 | Episodes: 46400 | Median Reward: 36.44 | Max Reward: 49.14
Iteration: 762 | Episodes: 46450 | Median Reward: 39.08 | Max Reward: 49.14
Iteration: 763 | Episodes: 46500 | Median Reward: 39.08 | Max Reward: 49.14
Iteration: 764 | Episodes: 46550 | Median Reward: 36.28 | Max Reward: 49.14
Iteration: 764 | Episodes: 46600 | Median Reward: 38.58 | Max Reward: 49.14
Iteration: 765 | Episodes: 46650 | Median Reward: 35.78 | Max Reward: 49.14
Iteration: 766 | Episodes: 46700 | Median Reward: 35.36 | Max Reward: 49.14
Iteration: 767 | Episodes: 46750 | Median Reward: 35.95 | Max Reward: 49.14
Iteration: 768 | Episodes: 46800 | Median Reward: 36.54 | Max Reward: 49.14
Iteration: 769 | Episodes: 46850 | Median Reward: 35.17 | Max Reward: 49.14
Iteration: 769 | Episodes: 46900 | Median Reward: 37.29 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.9        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 770          |
|    time_elapsed         | 13882        |
|    total_timesteps      | 4730880      |
| train/                  |              |
|    approx_kl            | 6.722842e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -94.2        |
|    explained_variance   | 0.204        |
|    learning_rate        | 0.0001       |
|    loss                 | 119          |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.000138    |
|    std                  | 1.54         |
|    value_loss           | 255          |
------------------------------------------
Iteration: 770 | Episodes: 46950 | Median Reward: 31.96 | Max Reward: 49.14
Iteration: 771 | Episodes: 47000 | Median Reward: 36.47 | Max Reward: 49.14
Iteration: 772 | Episodes: 47050 | Median Reward: 39.33 | Max Reward: 49.14
Iteration: 773 | Episodes: 47100 | Median Reward: 30.77 | Max Reward: 49.14
Iteration: 773 | Episodes: 47150 | Median Reward: 31.46 | Max Reward: 49.14
Iteration: 774 | Episodes: 47200 | Median Reward: 38.76 | Max Reward: 49.14
Iteration: 775 | Episodes: 47250 | Median Reward: 38.25 | Max Reward: 49.14
Iteration: 776 | Episodes: 47300 | Median Reward: 38.00 | Max Reward: 49.14
Iteration: 777 | Episodes: 47350 | Median Reward: 37.76 | Max Reward: 49.14
Iteration: 778 | Episodes: 47400 | Median Reward: 33.03 | Max Reward: 49.14
Iteration: 778 | Episodes: 47450 | Median Reward: 38.75 | Max Reward: 49.14
Iteration: 779 | Episodes: 47500 | Median Reward: 28.24 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.4         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 780           |
|    time_elapsed         | 14058         |
|    total_timesteps      | 4792320       |
| train/                  |               |
|    approx_kl            | 0.00011787369 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94.4         |
|    explained_variance   | 0.198         |
|    learning_rate        | 0.0001        |
|    loss                 | 135           |
|    n_updates            | 7790          |
|    policy_gradient_loss | -0.00022      |
|    std                  | 1.55          |
|    value_loss           | 270           |
-------------------------------------------
Iteration: 780 | Episodes: 47550 | Median Reward: 34.74 | Max Reward: 49.14
Iteration: 781 | Episodes: 47600 | Median Reward: 38.19 | Max Reward: 49.14
Iteration: 782 | Episodes: 47650 | Median Reward: 39.07 | Max Reward: 49.14
Iteration: 782 | Episodes: 47700 | Median Reward: 39.25 | Max Reward: 49.14
Iteration: 783 | Episodes: 47750 | Median Reward: 34.09 | Max Reward: 49.14
Iteration: 784 | Episodes: 47800 | Median Reward: 36.51 | Max Reward: 49.14
Iteration: 785 | Episodes: 47850 | Median Reward: 31.14 | Max Reward: 49.14
Iteration: 786 | Episodes: 47900 | Median Reward: 32.87 | Max Reward: 49.14
Iteration: 787 | Episodes: 47950 | Median Reward: 37.65 | Max Reward: 49.14
Iteration: 787 | Episodes: 48000 | Median Reward: 37.43 | Max Reward: 49.14
Iteration: 788 | Episodes: 48050 | Median Reward: 35.38 | Max Reward: 49.14
Iteration: 789 | Episodes: 48100 | Median Reward: 35.38 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.2        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 790          |
|    time_elapsed         | 14238        |
|    total_timesteps      | 4853760      |
| train/                  |              |
|    approx_kl            | 0.0001428544 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -94.5        |
|    explained_variance   | 0.21         |
|    learning_rate        | 0.0001       |
|    loss                 | 114          |
|    n_updates            | 7890         |
|    policy_gradient_loss | -0.000343    |
|    std                  | 1.56         |
|    value_loss           | 256          |
------------------------------------------
Iteration: 790 | Episodes: 48150 | Median Reward: 36.80 | Max Reward: 49.14
Iteration: 791 | Episodes: 48200 | Median Reward: 38.02 | Max Reward: 49.14
Iteration: 792 | Episodes: 48250 | Median Reward: 41.88 | Max Reward: 49.14
Iteration: 792 | Episodes: 48300 | Median Reward: 39.17 | Max Reward: 49.14
Iteration: 793 | Episodes: 48350 | Median Reward: 38.49 | Max Reward: 49.14
Iteration: 794 | Episodes: 48400 | Median Reward: 38.22 | Max Reward: 49.14
Iteration: 795 | Episodes: 48450 | Median Reward: 37.78 | Max Reward: 49.14
Iteration: 796 | Episodes: 48500 | Median Reward: 36.12 | Max Reward: 49.14
Iteration: 796 | Episodes: 48550 | Median Reward: 32.37 | Max Reward: 49.14
Iteration: 797 | Episodes: 48600 | Median Reward: 34.13 | Max Reward: 49.14
Iteration: 798 | Episodes: 48650 | Median Reward: 34.47 | Max Reward: 49.14
Iteration: 799 | Episodes: 48700 | Median Reward: 36.07 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.9         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 800           |
|    time_elapsed         | 14420         |
|    total_timesteps      | 4915200       |
| train/                  |               |
|    approx_kl            | 0.00018474061 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94.6         |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.0001        |
|    loss                 | 140           |
|    n_updates            | 7990          |
|    policy_gradient_loss | -0.000376     |
|    std                  | 1.57          |
|    value_loss           | 250           |
-------------------------------------------
Iteration: 800 | Episodes: 48750 | Median Reward: 36.36 | Max Reward: 49.14
Iteration: 801 | Episodes: 48800 | Median Reward: 33.28 | Max Reward: 49.14
Iteration: 801 | Episodes: 48850 | Median Reward: 33.03 | Max Reward: 49.14
Iteration: 802 | Episodes: 48900 | Median Reward: 36.48 | Max Reward: 49.14
Iteration: 803 | Episodes: 48950 | Median Reward: 36.79 | Max Reward: 49.14
Iteration: 804 | Episodes: 49000 | Median Reward: 37.95 | Max Reward: 49.14
Iteration: 805 | Episodes: 49050 | Median Reward: 34.81 | Max Reward: 49.14
Iteration: 806 | Episodes: 49100 | Median Reward: 36.02 | Max Reward: 49.14
Iteration: 806 | Episodes: 49150 | Median Reward: 37.09 | Max Reward: 49.14
Iteration: 807 | Episodes: 49200 | Median Reward: 34.37 | Max Reward: 49.14
Iteration: 808 | Episodes: 49250 | Median Reward: 34.37 | Max Reward: 49.14
Iteration: 809 | Episodes: 49300 | Median Reward: 36.62 | Max Reward: 49.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.7       |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 810         |
|    time_elapsed         | 14603       |
|    total_timesteps      | 4976640     |
| train/                  |             |
|    approx_kl            | 0.001071579 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -94.8       |
|    explained_variance   | 0.212       |
|    learning_rate        | 0.0001      |
|    loss                 | 144         |
|    n_updates            | 8090        |
|    policy_gradient_loss | -0.00131    |
|    std                  | 1.58        |
|    value_loss           | 277         |
-----------------------------------------
Iteration: 810 | Episodes: 49350 | Median Reward: 38.53 | Max Reward: 49.14
Iteration: 810 | Episodes: 49400 | Median Reward: 35.03 | Max Reward: 49.14
Iteration: 811 | Episodes: 49450 | Median Reward: 36.71 | Max Reward: 49.14
Iteration: 812 | Episodes: 49500 | Median Reward: 32.52 | Max Reward: 49.14
Iteration: 813 | Episodes: 49550 | Median Reward: 38.73 | Max Reward: 49.14
Iteration: 814 | Episodes: 49600 | Median Reward: 37.85 | Max Reward: 49.14
Iteration: 815 | Episodes: 49650 | Median Reward: 37.64 | Max Reward: 49.14
Iteration: 815 | Episodes: 49700 | Median Reward: 35.27 | Max Reward: 49.14
Iteration: 816 | Episodes: 49750 | Median Reward: 38.12 | Max Reward: 49.14
Iteration: 817 | Episodes: 49800 | Median Reward: 40.66 | Max Reward: 49.14
Iteration: 818 | Episodes: 49850 | Median Reward: 40.50 | Max Reward: 49.14
Iteration: 819 | Episodes: 49900 | Median Reward: 36.30 | Max Reward: 49.14
Iteration: 819 | Episodes: 49950 | Median Reward: 36.19 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.8        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 820          |
|    time_elapsed         | 14781        |
|    total_timesteps      | 5038080      |
| train/                  |              |
|    approx_kl            | 7.200697e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -94.9        |
|    explained_variance   | 0.21         |
|    learning_rate        | 0.0001       |
|    loss                 | 143          |
|    n_updates            | 8190         |
|    policy_gradient_loss | -0.000144    |
|    std                  | 1.59         |
|    value_loss           | 295          |
------------------------------------------
Iteration: 820 | Episodes: 50000 | Median Reward: 39.11 | Max Reward: 49.14
Iteration: 821 | Episodes: 50050 | Median Reward: 38.07 | Max Reward: 49.14
Iteration: 822 | Episodes: 50100 | Median Reward: 37.55 | Max Reward: 49.14
Iteration: 823 | Episodes: 50150 | Median Reward: 33.70 | Max Reward: 49.14
Iteration: 824 | Episodes: 50200 | Median Reward: 33.86 | Max Reward: 49.14
Iteration: 824 | Episodes: 50250 | Median Reward: 33.96 | Max Reward: 49.14
Iteration: 825 | Episodes: 50300 | Median Reward: 38.99 | Max Reward: 49.14
Iteration: 826 | Episodes: 50350 | Median Reward: 38.99 | Max Reward: 49.14
Iteration: 827 | Episodes: 50400 | Median Reward: 35.64 | Max Reward: 49.14
Iteration: 828 | Episodes: 50450 | Median Reward: 34.17 | Max Reward: 49.14
Iteration: 829 | Episodes: 50500 | Median Reward: 39.92 | Max Reward: 49.14
Iteration: 829 | Episodes: 50550 | Median Reward: 34.85 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.8         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 830           |
|    time_elapsed         | 14972         |
|    total_timesteps      | 5099520       |
| train/                  |               |
|    approx_kl            | 0.00010213429 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95.1         |
|    explained_variance   | 0.211         |
|    learning_rate        | 0.0001        |
|    loss                 | 145           |
|    n_updates            | 8290          |
|    policy_gradient_loss | -8.24e-06     |
|    std                  | 1.6           |
|    value_loss           | 301           |
-------------------------------------------
Iteration: 830 | Episodes: 50600 | Median Reward: 34.85 | Max Reward: 49.14
Iteration: 831 | Episodes: 50650 | Median Reward: 34.03 | Max Reward: 49.14
Iteration: 832 | Episodes: 50700 | Median Reward: 35.95 | Max Reward: 49.14
Iteration: 833 | Episodes: 50750 | Median Reward: 36.24 | Max Reward: 49.14
Iteration: 833 | Episodes: 50800 | Median Reward: 43.52 | Max Reward: 49.14
Iteration: 834 | Episodes: 50850 | Median Reward: 37.43 | Max Reward: 49.14
Iteration: 835 | Episodes: 50900 | Median Reward: 33.04 | Max Reward: 49.14
Iteration: 836 | Episodes: 50950 | Median Reward: 37.83 | Max Reward: 49.14
Iteration: 837 | Episodes: 51000 | Median Reward: 41.55 | Max Reward: 49.14
Iteration: 838 | Episodes: 51050 | Median Reward: 38.10 | Max Reward: 49.14
Iteration: 838 | Episodes: 51100 | Median Reward: 41.93 | Max Reward: 49.14
Iteration: 839 | Episodes: 51150 | Median Reward: 40.79 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61           |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 840           |
|    time_elapsed         | 15147         |
|    total_timesteps      | 5160960       |
| train/                  |               |
|    approx_kl            | 0.00061723165 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95.2         |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.0001        |
|    loss                 | 124           |
|    n_updates            | 8390          |
|    policy_gradient_loss | -0.000754     |
|    std                  | 1.61          |
|    value_loss           | 290           |
-------------------------------------------
Iteration: 840 | Episodes: 51200 | Median Reward: 37.77 | Max Reward: 49.14
Iteration: 841 | Episodes: 51250 | Median Reward: 38.59 | Max Reward: 49.14
Iteration: 842 | Episodes: 51300 | Median Reward: 39.65 | Max Reward: 49.14
Iteration: 843 | Episodes: 51350 | Median Reward: 38.79 | Max Reward: 49.14
Iteration: 843 | Episodes: 51400 | Median Reward: 32.78 | Max Reward: 49.14
Iteration: 844 | Episodes: 51450 | Median Reward: 39.22 | Max Reward: 49.14
Iteration: 845 | Episodes: 51500 | Median Reward: 39.68 | Max Reward: 49.14
Iteration: 846 | Episodes: 51550 | Median Reward: 39.68 | Max Reward: 49.14
Iteration: 847 | Episodes: 51600 | Median Reward: 38.18 | Max Reward: 49.14
Iteration: 847 | Episodes: 51650 | Median Reward: 40.71 | Max Reward: 49.14
Iteration: 848 | Episodes: 51700 | Median Reward: 38.97 | Max Reward: 49.14
Iteration: 849 | Episodes: 51750 | Median Reward: 38.16 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.7        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 850          |
|    time_elapsed         | 15335        |
|    total_timesteps      | 5222400      |
| train/                  |              |
|    approx_kl            | 0.0009270711 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -95.4        |
|    explained_variance   | 0.223        |
|    learning_rate        | 0.0001       |
|    loss                 | 125          |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.000689    |
|    std                  | 1.62         |
|    value_loss           | 286          |
------------------------------------------
Iteration: 850 | Episodes: 51800 | Median Reward: 35.04 | Max Reward: 49.14
Iteration: 851 | Episodes: 51850 | Median Reward: 38.41 | Max Reward: 49.14
Iteration: 851 | Episodes: 51900 | Median Reward: 35.61 | Max Reward: 49.14
Iteration: 852 | Episodes: 51950 | Median Reward: 38.11 | Max Reward: 49.14
Iteration: 853 | Episodes: 52000 | Median Reward: 37.62 | Max Reward: 49.14
Iteration: 854 | Episodes: 52050 | Median Reward: 42.73 | Max Reward: 49.14
Iteration: 855 | Episodes: 52100 | Median Reward: 40.42 | Max Reward: 49.14
Iteration: 856 | Episodes: 52150 | Median Reward: 34.02 | Max Reward: 49.14
Iteration: 856 | Episodes: 52200 | Median Reward: 31.86 | Max Reward: 49.14
Iteration: 857 | Episodes: 52250 | Median Reward: 34.46 | Max Reward: 49.14
Iteration: 858 | Episodes: 52300 | Median Reward: 38.50 | Max Reward: 49.14
Iteration: 859 | Episodes: 52350 | Median Reward: 38.10 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.7         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 860           |
|    time_elapsed         | 15517         |
|    total_timesteps      | 5283840       |
| train/                  |               |
|    approx_kl            | 0.00012279872 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95.6         |
|    explained_variance   | 0.224         |
|    learning_rate        | 0.0001        |
|    loss                 | 146           |
|    n_updates            | 8590          |
|    policy_gradient_loss | -6.63e-05     |
|    std                  | 1.63          |
|    value_loss           | 302           |
-------------------------------------------
Iteration: 860 | Episodes: 52400 | Median Reward: 34.40 | Max Reward: 49.14
Iteration: 861 | Episodes: 52450 | Median Reward: 36.67 | Max Reward: 49.14
Iteration: 861 | Episodes: 52500 | Median Reward: 36.68 | Max Reward: 49.14
Iteration: 862 | Episodes: 52550 | Median Reward: 38.77 | Max Reward: 49.14
Iteration: 863 | Episodes: 52600 | Median Reward: 40.39 | Max Reward: 49.14
Iteration: 864 | Episodes: 52650 | Median Reward: 36.89 | Max Reward: 49.14
Iteration: 865 | Episodes: 52700 | Median Reward: 39.09 | Max Reward: 49.14
Iteration: 865 | Episodes: 52750 | Median Reward: 39.71 | Max Reward: 49.14
Iteration: 866 | Episodes: 52800 | Median Reward: 39.27 | Max Reward: 49.14
Iteration: 867 | Episodes: 52850 | Median Reward: 38.91 | Max Reward: 49.14
Iteration: 868 | Episodes: 52900 | Median Reward: 40.46 | Max Reward: 49.14
Iteration: 869 | Episodes: 52950 | Median Reward: 38.42 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.6         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 870           |
|    time_elapsed         | 15696         |
|    total_timesteps      | 5345280       |
| train/                  |               |
|    approx_kl            | 0.00021983104 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95.7         |
|    explained_variance   | 0.23          |
|    learning_rate        | 0.0001        |
|    loss                 | 151           |
|    n_updates            | 8690          |
|    policy_gradient_loss | -0.000158     |
|    std                  | 1.64          |
|    value_loss           | 291           |
-------------------------------------------
Iteration: 870 | Episodes: 53000 | Median Reward: 36.86 | Max Reward: 49.14
Iteration: 870 | Episodes: 53050 | Median Reward: 36.39 | Max Reward: 49.14
Iteration: 871 | Episodes: 53100 | Median Reward: 40.34 | Max Reward: 49.14
Iteration: 872 | Episodes: 53150 | Median Reward: 39.16 | Max Reward: 49.14
Iteration: 873 | Episodes: 53200 | Median Reward: 38.50 | Max Reward: 49.14
Iteration: 874 | Episodes: 53250 | Median Reward: 38.44 | Max Reward: 49.14
Iteration: 875 | Episodes: 53300 | Median Reward: 39.43 | Max Reward: 49.14
Iteration: 875 | Episodes: 53350 | Median Reward: 38.50 | Max Reward: 49.14
Iteration: 876 | Episodes: 53400 | Median Reward: 34.29 | Max Reward: 49.14
Iteration: 877 | Episodes: 53450 | Median Reward: 39.41 | Max Reward: 49.14
Iteration: 878 | Episodes: 53500 | Median Reward: 40.45 | Max Reward: 49.14
Iteration: 879 | Episodes: 53550 | Median Reward: 40.83 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.1         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 880           |
|    time_elapsed         | 15883         |
|    total_timesteps      | 5406720       |
| train/                  |               |
|    approx_kl            | 0.00022623825 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95.9         |
|    explained_variance   | 0.23          |
|    learning_rate        | 0.0001        |
|    loss                 | 142           |
|    n_updates            | 8790          |
|    policy_gradient_loss | -0.000248     |
|    std                  | 1.65          |
|    value_loss           | 299           |
-------------------------------------------
Iteration: 880 | Episodes: 53600 | Median Reward: 39.21 | Max Reward: 49.14
Iteration: 880 | Episodes: 53650 | Median Reward: 35.14 | Max Reward: 49.14
Iteration: 881 | Episodes: 53700 | Median Reward: 35.08 | Max Reward: 49.14
Iteration: 882 | Episodes: 53750 | Median Reward: 34.04 | Max Reward: 49.14
Iteration: 883 | Episodes: 53800 | Median Reward: 38.05 | Max Reward: 49.14
Iteration: 884 | Episodes: 53850 | Median Reward: 38.22 | Max Reward: 49.14
Iteration: 884 | Episodes: 53900 | Median Reward: 38.17 | Max Reward: 49.14
Iteration: 885 | Episodes: 53950 | Median Reward: 38.87 | Max Reward: 49.14
Iteration: 886 | Episodes: 54000 | Median Reward: 36.46 | Max Reward: 49.14
Iteration: 887 | Episodes: 54050 | Median Reward: 38.06 | Max Reward: 49.14
Iteration: 888 | Episodes: 54100 | Median Reward: 38.06 | Max Reward: 49.14
Iteration: 888 | Episodes: 54150 | Median Reward: 33.98 | Max Reward: 49.14
Iteration: 889 | Episodes: 54200 | Median Reward: 35.48 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.6        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 890          |
|    time_elapsed         | 16061        |
|    total_timesteps      | 5468160      |
| train/                  |              |
|    approx_kl            | 0.0005536841 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -96.1        |
|    explained_variance   | 0.252        |
|    learning_rate        | 0.0001       |
|    loss                 | 123          |
|    n_updates            | 8890         |
|    policy_gradient_loss | -0.000688    |
|    std                  | 1.67         |
|    value_loss           | 253          |
------------------------------------------
Iteration: 890 | Episodes: 54250 | Median Reward: 39.55 | Max Reward: 49.14
Iteration: 891 | Episodes: 54300 | Median Reward: 35.22 | Max Reward: 49.14
Iteration: 892 | Episodes: 54350 | Median Reward: 34.78 | Max Reward: 49.14
Iteration: 893 | Episodes: 54400 | Median Reward: 40.40 | Max Reward: 49.14
Iteration: 893 | Episodes: 54450 | Median Reward: 39.00 | Max Reward: 49.14
Iteration: 894 | Episodes: 54500 | Median Reward: 37.50 | Max Reward: 49.14
Iteration: 895 | Episodes: 54550 | Median Reward: 36.68 | Max Reward: 49.14
Iteration: 896 | Episodes: 54600 | Median Reward: 37.58 | Max Reward: 49.14
Iteration: 897 | Episodes: 54650 | Median Reward: 37.30 | Max Reward: 49.14
Iteration: 898 | Episodes: 54700 | Median Reward: 33.75 | Max Reward: 49.14
Iteration: 898 | Episodes: 54750 | Median Reward: 40.92 | Max Reward: 49.14
Iteration: 899 | Episodes: 54800 | Median Reward: 38.89 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.7         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 900           |
|    time_elapsed         | 16241         |
|    total_timesteps      | 5529600       |
| train/                  |               |
|    approx_kl            | 5.5963086e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -96.3         |
|    explained_variance   | 0.238         |
|    learning_rate        | 0.0001        |
|    loss                 | 139           |
|    n_updates            | 8990          |
|    policy_gradient_loss | -1.31e-05     |
|    std                  | 1.68          |
|    value_loss           | 297           |
-------------------------------------------
Iteration: 900 | Episodes: 54850 | Median Reward: 35.83 | Max Reward: 49.14
Iteration: 901 | Episodes: 54900 | Median Reward: 38.01 | Max Reward: 49.14
Iteration: 902 | Episodes: 54950 | Median Reward: 38.58 | Max Reward: 49.14
Iteration: 902 | Episodes: 55000 | Median Reward: 37.39 | Max Reward: 49.14
Iteration: 903 | Episodes: 55050 | Median Reward: 36.61 | Max Reward: 49.14
Iteration: 904 | Episodes: 55100 | Median Reward: 35.22 | Max Reward: 49.14
Iteration: 905 | Episodes: 55150 | Median Reward: 36.17 | Max Reward: 49.14
Iteration: 906 | Episodes: 55200 | Median Reward: 38.20 | Max Reward: 49.14
Iteration: 907 | Episodes: 55250 | Median Reward: 35.39 | Max Reward: 49.14
Iteration: 907 | Episodes: 55300 | Median Reward: 39.07 | Max Reward: 49.14
Iteration: 908 | Episodes: 55350 | Median Reward: 36.43 | Max Reward: 49.14
Iteration: 909 | Episodes: 55400 | Median Reward: 37.96 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.6         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 910           |
|    time_elapsed         | 16427         |
|    total_timesteps      | 5591040       |
| train/                  |               |
|    approx_kl            | 0.00055585674 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -96.6         |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.0001        |
|    loss                 | 132           |
|    n_updates            | 9090          |
|    policy_gradient_loss | -0.000945     |
|    std                  | 1.7           |
|    value_loss           | 277           |
-------------------------------------------
Iteration: 910 | Episodes: 55450 | Median Reward: 36.93 | Max Reward: 49.14
Iteration: 911 | Episodes: 55500 | Median Reward: 33.86 | Max Reward: 49.14
Iteration: 912 | Episodes: 55550 | Median Reward: 36.97 | Max Reward: 49.14
Iteration: 912 | Episodes: 55600 | Median Reward: 36.60 | Max Reward: 49.14
Iteration: 913 | Episodes: 55650 | Median Reward: 33.99 | Max Reward: 49.14
Iteration: 914 | Episodes: 55700 | Median Reward: 38.90 | Max Reward: 49.14
Iteration: 915 | Episodes: 55750 | Median Reward: 39.15 | Max Reward: 49.14
Iteration: 916 | Episodes: 55800 | Median Reward: 38.12 | Max Reward: 49.14
Iteration: 916 | Episodes: 55850 | Median Reward: 38.98 | Max Reward: 49.14
Iteration: 917 | Episodes: 55900 | Median Reward: 36.04 | Max Reward: 49.14
Iteration: 918 | Episodes: 55950 | Median Reward: 36.75 | Max Reward: 49.14
Iteration: 919 | Episodes: 56000 | Median Reward: 38.89 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.8         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 920           |
|    time_elapsed         | 16611         |
|    total_timesteps      | 5652480       |
| train/                  |               |
|    approx_kl            | 0.00013428024 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -96.8         |
|    explained_variance   | 0.258         |
|    learning_rate        | 0.0001        |
|    loss                 | 110           |
|    n_updates            | 9190          |
|    policy_gradient_loss | -0.000121     |
|    std                  | 1.71          |
|    value_loss           | 262           |
-------------------------------------------
Iteration: 920 | Episodes: 56050 | Median Reward: 38.65 | Max Reward: 49.14
Iteration: 921 | Episodes: 56100 | Median Reward: 37.07 | Max Reward: 49.14
Iteration: 921 | Episodes: 56150 | Median Reward: 35.73 | Max Reward: 49.14
Iteration: 922 | Episodes: 56200 | Median Reward: 38.33 | Max Reward: 49.14
Iteration: 923 | Episodes: 56250 | Median Reward: 38.21 | Max Reward: 49.14
Iteration: 924 | Episodes: 56300 | Median Reward: 34.35 | Max Reward: 49.14
Iteration: 925 | Episodes: 56350 | Median Reward: 35.86 | Max Reward: 49.14
Iteration: 925 | Episodes: 56400 | Median Reward: 40.83 | Max Reward: 49.14
Iteration: 926 | Episodes: 56450 | Median Reward: 36.49 | Max Reward: 49.14
Iteration: 927 | Episodes: 56500 | Median Reward: 36.47 | Max Reward: 49.14
Iteration: 928 | Episodes: 56550 | Median Reward: 33.14 | Max Reward: 49.14
Iteration: 929 | Episodes: 56600 | Median Reward: 32.89 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.2        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 930          |
|    time_elapsed         | 16793        |
|    total_timesteps      | 5713920      |
| train/                  |              |
|    approx_kl            | 0.0006119098 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97          |
|    explained_variance   | 0.272        |
|    learning_rate        | 0.0001       |
|    loss                 | 102          |
|    n_updates            | 9290         |
|    policy_gradient_loss | -0.000966    |
|    std                  | 1.73         |
|    value_loss           | 232          |
------------------------------------------
Iteration: 930 | Episodes: 56650 | Median Reward: 38.14 | Max Reward: 49.14
Iteration: 930 | Episodes: 56700 | Median Reward: 40.20 | Max Reward: 49.14
Iteration: 931 | Episodes: 56750 | Median Reward: 36.22 | Max Reward: 49.14
Iteration: 932 | Episodes: 56800 | Median Reward: 35.13 | Max Reward: 49.14
Iteration: 933 | Episodes: 56850 | Median Reward: 35.30 | Max Reward: 49.14
Iteration: 934 | Episodes: 56900 | Median Reward: 39.56 | Max Reward: 49.14
Iteration: 935 | Episodes: 56950 | Median Reward: 38.28 | Max Reward: 49.14
Iteration: 935 | Episodes: 57000 | Median Reward: 39.19 | Max Reward: 49.14
Iteration: 936 | Episodes: 57050 | Median Reward: 36.73 | Max Reward: 49.14
Iteration: 937 | Episodes: 57100 | Median Reward: 35.89 | Max Reward: 49.14
Iteration: 938 | Episodes: 57150 | Median Reward: 37.59 | Max Reward: 49.14
Iteration: 939 | Episodes: 57200 | Median Reward: 38.89 | Max Reward: 49.14
Iteration: 939 | Episodes: 57250 | Median Reward: 36.56 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.8        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 940          |
|    time_elapsed         | 16980        |
|    total_timesteps      | 5775360      |
| train/                  |              |
|    approx_kl            | 0.0002973926 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.2        |
|    explained_variance   | 0.261        |
|    learning_rate        | 0.0001       |
|    loss                 | 134          |
|    n_updates            | 9390         |
|    policy_gradient_loss | -0.0002      |
|    std                  | 1.74         |
|    value_loss           | 273          |
------------------------------------------
Iteration: 940 | Episodes: 57300 | Median Reward: 37.18 | Max Reward: 49.14
Iteration: 941 | Episodes: 57350 | Median Reward: 37.00 | Max Reward: 49.14
Iteration: 942 | Episodes: 57400 | Median Reward: 40.87 | Max Reward: 49.14
Iteration: 943 | Episodes: 57450 | Median Reward: 41.92 | Max Reward: 49.14
Iteration: 944 | Episodes: 57500 | Median Reward: 40.55 | Max Reward: 49.14
Iteration: 944 | Episodes: 57550 | Median Reward: 35.23 | Max Reward: 49.14
Iteration: 945 | Episodes: 57600 | Median Reward: 38.81 | Max Reward: 49.14
Iteration: 946 | Episodes: 57650 | Median Reward: 40.01 | Max Reward: 49.14
Iteration: 947 | Episodes: 57700 | Median Reward: 40.01 | Max Reward: 49.14
Iteration: 948 | Episodes: 57750 | Median Reward: 35.99 | Max Reward: 49.14
Iteration: 949 | Episodes: 57800 | Median Reward: 37.18 | Max Reward: 49.14
Iteration: 949 | Episodes: 57850 | Median Reward: 42.06 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.5         |
| time/                   |               |
|    fps                  | 340           |
|    iterations           | 950           |
|    time_elapsed         | 17163         |
|    total_timesteps      | 5836800       |
| train/                  |               |
|    approx_kl            | 0.00047116075 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -97.4         |
|    explained_variance   | 0.267         |
|    learning_rate        | 0.0001        |
|    loss                 | 138           |
|    n_updates            | 9490          |
|    policy_gradient_loss | -0.000525     |
|    std                  | 1.76          |
|    value_loss           | 263           |
-------------------------------------------
Iteration: 950 | Episodes: 57900 | Median Reward: 35.71 | Max Reward: 49.14
Iteration: 951 | Episodes: 57950 | Median Reward: 34.79 | Max Reward: 49.14
Iteration: 952 | Episodes: 58000 | Median Reward: 34.59 | Max Reward: 49.14
Iteration: 953 | Episodes: 58050 | Median Reward: 32.50 | Max Reward: 49.14
Iteration: 953 | Episodes: 58100 | Median Reward: 42.28 | Max Reward: 49.14
Iteration: 954 | Episodes: 58150 | Median Reward: 33.12 | Max Reward: 49.14
Iteration: 955 | Episodes: 58200 | Median Reward: 34.52 | Max Reward: 49.14
Iteration: 956 | Episodes: 58250 | Median Reward: 38.67 | Max Reward: 49.14
Iteration: 957 | Episodes: 58300 | Median Reward: 40.21 | Max Reward: 49.14
Iteration: 958 | Episodes: 58350 | Median Reward: 39.75 | Max Reward: 49.14
Iteration: 958 | Episodes: 58400 | Median Reward: 40.92 | Max Reward: 49.14
Iteration: 959 | Episodes: 58450 | Median Reward: 38.85 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61           |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 960           |
|    time_elapsed         | 17353         |
|    total_timesteps      | 5898240       |
| train/                  |               |
|    approx_kl            | 0.00020454581 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -97.6         |
|    explained_variance   | 0.266         |
|    learning_rate        | 0.0001        |
|    loss                 | 139           |
|    n_updates            | 9590          |
|    policy_gradient_loss | -0.000327     |
|    std                  | 1.78          |
|    value_loss           | 269           |
-------------------------------------------
Iteration: 960 | Episodes: 58500 | Median Reward: 38.17 | Max Reward: 49.14
Iteration: 961 | Episodes: 58550 | Median Reward: 37.99 | Max Reward: 49.14
Iteration: 962 | Episodes: 58600 | Median Reward: 38.09 | Max Reward: 49.14
Iteration: 962 | Episodes: 58650 | Median Reward: 38.71 | Max Reward: 49.14
Iteration: 963 | Episodes: 58700 | Median Reward: 42.29 | Max Reward: 49.14
Iteration: 964 | Episodes: 58750 | Median Reward: 39.36 | Max Reward: 49.14
Iteration: 965 | Episodes: 58800 | Median Reward: 38.25 | Max Reward: 49.14
Iteration: 966 | Episodes: 58850 | Median Reward: 37.21 | Max Reward: 49.14
Iteration: 967 | Episodes: 58900 | Median Reward: 34.17 | Max Reward: 49.14
Iteration: 967 | Episodes: 58950 | Median Reward: 34.89 | Max Reward: 49.14
Iteration: 968 | Episodes: 59000 | Median Reward: 33.98 | Max Reward: 49.14
Iteration: 969 | Episodes: 59050 | Median Reward: 36.79 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.7        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 970          |
|    time_elapsed         | 17534        |
|    total_timesteps      | 5959680      |
| train/                  |              |
|    approx_kl            | 0.0021646465 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.8        |
|    explained_variance   | 0.285        |
|    learning_rate        | 0.0001       |
|    loss                 | 105          |
|    n_updates            | 9690         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 1.79         |
|    value_loss           | 237          |
------------------------------------------
Iteration: 970 | Episodes: 59100 | Median Reward: 35.67 | Max Reward: 49.14
Iteration: 971 | Episodes: 59150 | Median Reward: 36.80 | Max Reward: 49.14
Iteration: 972 | Episodes: 59200 | Median Reward: 38.17 | Max Reward: 49.14
Iteration: 972 | Episodes: 59250 | Median Reward: 39.90 | Max Reward: 49.14
Iteration: 973 | Episodes: 59300 | Median Reward: 39.21 | Max Reward: 49.14
Iteration: 974 | Episodes: 59350 | Median Reward: 38.86 | Max Reward: 49.14
Iteration: 975 | Episodes: 59400 | Median Reward: 37.83 | Max Reward: 49.14
Iteration: 976 | Episodes: 59450 | Median Reward: 37.57 | Max Reward: 49.14
Iteration: 976 | Episodes: 59500 | Median Reward: 38.19 | Max Reward: 49.14
Iteration: 977 | Episodes: 59550 | Median Reward: 41.99 | Max Reward: 49.14
Iteration: 978 | Episodes: 59600 | Median Reward: 39.23 | Max Reward: 49.14
Iteration: 979 | Episodes: 59650 | Median Reward: 38.01 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62           |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 980           |
|    time_elapsed         | 17716         |
|    total_timesteps      | 6021120       |
| train/                  |               |
|    approx_kl            | 0.00027961665 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -98.1         |
|    explained_variance   | 0.276         |
|    learning_rate        | 0.0001        |
|    loss                 | 138           |
|    n_updates            | 9790          |
|    policy_gradient_loss | -0.000552     |
|    std                  | 1.81          |
|    value_loss           | 265           |
-------------------------------------------
Iteration: 980 | Episodes: 59700 | Median Reward: 37.83 | Max Reward: 49.14
Iteration: 981 | Episodes: 59750 | Median Reward: 38.74 | Max Reward: 49.14
Iteration: 981 | Episodes: 59800 | Median Reward: 38.14 | Max Reward: 49.14
Iteration: 982 | Episodes: 59850 | Median Reward: 38.14 | Max Reward: 49.14
Iteration: 983 | Episodes: 59900 | Median Reward: 35.38 | Max Reward: 49.14
Iteration: 984 | Episodes: 59950 | Median Reward: 39.36 | Max Reward: 49.14
Iteration: 985 | Episodes: 60000 | Median Reward: 39.36 | Max Reward: 49.14
Iteration: 986 | Episodes: 60050 | Median Reward: 38.67 | Max Reward: 49.14
Iteration: 986 | Episodes: 60100 | Median Reward: 41.59 | Max Reward: 49.14
Iteration: 987 | Episodes: 60150 | Median Reward: 38.45 | Max Reward: 49.14
Iteration: 988 | Episodes: 60200 | Median Reward: 38.26 | Max Reward: 49.14
Iteration: 989 | Episodes: 60250 | Median Reward: 38.06 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.6        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 990          |
|    time_elapsed         | 17904        |
|    total_timesteps      | 6082560      |
| train/                  |              |
|    approx_kl            | 0.0002645196 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -98.3        |
|    explained_variance   | 0.281        |
|    learning_rate        | 0.0001       |
|    loss                 | 125          |
|    n_updates            | 9890         |
|    policy_gradient_loss | -0.000232    |
|    std                  | 1.83         |
|    value_loss           | 265          |
------------------------------------------
Iteration: 990 | Episodes: 60300 | Median Reward: 39.05 | Max Reward: 49.14
Iteration: 990 | Episodes: 60350 | Median Reward: 35.58 | Max Reward: 49.14
Iteration: 991 | Episodes: 60400 | Median Reward: 45.22 | Max Reward: 49.14
Iteration: 992 | Episodes: 60450 | Median Reward: 36.53 | Max Reward: 49.14
Iteration: 993 | Episodes: 60500 | Median Reward: 36.20 | Max Reward: 49.14
Iteration: 994 | Episodes: 60550 | Median Reward: 34.14 | Max Reward: 49.14
Iteration: 995 | Episodes: 60600 | Median Reward: 39.31 | Max Reward: 49.14
Iteration: 995 | Episodes: 60650 | Median Reward: 42.13 | Max Reward: 49.14
Iteration: 996 | Episodes: 60700 | Median Reward: 42.46 | Max Reward: 49.14
Iteration: 997 | Episodes: 60750 | Median Reward: 40.86 | Max Reward: 49.14
Iteration: 998 | Episodes: 60800 | Median Reward: 36.87 | Max Reward: 49.14
Iteration: 999 | Episodes: 60850 | Median Reward: 39.65 | Max Reward: 49.14
Iteration: 999 | Episodes: 60900 | Median Reward: 43.44 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.7        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 1000         |
|    time_elapsed         | 18085        |
|    total_timesteps      | 6144000      |
| train/                  |              |
|    approx_kl            | 0.0028147106 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -98.5        |
|    explained_variance   | 0.284        |
|    learning_rate        | 0.0001       |
|    loss                 | 109          |
|    n_updates            | 9990         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 1.85         |
|    value_loss           | 263          |
------------------------------------------
Iteration: 1000 | Episodes: 60950 | Median Reward: 39.10 | Max Reward: 49.14
Iteration: 1001 | Episodes: 61000 | Median Reward: 39.18 | Max Reward: 49.14
Iteration: 1002 | Episodes: 61050 | Median Reward: 37.27 | Max Reward: 49.14
Iteration: 1003 | Episodes: 61100 | Median Reward: 35.14 | Max Reward: 49.14
Iteration: 1004 | Episodes: 61150 | Median Reward: 39.86 | Max Reward: 49.14
Iteration: 1004 | Episodes: 61200 | Median Reward: 39.05 | Max Reward: 49.14
Iteration: 1005 | Episodes: 61250 | Median Reward: 42.16 | Max Reward: 49.14
Iteration: 1006 | Episodes: 61300 | Median Reward: 41.01 | Max Reward: 49.14
Iteration: 1007 | Episodes: 61350 | Median Reward: 39.06 | Max Reward: 49.14
Iteration: 1008 | Episodes: 61400 | Median Reward: 38.74 | Max Reward: 49.14
Iteration: 1009 | Episodes: 61450 | Median Reward: 36.79 | Max Reward: 49.14
Iteration: 1009 | Episodes: 61500 | Median Reward: 33.32 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65          |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 1010         |
|    time_elapsed         | 18263        |
|    total_timesteps      | 6205440      |
| train/                  |              |
|    approx_kl            | 0.0005350101 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.4          |
|    entropy_loss         | -98.7        |
|    explained_variance   | 0.291        |
|    learning_rate        | 0.0001       |
|    loss                 | 112          |
|    n_updates            | 10090        |
|    policy_gradient_loss | 0.000117     |
|    std                  | 1.87         |
|    value_loss           | 251          |
------------------------------------------
Iteration: 1010 | Episodes: 61550 | Median Reward: 39.29 | Max Reward: 49.14
Iteration: 1011 | Episodes: 61600 | Median Reward: 39.28 | Max Reward: 49.14
Iteration: 1012 | Episodes: 61650 | Median Reward: 38.89 | Max Reward: 49.14
Iteration: 1013 | Episodes: 61700 | Median Reward: 39.84 | Max Reward: 49.14
Iteration: 1013 | Episodes: 61750 | Median Reward: 34.74 | Max Reward: 49.14
Iteration: 1014 | Episodes: 61800 | Median Reward: 39.70 | Max Reward: 49.14
Iteration: 1015 | Episodes: 61850 | Median Reward: 40.03 | Max Reward: 49.14
Iteration: 1016 | Episodes: 61900 | Median Reward: 39.19 | Max Reward: 49.14
Iteration: 1017 | Episodes: 61950 | Median Reward: 37.57 | Max Reward: 49.14
Iteration: 1018 | Episodes: 62000 | Median Reward: 35.72 | Max Reward: 49.14
Iteration: 1018 | Episodes: 62050 | Median Reward: 41.42 | Max Reward: 49.14
Iteration: 1019 | Episodes: 62100 | Median Reward: 38.04 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.7         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1020          |
|    time_elapsed         | 18453         |
|    total_timesteps      | 6266880       |
| train/                  |               |
|    approx_kl            | 0.00072511396 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -99           |
|    explained_variance   | 0.291         |
|    learning_rate        | 0.0001        |
|    loss                 | 109           |
|    n_updates            | 10190         |
|    policy_gradient_loss | -0.000651     |
|    std                  | 1.88          |
|    value_loss           | 261           |
-------------------------------------------
Iteration: 1020 | Episodes: 62150 | Median Reward: 38.79 | Max Reward: 49.14
Iteration: 1021 | Episodes: 62200 | Median Reward: 41.91 | Max Reward: 49.14
Iteration: 1022 | Episodes: 62250 | Median Reward: 46.29 | Max Reward: 49.14
Iteration: 1023 | Episodes: 62300 | Median Reward: 37.96 | Max Reward: 49.14
Iteration: 1023 | Episodes: 62350 | Median Reward: 34.22 | Max Reward: 49.14
Iteration: 1024 | Episodes: 62400 | Median Reward: 39.03 | Max Reward: 49.14
Iteration: 1025 | Episodes: 62450 | Median Reward: 37.98 | Max Reward: 49.14
Iteration: 1026 | Episodes: 62500 | Median Reward: 37.98 | Max Reward: 49.14
Iteration: 1027 | Episodes: 62550 | Median Reward: 32.66 | Max Reward: 49.14
Iteration: 1027 | Episodes: 62600 | Median Reward: 34.11 | Max Reward: 49.14
Iteration: 1028 | Episodes: 62650 | Median Reward: 38.68 | Max Reward: 49.14
Iteration: 1029 | Episodes: 62700 | Median Reward: 40.11 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.4        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 1030         |
|    time_elapsed         | 18633        |
|    total_timesteps      | 6328320      |
| train/                  |              |
|    approx_kl            | 0.0005634716 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -99.3        |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.0001       |
|    loss                 | 113          |
|    n_updates            | 10290        |
|    policy_gradient_loss | -0.000932    |
|    std                  | 1.9          |
|    value_loss           | 246          |
------------------------------------------
Iteration: 1030 | Episodes: 62750 | Median Reward: 41.97 | Max Reward: 49.14
Iteration: 1031 | Episodes: 62800 | Median Reward: 42.11 | Max Reward: 49.14
Iteration: 1031 | Episodes: 62850 | Median Reward: 37.31 | Max Reward: 49.14
Iteration: 1032 | Episodes: 62900 | Median Reward: 39.57 | Max Reward: 49.14
Iteration: 1033 | Episodes: 62950 | Median Reward: 36.98 | Max Reward: 49.14
Iteration: 1034 | Episodes: 63000 | Median Reward: 36.98 | Max Reward: 49.14
Iteration: 1035 | Episodes: 63050 | Median Reward: 38.05 | Max Reward: 49.14
Iteration: 1036 | Episodes: 63100 | Median Reward: 37.60 | Max Reward: 49.14
Iteration: 1036 | Episodes: 63150 | Median Reward: 35.73 | Max Reward: 49.14
Iteration: 1037 | Episodes: 63200 | Median Reward: 39.31 | Max Reward: 49.14
Iteration: 1038 | Episodes: 63250 | Median Reward: 38.41 | Max Reward: 49.14
Iteration: 1039 | Episodes: 63300 | Median Reward: 39.20 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.1        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 1040         |
|    time_elapsed         | 18810        |
|    total_timesteps      | 6389760      |
| train/                  |              |
|    approx_kl            | 0.0017913927 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -99.5        |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.0001       |
|    loss                 | 123          |
|    n_updates            | 10390        |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.92         |
|    value_loss           | 257          |
------------------------------------------
Iteration: 1040 | Episodes: 63350 | Median Reward: 39.51 | Max Reward: 49.14
Iteration: 1041 | Episodes: 63400 | Median Reward: 38.36 | Max Reward: 49.14
Iteration: 1041 | Episodes: 63450 | Median Reward: 40.43 | Max Reward: 49.14
Iteration: 1042 | Episodes: 63500 | Median Reward: 37.23 | Max Reward: 49.14
Iteration: 1043 | Episodes: 63550 | Median Reward: 39.39 | Max Reward: 49.14
Iteration: 1044 | Episodes: 63600 | Median Reward: 40.83 | Max Reward: 49.14
Iteration: 1045 | Episodes: 63650 | Median Reward: 41.83 | Max Reward: 49.14
Iteration: 1046 | Episodes: 63700 | Median Reward: 36.64 | Max Reward: 49.14
Iteration: 1046 | Episodes: 63750 | Median Reward: 43.27 | Max Reward: 49.14
Iteration: 1047 | Episodes: 63800 | Median Reward: 38.51 | Max Reward: 49.14
Iteration: 1048 | Episodes: 63850 | Median Reward: 37.90 | Max Reward: 49.14
Iteration: 1049 | Episodes: 63900 | Median Reward: 37.99 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.3         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1050          |
|    time_elapsed         | 19002         |
|    total_timesteps      | 6451200       |
| train/                  |               |
|    approx_kl            | 0.00042215473 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -99.8         |
|    explained_variance   | 0.325         |
|    learning_rate        | 0.0001        |
|    loss                 | 97.2          |
|    n_updates            | 10490         |
|    policy_gradient_loss | -0.00047      |
|    std                  | 1.94          |
|    value_loss           | 215           |
-------------------------------------------
Iteration: 1050 | Episodes: 63950 | Median Reward: 39.79 | Max Reward: 49.14
Iteration: 1050 | Episodes: 64000 | Median Reward: 36.92 | Max Reward: 49.14
Iteration: 1051 | Episodes: 64050 | Median Reward: 42.29 | Max Reward: 49.14
Iteration: 1052 | Episodes: 64100 | Median Reward: 35.97 | Max Reward: 49.14
Iteration: 1053 | Episodes: 64150 | Median Reward: 38.02 | Max Reward: 49.14
Iteration: 1054 | Episodes: 64200 | Median Reward: 38.23 | Max Reward: 49.14
Iteration: 1055 | Episodes: 64250 | Median Reward: 42.02 | Max Reward: 49.14
Iteration: 1055 | Episodes: 64300 | Median Reward: 42.32 | Max Reward: 49.14
Iteration: 1056 | Episodes: 64350 | Median Reward: 42.41 | Max Reward: 49.14
Iteration: 1057 | Episodes: 64400 | Median Reward: 41.77 | Max Reward: 49.14
Iteration: 1058 | Episodes: 64450 | Median Reward: 41.05 | Max Reward: 49.14
Iteration: 1059 | Episodes: 64500 | Median Reward: 37.40 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.5         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1060          |
|    time_elapsed         | 19179         |
|    total_timesteps      | 6512640       |
| train/                  |               |
|    approx_kl            | 0.00036748708 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -100          |
|    explained_variance   | 0.308         |
|    learning_rate        | 0.0001        |
|    loss                 | 109           |
|    n_updates            | 10590         |
|    policy_gradient_loss | -0.000182     |
|    std                  | 1.96          |
|    value_loss           | 254           |
-------------------------------------------
Iteration: 1060 | Episodes: 64550 | Median Reward: 42.11 | Max Reward: 49.14
Iteration: 1060 | Episodes: 64600 | Median Reward: 36.59 | Max Reward: 49.14
Iteration: 1061 | Episodes: 64650 | Median Reward: 36.62 | Max Reward: 49.14
Iteration: 1062 | Episodes: 64700 | Median Reward: 36.62 | Max Reward: 49.14
Iteration: 1063 | Episodes: 64750 | Median Reward: 38.13 | Max Reward: 49.14
Iteration: 1064 | Episodes: 64800 | Median Reward: 35.99 | Max Reward: 49.14
Iteration: 1064 | Episodes: 64850 | Median Reward: 35.40 | Max Reward: 49.14
Iteration: 1065 | Episodes: 64900 | Median Reward: 38.89 | Max Reward: 49.14
Iteration: 1066 | Episodes: 64950 | Median Reward: 38.89 | Max Reward: 49.14
Iteration: 1067 | Episodes: 65000 | Median Reward: 40.18 | Max Reward: 49.14
Iteration: 1068 | Episodes: 65050 | Median Reward: 40.18 | Max Reward: 49.14
Iteration: 1068 | Episodes: 65100 | Median Reward: 32.31 | Max Reward: 49.14
Iteration: 1069 | Episodes: 65150 | Median Reward: 40.77 | Max Reward: 49.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.3       |
| time/                   |             |
|    fps                  | 339         |
|    iterations           | 1070        |
|    time_elapsed         | 19370       |
|    total_timesteps      | 6574080     |
| train/                  |             |
|    approx_kl            | 0.002042389 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -100        |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0001      |
|    loss                 | 106         |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.00227    |
|    std                  | 1.98        |
|    value_loss           | 211         |
-----------------------------------------
Iteration: 1070 | Episodes: 65200 | Median Reward: 39.66 | Max Reward: 49.14
Iteration: 1071 | Episodes: 65250 | Median Reward: 41.01 | Max Reward: 49.14
Iteration: 1072 | Episodes: 65300 | Median Reward: 43.51 | Max Reward: 49.14
Iteration: 1073 | Episodes: 65350 | Median Reward: 43.40 | Max Reward: 49.14
Iteration: 1073 | Episodes: 65400 | Median Reward: 43.37 | Max Reward: 49.14
Iteration: 1074 | Episodes: 65450 | Median Reward: 41.60 | Max Reward: 49.14
Iteration: 1075 | Episodes: 65500 | Median Reward: 39.55 | Max Reward: 49.14
Iteration: 1076 | Episodes: 65550 | Median Reward: 39.27 | Max Reward: 49.14
Iteration: 1077 | Episodes: 65600 | Median Reward: 41.22 | Max Reward: 49.14
Iteration: 1078 | Episodes: 65650 | Median Reward: 41.11 | Max Reward: 49.14
Iteration: 1078 | Episodes: 65700 | Median Reward: 42.17 | Max Reward: 49.14
Iteration: 1079 | Episodes: 65750 | Median Reward: 40.23 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.3         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1080          |
|    time_elapsed         | 19547         |
|    total_timesteps      | 6635520       |
| train/                  |               |
|    approx_kl            | 0.00035130675 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -101          |
|    explained_variance   | 0.309         |
|    learning_rate        | 0.0001        |
|    loss                 | 126           |
|    n_updates            | 10790         |
|    policy_gradient_loss | -0.000446     |
|    std                  | 2.01          |
|    value_loss           | 266           |
-------------------------------------------
Iteration: 1080 | Episodes: 65800 | Median Reward: 34.02 | Max Reward: 49.14
Iteration: 1081 | Episodes: 65850 | Median Reward: 34.02 | Max Reward: 49.14
Iteration: 1082 | Episodes: 65900 | Median Reward: 35.33 | Max Reward: 49.14
Iteration: 1082 | Episodes: 65950 | Median Reward: 43.62 | Max Reward: 49.14
Iteration: 1083 | Episodes: 66000 | Median Reward: 39.17 | Max Reward: 49.14
Iteration: 1084 | Episodes: 66050 | Median Reward: 39.68 | Max Reward: 49.14
Iteration: 1085 | Episodes: 66100 | Median Reward: 38.71 | Max Reward: 49.14
Iteration: 1086 | Episodes: 66150 | Median Reward: 37.98 | Max Reward: 49.14
Iteration: 1087 | Episodes: 66200 | Median Reward: 35.56 | Max Reward: 49.14
Iteration: 1087 | Episodes: 66250 | Median Reward: 37.18 | Max Reward: 49.14
Iteration: 1088 | Episodes: 66300 | Median Reward: 39.06 | Max Reward: 49.14
Iteration: 1089 | Episodes: 66350 | Median Reward: 41.93 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.5         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1090          |
|    time_elapsed         | 19729         |
|    total_timesteps      | 6696960       |
| train/                  |               |
|    approx_kl            | 5.5984885e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -101          |
|    explained_variance   | 0.31          |
|    learning_rate        | 0.0001        |
|    loss                 | 131           |
|    n_updates            | 10890         |
|    policy_gradient_loss | -6.97e-05     |
|    std                  | 2.03          |
|    value_loss           | 265           |
-------------------------------------------
Iteration: 1090 | Episodes: 66400 | Median Reward: 39.18 | Max Reward: 49.14
Iteration: 1091 | Episodes: 66450 | Median Reward: 34.00 | Max Reward: 49.14
Iteration: 1092 | Episodes: 66500 | Median Reward: 41.93 | Max Reward: 49.14
Iteration: 1092 | Episodes: 66550 | Median Reward: 44.60 | Max Reward: 49.14
Iteration: 1093 | Episodes: 66600 | Median Reward: 39.68 | Max Reward: 49.14
Iteration: 1094 | Episodes: 66650 | Median Reward: 39.68 | Max Reward: 49.14
Iteration: 1095 | Episodes: 66700 | Median Reward: 39.27 | Max Reward: 49.14
Iteration: 1096 | Episodes: 66750 | Median Reward: 39.92 | Max Reward: 49.14
Iteration: 1096 | Episodes: 66800 | Median Reward: 38.52 | Max Reward: 49.14
Iteration: 1097 | Episodes: 66850 | Median Reward: 39.39 | Max Reward: 49.14
Iteration: 1098 | Episodes: 66900 | Median Reward: 40.48 | Max Reward: 49.14
Iteration: 1099 | Episodes: 66950 | Median Reward: 36.36 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64           |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1100          |
|    time_elapsed         | 19915         |
|    total_timesteps      | 6758400       |
| train/                  |               |
|    approx_kl            | 0.00044522987 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -101          |
|    explained_variance   | 0.32          |
|    learning_rate        | 0.0001        |
|    loss                 | 118           |
|    n_updates            | 10990         |
|    policy_gradient_loss | -0.000888     |
|    std                  | 2.05          |
|    value_loss           | 255           |
-------------------------------------------
Iteration: 1100 | Episodes: 67000 | Median Reward: 35.67 | Max Reward: 49.14
Iteration: 1101 | Episodes: 67050 | Median Reward: 42.09 | Max Reward: 49.14
Iteration: 1101 | Episodes: 67100 | Median Reward: 39.69 | Max Reward: 49.14
Iteration: 1102 | Episodes: 67150 | Median Reward: 39.50 | Max Reward: 49.14
Iteration: 1103 | Episodes: 67200 | Median Reward: 43.43 | Max Reward: 49.14
Iteration: 1104 | Episodes: 67250 | Median Reward: 42.34 | Max Reward: 49.14
Iteration: 1105 | Episodes: 67300 | Median Reward: 39.31 | Max Reward: 49.14
Iteration: 1105 | Episodes: 67350 | Median Reward: 39.87 | Max Reward: 49.14
Iteration: 1106 | Episodes: 67400 | Median Reward: 38.35 | Max Reward: 49.14
Iteration: 1107 | Episodes: 67450 | Median Reward: 36.68 | Max Reward: 49.14
Iteration: 1108 | Episodes: 67500 | Median Reward: 37.04 | Max Reward: 49.14
Iteration: 1109 | Episodes: 67550 | Median Reward: 36.81 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.3         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1110          |
|    time_elapsed         | 20095         |
|    total_timesteps      | 6819840       |
| train/                  |               |
|    approx_kl            | 0.00069655053 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -101          |
|    explained_variance   | 0.33          |
|    learning_rate        | 0.0001        |
|    loss                 | 119           |
|    n_updates            | 11090         |
|    policy_gradient_loss | -0.000714     |
|    std                  | 2.07          |
|    value_loss           | 240           |
-------------------------------------------
Iteration: 1110 | Episodes: 67600 | Median Reward: 36.75 | Max Reward: 49.14
Iteration: 1110 | Episodes: 67650 | Median Reward: 41.32 | Max Reward: 49.14
Iteration: 1111 | Episodes: 67700 | Median Reward: 42.31 | Max Reward: 49.14
Iteration: 1112 | Episodes: 67750 | Median Reward: 43.37 | Max Reward: 49.14
Iteration: 1113 | Episodes: 67800 | Median Reward: 39.54 | Max Reward: 49.14
Iteration: 1114 | Episodes: 67850 | Median Reward: 38.41 | Max Reward: 49.14
Iteration: 1115 | Episodes: 67900 | Median Reward: 37.85 | Max Reward: 49.14
Iteration: 1115 | Episodes: 67950 | Median Reward: 35.38 | Max Reward: 49.14
Iteration: 1116 | Episodes: 68000 | Median Reward: 37.78 | Max Reward: 49.14
Iteration: 1117 | Episodes: 68050 | Median Reward: 41.78 | Max Reward: 49.14
Iteration: 1118 | Episodes: 68100 | Median Reward: 43.49 | Max Reward: 49.14
Iteration: 1119 | Episodes: 68150 | Median Reward: 37.44 | Max Reward: 49.14
Iteration: 1119 | Episodes: 68200 | Median Reward: 35.11 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.1         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1120          |
|    time_elapsed         | 20272         |
|    total_timesteps      | 6881280       |
| train/                  |               |
|    approx_kl            | 0.00021547821 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -101          |
|    explained_variance   | 0.32          |
|    learning_rate        | 0.0001        |
|    loss                 | 134           |
|    n_updates            | 11190         |
|    policy_gradient_loss | 0.000492      |
|    std                  | 2.09          |
|    value_loss           | 269           |
-------------------------------------------
Iteration: 1120 | Episodes: 68250 | Median Reward: 40.22 | Max Reward: 49.14
Iteration: 1121 | Episodes: 68300 | Median Reward: 40.45 | Max Reward: 49.14
Iteration: 1122 | Episodes: 68350 | Median Reward: 39.45 | Max Reward: 49.14
Iteration: 1123 | Episodes: 68400 | Median Reward: 39.86 | Max Reward: 49.14
Iteration: 1124 | Episodes: 68450 | Median Reward: 36.78 | Max Reward: 49.14
Iteration: 1124 | Episodes: 68500 | Median Reward: 38.97 | Max Reward: 49.14
Iteration: 1125 | Episodes: 68550 | Median Reward: 41.57 | Max Reward: 49.14
Iteration: 1126 | Episodes: 68600 | Median Reward: 41.78 | Max Reward: 49.14
Iteration: 1127 | Episodes: 68650 | Median Reward: 41.78 | Max Reward: 49.14
Iteration: 1128 | Episodes: 68700 | Median Reward: 38.14 | Max Reward: 49.14
Iteration: 1129 | Episodes: 68750 | Median Reward: 35.56 | Max Reward: 49.14
Iteration: 1129 | Episodes: 68800 | Median Reward: 40.65 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.5         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1130          |
|    time_elapsed         | 20461         |
|    total_timesteps      | 6942720       |
| train/                  |               |
|    approx_kl            | 0.00013001147 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0.331         |
|    learning_rate        | 0.0001        |
|    loss                 | 129           |
|    n_updates            | 11290         |
|    policy_gradient_loss | -0.000259     |
|    std                  | 2.11          |
|    value_loss           | 248           |
-------------------------------------------
Iteration: 1130 | Episodes: 68850 | Median Reward: 38.24 | Max Reward: 49.14
Iteration: 1131 | Episodes: 68900 | Median Reward: 40.05 | Max Reward: 49.14
Iteration: 1132 | Episodes: 68950 | Median Reward: 42.15 | Max Reward: 49.14
Iteration: 1133 | Episodes: 69000 | Median Reward: 38.94 | Max Reward: 49.14
Iteration: 1133 | Episodes: 69050 | Median Reward: 39.68 | Max Reward: 49.14
Iteration: 1134 | Episodes: 69100 | Median Reward: 39.77 | Max Reward: 49.14
Iteration: 1135 | Episodes: 69150 | Median Reward: 39.72 | Max Reward: 49.14
Iteration: 1136 | Episodes: 69200 | Median Reward: 39.65 | Max Reward: 49.14
Iteration: 1137 | Episodes: 69250 | Median Reward: 39.51 | Max Reward: 49.14
Iteration: 1138 | Episodes: 69300 | Median Reward: 38.48 | Max Reward: 49.14
Iteration: 1138 | Episodes: 69350 | Median Reward: 39.36 | Max Reward: 49.14
Iteration: 1139 | Episodes: 69400 | Median Reward: 39.61 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.6        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 1140         |
|    time_elapsed         | 20638        |
|    total_timesteps      | 7004160      |
| train/                  |              |
|    approx_kl            | 0.0008453901 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.342        |
|    learning_rate        | 0.0001       |
|    loss                 | 113          |
|    n_updates            | 11390        |
|    policy_gradient_loss | -0.000811    |
|    std                  | 2.13         |
|    value_loss           | 235          |
------------------------------------------
Iteration: 1140 | Episodes: 69450 | Median Reward: 38.78 | Max Reward: 49.14
Iteration: 1141 | Episodes: 69500 | Median Reward: 37.78 | Max Reward: 49.14
Iteration: 1142 | Episodes: 69550 | Median Reward: 40.88 | Max Reward: 49.14
Iteration: 1142 | Episodes: 69600 | Median Reward: 33.91 | Max Reward: 49.14
Iteration: 1143 | Episodes: 69650 | Median Reward: 36.24 | Max Reward: 49.14
Iteration: 1144 | Episodes: 69700 | Median Reward: 37.31 | Max Reward: 49.14
Iteration: 1145 | Episodes: 69750 | Median Reward: 41.79 | Max Reward: 49.14
Iteration: 1146 | Episodes: 69800 | Median Reward: 42.22 | Max Reward: 49.14
Iteration: 1147 | Episodes: 69850 | Median Reward: 43.47 | Max Reward: 49.14
Iteration: 1147 | Episodes: 69900 | Median Reward: 37.24 | Max Reward: 49.14
Iteration: 1148 | Episodes: 69950 | Median Reward: 39.52 | Max Reward: 49.14
Iteration: 1149 | Episodes: 70000 | Median Reward: 37.73 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.7        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 1150         |
|    time_elapsed         | 20820        |
|    total_timesteps      | 7065600      |
| train/                  |              |
|    approx_kl            | 0.0003838258 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.336        |
|    learning_rate        | 0.0001       |
|    loss                 | 96.2         |
|    n_updates            | 11490        |
|    policy_gradient_loss | -0.000536    |
|    std                  | 2.15         |
|    value_loss           | 253          |
------------------------------------------
Iteration: 1150 | Episodes: 70050 | Median Reward: 38.03 | Max Reward: 49.14
Iteration: 1151 | Episodes: 70100 | Median Reward: 38.32 | Max Reward: 49.14
Iteration: 1152 | Episodes: 70150 | Median Reward: 40.05 | Max Reward: 49.14
Iteration: 1152 | Episodes: 70200 | Median Reward: 36.88 | Max Reward: 49.14
Iteration: 1153 | Episodes: 70250 | Median Reward: 39.66 | Max Reward: 49.14
Iteration: 1154 | Episodes: 70300 | Median Reward: 39.83 | Max Reward: 49.14
Iteration: 1155 | Episodes: 70350 | Median Reward: 40.07 | Max Reward: 49.14
Iteration: 1156 | Episodes: 70400 | Median Reward: 39.42 | Max Reward: 49.14
Iteration: 1156 | Episodes: 70450 | Median Reward: 38.32 | Max Reward: 49.14
Iteration: 1157 | Episodes: 70500 | Median Reward: 40.51 | Max Reward: 49.14
Iteration: 1158 | Episodes: 70550 | Median Reward: 39.18 | Max Reward: 49.14
Iteration: 1159 | Episodes: 70600 | Median Reward: 38.43 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.9         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1160          |
|    time_elapsed         | 21004         |
|    total_timesteps      | 7127040       |
| train/                  |               |
|    approx_kl            | 0.00043592858 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0.354         |
|    learning_rate        | 0.0001        |
|    loss                 | 122           |
|    n_updates            | 11590         |
|    policy_gradient_loss | -0.000361     |
|    std                  | 2.17          |
|    value_loss           | 251           |
-------------------------------------------
Iteration: 1160 | Episodes: 70650 | Median Reward: 38.29 | Max Reward: 49.14
Iteration: 1161 | Episodes: 70700 | Median Reward: 37.09 | Max Reward: 49.14
Iteration: 1161 | Episodes: 70750 | Median Reward: 40.19 | Max Reward: 49.14
Iteration: 1162 | Episodes: 70800 | Median Reward: 38.80 | Max Reward: 49.14
Iteration: 1163 | Episodes: 70850 | Median Reward: 38.80 | Max Reward: 49.14
Iteration: 1164 | Episodes: 70900 | Median Reward: 39.07 | Max Reward: 49.14
Iteration: 1165 | Episodes: 70950 | Median Reward: 38.54 | Max Reward: 49.14
Iteration: 1166 | Episodes: 71000 | Median Reward: 37.59 | Max Reward: 49.14
Iteration: 1166 | Episodes: 71050 | Median Reward: 41.22 | Max Reward: 49.14
Iteration: 1167 | Episodes: 71100 | Median Reward: 38.42 | Max Reward: 49.14
Iteration: 1168 | Episodes: 71150 | Median Reward: 41.22 | Max Reward: 49.14
Iteration: 1169 | Episodes: 71200 | Median Reward: 40.80 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.5         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1170          |
|    time_elapsed         | 21186         |
|    total_timesteps      | 7188480       |
| train/                  |               |
|    approx_kl            | 0.00012266403 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | 0.36          |
|    learning_rate        | 0.0001        |
|    loss                 | 122           |
|    n_updates            | 11690         |
|    policy_gradient_loss | -0.000111     |
|    std                  | 2.2           |
|    value_loss           | 262           |
-------------------------------------------
Iteration: 1170 | Episodes: 71250 | Median Reward: 39.59 | Max Reward: 49.14
Iteration: 1170 | Episodes: 71300 | Median Reward: 39.23 | Max Reward: 49.14
Iteration: 1171 | Episodes: 71350 | Median Reward: 38.39 | Max Reward: 49.14
Iteration: 1172 | Episodes: 71400 | Median Reward: 39.57 | Max Reward: 49.14
Iteration: 1173 | Episodes: 71450 | Median Reward: 39.57 | Max Reward: 49.14
Iteration: 1174 | Episodes: 71500 | Median Reward: 38.56 | Max Reward: 49.14
Iteration: 1175 | Episodes: 71550 | Median Reward: 33.12 | Max Reward: 49.14
Iteration: 1175 | Episodes: 71600 | Median Reward: 37.42 | Max Reward: 49.14
Iteration: 1176 | Episodes: 71650 | Median Reward: 42.15 | Max Reward: 49.14
Iteration: 1177 | Episodes: 71700 | Median Reward: 42.16 | Max Reward: 49.14
Iteration: 1178 | Episodes: 71750 | Median Reward: 42.50 | Max Reward: 49.14
Iteration: 1179 | Episodes: 71800 | Median Reward: 41.51 | Max Reward: 49.14
Iteration: 1179 | Episodes: 71850 | Median Reward: 38.21 | Max Reward: 49.14
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60         |
| time/                   |             |
|    fps                  | 339         |
|    iterations           | 1180        |
|    time_elapsed         | 21364       |
|    total_timesteps      | 7249920     |
| train/                  |             |
|    approx_kl            | 8.75569e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -103        |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.0001      |
|    loss                 | 122         |
|    n_updates            | 11790       |
|    policy_gradient_loss | -0.000222   |
|    std                  | 2.22        |
|    value_loss           | 246         |
-----------------------------------------
Iteration: 1180 | Episodes: 71900 | Median Reward: 40.14 | Max Reward: 49.14
Iteration: 1181 | Episodes: 71950 | Median Reward: 41.50 | Max Reward: 49.14
Iteration: 1182 | Episodes: 72000 | Median Reward: 38.63 | Max Reward: 49.14
Iteration: 1183 | Episodes: 72050 | Median Reward: 39.48 | Max Reward: 49.14
Iteration: 1184 | Episodes: 72100 | Median Reward: 38.74 | Max Reward: 49.14
Iteration: 1184 | Episodes: 72150 | Median Reward: 32.49 | Max Reward: 49.14
Iteration: 1185 | Episodes: 72200 | Median Reward: 37.30 | Max Reward: 49.14
Iteration: 1186 | Episodes: 72250 | Median Reward: 32.94 | Max Reward: 49.14
Iteration: 1187 | Episodes: 72300 | Median Reward: 39.25 | Max Reward: 49.14
Iteration: 1188 | Episodes: 72350 | Median Reward: 42.37 | Max Reward: 49.14
Iteration: 1189 | Episodes: 72400 | Median Reward: 39.14 | Max Reward: 49.14
Iteration: 1189 | Episodes: 72450 | Median Reward: 40.13 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.6        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 1190         |
|    time_elapsed         | 21551        |
|    total_timesteps      | 7311360      |
| train/                  |              |
|    approx_kl            | 0.0010393067 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -103         |
|    explained_variance   | 0.378        |
|    learning_rate        | 0.0001       |
|    loss                 | 101          |
|    n_updates            | 11890        |
|    policy_gradient_loss | -0.00164     |
|    std                  | 2.24         |
|    value_loss           | 237          |
------------------------------------------
Iteration: 1190 | Episodes: 72500 | Median Reward: 41.47 | Max Reward: 49.14
Iteration: 1191 | Episodes: 72550 | Median Reward: 42.23 | Max Reward: 49.14
Iteration: 1192 | Episodes: 72600 | Median Reward: 42.83 | Max Reward: 49.14
Iteration: 1193 | Episodes: 72650 | Median Reward: 41.59 | Max Reward: 49.14
Iteration: 1193 | Episodes: 72700 | Median Reward: 41.06 | Max Reward: 49.14
Iteration: 1194 | Episodes: 72750 | Median Reward: 41.66 | Max Reward: 49.14
Iteration: 1195 | Episodes: 72800 | Median Reward: 40.23 | Max Reward: 49.14
Iteration: 1196 | Episodes: 72850 | Median Reward: 40.23 | Max Reward: 49.14
Iteration: 1197 | Episodes: 72900 | Median Reward: 43.41 | Max Reward: 49.14
Iteration: 1198 | Episodes: 72950 | Median Reward: 42.27 | Max Reward: 49.14
Iteration: 1198 | Episodes: 73000 | Median Reward: 36.47 | Max Reward: 49.14
Iteration: 1199 | Episodes: 73050 | Median Reward: 39.54 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.9         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1200          |
|    time_elapsed         | 21730         |
|    total_timesteps      | 7372800       |
| train/                  |               |
|    approx_kl            | 0.00025114993 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | 0.406         |
|    learning_rate        | 0.0001        |
|    loss                 | 92.1          |
|    n_updates            | 11990         |
|    policy_gradient_loss | -0.000418     |
|    std                  | 2.26          |
|    value_loss           | 200           |
-------------------------------------------
Iteration: 1200 | Episodes: 73100 | Median Reward: 39.35 | Max Reward: 49.14
Iteration: 1201 | Episodes: 73150 | Median Reward: 37.97 | Max Reward: 49.14
Iteration: 1202 | Episodes: 73200 | Median Reward: 37.09 | Max Reward: 49.14
Iteration: 1202 | Episodes: 73250 | Median Reward: 37.09 | Max Reward: 49.14
Iteration: 1203 | Episodes: 73300 | Median Reward: 40.36 | Max Reward: 49.14
Iteration: 1204 | Episodes: 73350 | Median Reward: 38.92 | Max Reward: 49.14
Iteration: 1205 | Episodes: 73400 | Median Reward: 42.51 | Max Reward: 49.14
Iteration: 1206 | Episodes: 73450 | Median Reward: 42.70 | Max Reward: 49.14
Iteration: 1207 | Episodes: 73500 | Median Reward: 39.45 | Max Reward: 49.14
Iteration: 1207 | Episodes: 73550 | Median Reward: 35.21 | Max Reward: 49.14
Iteration: 1208 | Episodes: 73600 | Median Reward: 39.06 | Max Reward: 49.14
Iteration: 1209 | Episodes: 73650 | Median Reward: 40.17 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.6         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1210          |
|    time_elapsed         | 21918         |
|    total_timesteps      | 7434240       |
| train/                  |               |
|    approx_kl            | 0.00019409458 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -104          |
|    explained_variance   | 0.385         |
|    learning_rate        | 0.0001        |
|    loss                 | 103           |
|    n_updates            | 12090         |
|    policy_gradient_loss | -0.000155     |
|    std                  | 2.28          |
|    value_loss           | 244           |
-------------------------------------------
Iteration: 1210 | Episodes: 73700 | Median Reward: 42.55 | Max Reward: 49.14
Iteration: 1211 | Episodes: 73750 | Median Reward: 42.74 | Max Reward: 49.14
Iteration: 1211 | Episodes: 73800 | Median Reward: 40.12 | Max Reward: 49.14
Iteration: 1212 | Episodes: 73850 | Median Reward: 43.85 | Max Reward: 49.14
Iteration: 1213 | Episodes: 73900 | Median Reward: 39.58 | Max Reward: 49.14
Iteration: 1214 | Episodes: 73950 | Median Reward: 39.49 | Max Reward: 49.14
Iteration: 1215 | Episodes: 74000 | Median Reward: 39.67 | Max Reward: 49.14
Iteration: 1216 | Episodes: 74050 | Median Reward: 39.85 | Max Reward: 49.14
Iteration: 1216 | Episodes: 74100 | Median Reward: 42.88 | Max Reward: 49.14
Iteration: 1217 | Episodes: 74150 | Median Reward: 39.76 | Max Reward: 49.14
Iteration: 1218 | Episodes: 74200 | Median Reward: 40.37 | Max Reward: 49.14
Iteration: 1219 | Episodes: 74250 | Median Reward: 42.42 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.3         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1220          |
|    time_elapsed         | 22098         |
|    total_timesteps      | 7495680       |
| train/                  |               |
|    approx_kl            | 0.00018202105 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -104          |
|    explained_variance   | 0.385         |
|    learning_rate        | 0.0001        |
|    loss                 | 117           |
|    n_updates            | 12190         |
|    policy_gradient_loss | -0.00032      |
|    std                  | 2.3           |
|    value_loss           | 244           |
-------------------------------------------
Iteration: 1220 | Episodes: 74300 | Median Reward: 40.94 | Max Reward: 49.14
Iteration: 1221 | Episodes: 74350 | Median Reward: 37.23 | Max Reward: 49.14
Iteration: 1221 | Episodes: 74400 | Median Reward: 39.93 | Max Reward: 49.14
Iteration: 1222 | Episodes: 74450 | Median Reward: 40.51 | Max Reward: 49.14
Iteration: 1223 | Episodes: 74500 | Median Reward: 38.38 | Max Reward: 49.14
Iteration: 1224 | Episodes: 74550 | Median Reward: 39.15 | Max Reward: 49.14
Iteration: 1225 | Episodes: 74600 | Median Reward: 41.12 | Max Reward: 49.14
Iteration: 1226 | Episodes: 74650 | Median Reward: 42.36 | Max Reward: 49.14
Iteration: 1226 | Episodes: 74700 | Median Reward: 39.02 | Max Reward: 49.14
Iteration: 1227 | Episodes: 74750 | Median Reward: 34.90 | Max Reward: 49.14
Iteration: 1228 | Episodes: 74800 | Median Reward: 36.51 | Max Reward: 49.14
Iteration: 1229 | Episodes: 74850 | Median Reward: 37.59 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.3         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1230          |
|    time_elapsed         | 22281         |
|    total_timesteps      | 7557120       |
| train/                  |               |
|    approx_kl            | 0.00021812142 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -104          |
|    explained_variance   | 0.397         |
|    learning_rate        | 0.0001        |
|    loss                 | 99            |
|    n_updates            | 12290         |
|    policy_gradient_loss | -8.38e-05     |
|    std                  | 2.33          |
|    value_loss           | 232           |
-------------------------------------------
Iteration: 1230 | Episodes: 74900 | Median Reward: 35.54 | Max Reward: 49.14
Iteration: 1230 | Episodes: 74950 | Median Reward: 42.26 | Max Reward: 49.14
Iteration: 1231 | Episodes: 75000 | Median Reward: 34.60 | Max Reward: 49.14
Iteration: 1232 | Episodes: 75050 | Median Reward: 38.09 | Max Reward: 49.14
Iteration: 1233 | Episodes: 75100 | Median Reward: 40.24 | Max Reward: 49.14
Iteration: 1234 | Episodes: 75150 | Median Reward: 41.07 | Max Reward: 49.14
Iteration: 1235 | Episodes: 75200 | Median Reward: 39.15 | Max Reward: 49.14
Iteration: 1235 | Episodes: 75250 | Median Reward: 42.04 | Max Reward: 49.14
Iteration: 1236 | Episodes: 75300 | Median Reward: 41.72 | Max Reward: 49.14
Iteration: 1237 | Episodes: 75350 | Median Reward: 40.68 | Max Reward: 49.14
Iteration: 1238 | Episodes: 75400 | Median Reward: 39.95 | Max Reward: 49.14
Iteration: 1239 | Episodes: 75450 | Median Reward: 33.64 | Max Reward: 49.14
Iteration: 1239 | Episodes: 75500 | Median Reward: 33.66 | Max Reward: 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.6     |
| time/                   |           |
|    fps                  | 339       |
|    iterations           | 1240      |
|    time_elapsed         | 22471     |
|    total_timesteps      | 7618560   |
| train/                  |           |
|    approx_kl            | 0.5131467 |
|    clip_fraction        | 0.0225    |
|    clip_range           | 0.4       |
|    entropy_loss         | -104      |
|    explained_variance   | 0.412     |
|    learning_rate        | 0.0001    |
|    loss                 | 108       |
|    n_updates            | 12390     |
|    policy_gradient_loss | 0.00546   |
|    std                  | 2.35      |
|    value_loss           | 215       |
---------------------------------------
Iteration: 1240 | Episodes: 75550 | Median Reward: 39.11 | Max Reward: 49.14
Iteration: 1241 | Episodes: 75600 | Median Reward: 38.95 | Max Reward: 49.14
Iteration: 1242 | Episodes: 75650 | Median Reward: 37.88 | Max Reward: 49.14
Iteration: 1243 | Episodes: 75700 | Median Reward: 38.78 | Max Reward: 49.14
Iteration: 1244 | Episodes: 75750 | Median Reward: 40.45 | Max Reward: 49.14
Iteration: 1244 | Episodes: 75800 | Median Reward: 42.55 | Max Reward: 49.14
Iteration: 1245 | Episodes: 75850 | Median Reward: 39.70 | Max Reward: 49.14
Iteration: 1246 | Episodes: 75900 | Median Reward: 40.39 | Max Reward: 49.14
Iteration: 1247 | Episodes: 75950 | Median Reward: 37.98 | Max Reward: 49.14
Iteration: 1248 | Episodes: 76000 | Median Reward: 37.32 | Max Reward: 49.14
Iteration: 1248 | Episodes: 76050 | Median Reward: 39.96 | Max Reward: 49.14
Iteration: 1249 | Episodes: 76100 | Median Reward: 44.10 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.7         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1250          |
|    time_elapsed         | 22651         |
|    total_timesteps      | 7680000       |
| train/                  |               |
|    approx_kl            | 0.00035031774 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -105          |
|    explained_variance   | 0.409         |
|    learning_rate        | 0.0001        |
|    loss                 | 109           |
|    n_updates            | 12490         |
|    policy_gradient_loss | -0.000439     |
|    std                  | 2.37          |
|    value_loss           | 224           |
-------------------------------------------
Iteration: 1250 | Episodes: 76150 | Median Reward: 41.55 | Max Reward: 49.14
Iteration: 1251 | Episodes: 76200 | Median Reward: 42.70 | Max Reward: 49.14
Iteration: 1252 | Episodes: 76250 | Median Reward: 43.16 | Max Reward: 49.14
Iteration: 1253 | Episodes: 76300 | Median Reward: 43.19 | Max Reward: 49.14
Iteration: 1253 | Episodes: 76350 | Median Reward: 42.09 | Max Reward: 49.14
Iteration: 1254 | Episodes: 76400 | Median Reward: 39.45 | Max Reward: 49.14
Iteration: 1255 | Episodes: 76450 | Median Reward: 37.49 | Max Reward: 49.14
Iteration: 1256 | Episodes: 76500 | Median Reward: 37.71 | Max Reward: 49.14
Iteration: 1257 | Episodes: 76550 | Median Reward: 39.08 | Max Reward: 49.14
Iteration: 1258 | Episodes: 76600 | Median Reward: 38.58 | Max Reward: 49.14
Iteration: 1258 | Episodes: 76650 | Median Reward: 41.66 | Max Reward: 49.14
Iteration: 1259 | Episodes: 76700 | Median Reward: 40.03 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.3        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 1260         |
|    time_elapsed         | 22832        |
|    total_timesteps      | 7741440      |
| train/                  |              |
|    approx_kl            | 0.0017995721 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -105         |
|    explained_variance   | 0.413        |
|    learning_rate        | 0.0001       |
|    loss                 | 91.2         |
|    n_updates            | 12590        |
|    policy_gradient_loss | -0.0017      |
|    std                  | 2.4          |
|    value_loss           | 223          |
------------------------------------------
Iteration: 1260 | Episodes: 76750 | Median Reward: 41.98 | Max Reward: 49.14
Iteration: 1261 | Episodes: 76800 | Median Reward: 42.00 | Max Reward: 49.14
Iteration: 1262 | Episodes: 76850 | Median Reward: 42.88 | Max Reward: 49.14
Iteration: 1262 | Episodes: 76900 | Median Reward: 40.94 | Max Reward: 49.14
Iteration: 1263 | Episodes: 76950 | Median Reward: 37.93 | Max Reward: 49.14
Iteration: 1264 | Episodes: 77000 | Median Reward: 41.01 | Max Reward: 49.14
Iteration: 1265 | Episodes: 77050 | Median Reward: 39.57 | Max Reward: 49.14
Iteration: 1266 | Episodes: 77100 | Median Reward: 38.53 | Max Reward: 49.14
Iteration: 1267 | Episodes: 77150 | Median Reward: 40.19 | Max Reward: 49.14
Iteration: 1267 | Episodes: 77200 | Median Reward: 42.89 | Max Reward: 49.14
Iteration: 1268 | Episodes: 77250 | Median Reward: 41.15 | Max Reward: 49.14
Iteration: 1269 | Episodes: 77300 | Median Reward: 39.54 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.4         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 1270          |
|    time_elapsed         | 23018         |
|    total_timesteps      | 7802880       |
| train/                  |               |
|    approx_kl            | 0.00044648678 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -105          |
|    explained_variance   | 0.408         |
|    learning_rate        | 0.0001        |
|    loss                 | 109           |
|    n_updates            | 12690         |
|    policy_gradient_loss | -0.000452     |
|    std                  | 2.42          |
|    value_loss           | 237           |
-------------------------------------------
Iteration: 1270 | Episodes: 77350 | Median Reward: 39.54 | Max Reward: 49.14
Iteration: 1271 | Episodes: 77400 | Median Reward: 44.00 | Max Reward: 49.14
Iteration: 1272 | Episodes: 77450 | Median Reward: 38.27 | Max Reward: 49.14
Iteration: 1272 | Episodes: 77500 | Median Reward: 25.24 | Max Reward: 49.14
Iteration: 1273 | Episodes: 77550 | Median Reward: 37.98 | Max Reward: 49.14
Iteration: 1274 | Episodes: 77600 | Median Reward: 39.23 | Max Reward: 49.14
Iteration: 1275 | Episodes: 77650 | Median Reward: 39.64 | Max Reward: 49.14
Iteration: 1276 | Episodes: 77700 | Median Reward: 38.90 | Max Reward: 49.14
Iteration: 1276 | Episodes: 77750 | Median Reward: 38.00 | Max Reward: 49.14
Iteration: 1277 | Episodes: 77800 | Median Reward: 42.46 | Max Reward: 49.14
Iteration: 1278 | Episodes: 77850 | Median Reward: 38.31 | Max Reward: 49.14
Iteration: 1279 | Episodes: 77900 | Median Reward: 38.59 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.3         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 1280          |
|    time_elapsed         | 23199         |
|    total_timesteps      | 7864320       |
| train/                  |               |
|    approx_kl            | 0.00028343892 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -105          |
|    explained_variance   | 0.428         |
|    learning_rate        | 0.0001        |
|    loss                 | 87.8          |
|    n_updates            | 12790         |
|    policy_gradient_loss | -0.000548     |
|    std                  | 2.45          |
|    value_loss           | 211           |
-------------------------------------------
Iteration: 1280 | Episodes: 77950 | Median Reward: 38.96 | Max Reward: 49.14
Iteration: 1281 | Episodes: 78000 | Median Reward: 36.89 | Max Reward: 49.14
Iteration: 1281 | Episodes: 78050 | Median Reward: 36.73 | Max Reward: 49.14
Iteration: 1282 | Episodes: 78100 | Median Reward: 42.45 | Max Reward: 49.14
Iteration: 1283 | Episodes: 78150 | Median Reward: 41.34 | Max Reward: 49.14
Iteration: 1284 | Episodes: 78200 | Median Reward: 40.08 | Max Reward: 49.14
Iteration: 1285 | Episodes: 78250 | Median Reward: 39.65 | Max Reward: 49.14
Iteration: 1285 | Episodes: 78300 | Median Reward: 40.69 | Max Reward: 49.14
Iteration: 1286 | Episodes: 78350 | Median Reward: 42.50 | Max Reward: 49.14
Iteration: 1287 | Episodes: 78400 | Median Reward: 42.26 | Max Reward: 49.14
Iteration: 1288 | Episodes: 78450 | Median Reward: 39.29 | Max Reward: 49.14
Iteration: 1289 | Episodes: 78500 | Median Reward: 39.50 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.3        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 1290         |
|    time_elapsed         | 23372        |
|    total_timesteps      | 7925760      |
| train/                  |              |
|    approx_kl            | 0.0010947685 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -106         |
|    explained_variance   | 0.434        |
|    learning_rate        | 0.0001       |
|    loss                 | 118          |
|    n_updates            | 12890        |
|    policy_gradient_loss | -0.00111     |
|    std                  | 2.47         |
|    value_loss           | 210          |
------------------------------------------
Iteration: 1290 | Episodes: 78550 | Median Reward: 43.20 | Max Reward: 49.14
Iteration: 1290 | Episodes: 78600 | Median Reward: 39.90 | Max Reward: 49.14
Iteration: 1291 | Episodes: 78650 | Median Reward: 42.03 | Max Reward: 49.14
Iteration: 1292 | Episodes: 78700 | Median Reward: 41.51 | Max Reward: 49.14
Iteration: 1293 | Episodes: 78750 | Median Reward: 41.64 | Max Reward: 49.14
Iteration: 1294 | Episodes: 78800 | Median Reward: 41.62 | Max Reward: 49.14
Iteration: 1295 | Episodes: 78850 | Median Reward: 33.55 | Max Reward: 49.14
Iteration: 1295 | Episodes: 78900 | Median Reward: 41.86 | Max Reward: 49.14
Iteration: 1296 | Episodes: 78950 | Median Reward: 41.86 | Max Reward: 49.14
Iteration: 1297 | Episodes: 79000 | Median Reward: 42.26 | Max Reward: 49.14
Iteration: 1298 | Episodes: 79050 | Median Reward: 41.85 | Max Reward: 49.14
Iteration: 1299 | Episodes: 79100 | Median Reward: 39.17 | Max Reward: 49.14
Iteration: 1299 | Episodes: 79150 | Median Reward: 39.02 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.1         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1300          |
|    time_elapsed         | 23558         |
|    total_timesteps      | 7987200       |
| train/                  |               |
|    approx_kl            | 0.00010902741 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -106          |
|    explained_variance   | 0.424         |
|    learning_rate        | 0.0001        |
|    loss                 | 117           |
|    n_updates            | 12990         |
|    policy_gradient_loss | -0.000193     |
|    std                  | 2.49          |
|    value_loss           | 232           |
-------------------------------------------
Iteration: 1300 | Episodes: 79200 | Median Reward: 36.94 | Max Reward: 49.14
Iteration: 1301 | Episodes: 79250 | Median Reward: 39.28 | Max Reward: 49.14
Iteration: 1302 | Episodes: 79300 | Median Reward: 38.89 | Max Reward: 49.14
Iteration: 1303 | Episodes: 79350 | Median Reward: 39.48 | Max Reward: 49.14
Iteration: 1304 | Episodes: 79400 | Median Reward: 42.44 | Max Reward: 49.14
Iteration: 1304 | Episodes: 79450 | Median Reward: 38.23 | Max Reward: 49.14
Iteration: 1305 | Episodes: 79500 | Median Reward: 38.82 | Max Reward: 49.14
Iteration: 1306 | Episodes: 79550 | Median Reward: 38.95 | Max Reward: 49.14
Iteration: 1307 | Episodes: 79600 | Median Reward: 39.61 | Max Reward: 49.14
Iteration: 1308 | Episodes: 79650 | Median Reward: 39.73 | Max Reward: 49.14
Iteration: 1309 | Episodes: 79700 | Median Reward: 38.64 | Max Reward: 49.14
Iteration: 1309 | Episodes: 79750 | Median Reward: 40.36 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.3        |
| time/                   |              |
|    fps                  | 339          |
|    iterations           | 1310         |
|    time_elapsed         | 23738        |
|    total_timesteps      | 8048640      |
| train/                  |              |
|    approx_kl            | 0.0002541789 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -106         |
|    explained_variance   | 0.439        |
|    learning_rate        | 0.0001       |
|    loss                 | 95.1         |
|    n_updates            | 13090        |
|    policy_gradient_loss | -0.000438    |
|    std                  | 2.52         |
|    value_loss           | 211          |
------------------------------------------
Iteration: 1310 | Episodes: 79800 | Median Reward: 41.64 | Max Reward: 49.14
Iteration: 1311 | Episodes: 79850 | Median Reward: 40.21 | Max Reward: 49.14
Iteration: 1312 | Episodes: 79900 | Median Reward: 39.10 | Max Reward: 49.14
Iteration: 1313 | Episodes: 79950 | Median Reward: 40.08 | Max Reward: 49.14
Iteration: 1313 | Episodes: 80000 | Median Reward: 34.10 | Max Reward: 49.14
Iteration: 1314 | Episodes: 80050 | Median Reward: 39.09 | Max Reward: 49.14
Iteration: 1315 | Episodes: 80100 | Median Reward: 43.08 | Max Reward: 49.14
Iteration: 1316 | Episodes: 80150 | Median Reward: 42.29 | Max Reward: 49.14
Iteration: 1317 | Episodes: 80200 | Median Reward: 39.82 | Max Reward: 49.14
Iteration: 1318 | Episodes: 80250 | Median Reward: 42.06 | Max Reward: 49.14
Iteration: 1318 | Episodes: 80300 | Median Reward: 38.25 | Max Reward: 49.14
Iteration: 1319 | Episodes: 80350 | Median Reward: 39.09 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.2         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1320          |
|    time_elapsed         | 23912         |
|    total_timesteps      | 8110080       |
| train/                  |               |
|    approx_kl            | 0.00011152851 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -106          |
|    explained_variance   | 0.441         |
|    learning_rate        | 0.0001        |
|    loss                 | 101           |
|    n_updates            | 13190         |
|    policy_gradient_loss | -0.000189     |
|    std                  | 2.55          |
|    value_loss           | 212           |
-------------------------------------------
Iteration: 1320 | Episodes: 80400 | Median Reward: 42.16 | Max Reward: 49.14
Iteration: 1321 | Episodes: 80450 | Median Reward: 42.16 | Max Reward: 49.14
Iteration: 1322 | Episodes: 80500 | Median Reward: 42.60 | Max Reward: 49.14
Iteration: 1322 | Episodes: 80550 | Median Reward: 40.50 | Max Reward: 49.14
Iteration: 1323 | Episodes: 80600 | Median Reward: 41.72 | Max Reward: 49.14
Iteration: 1324 | Episodes: 80650 | Median Reward: 34.82 | Max Reward: 49.14
Iteration: 1325 | Episodes: 80700 | Median Reward: 39.13 | Max Reward: 49.14
Iteration: 1326 | Episodes: 80750 | Median Reward: 39.76 | Max Reward: 49.14
Iteration: 1327 | Episodes: 80800 | Median Reward: 39.97 | Max Reward: 49.14
Iteration: 1327 | Episodes: 80850 | Median Reward: 37.34 | Max Reward: 49.14
Iteration: 1328 | Episodes: 80900 | Median Reward: 40.81 | Max Reward: 49.14
Iteration: 1329 | Episodes: 80950 | Median Reward: 41.83 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.6         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1330          |
|    time_elapsed         | 24103         |
|    total_timesteps      | 8171520       |
| train/                  |               |
|    approx_kl            | 0.00023721922 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -106          |
|    explained_variance   | 0.436         |
|    learning_rate        | 0.0001        |
|    loss                 | 114           |
|    n_updates            | 13290         |
|    policy_gradient_loss | -0.000411     |
|    std                  | 2.57          |
|    value_loss           | 227           |
-------------------------------------------
Iteration: 1330 | Episodes: 81000 | Median Reward: 40.20 | Max Reward: 49.14
Iteration: 1331 | Episodes: 81050 | Median Reward: 36.43 | Max Reward: 49.14
Iteration: 1332 | Episodes: 81100 | Median Reward: 39.61 | Max Reward: 49.14
Iteration: 1332 | Episodes: 81150 | Median Reward: 42.60 | Max Reward: 49.14
Iteration: 1333 | Episodes: 81200 | Median Reward: 43.17 | Max Reward: 49.14
Iteration: 1334 | Episodes: 81250 | Median Reward: 37.74 | Max Reward: 49.14
Iteration: 1335 | Episodes: 81300 | Median Reward: 36.77 | Max Reward: 49.14
Iteration: 1336 | Episodes: 81350 | Median Reward: 42.28 | Max Reward: 49.14
Iteration: 1336 | Episodes: 81400 | Median Reward: 42.88 | Max Reward: 49.14
Iteration: 1337 | Episodes: 81450 | Median Reward: 38.71 | Max Reward: 49.14
Iteration: 1338 | Episodes: 81500 | Median Reward: 38.56 | Max Reward: 49.14
Iteration: 1339 | Episodes: 81550 | Median Reward: 38.85 | Max Reward: 49.14
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -61.1          |
| time/                   |                |
|    fps                  | 339            |
|    iterations           | 1340           |
|    time_elapsed         | 24282          |
|    total_timesteps      | 8232960        |
| train/                  |                |
|    approx_kl            | 0.000108493834 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -107           |
|    explained_variance   | 0.453          |
|    learning_rate        | 0.0001         |
|    loss                 | 93.1           |
|    n_updates            | 13390          |
|    policy_gradient_loss | -0.000171      |
|    std                  | 2.6            |
|    value_loss           | 206            |
--------------------------------------------
Iteration: 1340 | Episodes: 81600 | Median Reward: 42.19 | Max Reward: 49.14
Iteration: 1341 | Episodes: 81650 | Median Reward: 42.10 | Max Reward: 49.14
Iteration: 1341 | Episodes: 81700 | Median Reward: 38.15 | Max Reward: 49.14
Iteration: 1342 | Episodes: 81750 | Median Reward: 38.15 | Max Reward: 49.14
Iteration: 1343 | Episodes: 81800 | Median Reward: 41.54 | Max Reward: 49.14
Iteration: 1344 | Episodes: 81850 | Median Reward: 41.93 | Max Reward: 49.14
Iteration: 1345 | Episodes: 81900 | Median Reward: 39.80 | Max Reward: 49.14
Iteration: 1346 | Episodes: 81950 | Median Reward: 39.02 | Max Reward: 49.14
Iteration: 1346 | Episodes: 82000 | Median Reward: 41.54 | Max Reward: 49.14
Iteration: 1347 | Episodes: 82050 | Median Reward: 42.29 | Max Reward: 49.14
Iteration: 1348 | Episodes: 82100 | Median Reward: 41.09 | Max Reward: 49.14
Iteration: 1349 | Episodes: 82150 | Median Reward: 39.21 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.4         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1350          |
|    time_elapsed         | 24466         |
|    total_timesteps      | 8294400       |
| train/                  |               |
|    approx_kl            | 0.00025436707 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -107          |
|    explained_variance   | 0.446         |
|    learning_rate        | 0.0001        |
|    loss                 | 98.5          |
|    n_updates            | 13490         |
|    policy_gradient_loss | -0.000666     |
|    std                  | 2.62          |
|    value_loss           | 213           |
-------------------------------------------
Iteration: 1350 | Episodes: 82200 | Median Reward: 39.22 | Max Reward: 49.14
Iteration: 1350 | Episodes: 82250 | Median Reward: 35.82 | Max Reward: 49.14
Iteration: 1351 | Episodes: 82300 | Median Reward: 42.58 | Max Reward: 49.14
Iteration: 1352 | Episodes: 82350 | Median Reward: 39.93 | Max Reward: 49.14
Iteration: 1353 | Episodes: 82400 | Median Reward: 39.93 | Max Reward: 49.14
Iteration: 1354 | Episodes: 82450 | Median Reward: 41.12 | Max Reward: 49.14
Iteration: 1355 | Episodes: 82500 | Median Reward: 39.84 | Max Reward: 49.14
Iteration: 1355 | Episodes: 82550 | Median Reward: 39.09 | Max Reward: 49.14
Iteration: 1356 | Episodes: 82600 | Median Reward: 43.00 | Max Reward: 49.14
Iteration: 1357 | Episodes: 82650 | Median Reward: 41.62 | Max Reward: 49.14
Iteration: 1358 | Episodes: 82700 | Median Reward: 38.96 | Max Reward: 49.14
Iteration: 1359 | Episodes: 82750 | Median Reward: 38.96 | Max Reward: 49.14
Iteration: 1359 | Episodes: 82800 | Median Reward: 37.75 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.4         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 1360          |
|    time_elapsed         | 24647         |
|    total_timesteps      | 8355840       |
| train/                  |               |
|    approx_kl            | 0.00031807314 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -107          |
|    explained_variance   | 0.455         |
|    learning_rate        | 0.0001        |
|    loss                 | 101           |
|    n_updates            | 13590         |
|    policy_gradient_loss | -0.000451     |
|    std                  | 2.65          |
|    value_loss           | 209           |
-------------------------------------------
Iteration: 1360 | Episodes: 82850 | Median Reward: 36.80 | Max Reward: 49.14
Iteration: 1361 | Episodes: 82900 | Median Reward: 39.09 | Max Reward: 49.14
Iteration: 1362 | Episodes: 82950 | Median Reward: 41.09 | Max Reward: 49.14
Iteration: 1363 | Episodes: 83000 | Median Reward: 38.95 | Max Reward: 49.14
Iteration: 1364 | Episodes: 83050 | Median Reward: 37.59 | Max Reward: 49.14
Iteration: 1364 | Episodes: 83100 | Median Reward: 38.49 | Max Reward: 49.14
Iteration: 1365 | Episodes: 83150 | Median Reward: 38.48 | Max Reward: 49.14
Iteration: 1366 | Episodes: 83200 | Median Reward: 39.23 | Max Reward: 49.14
Iteration: 1367 | Episodes: 83250 | Median Reward: 39.56 | Max Reward: 49.14
Iteration: 1368 | Episodes: 83300 | Median Reward: 38.93 | Max Reward: 49.14
Iteration: 1369 | Episodes: 83350 | Median Reward: 38.90 | Max Reward: 49.14
Iteration: 1369 | Episodes: 83400 | Median Reward: 42.16 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.4         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 1370          |
|    time_elapsed         | 24830         |
|    total_timesteps      | 8417280       |
| train/                  |               |
|    approx_kl            | 0.00033077516 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -107          |
|    explained_variance   | 0.452         |
|    learning_rate        | 0.0001        |
|    loss                 | 127           |
|    n_updates            | 13690         |
|    policy_gradient_loss | -0.000311     |
|    std                  | 2.67          |
|    value_loss           | 224           |
-------------------------------------------
Iteration: 1370 | Episodes: 83450 | Median Reward: 34.41 | Max Reward: 49.14
Iteration: 1371 | Episodes: 83500 | Median Reward: 40.01 | Max Reward: 49.14
Iteration: 1372 | Episodes: 83550 | Median Reward: 40.74 | Max Reward: 49.14
Iteration: 1373 | Episodes: 83600 | Median Reward: 41.92 | Max Reward: 49.14
Iteration: 1373 | Episodes: 83650 | Median Reward: 41.66 | Max Reward: 49.14
Iteration: 1374 | Episodes: 83700 | Median Reward: 39.30 | Max Reward: 49.14
Iteration: 1375 | Episodes: 83750 | Median Reward: 35.22 | Max Reward: 49.14
Iteration: 1376 | Episodes: 83800 | Median Reward: 39.27 | Max Reward: 49.14
Iteration: 1377 | Episodes: 83850 | Median Reward: 39.52 | Max Reward: 49.14
Iteration: 1378 | Episodes: 83900 | Median Reward: 39.30 | Max Reward: 49.14
Iteration: 1378 | Episodes: 83950 | Median Reward: 37.52 | Max Reward: 49.14
Iteration: 1379 | Episodes: 84000 | Median Reward: 36.25 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.8        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 1380         |
|    time_elapsed         | 25023        |
|    total_timesteps      | 8478720      |
| train/                  |              |
|    approx_kl            | 0.0001686597 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -108         |
|    explained_variance   | 0.462        |
|    learning_rate        | 0.0001       |
|    loss                 | 93.4         |
|    n_updates            | 13790        |
|    policy_gradient_loss | -9.13e-05    |
|    std                  | 2.7          |
|    value_loss           | 212          |
------------------------------------------
Iteration: 1380 | Episodes: 84050 | Median Reward: 37.96 | Max Reward: 49.14
Iteration: 1381 | Episodes: 84100 | Median Reward: 39.52 | Max Reward: 49.14
Iteration: 1382 | Episodes: 84150 | Median Reward: 40.00 | Max Reward: 49.14
Iteration: 1383 | Episodes: 84200 | Median Reward: 39.68 | Max Reward: 49.14
Iteration: 1383 | Episodes: 84250 | Median Reward: 43.30 | Max Reward: 49.14
Iteration: 1384 | Episodes: 84300 | Median Reward: 38.59 | Max Reward: 49.14
Iteration: 1385 | Episodes: 84350 | Median Reward: 43.33 | Max Reward: 49.14
Iteration: 1386 | Episodes: 84400 | Median Reward: 44.02 | Max Reward: 49.14
Iteration: 1387 | Episodes: 84450 | Median Reward: 37.30 | Max Reward: 49.14
Iteration: 1387 | Episodes: 84500 | Median Reward: 37.39 | Max Reward: 49.14
Iteration: 1388 | Episodes: 84550 | Median Reward: 41.22 | Max Reward: 49.14
Iteration: 1389 | Episodes: 84600 | Median Reward: 39.62 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.5        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 1390         |
|    time_elapsed         | 25201        |
|    total_timesteps      | 8540160      |
| train/                  |              |
|    approx_kl            | 0.0004987066 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -108         |
|    explained_variance   | 0.456        |
|    learning_rate        | 0.0001       |
|    loss                 | 102          |
|    n_updates            | 13890        |
|    policy_gradient_loss | -0.00112     |
|    std                  | 2.73         |
|    value_loss           | 223          |
------------------------------------------
Iteration: 1390 | Episodes: 84650 | Median Reward: 39.72 | Max Reward: 49.14
Iteration: 1391 | Episodes: 84700 | Median Reward: 38.96 | Max Reward: 49.14
Iteration: 1392 | Episodes: 84750 | Median Reward: 40.97 | Max Reward: 49.14
Iteration: 1392 | Episodes: 84800 | Median Reward: 37.99 | Max Reward: 49.14
Iteration: 1393 | Episodes: 84850 | Median Reward: 38.96 | Max Reward: 49.14
Iteration: 1394 | Episodes: 84900 | Median Reward: 40.22 | Max Reward: 49.14
Iteration: 1395 | Episodes: 84950 | Median Reward: 41.64 | Max Reward: 49.14
Iteration: 1396 | Episodes: 85000 | Median Reward: 39.75 | Max Reward: 49.14
Iteration: 1396 | Episodes: 85050 | Median Reward: 32.95 | Max Reward: 49.14
Iteration: 1397 | Episodes: 85100 | Median Reward: 39.18 | Max Reward: 49.14
Iteration: 1398 | Episodes: 85150 | Median Reward: 37.08 | Max Reward: 49.14
Iteration: 1399 | Episodes: 85200 | Median Reward: 38.08 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.7         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 1400          |
|    time_elapsed         | 25382         |
|    total_timesteps      | 8601600       |
| train/                  |               |
|    approx_kl            | 0.00027635825 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -108          |
|    explained_variance   | 0.48          |
|    learning_rate        | 0.0001        |
|    loss                 | 89.3          |
|    n_updates            | 13990         |
|    policy_gradient_loss | -0.000546     |
|    std                  | 2.75          |
|    value_loss           | 193           |
-------------------------------------------
Iteration: 1400 | Episodes: 85250 | Median Reward: 41.20 | Max Reward: 49.14
Iteration: 1401 | Episodes: 85300 | Median Reward: 42.51 | Max Reward: 49.14
Iteration: 1401 | Episodes: 85350 | Median Reward: 36.29 | Max Reward: 49.14
Iteration: 1402 | Episodes: 85400 | Median Reward: 45.61 | Max Reward: 49.14
Iteration: 1403 | Episodes: 85450 | Median Reward: 43.24 | Max Reward: 49.14
Iteration: 1404 | Episodes: 85500 | Median Reward: 38.19 | Max Reward: 49.14
Iteration: 1405 | Episodes: 85550 | Median Reward: 37.70 | Max Reward: 49.14
Iteration: 1406 | Episodes: 85600 | Median Reward: 37.91 | Max Reward: 49.14
Iteration: 1406 | Episodes: 85650 | Median Reward: 38.04 | Max Reward: 49.14
Iteration: 1407 | Episodes: 85700 | Median Reward: 38.37 | Max Reward: 49.14
Iteration: 1408 | Episodes: 85750 | Median Reward: 42.55 | Max Reward: 49.14
Iteration: 1409 | Episodes: 85800 | Median Reward: 43.40 | Max Reward: 49.14
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -60.2      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1410       |
|    time_elapsed         | 25577      |
|    total_timesteps      | 8663040    |
| train/                  |            |
|    approx_kl            | 0.00074029 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.459      |
|    learning_rate        | 0.0001     |
|    loss                 | 93.3       |
|    n_updates            | 14090      |
|    policy_gradient_loss | -0.00068   |
|    std                  | 2.78       |
|    value_loss           | 228        |
----------------------------------------
Iteration: 1410 | Episodes: 85850 | Median Reward: 38.79 | Max Reward: 49.14
Iteration: 1410 | Episodes: 85900 | Median Reward: 41.11 | Max Reward: 49.14
Iteration: 1411 | Episodes: 85950 | Median Reward: 39.81 | Max Reward: 49.14
Iteration: 1412 | Episodes: 86000 | Median Reward: 39.68 | Max Reward: 49.14
Iteration: 1413 | Episodes: 86050 | Median Reward: 38.74 | Max Reward: 49.14
Iteration: 1414 | Episodes: 86100 | Median Reward: 41.18 | Max Reward: 49.14
Iteration: 1415 | Episodes: 86150 | Median Reward: 41.71 | Max Reward: 49.14
Iteration: 1415 | Episodes: 86200 | Median Reward: 39.61 | Max Reward: 49.14
Iteration: 1416 | Episodes: 86250 | Median Reward: 40.71 | Max Reward: 49.14
Iteration: 1417 | Episodes: 86300 | Median Reward: 42.01 | Max Reward: 49.14
Iteration: 1418 | Episodes: 86350 | Median Reward: 42.01 | Max Reward: 49.14
Iteration: 1419 | Episodes: 86400 | Median Reward: 40.21 | Max Reward: 49.14
Iteration: 1419 | Episodes: 86450 | Median Reward: 40.67 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.8         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 1420          |
|    time_elapsed         | 25759         |
|    total_timesteps      | 8724480       |
| train/                  |               |
|    approx_kl            | 0.00095088256 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -108          |
|    explained_variance   | 0.477         |
|    learning_rate        | 0.0001        |
|    loss                 | 92.8          |
|    n_updates            | 14190         |
|    policy_gradient_loss | -0.000856     |
|    std                  | 2.81          |
|    value_loss           | 199           |
-------------------------------------------
Iteration: 1420 | Episodes: 86500 | Median Reward: 38.06 | Max Reward: 49.14
Iteration: 1421 | Episodes: 86550 | Median Reward: 38.71 | Max Reward: 49.14
Iteration: 1422 | Episodes: 86600 | Median Reward: 37.82 | Max Reward: 49.14
Iteration: 1423 | Episodes: 86650 | Median Reward: 40.72 | Max Reward: 49.14
Iteration: 1424 | Episodes: 86700 | Median Reward: 42.55 | Max Reward: 49.14
Iteration: 1424 | Episodes: 86750 | Median Reward: 42.69 | Max Reward: 49.14
Iteration: 1425 | Episodes: 86800 | Median Reward: 40.30 | Max Reward: 49.14
Iteration: 1426 | Episodes: 86850 | Median Reward: 38.96 | Max Reward: 49.14
Iteration: 1427 | Episodes: 86900 | Median Reward: 38.96 | Max Reward: 49.14
Iteration: 1428 | Episodes: 86950 | Median Reward: 39.66 | Max Reward: 49.14
Iteration: 1428 | Episodes: 87000 | Median Reward: 39.35 | Max Reward: 49.14
Iteration: 1429 | Episodes: 87050 | Median Reward: 42.14 | Max Reward: 49.14
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.1        |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 1430         |
|    time_elapsed         | 25946        |
|    total_timesteps      | 8785920      |
| train/                  |              |
|    approx_kl            | 0.0019205594 |
|    clip_fraction        | 0.00168      |
|    clip_range           | 0.4          |
|    entropy_loss         | -109         |
|    explained_variance   | 0.474        |
|    learning_rate        | 0.0001       |
|    loss                 | 87.2         |
|    n_updates            | 14290        |
|    policy_gradient_loss | 0.000764     |
|    std                  | 2.83         |
|    value_loss           | 217          |
------------------------------------------
Iteration: 1430 | Episodes: 87100 | Median Reward: 39.13 | Max Reward: 49.14
Iteration: 1431 | Episodes: 87150 | Median Reward: 37.76 | Max Reward: 49.14
Iteration: 1432 | Episodes: 87200 | Median Reward: 37.46 | Max Reward: 49.14
Iteration: 1433 | Episodes: 87250 | Median Reward: 38.71 | Max Reward: 49.14
Iteration: 1433 | Episodes: 87300 | Median Reward: 43.10 | Max Reward: 49.14
Iteration: 1434 | Episodes: 87350 | Median Reward: 39.31 | Max Reward: 49.14
Iteration: 1435 | Episodes: 87400 | Median Reward: 40.05 | Max Reward: 49.14
Iteration: 1436 | Episodes: 87450 | Median Reward: 39.88 | Max Reward: 49.14
Iteration: 1437 | Episodes: 87500 | Median Reward: 39.72 | Max Reward: 49.14
Iteration: 1438 | Episodes: 87550 | Median Reward: 40.04 | Max Reward: 49.14
Iteration: 1438 | Episodes: 87600 | Median Reward: 39.74 | Max Reward: 49.14
Iteration: 1439 | Episodes: 87650 | Median Reward: 42.01 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.3         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 1440          |
|    time_elapsed         | 26116         |
|    total_timesteps      | 8847360       |
| train/                  |               |
|    approx_kl            | 4.4089014e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -109          |
|    explained_variance   | 0.486         |
|    learning_rate        | 0.0001        |
|    loss                 | 94.5          |
|    n_updates            | 14390         |
|    policy_gradient_loss | -2.64e-05     |
|    std                  | 2.86          |
|    value_loss           | 203           |
-------------------------------------------
Iteration: 1440 | Episodes: 87700 | Median Reward: 40.42 | Max Reward: 49.14
Iteration: 1441 | Episodes: 87750 | Median Reward: 39.49 | Max Reward: 49.14
aaIteration: 1442 | Episodes: 87800 | Median Reward: 37.68 | Max Reward: 49.14
aIteration: 1442 | Episodes: 87850 | Median Reward: 38.36 | Max Reward: 49.14
Iteration: 1443 | Episodes: 87900 | Median Reward: 40.65 | Max Reward: 49.14
Iteration: 1444 | Episodes: 87950 | Median Reward: 42.04 | Max Reward: 49.14
Iteration: 1445 | Episodes: 88000 | Median Reward: 42.04 | Max Reward: 49.14
Iteration: 1446 | Episodes: 88050 | Median Reward: 42.65 | Max Reward: 49.14
Iteration: 1447 | Episodes: 88100 | Median Reward: 42.14 | Max Reward: 49.14
Iteration: 1447 | Episodes: 88150 | Median Reward: 42.58 | Max Reward: 49.14
Iteration: 1448 | Episodes: 88200 | Median Reward: 39.26 | Max Reward: 49.14
Iteration: 1449 | Episodes: 88250 | Median Reward: 41.43 | Max Reward: 49.14
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.1         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 1450          |
|    time_elapsed         | 26298         |
|    total_timesteps      | 8908800       |
| train/                  |               |
|    approx_kl            | 0.00026372386 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -109          |
|    explained_variance   | 0.485         |
|    learning_rate        | 0.0001        |
|    loss                 | 114           |
|    n_updates            | 14490         |
|    policy_gradient_loss | -0.000637     |
|    std                  | 2.89          |
|    value_loss           | 208           |
-------------------------------------------
Iteration: 1450 | Episodes: 88300 | Median Reward: 37.69 | Max Reward: 49.14
Iteration: 1451 | Episodes: 88350 | Median Reward: 34.86 | Max Reward: 49.14
Iteration: 1452 | Episodes: 88400 | Median Reward: 38.96 | Max Reward: 49.14
aaIteration: 1452 | Episodes: 88450 | Median Reward: 44.11 | Max Reward: 49.14
Iteration: 1453 | Episodes: 88500 | Median Reward: 40.15 | Max Reward: 49.14
Iteration: 1454 | Episodes: 88550 | Median Reward: 41.80 | Max Reward: 49.14
Iteration: 1455 | Episodes: 88600 | Median Reward: 40.85 | Max Reward: 49.14
Iteration: 1456 | Episodes: 88650 | Median Reward: 37.81 | Max Reward: 49.14
Iteration: 1456 | Episodes: 88700 | Median Reward: 14.65 | Max Reward: 49.14
Iteration: 1457 | Episodes: 88750 | Median Reward: 2.02 | Max Reward: 49.14
Iteration: 1458 | Episodes: 88800 | Median Reward: 2.63 | Max Reward: 49.14
Iteration: 1459 | Episodes: 88850 | Median Reward: 11.08 | Max Reward: 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -89.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1460      |
|    time_elapsed         | 26486     |
|    total_timesteps      | 8970240   |
| train/                  |           |
|    approx_kl            | 4.0452557 |
|    clip_fraction        | 0.0919    |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.438     |
|    learning_rate        | 0.0001    |
|    loss                 | 49        |
|    n_updates            | 14590     |
|    policy_gradient_loss | 0.0203    |
|    std                  | 2.91      |
|    value_loss           | 148       |
---------------------------------------
Iteration: 1460 | Episodes: 88900 | Median Reward: 22.68 | Max Reward: 49.14
Iteration: 1461 | Episodes: 88950 | Median Reward: 41.70 | Max Reward: 49.14
Iteration: 1461 | Episodes: 89000 | Median Reward: 5.68 | Max Reward: 49.14
Iteration: 1462 | Episodes: 89050 | Median Reward: 37.41 | Max Reward: 49.14
Iteration: 1463 | Episodes: 89100 | Median Reward: 23.48 | Max Reward: 49.14
Iteration: 1464 | Episodes: 89150 | Median Reward: 22.35 | Max Reward: 49.14
Iteration: 1465 | Episodes: 89200 | Median Reward: 42.27 | Max Reward: 49.14
Iteration: 1465 | Episodes: 89250 | Median Reward: 3.13 | Max Reward: 49.14
Iteration: 1466 | Episodes: 89300 | Median Reward: 29.56 | Max Reward: 49.14
Iteration: 1467 | Episodes: 89350 | Median Reward: -8.56 | Max Reward: 49.14
Iteration: 1468 | Episodes: 89400 | Median Reward: -9.53 | Max Reward: 49.14
Iteration: 1469 | Episodes: 89450 | Median Reward: 0.05 | Max Reward: 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -86.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1470      |
|    time_elapsed         | 26664     |
|    total_timesteps      | 9031680   |
| train/                  |           |
|    approx_kl            | 1.7909224 |
|    clip_fraction        | 0.0796    |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.444     |
|    learning_rate        | 0.0001    |
|    loss                 | 34.8      |
|    n_updates            | 14690     |
|    policy_gradient_loss | 0.00704   |
|    std                  | 2.92      |
|    value_loss           | 139       |
---------------------------------------
Iteration: 1470 | Episodes: 89500 | Median Reward: 17.23 | Max Reward: 49.14
Iteration: 1470 | Episodes: 89550 | Median Reward: 5.43 | Max Reward: 49.14
Iteration: 1471 | Episodes: 89600 | Median Reward: 14.22 | Max Reward: 49.14
Iteration: 1472 | Episodes: 89650 | Median Reward: 4.35 | Max Reward: 49.14
Iteration: 1473 | Episodes: 89700 | Median Reward: 4.35 | Max Reward: 49.14
Iteration: 1474 | Episodes: 89750 | Median Reward: 32.56 | Max Reward: 49.14
Iteration: 1475 | Episodes: 89800 | Median Reward: 24.44 | Max Reward: 49.14
Iteration: 1475 | Episodes: 89850 | Median Reward: 17.00 | Max Reward: 49.14
Iteration: 1476 | Episodes: 89900 | Median Reward: 5.23 | Max Reward: 49.14
Iteration: 1477 | Episodes: 89950 | Median Reward: 18.20 | Max Reward: 49.14
Iteration: 1478 | Episodes: 90000 | Median Reward: 31.81 | Max Reward: 49.14
Iteration: 1479 | Episodes: 90050 | Median Reward: 38.27 | Max Reward: 49.14
Iteration: 1479 | Episodes: 90100 | Median Reward: 24.89 | Max Reward: 49.14
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -75.4    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1480     |
|    time_elapsed         | 26840    |
|    total_timesteps      | 9093120  |
| train/                  |          |
|    approx_kl            | 2.00391  |
|    clip_fraction        | 0.0819   |
|    clip_range           | 0.4      |
|    entropy_loss         | -106     |
|    explained_variance   | 0.476    |
|    learning_rate        | 0.0001   |
|    loss                 | 91       |
|    n_updates            | 14790    |
|    policy_gradient_loss | 0.0213   |
|    std                  | 2.92     |
|    value_loss           | 202      |
--------------------------------------
Iteration: 1480 | Episodes: 90150 | Median Reward: 39.44 | Max Reward: 49.14
Iteration: 1481 | Episodes: 90200 | Median Reward: 36.39 | Max Reward: 49.14
Iteration: 1482 | Episodes: 90250 | Median Reward: 38.65 | Max Reward: 49.14
Iteration: 1483 | Episodes: 90300 | Median Reward: 41.59 | Max Reward: 49.14
Iteration: 1484 | Episodes: 90350 | Median Reward: 37.51 | Max Reward: 49.14
Iteration: 1484 | Episodes: 90400 | Median Reward: 17.26 | Max Reward: 49.14
Iteration: 1485 | Episodes: 90450 | Median Reward: 32.69 | Max Reward: 49.14
Iteration: 1486 | Episodes: 90500 | Median Reward: 39.28 | Max Reward: 49.14
Iteration: 1487 | Episodes: 90550 | Median Reward: 41.96 | Max Reward: 49.14
Iteration: 1488 | Episodes: 90600 | Median Reward: 40.58 | Max Reward: 49.14
Iteration: 1489 | Episodes: 90650 | Median Reward: -7.26 | Max Reward: 49.14
Iteration: 1489 | Episodes: 90700 | Median Reward: -7.36 | Max Reward: 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -95.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1490      |
|    time_elapsed         | 27030     |
|    total_timesteps      | 9154560   |
| train/                  |           |
|    approx_kl            | 3.0332484 |
|    clip_fraction        | 0.128     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.497     |
|    learning_rate        | 0.0001    |
|    loss                 | 71.5      |
|    n_updates            | 14890     |
|    policy_gradient_loss | 0.0376    |
|    std                  | 2.93      |
|    value_loss           | 136       |
---------------------------------------
Iteration: 1490 | Episodes: 90750 | Median Reward: -0.27 | Max Reward: 49.14
Iteration: 1491 | Episodes: 90800 | Median Reward: -2.69 | Max Reward: 49.14
Iteration: 1492 | Episodes: 90850 | Median Reward: -2.19 | Max Reward: 49.14
Iteration: 1493 | Episodes: 90900 | Median Reward: 13.81 | Max Reward: 49.14
Iteration: 1493 | Episodes: 90950 | Median Reward: 27.29 | Max Reward: 49.14
Iteration: 1494 | Episodes: 91000 | Median Reward: 8.59 | Max Reward: 49.14
Iteration: 1495 | Episodes: 91050 | Median Reward: 3.82 | Max Reward: 49.14
Iteration: 1496 | Episodes: 91100 | Median Reward: 7.22 | Max Reward: 49.14
Iteration: 1497 | Episodes: 91150 | Median Reward: -0.94 | Max Reward: 49.14
Iteration: 1498 | Episodes: 91200 | Median Reward: -2.56 | Max Reward: 49.14
Iteration: 1498 | Episodes: 91250 | Median Reward: 4.70 | Max Reward: 49.14
Iteration: 1499 | Episodes: 91300 | Median Reward: -0.97 | Max Reward: 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -96.4     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1500      |
|    time_elapsed         | 27206     |
|    total_timesteps      | 9216000   |
| train/                  |           |
|    approx_kl            | 7.9281416 |
|    clip_fraction        | 0.235     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.522     |
|    learning_rate        | 0.0001    |
|    loss                 | 33.3      |
|    n_updates            | 14990     |
|    policy_gradient_loss | 0.0244    |
|    std                  | 2.94      |
|    value_loss           | 131       |
---------------------------------------
Iteration: 1500 | Episodes: 91350 | Median Reward: -12.61 | Max Reward: 49.14
Iteration: 1501 | Episodes: 91400 | Median Reward: -11.31 | Max Reward: 49.14
Iteration: 1502 | Episodes: 91450 | Median Reward: -7.25 | Max Reward: 49.14
Iteration: 1502 | Episodes: 91500 | Median Reward: -27.69 | Max Reward: 49.14
Iteration: 1503 | Episodes: 91550 | Median Reward: -6.43 | Max Reward: 49.14
Iteration: 1504 | Episodes: 91600 | Median Reward: -3.62 | Max Reward: 49.14
Iteration: 1505 | Episodes: 91650 | Median Reward: -0.66 | Max Reward: 49.14
Iteration: 1506 | Episodes: 91700 | Median Reward: -0.66 | Max Reward: 49.14
Iteration: 1507 | Episodes: 91750 | Median Reward: -8.68 | Max Reward: 49.14
Iteration: 1507 | Episodes: 91800 | Median Reward: -14.27 | Max Reward: 49.14
Iteration: 1508 | Episodes: 91850 | Median Reward: -2.52 | Max Reward: 49.14
Iteration: 1509 | Episodes: 91900 | Median Reward: -15.47 | Max Reward: 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -96.4     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1510      |
|    time_elapsed         | 27386     |
|    total_timesteps      | 9277440   |
| train/                  |           |
|    approx_kl            | 11.059305 |
|    clip_fraction        | 0.195     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.423     |
|    learning_rate        | 0.0001    |
|    loss                 | 115       |
|    n_updates            | 15090     |
|    policy_gradient_loss | 0.0309    |
|    std                  | 2.95      |
|    value_loss           | 146       |
---------------------------------------
Iteration: 1510 | Episodes: 91950 | Median Reward: -17.10 | Max Reward: 49.14
Iteration: 1511 | Episodes: 92000 | Median Reward: -14.43 | Max Reward: 49.14
Iteration: 1512 | Episodes: 92050 | Median Reward: -19.50 | Max Reward: 49.14
Iteration: 1512 | Episodes: 92100 | Median Reward: 36.09 | Max Reward: 49.14
Iteration: 1513 | Episodes: 92150 | Median Reward: -20.52 | Max Reward: 49.14
Iteration: 1514 | Episodes: 92200 | Median Reward: -20.52 | Max Reward: 49.14
Iteration: 1515 | Episodes: 92250 | Median Reward: -15.47 | Max Reward: 49.14
Iteration: 1516 | Episodes: 92300 | Median Reward: -8.61 | Max Reward: 49.14
Iteration: 1516 | Episodes: 92350 | Median Reward: -10.84 | Max Reward: 49.14
Iteration: 1517 | Episodes: 92400 | Median Reward: -6.16 | Max Reward: 49.14
Iteration: 1518 | Episodes: 92450 | Median Reward: -9.36 | Max Reward: 49.14
Iteration: 1519 | Episodes: 92500 | Median Reward: -12.18 | Max Reward: 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -107      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1520      |
|    time_elapsed         | 27572     |
|    total_timesteps      | 9338880   |
| train/                  |           |
|    approx_kl            | 5.3707843 |
|    clip_fraction        | 0.0497    |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.451     |
|    learning_rate        | 0.0001    |
|    loss                 | 94        |
|    n_updates            | 15190     |
|    policy_gradient_loss | 0.0258    |
|    std                  | 2.96      |
|    value_loss           | 124       |
---------------------------------------
Iteration: 1520 | Episodes: 92550 | Median Reward: -12.33 | Max Reward: 49.14
Iteration: 1521 | Episodes: 92600 | Median Reward: -17.79 | Max Reward: 49.14
Iteration: 1521 | Episodes: 92650 | Median Reward: -4.53 | Max Reward: 49.14
Iteration: 1522 | Episodes: 92700 | Median Reward: -23.00 | Max Reward: 49.14
Iteration: 1523 | Episodes: 92750 | Median Reward: -24.44 | Max Reward: 49.14
Iteration: 1524 | Episodes: 92800 | Median Reward: -24.22 | Max Reward: 49.14
Iteration: 1525 | Episodes: 92850 | Median Reward: -23.11 | Max Reward: 49.14
Iteration: 1526 | Episodes: 92900 | Median Reward: -16.87 | Max Reward: 49.14
Iteration: 1526 | Episodes: 92950 | Median Reward: -26.16 | Max Reward: 49.14
Iteration: 1527 | Episodes: 93000 | Median Reward: -18.57 | Max Reward: 49.14
Iteration: 1528 | Episodes: 93050 | Median Reward: -16.94 | Max Reward: 49.14
Iteration: 1529 | Episodes: 93100 | Median Reward: -16.94 | Max Reward: 49.14
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -120     |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1530     |
|    time_elapsed         | 27754    |
|    total_timesteps      | 9400320  |
| train/                  |          |
|    approx_kl            | 0.73167  |
|    clip_fraction        | 0.0287   |
|    clip_range           | 0.4      |
|    entropy_loss         | -107     |
|    explained_variance   | 0.629    |
|    learning_rate        | 0.0001   |
|    loss                 | 31.4     |
|    n_updates            | 15290    |
|    policy_gradient_loss | -0.0087  |
|    std                  | 2.96     |
|    value_loss           | 68       |
--------------------------------------
Iteration: 1530 | Episodes: 93150 | Median Reward: -19.52 | Max Reward: 49.14
Iteration: 1530 | Episodes: 93200 | Median Reward: -9.16 | Max Reward: 49.14
Iteration: 1531 | Episodes: 93250 | Median Reward: -21.20 | Max Reward: 49.14
Iteration: 1532 | Episodes: 93300 | Median Reward: -12.57 | Max Reward: 49.14
Iteration: 1533 | Episodes: 93350 | Median Reward: -11.54 | Max Reward: 49.14
Iteration: 1534 | Episodes: 93400 | Median Reward: -10.89 | Max Reward: 49.14
Iteration: 1535 | Episodes: 93450 | Median Reward: -10.56 | Max Reward: 49.14
Iteration: 1535 | Episodes: 93500 | Median Reward: 15.16 | Max Reward: 49.14
Iteration: 1536 | Episodes: 93550 | Median Reward: -7.34 | Max Reward: 49.14
Iteration: 1537 | Episodes: 93600 | Median Reward: -7.95 | Max Reward: 49.14
Iteration: 1538 | Episodes: 93650 | Median Reward: -13.98 | Max Reward: 49.14
Iteration: 1539 | Episodes: 93700 | Median Reward: -13.56 | Max Reward: 49.14
Iteration: 1539 | Episodes: 93750 | Median Reward: -13.93 | Max Reward: 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -100      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1540      |
|    time_elapsed         | 27943     |
|    total_timesteps      | 9461760   |
| train/                  |           |
|    approx_kl            | 13.120322 |
|    clip_fraction        | 0.212     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.543     |
|    learning_rate        | 0.0001    |
|    loss                 | 28.4      |
|    n_updates            | 15390     |
|    policy_gradient_loss | 0.0643    |
|    std                  | 2.97      |
|    value_loss           | 96.9      |
---------------------------------------
Iteration: 1540 | Episodes: 93800 | Median Reward: -13.50 | Max Reward: 49.14
Iteration: 1541 | Episodes: 93850 | Median Reward: -13.92 | Max Reward: 49.14
Iteration: 1542 | Episodes: 93900 | Median Reward: -10.12 | Max Reward: 49.14
Iteration: 1543 | Episodes: 93950 | Median Reward: -9.92 | Max Reward: 49.14
Iteration: 1544 | Episodes: 94000 | Median Reward: -9.88 | Max Reward: 49.14
Iteration: 1544 | Episodes: 94050 | Median Reward: -18.19 | Max Reward: 49.14
Iteration: 1545 | Episodes: 94100 | Median Reward: -11.27 | Max Reward: 49.14
Iteration: 1546 | Episodes: 94150 | Median Reward: -16.39 | Max Reward: 49.14
Iteration: 1547 | Episodes: 94200 | Median Reward: -18.23 | Max Reward: 49.14
Iteration: 1548 | Episodes: 94250 | Median Reward: -23.53 | Max Reward: 49.14
Iteration: 1549 | Episodes: 94300 | Median Reward: -11.26 | Max Reward: 49.14
Iteration: 1549 | Episodes: 94350 | Median Reward: -19.34 | Max Reward: 49.14
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -106      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1550      |
|    time_elapsed         | 28122     |
|    total_timesteps      | 9523200   |
| train/                  |           |
|    approx_kl            | 21.143211 |
|    clip_fraction        | 0.224     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.467     |
|    learning_rate        | 0.0001    |
|    loss                 | 22.6      |
|    n_updates            | 15490     |
|    policy_gradient_loss | -0.0284   |
|    std                  | 2.99      |
|    value_loss           | 149       |
---------------------------------------
Iteration: 1550 | Episodes: 94400 | Median Reward: -16.99 | Max Reward: 49.14
Iteration: 1551 | Episodes: 94450 | Median Reward: -19.96 | Max Reward: 49.14
Iteration: 1552 | Episodes: 94500 | Median Reward: -24.18 | Max Reward: 49.14
Iteration: 1553 | Episodes: 94550 | Median Reward: -23.45 | Max Reward: 49.14
Iteration: 1553 | Episodes: 94600 | Median Reward: -25.30 | Max Reward: 49.14
Iteration: 1554 | Episodes: 94650 | Median Reward: -19.07 | Max Reward: 49.14
Iteration: 1555 | Episodes: 94700 | Median Reward: -22.25 | Max Reward: 49.14
Iteration: 1556 | Episodes: 94750 | Median Reward: -19.98 | Max Reward: 49.14
Iteration: 1557 | Episodes: 94800 | Median Reward: -13.11 | Max Reward: 49.14
Iteration: 1558 | Episodes: 94850 | Median Reward: -13.11 | Max Reward: 49.14
Iteration: 1558 | Episodes: 94900 | Median Reward: -21.00 | Max Reward: 49.14
Iteration: 1559 | Episodes: 94950 | Median Reward: -24.34 | Max Reward: 49.14
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -115     |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1560     |
|    time_elapsed         | 28302    |
|    total_timesteps      | 9584640  |
| train/                  |          |
|    approx_kl            | 3.844751 |
|    clip_fraction        | 0.0433   |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0.521    |
|    learning_rate        | 0.0001   |
|    loss                 | 55.7     |
|    n_updates            | 15590    |
|    policy_gradient_loss | -0.00281 |
|    std                  | 3        |
|    value_loss           | 103      |
--------------------------------------
Iteration: 1560 | Episodes: 95000 | Median Reward: -19.72 | Max Reward: 49.14
Iteration: 1561 | Episodes: 95050 | Median Reward: -19.72 | Max Reward: 49.14
Iteration: 1562 | Episodes: 95100 | Median Reward: -22.12 | Max Reward: 49.14
Iteration: 1563 | Episodes: 95150 | Median Reward: -19.07 | Max Reward: 49.14
Iteration: 1563 | Episodes: 95200 | Median Reward: -19.31 | Max Reward: 49.14
Iteration: 1564 | Episodes: 95250 | Median Reward: -17.60 | Max Reward: 49.14
Iteration: 1565 | Episodes: 95300 | Median Reward: -16.26 | Max Reward: 49.14
Iteration: 1566 | Episodes: 95350 | Median Reward: -16.26 | Max Reward: 49.14
Iteration: 1567 | Episodes: 95400 | Median Reward: -16.29 | Max Reward: 49.14
Iteration: 1567 | Episodes: 95450 | Median Reward: -15.46 | Max Reward: 49.14
Iteration: 1568 | Episodes: 95500 | Median Reward: -6.57 | Max Reward: 49.14
Iteration: 1569 | Episodes: 95550 | Median Reward: -14.99 | Max Reward: 49.14
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -103       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1570       |
|    time_elapsed         | 28488      |
|    total_timesteps      | 9646080    |
| train/                  |            |
|    approx_kl            | 0.21607399 |
|    clip_fraction        | 0.0516     |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.0001     |
|    loss                 | 18.8       |
|    n_updates            | 15690      |
|    policy_gradient_loss | 0.00743    |
|    std                  | 3.01       |
|    value_loss           | 190        |
----------------------------------------
Iteration: 1570 | Episodes: 95600 | Median Reward: -21.30 | Max Reward: 49.14
Iteration: 1571 | Episodes: 95650 | Median Reward: -24.12 | Max Reward: 49.14
Iteration: 1572 | Episodes: 95700 | Median Reward: -17.93 | Max Reward: 49.14
Iteration: 1572 | Episodes: 95750 | Median Reward: -14.36 | Max Reward: 49.14
Iteration: 1573 | Episodes: 95800 | Median Reward: -17.06 | Max Reward: 49.14
Iteration: 1574 | Episodes: 95850 | Median Reward: -15.74 | Max Reward: 49.14
Iteration: 1575 | Episodes: 95900 | Median Reward: -14.84 | Max Reward: 49.19
Iteration: 1576 | Episodes: 95950 | Median Reward: -9.47 | Max Reward: 49.19
Iteration: 1576 | Episodes: 96000 | Median Reward: 21.42 | Max Reward: 49.19
Iteration: 1577 | Episodes: 96050 | Median Reward: 40.92 | Max Reward: 49.19
Iteration: 1578 | Episodes: 96100 | Median Reward: 36.84 | Max Reward: 49.19
Iteration: 1579 | Episodes: 96150 | Median Reward: 38.70 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -73.8     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1580      |
|    time_elapsed         | 28670     |
|    total_timesteps      | 9707520   |
| train/                  |           |
|    approx_kl            | 1.4592074 |
|    clip_fraction        | 0.0667    |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.481     |
|    learning_rate        | 0.0001    |
|    loss                 | 37.5      |
|    n_updates            | 15790     |
|    policy_gradient_loss | -0.0137   |
|    std                  | 3.02      |
|    value_loss           | 184       |
---------------------------------------
Iteration: 1580 | Episodes: 96200 | Median Reward: 40.74 | Max Reward: 49.19
Iteration: 1581 | Episodes: 96250 | Median Reward: 41.27 | Max Reward: 49.19
Iteration: 1581 | Episodes: 96300 | Median Reward: 37.79 | Max Reward: 49.19
Iteration: 1582 | Episodes: 96350 | Median Reward: 27.41 | Max Reward: 49.19
Iteration: 1583 | Episodes: 96400 | Median Reward: 23.19 | Max Reward: 49.19
Iteration: 1584 | Episodes: 96450 | Median Reward: -12.75 | Max Reward: 49.19
Iteration: 1585 | Episodes: 96500 | Median Reward: -5.52 | Max Reward: 49.19
Iteration: 1586 | Episodes: 96550 | Median Reward: 35.69 | Max Reward: 49.19
Iteration: 1586 | Episodes: 96600 | Median Reward: -18.47 | Max Reward: 49.19
Iteration: 1587 | Episodes: 96650 | Median Reward: -12.15 | Max Reward: 49.19
Iteration: 1588 | Episodes: 96700 | Median Reward: -12.62 | Max Reward: 49.19
Iteration: 1589 | Episodes: 96750 | Median Reward: -16.64 | Max Reward: 49.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -119         |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 1590         |
|    time_elapsed         | 28849        |
|    total_timesteps      | 9768960      |
| train/                  |              |
|    approx_kl            | 0.0041042273 |
|    clip_fraction        | 0.00061      |
|    clip_range           | 0.4          |
|    entropy_loss         | -110         |
|    explained_variance   | 0.768        |
|    learning_rate        | 0.0001       |
|    loss                 | 15           |
|    n_updates            | 15890        |
|    policy_gradient_loss | -0.00195     |
|    std                  | 3.03         |
|    value_loss           | 43.2         |
------------------------------------------
Iteration: 1590 | Episodes: 96800 | Median Reward: -23.79 | Max Reward: 49.19
Iteration: 1590 | Episodes: 96850 | Median Reward: -15.08 | Max Reward: 49.19
Iteration: 1591 | Episodes: 96900 | Median Reward: -16.01 | Max Reward: 49.19
Iteration: 1592 | Episodes: 96950 | Median Reward: -9.14 | Max Reward: 49.19
Iteration: 1593 | Episodes: 97000 | Median Reward: -9.14 | Max Reward: 49.19
Iteration: 1594 | Episodes: 97050 | Median Reward: -13.66 | Max Reward: 49.19
Iteration: 1595 | Episodes: 97100 | Median Reward: -13.16 | Max Reward: 49.19
Iteration: 1595 | Episodes: 97150 | Median Reward: 20.45 | Max Reward: 49.19
Iteration: 1596 | Episodes: 97200 | Median Reward: -22.79 | Max Reward: 49.19
Iteration: 1597 | Episodes: 97250 | Median Reward: -22.60 | Max Reward: 49.19
Iteration: 1598 | Episodes: 97300 | Median Reward: -15.20 | Max Reward: 49.19
Iteration: 1599 | Episodes: 97350 | Median Reward: -17.09 | Max Reward: 49.19
Iteration: 1599 | Episodes: 97400 | Median Reward: -9.41 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -108      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1600      |
|    time_elapsed         | 29038     |
|    total_timesteps      | 9830400   |
| train/                  |           |
|    approx_kl            | 1.6667807 |
|    clip_fraction        | 0.0651    |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.555     |
|    learning_rate        | 0.0001    |
|    loss                 | 20.8      |
|    n_updates            | 15990     |
|    policy_gradient_loss | -0.00857  |
|    std                  | 3.04      |
|    value_loss           | 106       |
---------------------------------------
Iteration: 1600 | Episodes: 97450 | Median Reward: -15.97 | Max Reward: 49.19
Iteration: 1601 | Episodes: 97500 | Median Reward: -21.10 | Max Reward: 49.19
Iteration: 1602 | Episodes: 97550 | Median Reward: -18.57 | Max Reward: 49.19
Iteration: 1603 | Episodes: 97600 | Median Reward: -16.79 | Max Reward: 49.19
Iteration: 1604 | Episodes: 97650 | Median Reward: -13.72 | Max Reward: 49.19
Iteration: 1604 | Episodes: 97700 | Median Reward: -18.04 | Max Reward: 49.19
Iteration: 1605 | Episodes: 97750 | Median Reward: -18.93 | Max Reward: 49.19
Iteration: 1606 | Episodes: 97800 | Median Reward: -9.53 | Max Reward: 49.19
Iteration: 1607 | Episodes: 97850 | Median Reward: -9.53 | Max Reward: 49.19
Iteration: 1608 | Episodes: 97900 | Median Reward: -19.38 | Max Reward: 49.19
Iteration: 1608 | Episodes: 97950 | Median Reward: -14.64 | Max Reward: 49.19
Iteration: 1609 | Episodes: 98000 | Median Reward: -13.98 | Max Reward: 49.19
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -109       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1610       |
|    time_elapsed         | 29215      |
|    total_timesteps      | 9891840    |
| train/                  |            |
|    approx_kl            | 0.25212407 |
|    clip_fraction        | 0.00925    |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.0001     |
|    loss                 | 15         |
|    n_updates            | 16090      |
|    policy_gradient_loss | -0.00419   |
|    std                  | 3.05       |
|    value_loss           | 88.8       |
----------------------------------------
Iteration: 1610 | Episodes: 98050 | Median Reward: -14.99 | Max Reward: 49.19
Iteration: 1611 | Episodes: 98100 | Median Reward: -13.11 | Max Reward: 49.19
Iteration: 1612 | Episodes: 98150 | Median Reward: -13.46 | Max Reward: 49.19
Iteration: 1613 | Episodes: 98200 | Median Reward: -23.30 | Max Reward: 49.19
Iteration: 1613 | Episodes: 98250 | Median Reward: -21.47 | Max Reward: 49.19
Iteration: 1614 | Episodes: 98300 | Median Reward: -20.28 | Max Reward: 49.19
Iteration: 1615 | Episodes: 98350 | Median Reward: -24.08 | Max Reward: 49.19
Iteration: 1616 | Episodes: 98400 | Median Reward: -24.08 | Max Reward: 49.19
Iteration: 1617 | Episodes: 98450 | Median Reward: -22.43 | Max Reward: 49.19
Iteration: 1618 | Episodes: 98500 | Median Reward: -18.21 | Max Reward: 49.19
Iteration: 1618 | Episodes: 98550 | Median Reward: -20.89 | Max Reward: 49.19
Iteration: 1619 | Episodes: 98600 | Median Reward: -17.46 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -119      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1620      |
|    time_elapsed         | 29393     |
|    total_timesteps      | 9953280   |
| train/                  |           |
|    approx_kl            | 4.8267603 |
|    clip_fraction        | 0.0515    |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.714     |
|    learning_rate        | 0.0001    |
|    loss                 | 14.7      |
|    n_updates            | 16190     |
|    policy_gradient_loss | 0.00468   |
|    std                  | 3.06      |
|    value_loss           | 47.1      |
---------------------------------------
Iteration: 1620 | Episodes: 98650 | Median Reward: -24.01 | Max Reward: 49.19
Iteration: 1621 | Episodes: 98700 | Median Reward: -20.29 | Max Reward: 49.19
Iteration: 1622 | Episodes: 98750 | Median Reward: -13.40 | Max Reward: 49.19
Iteration: 1623 | Episodes: 98800 | Median Reward: -19.44 | Max Reward: 49.19
Iteration: 1623 | Episodes: 98850 | Median Reward: -28.05 | Max Reward: 49.19
Iteration: 1624 | Episodes: 98900 | Median Reward: -23.03 | Max Reward: 49.19
Iteration: 1625 | Episodes: 98950 | Median Reward: -21.89 | Max Reward: 49.19
Iteration: 1626 | Episodes: 99000 | Median Reward: -19.77 | Max Reward: 49.19
Iteration: 1627 | Episodes: 99050 | Median Reward: -19.54 | Max Reward: 49.19
Iteration: 1627 | Episodes: 99100 | Median Reward: -13.49 | Max Reward: 49.19
Iteration: 1628 | Episodes: 99150 | Median Reward: -2.87 | Max Reward: 49.19
Iteration: 1629 | Episodes: 99200 | Median Reward: -19.28 | Max Reward: 49.19
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -105     |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1630     |
|    time_elapsed         | 29579    |
|    total_timesteps      | 10014720 |
| train/                  |          |
|    approx_kl            | 1.945396 |
|    clip_fraction        | 0.052    |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0.467    |
|    learning_rate        | 0.0001   |
|    loss                 | 55.1     |
|    n_updates            | 16290    |
|    policy_gradient_loss | -0.0111  |
|    std                  | 3.07     |
|    value_loss           | 219      |
--------------------------------------
Iteration: 1630 | Episodes: 99250 | Median Reward: -23.80 | Max Reward: 49.19
Iteration: 1631 | Episodes: 99300 | Median Reward: -23.80 | Max Reward: 49.19
Iteration: 1632 | Episodes: 99350 | Median Reward: -15.95 | Max Reward: 49.19
Iteration: 1632 | Episodes: 99400 | Median Reward: -12.16 | Max Reward: 49.19
Iteration: 1633 | Episodes: 99450 | Median Reward: 41.74 | Max Reward: 49.19
Iteration: 1634 | Episodes: 99500 | Median Reward: 40.24 | Max Reward: 49.19
Iteration: 1635 | Episodes: 99550 | Median Reward: 41.24 | Max Reward: 49.19
Iteration: 1635 | Episodes: 99600 | Median Reward: 43.73 | Max Reward: 49.19
Iteration: 1636 | Episodes: 99650 | Median Reward: -5.51 | Max Reward: 49.19
Iteration: 1637 | Episodes: 99700 | Median Reward: -6.58 | Max Reward: 49.19
Iteration: 1638 | Episodes: 99750 | Median Reward: 36.88 | Max Reward: 49.19
Iteration: 1639 | Episodes: 99800 | Median Reward: 11.63 | Max Reward: 49.19
Iteration: 1639 | Episodes: 99850 | Median Reward: 16.50 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -85.7     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1640      |
|    time_elapsed         | 29758     |
|    total_timesteps      | 10076160  |
| train/                  |           |
|    approx_kl            | 13.462232 |
|    clip_fraction        | 0.172     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.465     |
|    learning_rate        | 0.0001    |
|    loss                 | 105       |
|    n_updates            | 16390     |
|    policy_gradient_loss | 0.0069    |
|    std                  | 3.08      |
|    value_loss           | 226       |
---------------------------------------
Iteration: 1640 | Episodes: 99900 | Median Reward: 12.46 | Max Reward: 49.19
Iteration: 1641 | Episodes: 99950 | Median Reward: -14.71 | Max Reward: 49.19
Iteration: 1642 | Episodes: 100000 | Median Reward: -12.58 | Max Reward: 49.19
Iteration: 1643 | Episodes: 100050 | Median Reward: -6.57 | Max Reward: 49.19
Iteration: 1643 | Episodes: 100100 | Median Reward: 8.44 | Max Reward: 49.19
Iteration: 1644 | Episodes: 100150 | Median Reward: -19.09 | Max Reward: 49.19
Iteration: 1645 | Episodes: 100200 | Median Reward: 39.00 | Max Reward: 49.19
Iteration: 1646 | Episodes: 100250 | Median Reward: 41.55 | Max Reward: 49.19
Iteration: 1647 | Episodes: 100300 | Median Reward: 38.71 | Max Reward: 49.19
Iteration: 1648 | Episodes: 100350 | Median Reward: 36.47 | Max Reward: 49.19
Iteration: 1648 | Episodes: 100400 | Median Reward: -11.81 | Max Reward: 49.19
Iteration: 1649 | Episodes: 100450 | Median Reward: -9.88 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -97.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1650      |
|    time_elapsed         | 29939     |
|    total_timesteps      | 10137600  |
| train/                  |           |
|    approx_kl            | 18.067108 |
|    clip_fraction        | 0.247     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.531     |
|    learning_rate        | 0.0001    |
|    loss                 | 75.2      |
|    n_updates            | 16490     |
|    policy_gradient_loss | -0.0268   |
|    std                  | 3.09      |
|    value_loss           | 134       |
---------------------------------------
Iteration: 1650 | Episodes: 100500 | Median Reward: 23.70 | Max Reward: 49.19
Iteration: 1651 | Episodes: 100550 | Median Reward: 41.11 | Max Reward: 49.19
Iteration: 1652 | Episodes: 100600 | Median Reward: 39.12 | Max Reward: 49.19
Iteration: 1652 | Episodes: 100650 | Median Reward: -3.48 | Max Reward: 49.19
Iteration: 1653 | Episodes: 100700 | Median Reward: -7.72 | Max Reward: 49.19
Iteration: 1654 | Episodes: 100750 | Median Reward: 34.30 | Max Reward: 49.19
Iteration: 1655 | Episodes: 100800 | Median Reward: 41.24 | Max Reward: 49.19
Iteration: 1656 | Episodes: 100850 | Median Reward: 43.42 | Max Reward: 49.19
Iteration: 1657 | Episodes: 100900 | Median Reward: 42.98 | Max Reward: 49.19
Iteration: 1657 | Episodes: 100950 | Median Reward: 38.38 | Max Reward: 49.19
Iteration: 1658 | Episodes: 101000 | Median Reward: 37.56 | Max Reward: 49.19
Iteration: 1659 | Episodes: 101050 | Median Reward: 40.11 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78       |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1660      |
|    time_elapsed         | 30126     |
|    total_timesteps      | 10199040  |
| train/                  |           |
|    approx_kl            | 5.9019547 |
|    clip_fraction        | 0.115     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.492     |
|    learning_rate        | 0.0001    |
|    loss                 | 130       |
|    n_updates            | 16590     |
|    policy_gradient_loss | -0.0241   |
|    std                  | 3.1       |
|    value_loss           | 203       |
---------------------------------------
Iteration: 1660 | Episodes: 101100 | Median Reward: 39.65 | Max Reward: 49.19
Iteration: 1661 | Episodes: 101150 | Median Reward: 39.09 | Max Reward: 49.19
Iteration: 1662 | Episodes: 101200 | Median Reward: 42.76 | Max Reward: 49.19
Iteration: 1662 | Episodes: 101250 | Median Reward: 41.22 | Max Reward: 49.19
Iteration: 1663 | Episodes: 101300 | Median Reward: 44.06 | Max Reward: 49.19
Iteration: 1664 | Episodes: 101350 | Median Reward: 40.15 | Max Reward: 49.19
Iteration: 1665 | Episodes: 101400 | Median Reward: 40.15 | Max Reward: 49.19
Iteration: 1666 | Episodes: 101450 | Median Reward: 41.61 | Max Reward: 49.19
Iteration: 1666 | Episodes: 101500 | Median Reward: 42.09 | Max Reward: 49.19
Iteration: 1667 | Episodes: 101550 | Median Reward: 8.84 | Max Reward: 49.19
Iteration: 1668 | Episodes: 101600 | Median Reward: 3.29 | Max Reward: 49.19
Iteration: 1669 | Episodes: 101650 | Median Reward: 34.78 | Max Reward: 49.19
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -83.9    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1670     |
|    time_elapsed         | 30306    |
|    total_timesteps      | 10260480 |
| train/                  |          |
|    approx_kl            | 6.835541 |
|    clip_fraction        | 0.162    |
|    clip_range           | 0.4      |
|    entropy_loss         | -110     |
|    explained_variance   | 0.523    |
|    learning_rate        | 0.0001   |
|    loss                 | 80.3     |
|    n_updates            | 16690    |
|    policy_gradient_loss | 0.0358   |
|    std                  | 3.11     |
|    value_loss           | 172      |
--------------------------------------
Iteration: 1670 | Episodes: 101700 | Median Reward: 29.35 | Max Reward: 49.19
Iteration: 1671 | Episodes: 101750 | Median Reward: 33.58 | Max Reward: 49.19
Iteration: 1671 | Episodes: 101800 | Median Reward: 42.13 | Max Reward: 49.19
Iteration: 1672 | Episodes: 101850 | Median Reward: 40.92 | Max Reward: 49.19
Iteration: 1673 | Episodes: 101900 | Median Reward: 42.90 | Max Reward: 49.19
Iteration: 1674 | Episodes: 101950 | Median Reward: 43.61 | Max Reward: 49.19
Iteration: 1675 | Episodes: 102000 | Median Reward: 39.21 | Max Reward: 49.19
Iteration: 1675 | Episodes: 102050 | Median Reward: 41.90 | Max Reward: 49.19
Iteration: 1676 | Episodes: 102100 | Median Reward: 33.16 | Max Reward: 49.19
Iteration: 1677 | Episodes: 102150 | Median Reward: -2.70 | Max Reward: 49.19
Iteration: 1678 | Episodes: 102200 | Median Reward: -2.70 | Max Reward: 49.19
Iteration: 1679 | Episodes: 102250 | Median Reward: 35.88 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -80.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1680      |
|    time_elapsed         | 30493     |
|    total_timesteps      | 10321920  |
| train/                  |           |
|    approx_kl            | 23.280102 |
|    clip_fraction        | 0.197     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.514     |
|    learning_rate        | 0.0001    |
|    loss                 | 74.1      |
|    n_updates            | 16790     |
|    policy_gradient_loss | -0.0133   |
|    std                  | 3.11      |
|    value_loss           | 180       |
---------------------------------------
Iteration: 1680 | Episodes: 102300 | Median Reward: 40.23 | Max Reward: 49.19
Iteration: 1680 | Episodes: 102350 | Median Reward: 41.06 | Max Reward: 49.19
Iteration: 1681 | Episodes: 102400 | Median Reward: 42.18 | Max Reward: 49.19
Iteration: 1682 | Episodes: 102450 | Median Reward: 40.50 | Max Reward: 49.19
Iteration: 1683 | Episodes: 102500 | Median Reward: 40.50 | Max Reward: 49.19
Iteration: 1684 | Episodes: 102550 | Median Reward: 35.10 | Max Reward: 49.19
Iteration: 1685 | Episodes: 102600 | Median Reward: 13.77 | Max Reward: 49.19
Iteration: 1685 | Episodes: 102650 | Median Reward: 41.61 | Max Reward: 49.19
Iteration: 1686 | Episodes: 102700 | Median Reward: 36.65 | Max Reward: 49.19
Iteration: 1687 | Episodes: 102750 | Median Reward: 36.80 | Max Reward: 49.19
Iteration: 1688 | Episodes: 102800 | Median Reward: 38.76 | Max Reward: 49.19
Iteration: 1689 | Episodes: 102850 | Median Reward: 38.79 | Max Reward: 49.19
Iteration: 1689 | Episodes: 102900 | Median Reward: 35.37 | Max Reward: 49.19
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -79.3    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1690     |
|    time_elapsed         | 30672    |
|    total_timesteps      | 10383360 |
| train/                  |          |
|    approx_kl            | 6.220276 |
|    clip_fraction        | 0.121    |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0.504    |
|    learning_rate        | 0.0001   |
|    loss                 | 89.7     |
|    n_updates            | 16890    |
|    policy_gradient_loss | -0.0201  |
|    std                  | 3.12     |
|    value_loss           | 212      |
--------------------------------------
Iteration: 1690 | Episodes: 102950 | Median Reward: 41.87 | Max Reward: 49.19
Iteration: 1691 | Episodes: 103000 | Median Reward: 32.65 | Max Reward: 49.19
Iteration: 1692 | Episodes: 103050 | Median Reward: -12.17 | Max Reward: 49.19
Iteration: 1693 | Episodes: 103100 | Median Reward: -20.84 | Max Reward: 49.19
Iteration: 1694 | Episodes: 103150 | Median Reward: -16.39 | Max Reward: 49.19
Iteration: 1694 | Episodes: 103200 | Median Reward: -3.11 | Max Reward: 49.19
Iteration: 1695 | Episodes: 103250 | Median Reward: 3.58 | Max Reward: 49.19
Iteration: 1696 | Episodes: 103300 | Median Reward: 39.29 | Max Reward: 49.19
Iteration: 1697 | Episodes: 103350 | Median Reward: 40.50 | Max Reward: 49.19
Iteration: 1698 | Episodes: 103400 | Median Reward: 33.93 | Max Reward: 49.19
Iteration: 1698 | Episodes: 103450 | Median Reward: 39.81 | Max Reward: 49.19
Iteration: 1699 | Episodes: 103500 | Median Reward: 41.58 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -75.8     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1700      |
|    time_elapsed         | 30850     |
|    total_timesteps      | 10444800  |
| train/                  |           |
|    approx_kl            | 16.543398 |
|    clip_fraction        | 0.238     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.493     |
|    learning_rate        | 0.0001    |
|    loss                 | 154       |
|    n_updates            | 16990     |
|    policy_gradient_loss | -0.0472   |
|    std                  | 3.12      |
|    value_loss           | 230       |
---------------------------------------
Iteration: 1700 | Episodes: 103550 | Median Reward: 42.12 | Max Reward: 49.19
Iteration: 1701 | Episodes: 103600 | Median Reward: 36.23 | Max Reward: 49.19
Iteration: 1702 | Episodes: 103650 | Median Reward: -16.81 | Max Reward: 49.19
Iteration: 1703 | Episodes: 103700 | Median Reward: -10.53 | Max Reward: 49.19
Iteration: 1703 | Episodes: 103750 | Median Reward: 39.61 | Max Reward: 49.19
Iteration: 1704 | Episodes: 103800 | Median Reward: -9.56 | Max Reward: 49.19
Iteration: 1705 | Episodes: 103850 | Median Reward: -8.91 | Max Reward: 49.19
Iteration: 1706 | Episodes: 103900 | Median Reward: 38.60 | Max Reward: 49.19
Iteration: 1707 | Episodes: 103950 | Median Reward: 37.12 | Max Reward: 49.19
Iteration: 1707 | Episodes: 104000 | Median Reward: -10.82 | Max Reward: 49.19
Iteration: 1708 | Episodes: 104050 | Median Reward: 42.03 | Max Reward: 49.19
Iteration: 1709 | Episodes: 104100 | Median Reward: 36.63 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -81       |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1710      |
|    time_elapsed         | 31039     |
|    total_timesteps      | 10506240  |
| train/                  |           |
|    approx_kl            | 3.7060404 |
|    clip_fraction        | 0.126     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.478     |
|    learning_rate        | 0.0001    |
|    loss                 | 138       |
|    n_updates            | 17090     |
|    policy_gradient_loss | 0.0395    |
|    std                  | 3.13      |
|    value_loss           | 292       |
---------------------------------------
Iteration: 1710 | Episodes: 104150 | Median Reward: 24.16 | Max Reward: 49.19
Iteration: 1711 | Episodes: 104200 | Median Reward: 40.88 | Max Reward: 49.19
Iteration: 1712 | Episodes: 104250 | Median Reward: 41.72 | Max Reward: 49.19
Iteration: 1712 | Episodes: 104300 | Median Reward: 46.17 | Max Reward: 49.19
Iteration: 1713 | Episodes: 104350 | Median Reward: 42.98 | Max Reward: 49.19
Iteration: 1714 | Episodes: 104400 | Median Reward: 42.56 | Max Reward: 49.19
Iteration: 1715 | Episodes: 104450 | Median Reward: 38.26 | Max Reward: 49.19
Iteration: 1716 | Episodes: 104500 | Median Reward: -18.99 | Max Reward: 49.19
Iteration: 1717 | Episodes: 104550 | Median Reward: 40.07 | Max Reward: 49.19
Iteration: 1717 | Episodes: 104600 | Median Reward: -10.33 | Max Reward: 49.19
Iteration: 1718 | Episodes: 104650 | Median Reward: 40.95 | Max Reward: 49.19
Iteration: 1719 | Episodes: 104700 | Median Reward: 43.57 | Max Reward: 49.19
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -71.1    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1720     |
|    time_elapsed         | 31218    |
|    total_timesteps      | 10567680 |
| train/                  |          |
|    approx_kl            | 9.10029  |
|    clip_fraction        | 0.0824   |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0.495    |
|    learning_rate        | 0.0001   |
|    loss                 | 88.6     |
|    n_updates            | 17190    |
|    policy_gradient_loss | -0.0139  |
|    std                  | 3.14     |
|    value_loss           | 225      |
--------------------------------------
Iteration: 1720 | Episodes: 104750 | Median Reward: 43.57 | Max Reward: 49.19
Iteration: 1721 | Episodes: 104800 | Median Reward: 44.46 | Max Reward: 49.19
Iteration: 1721 | Episodes: 104850 | Median Reward: -2.06 | Max Reward: 49.19
Iteration: 1722 | Episodes: 104900 | Median Reward: 39.20 | Max Reward: 49.19
Iteration: 1723 | Episodes: 104950 | Median Reward: 39.40 | Max Reward: 49.19
Iteration: 1724 | Episodes: 105000 | Median Reward: -2.38 | Max Reward: 49.19
Iteration: 1725 | Episodes: 105050 | Median Reward: 38.64 | Max Reward: 49.19
Iteration: 1726 | Episodes: 105100 | Median Reward: 43.27 | Max Reward: 49.19
Iteration: 1726 | Episodes: 105150 | Median Reward: -13.46 | Max Reward: 49.19
Iteration: 1727 | Episodes: 105200 | Median Reward: -12.97 | Max Reward: 49.19
Iteration: 1728 | Episodes: 105250 | Median Reward: -16.53 | Max Reward: 49.19
Iteration: 1729 | Episodes: 105300 | Median Reward: -15.68 | Max Reward: 49.19
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -96.9      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1730       |
|    time_elapsed         | 31400      |
|    total_timesteps      | 10629120   |
| train/                  |            |
|    approx_kl            | 0.06098509 |
|    clip_fraction        | 0.0205     |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.0001     |
|    loss                 | 72.8       |
|    n_updates            | 17290      |
|    policy_gradient_loss | -0.00157   |
|    std                  | 3.15       |
|    value_loss           | 109        |
----------------------------------------
Iteration: 1730 | Episodes: 105350 | Median Reward: -1.27 | Max Reward: 49.19
Iteration: 1731 | Episodes: 105400 | Median Reward: -18.09 | Max Reward: 49.19
Iteration: 1731 | Episodes: 105450 | Median Reward: -25.12 | Max Reward: 49.19
Iteration: 1732 | Episodes: 105500 | Median Reward: -15.54 | Max Reward: 49.19
Iteration: 1733 | Episodes: 105550 | Median Reward: -8.01 | Max Reward: 49.19
Iteration: 1734 | Episodes: 105600 | Median Reward: 39.50 | Max Reward: 49.19
Iteration: 1734 | Episodes: 105650 | Median Reward: 3.85 | Max Reward: 49.19
Iteration: 1735 | Episodes: 105700 | Median Reward: 40.84 | Max Reward: 49.19
Iteration: 1736 | Episodes: 105750 | Median Reward: 0.34 | Max Reward: 49.19
Iteration: 1737 | Episodes: 105800 | Median Reward: -1.22 | Max Reward: 49.19
Iteration: 1738 | Episodes: 105850 | Median Reward: 41.47 | Max Reward: 49.19
Iteration: 1739 | Episodes: 105900 | Median Reward: 42.16 | Max Reward: 49.19
Iteration: 1739 | Episodes: 105950 | Median Reward: 41.86 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -73.4     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1740      |
|    time_elapsed         | 31586     |
|    total_timesteps      | 10690560  |
| train/                  |           |
|    approx_kl            | 1.4363954 |
|    clip_fraction        | 0.104     |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.507     |
|    learning_rate        | 0.0001    |
|    loss                 | 129       |
|    n_updates            | 17390     |
|    policy_gradient_loss | -0.0041   |
|    std                  | 3.16      |
|    value_loss           | 221       |
---------------------------------------
Iteration: 1740 | Episodes: 106000 | Median Reward: 42.38 | Max Reward: 49.19
Iteration: 1741 | Episodes: 106050 | Median Reward: 42.20 | Max Reward: 49.19
Iteration: 1742 | Episodes: 106100 | Median Reward: 40.87 | Max Reward: 49.19
Iteration: 1743 | Episodes: 106150 | Median Reward: 14.23 | Max Reward: 49.19
Iteration: 1743 | Episodes: 106200 | Median Reward: 38.88 | Max Reward: 49.19
Iteration: 1744 | Episodes: 106250 | Median Reward: 32.35 | Max Reward: 49.19
Iteration: 1745 | Episodes: 106300 | Median Reward: 39.69 | Max Reward: 49.19
Iteration: 1746 | Episodes: 106350 | Median Reward: 41.92 | Max Reward: 49.19
Iteration: 1747 | Episodes: 106400 | Median Reward: 42.05 | Max Reward: 49.19
Iteration: 1748 | Episodes: 106450 | Median Reward: 41.26 | Max Reward: 49.19
Iteration: 1748 | Episodes: 106500 | Median Reward: 40.63 | Max Reward: 49.19
Iteration: 1749 | Episodes: 106550 | Median Reward: 39.70 | Max Reward: 49.19
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -65.8      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1750       |
|    time_elapsed         | 31770      |
|    total_timesteps      | 10752000   |
| train/                  |            |
|    approx_kl            | 0.04862047 |
|    clip_fraction        | 0.0338     |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.0001     |
|    loss                 | 147        |
|    n_updates            | 17490      |
|    policy_gradient_loss | -0.00262   |
|    std                  | 3.16       |
|    value_loss           | 278        |
----------------------------------------
Iteration: 1750 | Episodes: 106600 | Median Reward: 43.18 | Max Reward: 49.19
Iteration: 1751 | Episodes: 106650 | Median Reward: 43.19 | Max Reward: 49.19
Iteration: 1752 | Episodes: 106700 | Median Reward: 35.00 | Max Reward: 49.19
Iteration: 1753 | Episodes: 106750 | Median Reward: 39.08 | Max Reward: 49.19
Iteration: 1753 | Episodes: 106800 | Median Reward: 31.09 | Max Reward: 49.19
Iteration: 1754 | Episodes: 106850 | Median Reward: 42.63 | Max Reward: 49.19
Iteration: 1755 | Episodes: 106900 | Median Reward: 40.30 | Max Reward: 49.19
Iteration: 1756 | Episodes: 106950 | Median Reward: 38.79 | Max Reward: 49.19
Iteration: 1757 | Episodes: 107000 | Median Reward: 39.61 | Max Reward: 49.19
Iteration: 1757 | Episodes: 107050 | Median Reward: 36.97 | Max Reward: 49.19
Iteration: 1758 | Episodes: 107100 | Median Reward: 42.02 | Max Reward: 49.19
Iteration: 1759 | Episodes: 107150 | Median Reward: 38.01 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -76.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1760      |
|    time_elapsed         | 31956     |
|    total_timesteps      | 10813440  |
| train/                  |           |
|    approx_kl            | 36.677116 |
|    clip_fraction        | 0.683     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.503     |
|    learning_rate        | 0.0001    |
|    loss                 | 129       |
|    n_updates            | 17590     |
|    policy_gradient_loss | 0.153     |
|    std                  | 3.17      |
|    value_loss           | 270       |
---------------------------------------
Iteration: 1760 | Episodes: 107200 | Median Reward: -14.18 | Max Reward: 49.19
Iteration: 1761 | Episodes: 107250 | Median Reward: -8.91 | Max Reward: 49.19
Iteration: 1762 | Episodes: 107300 | Median Reward: 39.76 | Max Reward: 49.19
Iteration: 1762 | Episodes: 107350 | Median Reward: 30.70 | Max Reward: 49.19
Iteration: 1763 | Episodes: 107400 | Median Reward: 33.32 | Max Reward: 49.19
Iteration: 1764 | Episodes: 107450 | Median Reward: 34.21 | Max Reward: 49.19
Iteration: 1765 | Episodes: 107500 | Median Reward: -0.55 | Max Reward: 49.19
Iteration: 1766 | Episodes: 107550 | Median Reward: -8.53 | Max Reward: 49.19
Iteration: 1766 | Episodes: 107600 | Median Reward: 17.08 | Max Reward: 49.19
Iteration: 1767 | Episodes: 107650 | Median Reward: -12.96 | Max Reward: 49.19
Iteration: 1768 | Episodes: 107700 | Median Reward: 37.37 | Max Reward: 49.19
Iteration: 1769 | Episodes: 107750 | Median Reward: 41.67 | Max Reward: 49.19
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -71.3      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1770       |
|    time_elapsed         | 32139      |
|    total_timesteps      | 10874880   |
| train/                  |            |
|    approx_kl            | 0.08996654 |
|    clip_fraction        | 0.0322     |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.519      |
|    learning_rate        | 0.0001     |
|    loss                 | 113        |
|    n_updates            | 17690      |
|    policy_gradient_loss | -0.00778   |
|    std                  | 3.18       |
|    value_loss           | 216        |
----------------------------------------
Iteration: 1770 | Episodes: 107800 | Median Reward: 41.52 | Max Reward: 49.19
Iteration: 1771 | Episodes: 107850 | Median Reward: 39.03 | Max Reward: 49.19
Iteration: 1771 | Episodes: 107900 | Median Reward: 42.12 | Max Reward: 49.19
Iteration: 1772 | Episodes: 107950 | Median Reward: 42.11 | Max Reward: 49.19
Iteration: 1773 | Episodes: 108000 | Median Reward: 43.49 | Max Reward: 49.19
Iteration: 1774 | Episodes: 108050 | Median Reward: 41.52 | Max Reward: 49.19
Iteration: 1775 | Episodes: 108100 | Median Reward: 40.35 | Max Reward: 49.19
Iteration: 1776 | Episodes: 108150 | Median Reward: 37.43 | Max Reward: 49.19
Iteration: 1776 | Episodes: 108200 | Median Reward: 40.93 | Max Reward: 49.19
Iteration: 1777 | Episodes: 108250 | Median Reward: 40.54 | Max Reward: 49.19
Iteration: 1778 | Episodes: 108300 | Median Reward: 37.05 | Max Reward: 49.19
Iteration: 1779 | Episodes: 108350 | Median Reward: 37.05 | Max Reward: 49.19
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -63.1      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1780       |
|    time_elapsed         | 32321      |
|    total_timesteps      | 10936320   |
| train/                  |            |
|    approx_kl            | 0.03437804 |
|    clip_fraction        | 0.0187     |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.538      |
|    learning_rate        | 0.0001     |
|    loss                 | 113        |
|    n_updates            | 17790      |
|    policy_gradient_loss | -0.0093    |
|    std                  | 3.19       |
|    value_loss           | 219        |
----------------------------------------
Iteration: 1780 | Episodes: 108400 | Median Reward: 38.24 | Max Reward: 49.19
Iteration: 1780 | Episodes: 108450 | Median Reward: 38.08 | Max Reward: 49.19
Iteration: 1781 | Episodes: 108500 | Median Reward: 36.22 | Max Reward: 49.19
Iteration: 1782 | Episodes: 108550 | Median Reward: 39.82 | Max Reward: 49.19
Iteration: 1783 | Episodes: 108600 | Median Reward: 40.84 | Max Reward: 49.19
Iteration: 1784 | Episodes: 108650 | Median Reward: 40.87 | Max Reward: 49.19
Iteration: 1785 | Episodes: 108700 | Median Reward: 39.31 | Max Reward: 49.19
Iteration: 1785 | Episodes: 108750 | Median Reward: 38.61 | Max Reward: 49.19
Iteration: 1786 | Episodes: 108800 | Median Reward: 39.73 | Max Reward: 49.19
Iteration: 1787 | Episodes: 108850 | Median Reward: 40.82 | Max Reward: 49.19
Iteration: 1788 | Episodes: 108900 | Median Reward: 42.22 | Max Reward: 49.19
Iteration: 1789 | Episodes: 108950 | Median Reward: 44.06 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -59.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1790      |
|    time_elapsed         | 32512     |
|    total_timesteps      | 10997760  |
| train/                  |           |
|    approx_kl            | 5.0621595 |
|    clip_fraction        | 0.0404    |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.518     |
|    learning_rate        | 0.0001    |
|    loss                 | 139       |
|    n_updates            | 17890     |
|    policy_gradient_loss | 0.000725  |
|    std                  | 3.2       |
|    value_loss           | 265       |
---------------------------------------
Iteration: 1790 | Episodes: 109000 | Median Reward: 41.48 | Max Reward: 49.19
Iteration: 1790 | Episodes: 109050 | Median Reward: 39.25 | Max Reward: 49.19
Iteration: 1791 | Episodes: 109100 | Median Reward: 41.91 | Max Reward: 49.19
Iteration: 1792 | Episodes: 109150 | Median Reward: 41.91 | Max Reward: 49.19
Iteration: 1793 | Episodes: 109200 | Median Reward: 41.76 | Max Reward: 49.19
Iteration: 1794 | Episodes: 109250 | Median Reward: 39.59 | Max Reward: 49.19
Iteration: 1794 | Episodes: 109300 | Median Reward: 40.40 | Max Reward: 49.19
Iteration: 1795 | Episodes: 109350 | Median Reward: 39.82 | Max Reward: 49.19
Iteration: 1796 | Episodes: 109400 | Median Reward: 39.71 | Max Reward: 49.19
Iteration: 1797 | Episodes: 109450 | Median Reward: 42.03 | Max Reward: 49.19
Iteration: 1798 | Episodes: 109500 | Median Reward: 40.11 | Max Reward: 49.19
Iteration: 1799 | Episodes: 109550 | Median Reward: 2.49 | Max Reward: 49.19
Iteration: 1799 | Episodes: 109600 | Median Reward: 40.35 | Max Reward: 49.19
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1800      |
|    time_elapsed         | 32692     |
|    total_timesteps      | 11059200  |
| train/                  |           |
|    approx_kl            | 19.521982 |
|    clip_fraction        | 0.265     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.618     |
|    learning_rate        | 0.0001    |
|    loss                 | 100       |
|    n_updates            | 17990     |
|    policy_gradient_loss | 0.108     |
|    std                  | 3.21      |
|    value_loss           | 114       |
---------------------------------------
Iteration: 1800 | Episodes: 109650 | Median Reward: 37.63 | Max Reward: 49.19
Iteration: 1801 | Episodes: 109700 | Median Reward: 37.31 | Max Reward: 49.19
Iteration: 1802 | Episodes: 109750 | Median Reward: 37.61 | Max Reward: 49.19
Iteration: 1803 | Episodes: 109800 | Median Reward: 43.92 | Max Reward: 49.19
Iteration: 1803 | Episodes: 109850 | Median Reward: -13.29 | Max Reward: 49.19
Iteration: 1804 | Episodes: 109900 | Median Reward: -9.56 | Max Reward: 49.19
Iteration: 1805 | Episodes: 109950 | Median Reward: 41.98 | Max Reward: 49.19
Iteration: 1806 | Episodes: 110000 | Median Reward: 41.98 | Max Reward: 49.22
Iteration: 1806 | Episodes: 110050 | Median Reward: 39.63 | Max Reward: 49.22
Iteration: 1807 | Episodes: 110100 | Median Reward: 38.51 | Max Reward: 49.22
Iteration: 1808 | Episodes: 110150 | Median Reward: 37.79 | Max Reward: 49.22
Iteration: 1809 | Episodes: 110200 | Median Reward: 7.26 | Max Reward: 49.22
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -89.7    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1810     |
|    time_elapsed         | 32870    |
|    total_timesteps      | 11120640 |
| train/                  |          |
|    approx_kl            | 9.459898 |
|    clip_fraction        | 0.313    |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0.565    |
|    learning_rate        | 0.0001   |
|    loss                 | 78.6     |
|    n_updates            | 18090    |
|    policy_gradient_loss | 0.103    |
|    std                  | 3.22     |
|    value_loss           | 158      |
--------------------------------------
Iteration: 1810 | Episodes: 110250 | Median Reward: -6.01 | Max Reward: 49.22
Iteration: 1811 | Episodes: 110300 | Median Reward: -9.43 | Max Reward: 49.22
Iteration: 1811 | Episodes: 110350 | Median Reward: 44.99 | Max Reward: 49.22
Iteration: 1812 | Episodes: 110400 | Median Reward: 38.75 | Max Reward: 49.22
Iteration: 1813 | Episodes: 110450 | Median Reward: 16.99 | Max Reward: 49.22
Iteration: 1814 | Episodes: 110500 | Median Reward: -10.25 | Max Reward: 49.22
Iteration: 1815 | Episodes: 110550 | Median Reward: -2.08 | Max Reward: 49.22
Iteration: 1815 | Episodes: 110600 | Median Reward: -9.98 | Max Reward: 49.22
Iteration: 1816 | Episodes: 110650 | Median Reward: 3.51 | Max Reward: 49.22
Iteration: 1817 | Episodes: 110700 | Median Reward: -17.64 | Max Reward: 49.22
Iteration: 1818 | Episodes: 110750 | Median Reward: -17.64 | Max Reward: 49.22
Iteration: 1819 | Episodes: 110800 | Median Reward: 11.10 | Max Reward: 49.22
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -87.1    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1820     |
|    time_elapsed         | 33063    |
|    total_timesteps      | 11182080 |
| train/                  |          |
|    approx_kl            | 1.256871 |
|    clip_fraction        | 0.0478   |
|    clip_range           | 0.4      |
|    entropy_loss         | -109     |
|    explained_variance   | 0.549    |
|    learning_rate        | 0.0001   |
|    loss                 | 126      |
|    n_updates            | 18190    |
|    policy_gradient_loss | -0.0196  |
|    std                  | 3.23     |
|    value_loss           | 171      |
--------------------------------------
Iteration: 1820 | Episodes: 110850 | Median Reward: 38.89 | Max Reward: 49.22
Iteration: 1820 | Episodes: 110900 | Median Reward: 45.04 | Max Reward: 49.22
Iteration: 1821 | Episodes: 110950 | Median Reward: 42.34 | Max Reward: 49.22
Iteration: 1822 | Episodes: 111000 | Median Reward: 39.23 | Max Reward: 49.22
Iteration: 1823 | Episodes: 111050 | Median Reward: -11.47 | Max Reward: 49.22
Iteration: 1824 | Episodes: 111100 | Median Reward: -12.37 | Max Reward: 49.22
Iteration: 1824 | Episodes: 111150 | Median Reward: 18.68 | Max Reward: 49.22
Iteration: 1825 | Episodes: 111200 | Median Reward: 10.31 | Max Reward: 49.22
Iteration: 1826 | Episodes: 111250 | Median Reward: -10.01 | Max Reward: 49.22
Iteration: 1827 | Episodes: 111300 | Median Reward: -12.46 | Max Reward: 49.22
Iteration: 1828 | Episodes: 111350 | Median Reward: -12.46 | Max Reward: 49.22
Iteration: 1829 | Episodes: 111400 | Median Reward: 41.54 | Max Reward: 49.22
Iteration: 1829 | Episodes: 111450 | Median Reward: -19.19 | Max Reward: 49.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -91.1       |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 1830        |
|    time_elapsed         | 33243       |
|    total_timesteps      | 11243520    |
| train/                  |             |
|    approx_kl            | 0.051764872 |
|    clip_fraction        | 0.0134      |
|    clip_range           | 0.4         |
|    entropy_loss         | -108        |
|    explained_variance   | 0.542       |
|    learning_rate        | 0.0001      |
|    loss                 | 112         |
|    n_updates            | 18290       |
|    policy_gradient_loss | 0.000125    |
|    std                  | 3.23        |
|    value_loss           | 194         |
-----------------------------------------
Iteration: 1830 | Episodes: 111500 | Median Reward: -14.48 | Max Reward: 49.22
Iteration: 1831 | Episodes: 111550 | Median Reward: -9.12 | Max Reward: 49.22
Iteration: 1832 | Episodes: 111600 | Median Reward: 41.82 | Max Reward: 49.22
Iteration: 1833 | Episodes: 111650 | Median Reward: 44.90 | Max Reward: 49.22
Iteration: 1833 | Episodes: 111700 | Median Reward: 15.06 | Max Reward: 49.22
Iteration: 1834 | Episodes: 111750 | Median Reward: -15.21 | Max Reward: 49.22
Iteration: 1835 | Episodes: 111800 | Median Reward: -15.45 | Max Reward: 49.22
Iteration: 1836 | Episodes: 111850 | Median Reward: -15.92 | Max Reward: 49.22
Iteration: 1837 | Episodes: 111900 | Median Reward: -15.84 | Max Reward: 49.22
Iteration: 1838 | Episodes: 111950 | Median Reward: -8.06 | Max Reward: 49.22
Iteration: 1838 | Episodes: 112000 | Median Reward: -23.49 | Max Reward: 49.22
Iteration: 1839 | Episodes: 112050 | Median Reward: 40.81 | Max Reward: 49.22
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -80.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1840      |
|    time_elapsed         | 33430     |
|    total_timesteps      | 11304960  |
| train/                  |           |
|    approx_kl            | 7.7362037 |
|    clip_fraction        | 0.076     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.558     |
|    learning_rate        | 0.0001    |
|    loss                 | 71.9      |
|    n_updates            | 18390     |
|    policy_gradient_loss | 0.00397   |
|    std                  | 3.24      |
|    value_loss           | 132       |
---------------------------------------
Iteration: 1840 | Episodes: 112100 | Median Reward: 39.14 | Max Reward: 49.22
Iteration: 1841 | Episodes: 112150 | Median Reward: -13.38 | Max Reward: 49.25
Iteration: 1842 | Episodes: 112200 | Median Reward: -13.38 | Max Reward: 49.25
Iteration: 1842 | Episodes: 112250 | Median Reward: -7.99 | Max Reward: 49.25
Iteration: 1843 | Episodes: 112300 | Median Reward: -27.56 | Max Reward: 49.25
Iteration: 1844 | Episodes: 112350 | Median Reward: -21.64 | Max Reward: 49.25
Iteration: 1845 | Episodes: 112400 | Median Reward: -15.08 | Max Reward: 49.25
Iteration: 1846 | Episodes: 112450 | Median Reward: -20.60 | Max Reward: 49.25
Iteration: 1847 | Episodes: 112500 | Median Reward: -18.79 | Max Reward: 49.25
Iteration: 1847 | Episodes: 112550 | Median Reward: -23.24 | Max Reward: 49.25
Iteration: 1848 | Episodes: 112600 | Median Reward: -20.77 | Max Reward: 49.25
Iteration: 1849 | Episodes: 112650 | Median Reward: 5.95 | Max Reward: 49.25
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 90        |
|    ep_rew_mean          | -72.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1850      |
|    time_elapsed         | 33610     |
|    total_timesteps      | 11366400  |
| train/                  |           |
|    approx_kl            | 6.7945886 |
|    clip_fraction        | 0.14      |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.546     |
|    learning_rate        | 0.0001    |
|    loss                 | 84.2      |
|    n_updates            | 18490     |
|    policy_gradient_loss | 0.0497    |
|    std                  | 3.25      |
|    value_loss           | 134       |
---------------------------------------
Iteration: 1850 | Episodes: 112700 | Median Reward: 36.75 | Max Reward: 49.25
Iteration: 1850 | Episodes: 112750 | Median Reward: -14.30 | Max Reward: 49.25
Iteration: 1851 | Episodes: 112800 | Median Reward: 4.10 | Max Reward: 49.25
Iteration: 1852 | Episodes: 112850 | Median Reward: -9.31 | Max Reward: 49.25
Iteration: 1853 | Episodes: 112900 | Median Reward: -20.69 | Max Reward: 49.25
Iteration: 1854 | Episodes: 112950 | Median Reward: -24.15 | Max Reward: 49.25
Iteration: 1855 | Episodes: 113000 | Median Reward: -15.68 | Max Reward: 49.25
Iteration: 1855 | Episodes: 113050 | Median Reward: -18.14 | Max Reward: 49.25
Iteration: 1856 | Episodes: 113100 | Median Reward: -22.64 | Max Reward: 49.25
Iteration: 1857 | Episodes: 113150 | Median Reward: -20.20 | Max Reward: 49.25
Iteration: 1858 | Episodes: 113200 | Median Reward: -6.50 | Max Reward: 49.25
Iteration: 1859 | Episodes: 113250 | Median Reward: 2.88 | Max Reward: 49.25
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -93.5      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1860       |
|    time_elapsed         | 33788      |
|    total_timesteps      | 11427840   |
| train/                  |            |
|    approx_kl            | 0.92910916 |
|    clip_fraction        | 0.0661     |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.0001     |
|    loss                 | 82.3       |
|    n_updates            | 18590      |
|    policy_gradient_loss | 0.014      |
|    std                  | 3.25       |
|    value_loss           | 131        |
----------------------------------------
Iteration: 1860 | Episodes: 113300 | Median Reward: -15.91 | Max Reward: 49.25
Iteration: 1860 | Episodes: 113350 | Median Reward: 39.19 | Max Reward: 49.25
Iteration: 1861 | Episodes: 113400 | Median Reward: -4.97 | Max Reward: 49.25
Iteration: 1862 | Episodes: 113450 | Median Reward: -14.29 | Max Reward: 49.25
Iteration: 1863 | Episodes: 113500 | Median Reward: -10.04 | Max Reward: 49.25
Iteration: 1864 | Episodes: 113550 | Median Reward: -14.58 | Max Reward: 49.25
Iteration: 1864 | Episodes: 113600 | Median Reward: -15.06 | Max Reward: 49.25
Iteration: 1865 | Episodes: 113650 | Median Reward: 6.99 | Max Reward: 49.25
Iteration: 1866 | Episodes: 113700 | Median Reward: 1.68 | Max Reward: 49.25
Iteration: 1867 | Episodes: 113750 | Median Reward: -6.83 | Max Reward: 49.25
Iteration: 1868 | Episodes: 113800 | Median Reward: -5.80 | Max Reward: 49.25
Iteration: 1869 | Episodes: 113850 | Median Reward: -11.18 | Max Reward: 49.25
Iteration: 1869 | Episodes: 113900 | Median Reward: 2.22 | Max Reward: 49.25
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -95.1     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1870      |
|    time_elapsed         | 33974     |
|    total_timesteps      | 11489280  |
| train/                  |           |
|    approx_kl            | 0.3605416 |
|    clip_fraction        | 0.0462    |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.588     |
|    learning_rate        | 0.0001    |
|    loss                 | 67.8      |
|    n_updates            | 18690     |
|    policy_gradient_loss | 0.00224   |
|    std                  | 3.25      |
|    value_loss           | 121       |
---------------------------------------
Iteration: 1870 | Episodes: 113950 | Median Reward: -14.35 | Max Reward: 49.25
Iteration: 1871 | Episodes: 114000 | Median Reward: -5.43 | Max Reward: 49.25
Iteration: 1872 | Episodes: 114050 | Median Reward: -5.43 | Max Reward: 49.25
Iteration: 1873 | Episodes: 114100 | Median Reward: -11.35 | Max Reward: 49.25
Iteration: 1873 | Episodes: 114150 | Median Reward: 13.02 | Max Reward: 49.25
Iteration: 1874 | Episodes: 114200 | Median Reward: 18.44 | Max Reward: 49.25
Iteration: 1875 | Episodes: 114250 | Median Reward: -11.71 | Max Reward: 49.25
Iteration: 1876 | Episodes: 114300 | Median Reward: -8.57 | Max Reward: 49.25
Iteration: 1877 | Episodes: 114350 | Median Reward: 9.78 | Max Reward: 49.25
Iteration: 1877 | Episodes: 114400 | Median Reward: -2.22 | Max Reward: 49.25
Iteration: 1878 | Episodes: 114450 | Median Reward: -5.62 | Max Reward: 49.25
Iteration: 1879 | Episodes: 114500 | Median Reward: -0.56 | Max Reward: 49.25
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -92.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1880      |
|    time_elapsed         | 34155     |
|    total_timesteps      | 11550720  |
| train/                  |           |
|    approx_kl            | 0.7977679 |
|    clip_fraction        | 0.0657    |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.563     |
|    learning_rate        | 0.0001    |
|    loss                 | 80.3      |
|    n_updates            | 18790     |
|    policy_gradient_loss | -0.0197   |
|    std                  | 3.26      |
|    value_loss           | 144       |
---------------------------------------
Iteration: 1880 | Episodes: 114550 | Median Reward: -13.92 | Max Reward: 49.25
Iteration: 1881 | Episodes: 114600 | Median Reward: -13.92 | Max Reward: 49.25
Iteration: 1881 | Episodes: 114650 | Median Reward: 1.35 | Max Reward: 49.25
Iteration: 1882 | Episodes: 114700 | Median Reward: -10.70 | Max Reward: 49.25
Iteration: 1883 | Episodes: 114750 | Median Reward: -0.15 | Max Reward: 49.25
Iteration: 1884 | Episodes: 114800 | Median Reward: -7.99 | Max Reward: 49.25
Iteration: 1885 | Episodes: 114850 | Median Reward: -11.55 | Max Reward: 49.25
Iteration: 1886 | Episodes: 114900 | Median Reward: -11.68 | Max Reward: 49.25
Iteration: 1886 | Episodes: 114950 | Median Reward: -13.57 | Max Reward: 49.25
Iteration: 1887 | Episodes: 115000 | Median Reward: 8.42 | Max Reward: 49.25
Iteration: 1888 | Episodes: 115050 | Median Reward: -10.10 | Max Reward: 49.25
Iteration: 1889 | Episodes: 115100 | Median Reward: -14.57 | Max Reward: 49.25
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -107       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1890       |
|    time_elapsed         | 34339      |
|    total_timesteps      | 11612160   |
| train/                  |            |
|    approx_kl            | 0.16688421 |
|    clip_fraction        | 0.0563     |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0001     |
|    loss                 | 19.4       |
|    n_updates            | 18890      |
|    policy_gradient_loss | -0.0297    |
|    std                  | 3.26       |
|    value_loss           | 47.3       |
----------------------------------------
Iteration: 1890 | Episodes: 115150 | Median Reward: -0.74 | Max Reward: 49.25
Iteration: 1891 | Episodes: 115200 | Median Reward: 14.06 | Max Reward: 49.25
Iteration: 1891 | Episodes: 115250 | Median Reward: 5.96 | Max Reward: 49.25
Iteration: 1892 | Episodes: 115300 | Median Reward: -20.04 | Max Reward: 49.25
Iteration: 1893 | Episodes: 115350 | Median Reward: -16.92 | Max Reward: 49.25
Iteration: 1894 | Episodes: 115400 | Median Reward: -3.01 | Max Reward: 49.25
Iteration: 1895 | Episodes: 115450 | Median Reward: -16.98 | Max Reward: 49.25
Iteration: 1895 | Episodes: 115500 | Median Reward: -8.42 | Max Reward: 49.25
Iteration: 1896 | Episodes: 115550 | Median Reward: -6.43 | Max Reward: 49.25
Iteration: 1897 | Episodes: 115600 | Median Reward: -10.91 | Max Reward: 49.25
Iteration: 1898 | Episodes: 115650 | Median Reward: -8.71 | Max Reward: 49.25
Iteration: 1899 | Episodes: 115700 | Median Reward: -3.04 | Max Reward: 49.25
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -104      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1900      |
|    time_elapsed         | 34528     |
|    total_timesteps      | 11673600  |
| train/                  |           |
|    approx_kl            | 0.1593433 |
|    clip_fraction        | 0.114     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.598     |
|    learning_rate        | 0.0001    |
|    loss                 | 39.6      |
|    n_updates            | 18990     |
|    policy_gradient_loss | 0.0266    |
|    std                  | 3.27      |
|    value_loss           | 107       |
---------------------------------------
Iteration: 1900 | Episodes: 115750 | Median Reward: -16.47 | Max Reward: 49.25
Iteration: 1900 | Episodes: 115800 | Median Reward: -15.49 | Max Reward: 49.25
Iteration: 1901 | Episodes: 115850 | Median Reward: -12.82 | Max Reward: 49.25
Iteration: 1902 | Episodes: 115900 | Median Reward: -6.49 | Max Reward: 49.25
Iteration: 1903 | Episodes: 115950 | Median Reward: 24.55 | Max Reward: 49.25
Iteration: 1903 | Episodes: 116000 | Median Reward: 39.44 | Max Reward: 49.25
Iteration: 1904 | Episodes: 116050 | Median Reward: -4.87 | Max Reward: 49.25
Iteration: 1905 | Episodes: 116100 | Median Reward: -1.06 | Max Reward: 49.25
Iteration: 1906 | Episodes: 116150 | Median Reward: -5.90 | Max Reward: 49.25
Iteration: 1907 | Episodes: 116200 | Median Reward: -6.55 | Max Reward: 49.25
Iteration: 1908 | Episodes: 116250 | Median Reward: 2.24 | Max Reward: 49.25
Iteration: 1908 | Episodes: 116300 | Median Reward: -15.37 | Max Reward: 49.25
Iteration: 1909 | Episodes: 116350 | Median Reward: 29.56 | Max Reward: 49.25
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -86.9    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1910     |
|    time_elapsed         | 34706    |
|    total_timesteps      | 11735040 |
| train/                  |          |
|    approx_kl            | 5.378924 |
|    clip_fraction        | 0.126    |
|    clip_range           | 0.4      |
|    entropy_loss         | -106     |
|    explained_variance   | 0.626    |
|    learning_rate        | 0.0001   |
|    loss                 | 64.1     |
|    n_updates            | 19090    |
|    policy_gradient_loss | -0.00988 |
|    std                  | 3.28     |
|    value_loss           | 97.7     |
--------------------------------------
Iteration: 1910 | Episodes: 116400 | Median Reward: -4.84 | Max Reward: 49.25
Iteration: 1911 | Episodes: 116450 | Median Reward: -11.46 | Max Reward: 49.25
Iteration: 1912 | Episodes: 116500 | Median Reward: -6.50 | Max Reward: 49.25
Iteration: 1912 | Episodes: 116550 | Median Reward: -16.50 | Max Reward: 49.25
Iteration: 1913 | Episodes: 116600 | Median Reward: 37.35 | Max Reward: 49.25
Iteration: 1914 | Episodes: 116650 | Median Reward: 7.83 | Max Reward: 49.25
Iteration: 1915 | Episodes: 116700 | Median Reward: -6.43 | Max Reward: 49.25
Iteration: 1915 | Episodes: 116750 | Median Reward: -16.69 | Max Reward: 49.25
Iteration: 1916 | Episodes: 116800 | Median Reward: -9.54 | Max Reward: 49.25
Iteration: 1917 | Episodes: 116850 | Median Reward: -12.62 | Max Reward: 49.25
Iteration: 1918 | Episodes: 116900 | Median Reward: -10.96 | Max Reward: 49.25
Iteration: 1919 | Episodes: 116950 | Median Reward: -2.18 | Max Reward: 49.25
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -97.6    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1920     |
|    time_elapsed         | 34895    |
|    total_timesteps      | 11796480 |
| train/                  |          |
|    approx_kl            | 2.428896 |
|    clip_fraction        | 0.096    |
|    clip_range           | 0.4      |
|    entropy_loss         | -108     |
|    explained_variance   | 0.684    |
|    learning_rate        | 0.0001   |
|    loss                 | 24.8     |
|    n_updates            | 19190    |
|    policy_gradient_loss | 0.016    |
|    std                  | 3.29     |
|    value_loss           | 79.8     |
--------------------------------------
Iteration: 1920 | Episodes: 117000 | Median Reward: 0.22 | Max Reward: 49.25
Iteration: 1920 | Episodes: 117050 | Median Reward: -8.39 | Max Reward: 49.25
Iteration: 1921 | Episodes: 117100 | Median Reward: -16.01 | Max Reward: 49.25
Iteration: 1922 | Episodes: 117150 | Median Reward: -8.49 | Max Reward: 49.25
Iteration: 1923 | Episodes: 117200 | Median Reward: -4.81 | Max Reward: 49.25
Iteration: 1924 | Episodes: 117250 | Median Reward: 22.46 | Max Reward: 49.25
Iteration: 1924 | Episodes: 117300 | Median Reward: -22.75 | Max Reward: 49.25
Iteration: 1925 | Episodes: 117350 | Median Reward: -4.67 | Max Reward: 49.25
Iteration: 1926 | Episodes: 117400 | Median Reward: 0.90 | Max Reward: 49.25
Iteration: 1927 | Episodes: 117450 | Median Reward: -1.56 | Max Reward: 49.25
Iteration: 1928 | Episodes: 117500 | Median Reward: -8.36 | Max Reward: 49.25
Iteration: 1929 | Episodes: 117550 | Median Reward: -9.34 | Max Reward: 49.25
Iteration: 1929 | Episodes: 117600 | Median Reward: -13.74 | Max Reward: 49.25
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -103       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 1930       |
|    time_elapsed         | 35073      |
|    total_timesteps      | 11857920   |
| train/                  |            |
|    approx_kl            | 0.59684485 |
|    clip_fraction        | 0.0627     |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | 0.647      |
|    learning_rate        | 0.0001     |
|    loss                 | 12.7       |
|    n_updates            | 19290      |
|    policy_gradient_loss | -0.0131    |
|    std                  | 3.29       |
|    value_loss           | 94.3       |
----------------------------------------
Iteration: 1930 | Episodes: 117650 | Median Reward: -17.42 | Max Reward: 49.25
Iteration: 1931 | Episodes: 117700 | Median Reward: -18.83 | Max Reward: 49.25
Iteration: 1932 | Episodes: 117750 | Median Reward: -9.25 | Max Reward: 49.25
Iteration: 1933 | Episodes: 117800 | Median Reward: -1.03 | Max Reward: 49.25
Iteration: 1934 | Episodes: 117850 | Median Reward: -17.72 | Max Reward: 49.25
Iteration: 1934 | Episodes: 117900 | Median Reward: -9.53 | Max Reward: 49.25
Iteration: 1935 | Episodes: 117950 | Median Reward: -12.74 | Max Reward: 49.25
Iteration: 1936 | Episodes: 118000 | Median Reward: -12.74 | Max Reward: 49.25
Iteration: 1937 | Episodes: 118050 | Median Reward: -7.02 | Max Reward: 49.25
Iteration: 1938 | Episodes: 118100 | Median Reward: -2.49 | Max Reward: 49.25
Iteration: 1938 | Episodes: 118150 | Median Reward: -10.78 | Max Reward: 49.25
Iteration: 1939 | Episodes: 118200 | Median Reward: -10.60 | Max Reward: 49.25
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -99.7     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1940      |
|    time_elapsed         | 35254     |
|    total_timesteps      | 11919360  |
| train/                  |           |
|    approx_kl            | 9.6686735 |
|    clip_fraction        | 0.0927    |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.59      |
|    learning_rate        | 0.0001    |
|    loss                 | 61.6      |
|    n_updates            | 19390     |
|    policy_gradient_loss | 0.0219    |
|    std                  | 3.3       |
|    value_loss           | 132       |
---------------------------------------
Iteration: 1940 | Episodes: 118250 | Median Reward: -3.59 | Max Reward: 49.25
Iteration: 1941 | Episodes: 118300 | Median Reward: -4.16 | Max Reward: 49.25
Iteration: 1942 | Episodes: 118350 | Median Reward: -10.78 | Max Reward: 49.25
Iteration: 1943 | Episodes: 118400 | Median Reward: -11.46 | Max Reward: 49.25
Iteration: 1943 | Episodes: 118450 | Median Reward: -20.17 | Max Reward: 49.25
Iteration: 1944 | Episodes: 118500 | Median Reward: -11.99 | Max Reward: 49.25
Iteration: 1945 | Episodes: 118550 | Median Reward: 0.29 | Max Reward: 49.25
Iteration: 1946 | Episodes: 118600 | Median Reward: -12.02 | Max Reward: 49.25
Iteration: 1947 | Episodes: 118650 | Median Reward: -12.24 | Max Reward: 49.25
Iteration: 1947 | Episodes: 118700 | Median Reward: 0.69 | Max Reward: 49.25
Iteration: 1948 | Episodes: 118750 | Median Reward: -6.29 | Max Reward: 49.25
Iteration: 1949 | Episodes: 118800 | Median Reward: 17.47 | Max Reward: 49.25
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -90.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1950      |
|    time_elapsed         | 35443     |
|    total_timesteps      | 11980800  |
| train/                  |           |
|    approx_kl            | 3.5734086 |
|    clip_fraction        | 0.157     |
|    clip_range           | 0.4       |
|    entropy_loss         | -106      |
|    explained_variance   | 0.615     |
|    learning_rate        | 0.0001    |
|    loss                 | 16.2      |
|    n_updates            | 19490     |
|    policy_gradient_loss | 0.0276    |
|    std                  | 3.31      |
|    value_loss           | 98.9      |
---------------------------------------
Iteration: 1950 | Episodes: 118850 | Median Reward: -2.11 | Max Reward: 49.25
Iteration: 1951 | Episodes: 118900 | Median Reward: -15.66 | Max Reward: 49.25
Iteration: 1952 | Episodes: 118950 | Median Reward: 11.76 | Max Reward: 49.25
Iteration: 1952 | Episodes: 119000 | Median Reward: 10.02 | Max Reward: 49.25
Iteration: 1953 | Episodes: 119050 | Median Reward: 6.39 | Max Reward: 49.25
Iteration: 1954 | Episodes: 119100 | Median Reward: -18.38 | Max Reward: 49.25
Iteration: 1955 | Episodes: 119150 | Median Reward: -15.70 | Max Reward: 49.25
Iteration: 1956 | Episodes: 119200 | Median Reward: -13.48 | Max Reward: 49.25
Iteration: 1957 | Episodes: 119250 | Median Reward: -23.51 | Max Reward: 49.25
Iteration: 1957 | Episodes: 119300 | Median Reward: -17.06 | Max Reward: 49.25
Iteration: 1958 | Episodes: 119350 | Median Reward: -14.11 | Max Reward: 49.25
Iteration: 1959 | Episodes: 119400 | Median Reward: -12.25 | Max Reward: 49.25
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -103      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1960      |
|    time_elapsed         | 35624     |
|    total_timesteps      | 12042240  |
| train/                  |           |
|    approx_kl            | 1.4573575 |
|    clip_fraction        | 0.143     |
|    clip_range           | 0.4       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.598     |
|    learning_rate        | 0.0001    |
|    loss                 | 66.4      |
|    n_updates            | 19590     |
|    policy_gradient_loss | 0.00414   |
|    std                  | 3.32      |
|    value_loss           | 128       |
---------------------------------------
Iteration: 1960 | Episodes: 119450 | Median Reward: -13.29 | Max Reward: 49.25
Iteration: 1961 | Episodes: 119500 | Median Reward: -4.14 | Max Reward: 49.25
Iteration: 1961 | Episodes: 119550 | Median Reward: -14.50 | Max Reward: 49.25
Iteration: 1962 | Episodes: 119600 | Median Reward: -7.45 | Max Reward: 49.25
Iteration: 1963 | Episodes: 119650 | Median Reward: 5.61 | Max Reward: 49.25
Iteration: 1964 | Episodes: 119700 | Median Reward: 5.93 | Max Reward: 49.25
Iteration: 1965 | Episodes: 119750 | Median Reward: 3.94 | Max Reward: 49.26
Iteration: 1966 | Episodes: 119800 | Median Reward: 3.94 | Max Reward: 49.26
Iteration: 1966 | Episodes: 119850 | Median Reward: -12.68 | Max Reward: 49.26
Iteration: 1967 | Episodes: 119900 | Median Reward: -7.72 | Max Reward: 49.26
Iteration: 1968 | Episodes: 119950 | Median Reward: 3.46 | Max Reward: 49.26
Iteration: 1969 | Episodes: 120000 | Median Reward: 5.53 | Max Reward: 49.26
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -101      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 1970      |
|    time_elapsed         | 35804     |
|    total_timesteps      | 12103680  |
| train/                  |           |
|    approx_kl            | 2.9869952 |
|    clip_fraction        | 0.17      |
|    clip_range           | 0.4       |
|    entropy_loss         | -107      |
|    explained_variance   | 0.593     |
|    learning_rate        | 0.0001    |
|    loss                 | 25.2      |
|    n_updates            | 19690     |
|    policy_gradient_loss | 0.00843   |
|    std                  | 3.33      |
|    value_loss           | 161       |
---------------------------------------
Iteration: 1970 | Episodes: 120050 | Median Reward: -12.73 | Max Reward: 49.26
Iteration: 1970 | Episodes: 120100 | Median Reward: -1.58 | Max Reward: 49.26
Iteration: 1971 | Episodes: 120150 | Median Reward: 6.87 | Max Reward: 49.26
Iteration: 1972 | Episodes: 120200 | Median Reward: 12.30 | Max Reward: 49.26
Iteration: 1973 | Episodes: 120250 | Median Reward: -15.16 | Max Reward: 49.26
Iteration: 1974 | Episodes: 120300 | Median Reward: -15.55 | Max Reward: 49.26
Iteration: 1975 | Episodes: 120350 | Median Reward: -7.07 | Max Reward: 49.26
Iteration: 1975 | Episodes: 120400 | Median Reward: -7.97 | Max Reward: 49.26
Iteration: 1976 | Episodes: 120450 | Median Reward: 1.71 | Max Reward: 49.26
Iteration: 1977 | Episodes: 120500 | Median Reward: 18.12 | Max Reward: 49.26
Iteration: 1978 | Episodes: 120550 | Median Reward: -5.05 | Max Reward: 49.26
Iteration: 1979 | Episodes: 120600 | Median Reward: -11.39 | Max Reward: 49.26
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -102     |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1980     |
|    time_elapsed         | 35989    |
|    total_timesteps      | 12165120 |
| train/                  |          |
|    approx_kl            | 6.823755 |
|    clip_fraction        | 0.174    |
|    clip_range           | 0.4      |
|    entropy_loss         | -107     |
|    explained_variance   | 0.675    |
|    learning_rate        | 0.0001   |
|    loss                 | 7.08     |
|    n_updates            | 19790    |
|    policy_gradient_loss | 0.0319   |
|    std                  | 3.34     |
|    value_loss           | 68.5     |
--------------------------------------
Iteration: 1980 | Episodes: 120650 | Median Reward: -7.86 | Max Reward: 49.26
Iteration: 1980 | Episodes: 120700 | Median Reward: -12.18 | Max Reward: 49.26
Iteration: 1981 | Episodes: 120750 | Median Reward: -15.55 | Max Reward: 49.26
Iteration: 1982 | Episodes: 120800 | Median Reward: -7.33 | Max Reward: 49.26
Iteration: 1983 | Episodes: 120850 | Median Reward: -8.53 | Max Reward: 49.26
Iteration: 1984 | Episodes: 120900 | Median Reward: -15.08 | Max Reward: 49.26
Iteration: 1984 | Episodes: 120950 | Median Reward: -16.07 | Max Reward: 49.26
Iteration: 1985 | Episodes: 121000 | Median Reward: -3.95 | Max Reward: 49.29
Iteration: 1986 | Episodes: 121050 | Median Reward: 11.66 | Max Reward: 49.29
Iteration: 1987 | Episodes: 121100 | Median Reward: 2.61 | Max Reward: 49.29
Iteration: 1988 | Episodes: 121150 | Median Reward: -9.54 | Max Reward: 49.29
Iteration: 1989 | Episodes: 121200 | Median Reward: 14.69 | Max Reward: 49.29
Iteration: 1989 | Episodes: 121250 | Median Reward: -11.93 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -97.1    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 1990     |
|    time_elapsed         | 36164    |
|    total_timesteps      | 12226560 |
| train/                  |          |
|    approx_kl            | 2.013901 |
|    clip_fraction        | 0.0673   |
|    clip_range           | 0.4      |
|    entropy_loss         | -107     |
|    explained_variance   | 0.585    |
|    learning_rate        | 0.0001   |
|    loss                 | 86.4     |
|    n_updates            | 19890    |
|    policy_gradient_loss | -0.00481 |
|    std                  | 3.34     |
|    value_loss           | 151      |
--------------------------------------
Iteration: 1990 | Episodes: 121300 | Median Reward: -11.39 | Max Reward: 49.29
Iteration: 1991 | Episodes: 121350 | Median Reward: 11.10 | Max Reward: 49.29
Iteration: 1992 | Episodes: 121400 | Median Reward: 13.99 | Max Reward: 49.29
Iteration: 1992 | Episodes: 121450 | Median Reward: 20.76 | Max Reward: 49.29
Iteration: 1993 | Episodes: 121500 | Median Reward: -15.23 | Max Reward: 49.29
Iteration: 1994 | Episodes: 121550 | Median Reward: -4.40 | Max Reward: 49.29
Iteration: 1995 | Episodes: 121600 | Median Reward: -5.20 | Max Reward: 49.29
Iteration: 1996 | Episodes: 121650 | Median Reward: -8.02 | Max Reward: 49.29
Iteration: 1997 | Episodes: 121700 | Median Reward: -6.69 | Max Reward: 49.29
Iteration: 1997 | Episodes: 121750 | Median Reward: -14.90 | Max Reward: 49.29
Iteration: 1998 | Episodes: 121800 | Median Reward: -20.09 | Max Reward: 49.29
Iteration: 1999 | Episodes: 121850 | Median Reward: -13.89 | Max Reward: 49.29
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 86         |
|    ep_rew_mean          | -72        |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2000       |
|    time_elapsed         | 36345      |
|    total_timesteps      | 12288000   |
| train/                  |            |
|    approx_kl            | 0.38258344 |
|    clip_fraction        | 0.0169     |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0001     |
|    loss                 | 12.4       |
|    n_updates            | 19990      |
|    policy_gradient_loss | -0.00187   |
|    std                  | 3.35       |
|    value_loss           | 40.9       |
----------------------------------------
Iteration: 2000 | Episodes: 121900 | Median Reward: 35.06 | Max Reward: 49.29
Iteration: 2000 | Episodes: 121950 | Median Reward: 41.95 | Max Reward: 49.29
Iteration: 2001 | Episodes: 122000 | Median Reward: -9.58 | Max Reward: 49.29
Iteration: 2002 | Episodes: 122050 | Median Reward: -10.90 | Max Reward: 49.29
Iteration: 2003 | Episodes: 122100 | Median Reward: 41.76 | Max Reward: 49.29
Iteration: 2003 | Episodes: 122150 | Median Reward: -19.43 | Max Reward: 49.29
Iteration: 2004 | Episodes: 122200 | Median Reward: -15.26 | Max Reward: 49.29
Iteration: 2005 | Episodes: 122250 | Median Reward: -13.97 | Max Reward: 49.29
Iteration: 2006 | Episodes: 122300 | Median Reward: -14.56 | Max Reward: 49.29
Iteration: 2007 | Episodes: 122350 | Median Reward: -14.56 | Max Reward: 49.29
Iteration: 2008 | Episodes: 122400 | Median Reward: 8.90 | Max Reward: 49.29
Iteration: 2008 | Episodes: 122450 | Median Reward: -6.63 | Max Reward: 49.29
Iteration: 2009 | Episodes: 122500 | Median Reward: -11.11 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -94      |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2010     |
|    time_elapsed         | 36533    |
|    total_timesteps      | 12349440 |
| train/                  |          |
|    approx_kl            | 2.242621 |
|    clip_fraction        | 0.125    |
|    clip_range           | 0.4      |
|    entropy_loss         | -109     |
|    explained_variance   | 0.645    |
|    learning_rate        | 0.0001   |
|    loss                 | 21       |
|    n_updates            | 20090    |
|    policy_gradient_loss | 0.0038   |
|    std                  | 3.36     |
|    value_loss           | 98.7     |
--------------------------------------
Iteration: 2010 | Episodes: 122550 | Median Reward: -11.91 | Max Reward: 49.29
Iteration: 2011 | Episodes: 122600 | Median Reward: -23.68 | Max Reward: 49.29
Iteration: 2012 | Episodes: 122650 | Median Reward: -14.84 | Max Reward: 49.29
Iteration: 2013 | Episodes: 122700 | Median Reward: -24.59 | Max Reward: 49.29
Iteration: 2013 | Episodes: 122750 | Median Reward: 2.08 | Max Reward: 49.29
Iteration: 2014 | Episodes: 122800 | Median Reward: 38.56 | Max Reward: 49.29
Iteration: 2015 | Episodes: 122850 | Median Reward: 11.42 | Max Reward: 49.29
Iteration: 2016 | Episodes: 122900 | Median Reward: -15.51 | Max Reward: 49.29
Iteration: 2017 | Episodes: 122950 | Median Reward: 39.80 | Max Reward: 49.29
Iteration: 2017 | Episodes: 123000 | Median Reward: -24.32 | Max Reward: 49.29
Iteration: 2018 | Episodes: 123050 | Median Reward: -6.65 | Max Reward: 49.29
Iteration: 2019 | Episodes: 123100 | Median Reward: 41.38 | Max Reward: 49.29
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -81.9      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2020       |
|    time_elapsed         | 36716      |
|    total_timesteps      | 12410880   |
| train/                  |            |
|    approx_kl            | 12.3677635 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.4        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0001     |
|    loss                 | 38.2       |
|    n_updates            | 20190      |
|    policy_gradient_loss | -0.00804   |
|    std                  | 3.37       |
|    value_loss           | 89.7       |
----------------------------------------
Iteration: 2020 | Episodes: 123150 | Median Reward: 41.38 | Max Reward: 49.29
Iteration: 2021 | Episodes: 123200 | Median Reward: -10.14 | Max Reward: 49.29
Iteration: 2022 | Episodes: 123250 | Median Reward: 36.81 | Max Reward: 49.29
Iteration: 2022 | Episodes: 123300 | Median Reward: -6.57 | Max Reward: 49.29
Iteration: 2023 | Episodes: 123350 | Median Reward: -9.01 | Max Reward: 49.29
Iteration: 2024 | Episodes: 123400 | Median Reward: -9.33 | Max Reward: 49.29
Iteration: 2025 | Episodes: 123450 | Median Reward: 21.65 | Max Reward: 49.29
Iteration: 2026 | Episodes: 123500 | Median Reward: 37.92 | Max Reward: 49.29
Iteration: 2026 | Episodes: 123550 | Median Reward: -3.60 | Max Reward: 49.29
Iteration: 2027 | Episodes: 123600 | Median Reward: 16.53 | Max Reward: 49.29
Iteration: 2028 | Episodes: 123650 | Median Reward: 45.44 | Max Reward: 49.29
Iteration: 2029 | Episodes: 123700 | Median Reward: 41.54 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -85.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2030      |
|    time_elapsed         | 36904     |
|    total_timesteps      | 12472320  |
| train/                  |           |
|    approx_kl            | 11.795685 |
|    clip_fraction        | 0.196     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.505     |
|    learning_rate        | 0.0001    |
|    loss                 | 170       |
|    n_updates            | 20290     |
|    policy_gradient_loss | -0.0198   |
|    std                  | 3.38      |
|    value_loss           | 310       |
---------------------------------------
Iteration: 2030 | Episodes: 123750 | Median Reward: -3.03 | Max Reward: 49.29
Iteration: 2031 | Episodes: 123800 | Median Reward: 42.38 | Max Reward: 49.29
Iteration: 2031 | Episodes: 123850 | Median Reward: 20.78 | Max Reward: 49.29
Iteration: 2032 | Episodes: 123900 | Median Reward: 39.09 | Max Reward: 49.29
Iteration: 2033 | Episodes: 123950 | Median Reward: 8.37 | Max Reward: 49.29
Iteration: 2034 | Episodes: 124000 | Median Reward: 40.36 | Max Reward: 49.29
Iteration: 2035 | Episodes: 124050 | Median Reward: 37.92 | Max Reward: 49.29
Iteration: 2036 | Episodes: 124100 | Median Reward: -17.56 | Max Reward: 49.29
Iteration: 2036 | Episodes: 124150 | Median Reward: 40.10 | Max Reward: 49.29
Iteration: 2037 | Episodes: 124200 | Median Reward: -13.05 | Max Reward: 49.29
Iteration: 2038 | Episodes: 124250 | Median Reward: -0.13 | Max Reward: 49.29
Iteration: 2039 | Episodes: 124300 | Median Reward: 35.65 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -86.8    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2040     |
|    time_elapsed         | 37085    |
|    total_timesteps      | 12533760 |
| train/                  |          |
|    approx_kl            | 9.732883 |
|    clip_fraction        | 0.217    |
|    clip_range           | 0.4      |
|    entropy_loss         | -109     |
|    explained_variance   | 0.562    |
|    learning_rate        | 0.0001   |
|    loss                 | 73.7     |
|    n_updates            | 20390    |
|    policy_gradient_loss | -0.0457  |
|    std                  | 3.39     |
|    value_loss           | 199      |
--------------------------------------
Iteration: 2040 | Episodes: 124350 | Median Reward: -11.38 | Max Reward: 49.29
Iteration: 2040 | Episodes: 124400 | Median Reward: -9.66 | Max Reward: 49.29
Iteration: 2041 | Episodes: 124450 | Median Reward: -11.25 | Max Reward: 49.29
Iteration: 2042 | Episodes: 124500 | Median Reward: 40.00 | Max Reward: 49.29
Iteration: 2043 | Episodes: 124550 | Median Reward: 38.97 | Max Reward: 49.29
Iteration: 2044 | Episodes: 124600 | Median Reward: 37.63 | Max Reward: 49.29
Iteration: 2045 | Episodes: 124650 | Median Reward: 12.37 | Max Reward: 49.29
Iteration: 2045 | Episodes: 124700 | Median Reward: -10.63 | Max Reward: 49.29
Iteration: 2046 | Episodes: 124750 | Median Reward: 37.04 | Max Reward: 49.29
Iteration: 2047 | Episodes: 124800 | Median Reward: 41.22 | Max Reward: 49.29
Iteration: 2048 | Episodes: 124850 | Median Reward: 41.34 | Max Reward: 49.29
Iteration: 2049 | Episodes: 124900 | Median Reward: 41.00 | Max Reward: 49.29
Iteration: 2049 | Episodes: 124950 | Median Reward: 38.50 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2050      |
|    time_elapsed         | 37266     |
|    total_timesteps      | 12595200  |
| train/                  |           |
|    approx_kl            | 3.1697907 |
|    clip_fraction        | 0.103     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.566     |
|    learning_rate        | 0.0001    |
|    loss                 | 87        |
|    n_updates            | 20490     |
|    policy_gradient_loss | -0.00475  |
|    std                  | 3.39      |
|    value_loss           | 211       |
---------------------------------------
Iteration: 2050 | Episodes: 125000 | Median Reward: 38.88 | Max Reward: 49.29
Iteration: 2051 | Episodes: 125050 | Median Reward: 38.68 | Max Reward: 49.29
Iteration: 2052 | Episodes: 125100 | Median Reward: 38.68 | Max Reward: 49.29
Iteration: 2053 | Episodes: 125150 | Median Reward: 40.46 | Max Reward: 49.29
Iteration: 2054 | Episodes: 125200 | Median Reward: 39.92 | Max Reward: 49.29
Iteration: 2054 | Episodes: 125250 | Median Reward: 40.87 | Max Reward: 49.29
Iteration: 2055 | Episodes: 125300 | Median Reward: 39.14 | Max Reward: 49.29
Iteration: 2056 | Episodes: 125350 | Median Reward: 38.58 | Max Reward: 49.29
Iteration: 2057 | Episodes: 125400 | Median Reward: 39.69 | Max Reward: 49.29
Iteration: 2058 | Episodes: 125450 | Median Reward: 40.28 | Max Reward: 49.29
Iteration: 2058 | Episodes: 125500 | Median Reward: 41.93 | Max Reward: 49.29
Iteration: 2059 | Episodes: 125550 | Median Reward: -14.51 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -84.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2060      |
|    time_elapsed         | 37452     |
|    total_timesteps      | 12656640  |
| train/                  |           |
|    approx_kl            | 4.0749345 |
|    clip_fraction        | 0.127     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.563     |
|    learning_rate        | 0.0001    |
|    loss                 | 128       |
|    n_updates            | 20590     |
|    policy_gradient_loss | -0.0222   |
|    std                  | 3.4       |
|    value_loss           | 248       |
---------------------------------------
Iteration: 2060 | Episodes: 125600 | Median Reward: 40.87 | Max Reward: 49.29
Iteration: 2061 | Episodes: 125650 | Median Reward: 40.95 | Max Reward: 49.29
Iteration: 2062 | Episodes: 125700 | Median Reward: 39.28 | Max Reward: 49.29
Iteration: 2063 | Episodes: 125750 | Median Reward: 40.29 | Max Reward: 49.29
Iteration: 2063 | Episodes: 125800 | Median Reward: 43.00 | Max Reward: 49.29
Iteration: 2064 | Episodes: 125850 | Median Reward: 42.47 | Max Reward: 49.29
Iteration: 2065 | Episodes: 125900 | Median Reward: 41.65 | Max Reward: 49.29
Iteration: 2066 | Episodes: 125950 | Median Reward: 41.65 | Max Reward: 49.29
Iteration: 2067 | Episodes: 126000 | Median Reward: 36.34 | Max Reward: 49.29
Iteration: 2067 | Episodes: 126050 | Median Reward: 38.80 | Max Reward: 49.29
Iteration: 2068 | Episodes: 126100 | Median Reward: 40.79 | Max Reward: 49.29
Iteration: 2069 | Episodes: 126150 | Median Reward: 40.60 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2070      |
|    time_elapsed         | 37631     |
|    total_timesteps      | 12718080  |
| train/                  |           |
|    approx_kl            | 0.8844474 |
|    clip_fraction        | 0.0948    |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.567     |
|    learning_rate        | 0.0001    |
|    loss                 | 72.5      |
|    n_updates            | 20690     |
|    policy_gradient_loss | -0.0231   |
|    std                  | 3.41      |
|    value_loss           | 201       |
---------------------------------------
Iteration: 2070 | Episodes: 126200 | Median Reward: 38.45 | Max Reward: 49.29
Iteration: 2071 | Episodes: 126250 | Median Reward: 36.95 | Max Reward: 49.29
Iteration: 2072 | Episodes: 126300 | Median Reward: -9.86 | Max Reward: 49.29
Iteration: 2072 | Episodes: 126350 | Median Reward: 9.62 | Max Reward: 49.29
Iteration: 2073 | Episodes: 126400 | Median Reward: 39.72 | Max Reward: 49.29
Iteration: 2074 | Episodes: 126450 | Median Reward: 39.40 | Max Reward: 49.29
Iteration: 2075 | Episodes: 126500 | Median Reward: 40.60 | Max Reward: 49.29
Iteration: 2076 | Episodes: 126550 | Median Reward: 41.28 | Max Reward: 49.29
Iteration: 2077 | Episodes: 126600 | Median Reward: 41.27 | Max Reward: 49.29
Iteration: 2077 | Episodes: 126650 | Median Reward: 37.23 | Max Reward: 49.29
Iteration: 2078 | Episodes: 126700 | Median Reward: 37.81 | Max Reward: 49.29
Iteration: 2079 | Episodes: 126750 | Median Reward: 37.57 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -81.2    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2080     |
|    time_elapsed         | 37808    |
|    total_timesteps      | 12779520 |
| train/                  |          |
|    approx_kl            | 6.836967 |
|    clip_fraction        | 0.192    |
|    clip_range           | 0.4      |
|    entropy_loss         | -109     |
|    explained_variance   | 0.583    |
|    learning_rate        | 0.0001   |
|    loss                 | 69.3     |
|    n_updates            | 20790    |
|    policy_gradient_loss | 0.0193   |
|    std                  | 3.42     |
|    value_loss           | 194      |
--------------------------------------
Iteration: 2080 | Episodes: 126800 | Median Reward: 42.04 | Max Reward: 49.29
Iteration: 2081 | Episodes: 126850 | Median Reward: 41.40 | Max Reward: 49.29
Iteration: 2081 | Episodes: 126900 | Median Reward: -10.87 | Max Reward: 49.29
Iteration: 2082 | Episodes: 126950 | Median Reward: -12.00 | Max Reward: 49.29
Iteration: 2083 | Episodes: 127000 | Median Reward: 4.47 | Max Reward: 49.29
Iteration: 2084 | Episodes: 127050 | Median Reward: 34.91 | Max Reward: 49.29
Iteration: 2085 | Episodes: 127100 | Median Reward: 41.89 | Max Reward: 49.29
Iteration: 2086 | Episodes: 127150 | Median Reward: 43.19 | Max Reward: 49.29
Iteration: 2086 | Episodes: 127200 | Median Reward: -11.58 | Max Reward: 49.29
Iteration: 2087 | Episodes: 127250 | Median Reward: 26.59 | Max Reward: 49.29
Iteration: 2088 | Episodes: 127300 | Median Reward: 38.79 | Max Reward: 49.29
Iteration: 2089 | Episodes: 127350 | Median Reward: 39.51 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2090      |
|    time_elapsed         | 38000     |
|    total_timesteps      | 12840960  |
| train/                  |           |
|    approx_kl            | 1.2196342 |
|    clip_fraction        | 0.093     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.599     |
|    learning_rate        | 0.0001    |
|    loss                 | 69.4      |
|    n_updates            | 20890     |
|    policy_gradient_loss | -0.0215   |
|    std                  | 3.43      |
|    value_loss           | 205       |
---------------------------------------
Iteration: 2090 | Episodes: 127400 | Median Reward: 43.37 | Max Reward: 49.29
Iteration: 2091 | Episodes: 127450 | Median Reward: 41.53 | Max Reward: 49.29
Iteration: 2091 | Episodes: 127500 | Median Reward: 39.17 | Max Reward: 49.29
Iteration: 2092 | Episodes: 127550 | Median Reward: 39.22 | Max Reward: 49.29
Iteration: 2093 | Episodes: 127600 | Median Reward: 41.80 | Max Reward: 49.29
Iteration: 2094 | Episodes: 127650 | Median Reward: 41.83 | Max Reward: 49.29
Iteration: 2095 | Episodes: 127700 | Median Reward: 36.36 | Max Reward: 49.29
Iteration: 2095 | Episodes: 127750 | Median Reward: 40.46 | Max Reward: 49.29
Iteration: 2096 | Episodes: 127800 | Median Reward: 40.66 | Max Reward: 49.29
Iteration: 2097 | Episodes: 127850 | Median Reward: 37.68 | Max Reward: 49.29
Iteration: 2098 | Episodes: 127900 | Median Reward: 37.45 | Max Reward: 49.29
Iteration: 2099 | Episodes: 127950 | Median Reward: 41.23 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -61      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2100     |
|    time_elapsed         | 38177    |
|    total_timesteps      | 12902400 |
| train/                  |          |
|    approx_kl            | 9.948109 |
|    clip_fraction        | 0.22     |
|    clip_range           | 0.4      |
|    entropy_loss         | -110     |
|    explained_variance   | 0.588    |
|    learning_rate        | 0.0001   |
|    loss                 | 107      |
|    n_updates            | 20990    |
|    policy_gradient_loss | 0.0196   |
|    std                  | 3.44     |
|    value_loss           | 224      |
--------------------------------------
Iteration: 2100 | Episodes: 128000 | Median Reward: 41.24 | Max Reward: 49.29
Iteration: 2100 | Episodes: 128050 | Median Reward: 39.68 | Max Reward: 49.29
Iteration: 2101 | Episodes: 128100 | Median Reward: 41.53 | Max Reward: 49.29
Iteration: 2102 | Episodes: 128150 | Median Reward: 39.91 | Max Reward: 49.29
Iteration: 2103 | Episodes: 128200 | Median Reward: 40.28 | Max Reward: 49.29
Iteration: 2104 | Episodes: 128250 | Median Reward: 40.01 | Max Reward: 49.29
Iteration: 2104 | Episodes: 128300 | Median Reward: 27.56 | Max Reward: 49.29
Iteration: 2105 | Episodes: 128350 | Median Reward: 39.68 | Max Reward: 49.29
Iteration: 2106 | Episodes: 128400 | Median Reward: 41.82 | Max Reward: 49.29
Iteration: 2107 | Episodes: 128450 | Median Reward: 42.94 | Max Reward: 49.29
Iteration: 2108 | Episodes: 128500 | Median Reward: 44.82 | Max Reward: 49.29
Iteration: 2109 | Episodes: 128550 | Median Reward: 43.80 | Max Reward: 49.29
Iteration: 2109 | Episodes: 128600 | Median Reward: 39.34 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -69.2    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2110     |
|    time_elapsed         | 38360    |
|    total_timesteps      | 12963840 |
| train/                  |          |
|    approx_kl            | 3.305551 |
|    clip_fraction        | 0.0598   |
|    clip_range           | 0.4      |
|    entropy_loss         | -111     |
|    explained_variance   | 0.579    |
|    learning_rate        | 0.0001   |
|    loss                 | 113      |
|    n_updates            | 21090    |
|    policy_gradient_loss | -0.0147  |
|    std                  | 3.45     |
|    value_loss           | 247      |
--------------------------------------
Iteration: 2110 | Episodes: 128650 | Median Reward: 43.32 | Max Reward: 49.29
Iteration: 2111 | Episodes: 128700 | Median Reward: 42.63 | Max Reward: 49.29
Iteration: 2112 | Episodes: 128750 | Median Reward: 42.63 | Max Reward: 49.29
Iteration: 2113 | Episodes: 128800 | Median Reward: 42.72 | Max Reward: 49.29
Iteration: 2114 | Episodes: 128850 | Median Reward: 41.89 | Max Reward: 49.29
Iteration: 2114 | Episodes: 128900 | Median Reward: 43.50 | Max Reward: 49.29
Iteration: 2115 | Episodes: 128950 | Median Reward: 42.69 | Max Reward: 49.29
Iteration: 2116 | Episodes: 129000 | Median Reward: 37.95 | Max Reward: 49.29
Iteration: 2117 | Episodes: 129050 | Median Reward: 41.60 | Max Reward: 49.29
Iteration: 2118 | Episodes: 129100 | Median Reward: 41.67 | Max Reward: 49.29
Iteration: 2118 | Episodes: 129150 | Median Reward: 39.85 | Max Reward: 49.29
Iteration: 2119 | Episodes: 129200 | Median Reward: 41.58 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -66.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2120      |
|    time_elapsed         | 38545     |
|    total_timesteps      | 13025280  |
| train/                  |           |
|    approx_kl            | 1.4960611 |
|    clip_fraction        | 0.0569    |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.618     |
|    learning_rate        | 0.0001    |
|    loss                 | 88        |
|    n_updates            | 21190     |
|    policy_gradient_loss | -0.0156   |
|    std                  | 3.46      |
|    value_loss           | 184       |
---------------------------------------
Iteration: 2120 | Episodes: 129250 | Median Reward: 37.88 | Max Reward: 49.29
Iteration: 2121 | Episodes: 129300 | Median Reward: 41.93 | Max Reward: 49.29
Iteration: 2122 | Episodes: 129350 | Median Reward: 43.52 | Max Reward: 49.29
Iteration: 2123 | Episodes: 129400 | Median Reward: 37.38 | Max Reward: 49.29
Iteration: 2123 | Episodes: 129450 | Median Reward: 41.46 | Max Reward: 49.29
Iteration: 2124 | Episodes: 129500 | Median Reward: 42.74 | Max Reward: 49.29
Iteration: 2125 | Episodes: 129550 | Median Reward: 40.71 | Max Reward: 49.29
Iteration: 2126 | Episodes: 129600 | Median Reward: 39.36 | Max Reward: 49.29
Iteration: 2127 | Episodes: 129650 | Median Reward: 41.59 | Max Reward: 49.29
Iteration: 2127 | Episodes: 129700 | Median Reward: 43.25 | Max Reward: 49.29
Iteration: 2128 | Episodes: 129750 | Median Reward: 45.56 | Max Reward: 49.29
Iteration: 2129 | Episodes: 129800 | Median Reward: 39.27 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -60.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2130      |
|    time_elapsed         | 38726     |
|    total_timesteps      | 13086720  |
| train/                  |           |
|    approx_kl            | 1.4920335 |
|    clip_fraction        | 0.0822    |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.589     |
|    learning_rate        | 0.0001    |
|    loss                 | 122       |
|    n_updates            | 21290     |
|    policy_gradient_loss | -0.0159   |
|    std                  | 3.48      |
|    value_loss           | 230       |
---------------------------------------
Iteration: 2130 | Episodes: 129850 | Median Reward: 38.99 | Max Reward: 49.29
Iteration: 2131 | Episodes: 129900 | Median Reward: 40.21 | Max Reward: 49.29
Iteration: 2132 | Episodes: 129950 | Median Reward: 41.07 | Max Reward: 49.29
Iteration: 2132 | Episodes: 130000 | Median Reward: 41.01 | Max Reward: 49.29
Iteration: 2133 | Episodes: 130050 | Median Reward: 40.47 | Max Reward: 49.29
Iteration: 2134 | Episodes: 130100 | Median Reward: 42.96 | Max Reward: 49.29
Iteration: 2135 | Episodes: 130150 | Median Reward: 42.14 | Max Reward: 49.29
Iteration: 2136 | Episodes: 130200 | Median Reward: 39.39 | Max Reward: 49.29
Iteration: 2136 | Episodes: 130250 | Median Reward: 41.14 | Max Reward: 49.29
Iteration: 2137 | Episodes: 130300 | Median Reward: 39.25 | Max Reward: 49.29
Iteration: 2138 | Episodes: 130350 | Median Reward: 41.08 | Max Reward: 49.29
Iteration: 2139 | Episodes: 130400 | Median Reward: 41.34 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -61.4    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2140     |
|    time_elapsed         | 38906    |
|    total_timesteps      | 13148160 |
| train/                  |          |
|    approx_kl            | 2.495412 |
|    clip_fraction        | 0.0622   |
|    clip_range           | 0.4      |
|    entropy_loss         | -111     |
|    explained_variance   | 0.63     |
|    learning_rate        | 0.0001   |
|    loss                 | 92.9     |
|    n_updates            | 21390    |
|    policy_gradient_loss | -0.0201  |
|    std                  | 3.49     |
|    value_loss           | 181      |
--------------------------------------
Iteration: 2140 | Episodes: 130450 | Median Reward: 40.56 | Max Reward: 49.29
Iteration: 2141 | Episodes: 130500 | Median Reward: 42.82 | Max Reward: 49.29
Iteration: 2141 | Episodes: 130550 | Median Reward: 40.02 | Max Reward: 49.29
Iteration: 2142 | Episodes: 130600 | Median Reward: 39.50 | Max Reward: 49.29
Iteration: 2143 | Episodes: 130650 | Median Reward: 39.86 | Max Reward: 49.29
Iteration: 2144 | Episodes: 130700 | Median Reward: 42.80 | Max Reward: 49.29
Iteration: 2145 | Episodes: 130750 | Median Reward: 42.79 | Max Reward: 49.29
Iteration: 2145 | Episodes: 130800 | Median Reward: 32.49 | Max Reward: 49.29
Iteration: 2146 | Episodes: 130850 | Median Reward: 42.22 | Max Reward: 49.29
Iteration: 2147 | Episodes: 130900 | Median Reward: 40.44 | Max Reward: 49.29
Iteration: 2148 | Episodes: 130950 | Median Reward: 39.85 | Max Reward: 49.29
Iteration: 2149 | Episodes: 131000 | Median Reward: 35.48 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -73.4    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2150     |
|    time_elapsed         | 39093    |
|    total_timesteps      | 13209600 |
| train/                  |          |
|    approx_kl            | 9.114502 |
|    clip_fraction        | 0.177    |
|    clip_range           | 0.4      |
|    entropy_loss         | -111     |
|    explained_variance   | 0.635    |
|    learning_rate        | 0.0001   |
|    loss                 | 87.5     |
|    n_updates            | 21490    |
|    policy_gradient_loss | -0.0425  |
|    std                  | 3.51     |
|    value_loss           | 160      |
--------------------------------------
Iteration: 2150 | Episodes: 131050 | Median Reward: 36.76 | Max Reward: 49.29
Iteration: 2150 | Episodes: 131100 | Median Reward: 43.29 | Max Reward: 49.29
Iteration: 2151 | Episodes: 131150 | Median Reward: -5.92 | Max Reward: 49.29
Iteration: 2152 | Episodes: 131200 | Median Reward: 42.49 | Max Reward: 49.29
Iteration: 2153 | Episodes: 131250 | Median Reward: 41.05 | Max Reward: 49.29
Iteration: 2154 | Episodes: 131300 | Median Reward: -0.24 | Max Reward: 49.29
Iteration: 2155 | Episodes: 131350 | Median Reward: 41.03 | Max Reward: 49.29
Iteration: 2155 | Episodes: 131400 | Median Reward: 43.06 | Max Reward: 49.29
Iteration: 2156 | Episodes: 131450 | Median Reward: 40.45 | Max Reward: 49.29
Iteration: 2157 | Episodes: 131500 | Median Reward: 39.19 | Max Reward: 49.29
Iteration: 2158 | Episodes: 131550 | Median Reward: 39.19 | Max Reward: 49.29
Iteration: 2159 | Episodes: 131600 | Median Reward: 38.38 | Max Reward: 49.29
Iteration: 2159 | Episodes: 131650 | Median Reward: 37.23 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -81.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2160      |
|    time_elapsed         | 39272     |
|    total_timesteps      | 13271040  |
| train/                  |           |
|    approx_kl            | 2.0984998 |
|    clip_fraction        | 0.0644    |
|    clip_range           | 0.4       |
|    entropy_loss         | -112      |
|    explained_variance   | 0.651     |
|    learning_rate        | 0.0001    |
|    loss                 | 62.1      |
|    n_updates            | 21590     |
|    policy_gradient_loss | 0.0513    |
|    std                  | 3.52      |
|    value_loss           | 138       |
---------------------------------------
Iteration: 2160 | Episodes: 131700 | Median Reward: 43.07 | Max Reward: 49.29
Iteration: 2161 | Episodes: 131750 | Median Reward: 43.10 | Max Reward: 49.29
Iteration: 2162 | Episodes: 131800 | Median Reward: 42.19 | Max Reward: 49.29
Iteration: 2163 | Episodes: 131850 | Median Reward: 36.77 | Max Reward: 49.29
Iteration: 2164 | Episodes: 131900 | Median Reward: 40.00 | Max Reward: 49.29
Iteration: 2164 | Episodes: 131950 | Median Reward: 42.31 | Max Reward: 49.29
Iteration: 2165 | Episodes: 132000 | Median Reward: 42.67 | Max Reward: 49.29
Iteration: 2166 | Episodes: 132050 | Median Reward: 38.82 | Max Reward: 49.29
Iteration: 2167 | Episodes: 132100 | Median Reward: 36.52 | Max Reward: 49.29
Iteration: 2168 | Episodes: 132150 | Median Reward: 40.50 | Max Reward: 49.29
Iteration: 2169 | Episodes: 132200 | Median Reward: 39.92 | Max Reward: 49.29
Iteration: 2169 | Episodes: 132250 | Median Reward: 41.18 | Max Reward: 49.29
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -67.5      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2170       |
|    time_elapsed         | 39457      |
|    total_timesteps      | 13332480   |
| train/                  |            |
|    approx_kl            | 12.6799555 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.4        |
|    entropy_loss         | -112       |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.0001     |
|    loss                 | 92.7       |
|    n_updates            | 21690      |
|    policy_gradient_loss | 0.03       |
|    std                  | 3.53       |
|    value_loss           | 152        |
----------------------------------------
Iteration: 2170 | Episodes: 132300 | Median Reward: 32.51 | Max Reward: 49.29
Iteration: 2171 | Episodes: 132350 | Median Reward: -8.19 | Max Reward: 49.29
Iteration: 2172 | Episodes: 132400 | Median Reward: 42.72 | Max Reward: 49.29
Iteration: 2173 | Episodes: 132450 | Median Reward: 41.50 | Max Reward: 49.29
Iteration: 2173 | Episodes: 132500 | Median Reward: -9.39 | Max Reward: 49.29
Iteration: 2174 | Episodes: 132550 | Median Reward: -13.81 | Max Reward: 49.29
Iteration: 2175 | Episodes: 132600 | Median Reward: -9.62 | Max Reward: 49.29
Iteration: 2176 | Episodes: 132650 | Median Reward: 24.98 | Max Reward: 49.29
Iteration: 2177 | Episodes: 132700 | Median Reward: 40.38 | Max Reward: 49.29
Iteration: 2178 | Episodes: 132750 | Median Reward: -11.63 | Max Reward: 49.29
Iteration: 2178 | Episodes: 132800 | Median Reward: 14.09 | Max Reward: 49.29
Iteration: 2179 | Episodes: 132850 | Median Reward: -10.45 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 88.7     |
|    ep_rew_mean          | -76      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2180     |
|    time_elapsed         | 39639    |
|    total_timesteps      | 13393920 |
| train/                  |          |
|    approx_kl            | 6.277639 |
|    clip_fraction        | 0.252    |
|    clip_range           | 0.4      |
|    entropy_loss         | -112     |
|    explained_variance   | 0.65     |
|    learning_rate        | 0.0001   |
|    loss                 | 22.3     |
|    n_updates            | 21790    |
|    policy_gradient_loss | 0.0879   |
|    std                  | 3.53     |
|    value_loss           | 130      |
--------------------------------------
Iteration: 2180 | Episodes: 132900 | Median Reward: -6.49 | Max Reward: 49.29
Iteration: 2181 | Episodes: 132950 | Median Reward: -2.98 | Max Reward: 49.29
Iteration: 2181 | Episodes: 133000 | Median Reward: -22.69 | Max Reward: 49.29
Iteration: 2182 | Episodes: 133050 | Median Reward: -12.30 | Max Reward: 49.29
Iteration: 2183 | Episodes: 133100 | Median Reward: 8.32 | Max Reward: 49.29
Iteration: 2184 | Episodes: 133150 | Median Reward: -10.88 | Max Reward: 49.29
Iteration: 2184 | Episodes: 133200 | Median Reward: -17.82 | Max Reward: 49.29
Iteration: 2185 | Episodes: 133250 | Median Reward: 40.15 | Max Reward: 49.29
Iteration: 2186 | Episodes: 133300 | Median Reward: -9.73 | Max Reward: 49.29
Iteration: 2187 | Episodes: 133350 | Median Reward: -17.30 | Max Reward: 49.29
Iteration: 2187 | Episodes: 133400 | Median Reward: 26.27 | Max Reward: 49.29
Iteration: 2188 | Episodes: 133450 | Median Reward: -17.63 | Max Reward: 49.29
Iteration: 2189 | Episodes: 133500 | Median Reward: -18.13 | Max Reward: 49.29
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -114       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2190       |
|    time_elapsed         | 39819      |
|    total_timesteps      | 13455360   |
| train/                  |            |
|    approx_kl            | 0.01866164 |
|    clip_fraction        | 0.0119     |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0001     |
|    loss                 | 26.1       |
|    n_updates            | 21890      |
|    policy_gradient_loss | 0.00516    |
|    std                  | 3.53       |
|    value_loss           | 80.1       |
----------------------------------------
Iteration: 2190 | Episodes: 133550 | Median Reward: -16.08 | Max Reward: 49.29
Iteration: 2191 | Episodes: 133600 | Median Reward: 21.10 | Max Reward: 49.29
Iteration: 2192 | Episodes: 133650 | Median Reward: 19.74 | Max Reward: 49.29
Iteration: 2192 | Episodes: 133700 | Median Reward: 37.56 | Max Reward: 49.29
Iteration: 2193 | Episodes: 133750 | Median Reward: -19.24 | Max Reward: 49.29
Iteration: 2194 | Episodes: 133800 | Median Reward: -8.61 | Max Reward: 49.29
Iteration: 2195 | Episodes: 133850 | Median Reward: -6.17 | Max Reward: 49.29
Iteration: 2196 | Episodes: 133900 | Median Reward: -5.82 | Max Reward: 49.29
Iteration: 2196 | Episodes: 133950 | Median Reward: -6.74 | Max Reward: 49.29
Iteration: 2197 | Episodes: 134000 | Median Reward: 36.77 | Max Reward: 49.29
Iteration: 2198 | Episodes: 134050 | Median Reward: 40.61 | Max Reward: 49.29
Iteration: 2199 | Episodes: 134100 | Median Reward: 40.10 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -83       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2200      |
|    time_elapsed         | 40005     |
|    total_timesteps      | 13516800  |
| train/                  |           |
|    approx_kl            | 4.9585114 |
|    clip_fraction        | 0.233     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.633     |
|    learning_rate        | 0.0001    |
|    loss                 | 96.1      |
|    n_updates            | 21990     |
|    policy_gradient_loss | 0.0285    |
|    std                  | 3.54      |
|    value_loss           | 154       |
---------------------------------------
Iteration: 2200 | Episodes: 134150 | Median Reward: 21.19 | Max Reward: 49.29
Iteration: 2201 | Episodes: 134200 | Median Reward: 41.16 | Max Reward: 49.29
Iteration: 2201 | Episodes: 134250 | Median Reward: 36.02 | Max Reward: 49.29
Iteration: 2202 | Episodes: 134300 | Median Reward: 36.02 | Max Reward: 49.29
Iteration: 2203 | Episodes: 134350 | Median Reward: -3.50 | Max Reward: 49.29
Iteration: 2204 | Episodes: 134400 | Median Reward: 6.63 | Max Reward: 49.29
Iteration: 2205 | Episodes: 134450 | Median Reward: 40.05 | Max Reward: 49.29
Iteration: 2205 | Episodes: 134500 | Median Reward: 38.47 | Max Reward: 49.29
Iteration: 2206 | Episodes: 134550 | Median Reward: 40.79 | Max Reward: 49.29
Iteration: 2207 | Episodes: 134600 | Median Reward: 37.73 | Max Reward: 49.29
Iteration: 2208 | Episodes: 134650 | Median Reward: 37.33 | Max Reward: 49.29
Iteration: 2209 | Episodes: 134700 | Median Reward: 2.28 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2210      |
|    time_elapsed         | 40182     |
|    total_timesteps      | 13578240  |
| train/                  |           |
|    approx_kl            | 1.0874311 |
|    clip_fraction        | 0.0999    |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.645     |
|    learning_rate        | 0.0001    |
|    loss                 | 65.3      |
|    n_updates            | 22090     |
|    policy_gradient_loss | -0.0159   |
|    std                  | 3.54      |
|    value_loss           | 136       |
---------------------------------------
Iteration: 2210 | Episodes: 134750 | Median Reward: 41.76 | Max Reward: 49.29
Iteration: 2210 | Episodes: 134800 | Median Reward: 42.03 | Max Reward: 49.29
Iteration: 2211 | Episodes: 134850 | Median Reward: 43.60 | Max Reward: 49.29
Iteration: 2212 | Episodes: 134900 | Median Reward: 42.04 | Max Reward: 49.29
Iteration: 2213 | Episodes: 134950 | Median Reward: 39.20 | Max Reward: 49.29
Iteration: 2214 | Episodes: 135000 | Median Reward: 37.79 | Max Reward: 49.29
Iteration: 2214 | Episodes: 135050 | Median Reward: 13.26 | Max Reward: 49.29
Iteration: 2215 | Episodes: 135100 | Median Reward: 42.15 | Max Reward: 49.29
Iteration: 2216 | Episodes: 135150 | Median Reward: 41.88 | Max Reward: 49.29
Iteration: 2217 | Episodes: 135200 | Median Reward: 29.32 | Max Reward: 49.29
Iteration: 2218 | Episodes: 135250 | Median Reward: -9.37 | Max Reward: 49.29
Iteration: 2219 | Episodes: 135300 | Median Reward: 42.04 | Max Reward: 49.29
Iteration: 2219 | Episodes: 135350 | Median Reward: 43.67 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2220      |
|    time_elapsed         | 40361     |
|    total_timesteps      | 13639680  |
| train/                  |           |
|    approx_kl            | 4.9855804 |
|    clip_fraction        | 0.112     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.636     |
|    learning_rate        | 0.0001    |
|    loss                 | 96.4      |
|    n_updates            | 22190     |
|    policy_gradient_loss | -0.0105   |
|    std                  | 3.55      |
|    value_loss           | 160       |
---------------------------------------
Iteration: 2220 | Episodes: 135400 | Median Reward: 42.63 | Max Reward: 49.29
Iteration: 2221 | Episodes: 135450 | Median Reward: 42.33 | Max Reward: 49.29
Iteration: 2222 | Episodes: 135500 | Median Reward: 42.80 | Max Reward: 49.29
Iteration: 2223 | Episodes: 135550 | Median Reward: 43.18 | Max Reward: 49.29
Iteration: 2223 | Episodes: 135600 | Median Reward: 43.73 | Max Reward: 49.29
Iteration: 2224 | Episodes: 135650 | Median Reward: 39.74 | Max Reward: 49.29
Iteration: 2225 | Episodes: 135700 | Median Reward: 39.74 | Max Reward: 49.29
Iteration: 2226 | Episodes: 135750 | Median Reward: 38.02 | Max Reward: 49.29
Iteration: 2227 | Episodes: 135800 | Median Reward: 37.19 | Max Reward: 49.29
Iteration: 2228 | Episodes: 135850 | Median Reward: 37.86 | Max Reward: 49.29
Iteration: 2228 | Episodes: 135900 | Median Reward: 46.26 | Max Reward: 49.29
Iteration: 2229 | Episodes: 135950 | Median Reward: 42.28 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -64.5    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2230     |
|    time_elapsed         | 40547    |
|    total_timesteps      | 13701120 |
| train/                  |          |
|    approx_kl            | 3.866254 |
|    clip_fraction        | 0.113    |
|    clip_range           | 0.4      |
|    entropy_loss         | -111     |
|    explained_variance   | 0.628    |
|    learning_rate        | 0.0001   |
|    loss                 | 110      |
|    n_updates            | 22290    |
|    policy_gradient_loss | -0.0244  |
|    std                  | 3.56     |
|    value_loss           | 196      |
--------------------------------------
Iteration: 2230 | Episodes: 136000 | Median Reward: 39.88 | Max Reward: 49.29
Iteration: 2231 | Episodes: 136050 | Median Reward: 39.58 | Max Reward: 49.29
Iteration: 2232 | Episodes: 136100 | Median Reward: 43.96 | Max Reward: 49.29
Iteration: 2233 | Episodes: 136150 | Median Reward: 43.73 | Max Reward: 49.29
Iteration: 2233 | Episodes: 136200 | Median Reward: 38.35 | Max Reward: 49.29
Iteration: 2234 | Episodes: 136250 | Median Reward: 41.25 | Max Reward: 49.29
Iteration: 2235 | Episodes: 136300 | Median Reward: 42.68 | Max Reward: 49.29
Iteration: 2236 | Episodes: 136350 | Median Reward: 42.68 | Max Reward: 49.29
Iteration: 2237 | Episodes: 136400 | Median Reward: 42.08 | Max Reward: 49.29
Iteration: 2237 | Episodes: 136450 | Median Reward: 40.70 | Max Reward: 49.29
Iteration: 2238 | Episodes: 136500 | Median Reward: 34.24 | Max Reward: 49.29
Iteration: 2239 | Episodes: 136550 | Median Reward: 42.14 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2240      |
|    time_elapsed         | 40728     |
|    total_timesteps      | 13762560  |
| train/                  |           |
|    approx_kl            | 11.811842 |
|    clip_fraction        | 0.192     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.666     |
|    learning_rate        | 0.0001    |
|    loss                 | 36.6      |
|    n_updates            | 22390     |
|    policy_gradient_loss | -0.0197   |
|    std                  | 3.58      |
|    value_loss           | 118       |
---------------------------------------
Iteration: 2240 | Episodes: 136600 | Median Reward: 41.75 | Max Reward: 49.29
Iteration: 2241 | Episodes: 136650 | Median Reward: 39.10 | Max Reward: 49.29
Iteration: 2242 | Episodes: 136700 | Median Reward: 38.51 | Max Reward: 49.29
Iteration: 2242 | Episodes: 136750 | Median Reward: 40.50 | Max Reward: 49.29
Iteration: 2243 | Episodes: 136800 | Median Reward: 43.36 | Max Reward: 49.29
Iteration: 2244 | Episodes: 136850 | Median Reward: 41.14 | Max Reward: 49.29
Iteration: 2245 | Episodes: 136900 | Median Reward: 39.38 | Max Reward: 49.29
Iteration: 2246 | Episodes: 136950 | Median Reward: 41.61 | Max Reward: 49.29
Iteration: 2247 | Episodes: 137000 | Median Reward: 42.37 | Max Reward: 49.29
Iteration: 2247 | Episodes: 137050 | Median Reward: 39.85 | Max Reward: 49.29
Iteration: 2248 | Episodes: 137100 | Median Reward: 42.10 | Max Reward: 49.29
Iteration: 2249 | Episodes: 137150 | Median Reward: 41.50 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2250      |
|    time_elapsed         | 40907     |
|    total_timesteps      | 13824000  |
| train/                  |           |
|    approx_kl            | 15.330867 |
|    clip_fraction        | 0.179     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.0001    |
|    loss                 | 79.4      |
|    n_updates            | 22490     |
|    policy_gradient_loss | -0.0554   |
|    std                  | 3.59      |
|    value_loss           | 179       |
---------------------------------------
Iteration: 2250 | Episodes: 137200 | Median Reward: 41.50 | Max Reward: 49.29
Iteration: 2251 | Episodes: 137250 | Median Reward: 40.23 | Max Reward: 49.29
Iteration: 2251 | Episodes: 137300 | Median Reward: 43.72 | Max Reward: 49.29
Iteration: 2252 | Episodes: 137350 | Median Reward: 42.42 | Max Reward: 49.29
Iteration: 2253 | Episodes: 137400 | Median Reward: 40.22 | Max Reward: 49.29
Iteration: 2254 | Episodes: 137450 | Median Reward: 40.07 | Max Reward: 49.29
Iteration: 2255 | Episodes: 137500 | Median Reward: 46.72 | Max Reward: 49.29
Iteration: 2256 | Episodes: 137550 | Median Reward: 40.22 | Max Reward: 49.29
Iteration: 2256 | Episodes: 137600 | Median Reward: 40.79 | Max Reward: 49.29
Iteration: 2257 | Episodes: 137650 | Median Reward: 39.75 | Max Reward: 49.29
Iteration: 2258 | Episodes: 137700 | Median Reward: 37.49 | Max Reward: 49.29
Iteration: 2259 | Episodes: 137750 | Median Reward: 39.05 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -81.1    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2260     |
|    time_elapsed         | 41099    |
|    total_timesteps      | 13885440 |
| train/                  |          |
|    approx_kl            | 34.67138 |
|    clip_fraction        | 0.457    |
|    clip_range           | 0.4      |
|    entropy_loss         | -111     |
|    explained_variance   | 0.671    |
|    learning_rate        | 0.0001   |
|    loss                 | 64.3     |
|    n_updates            | 22590    |
|    policy_gradient_loss | 0.174    |
|    std                  | 3.61     |
|    value_loss           | 138      |
--------------------------------------
Iteration: 2260 | Episodes: 137800 | Median Reward: 37.65 | Max Reward: 49.29
Iteration: 2260 | Episodes: 137850 | Median Reward: 40.14 | Max Reward: 49.29
Iteration: 2261 | Episodes: 137900 | Median Reward: 40.07 | Max Reward: 49.29
Iteration: 2262 | Episodes: 137950 | Median Reward: 39.14 | Max Reward: 49.29
Iteration: 2263 | Episodes: 138000 | Median Reward: 38.69 | Max Reward: 49.29
Iteration: 2264 | Episodes: 138050 | Median Reward: 39.27 | Max Reward: 49.29
Iteration: 2265 | Episodes: 138100 | Median Reward: 39.83 | Max Reward: 49.29
Iteration: 2265 | Episodes: 138150 | Median Reward: 38.13 | Max Reward: 49.29
Iteration: 2266 | Episodes: 138200 | Median Reward: 39.67 | Max Reward: 49.29
Iteration: 2267 | Episodes: 138250 | Median Reward: 39.25 | Max Reward: 49.29
Iteration: 2268 | Episodes: 138300 | Median Reward: 40.29 | Max Reward: 49.29
Iteration: 2269 | Episodes: 138350 | Median Reward: 39.98 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -66.1     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2270      |
|    time_elapsed         | 41279     |
|    total_timesteps      | 13946880  |
| train/                  |           |
|    approx_kl            | 2.0699973 |
|    clip_fraction        | 0.0658    |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.668     |
|    learning_rate        | 0.0001    |
|    loss                 | 85.2      |
|    n_updates            | 22690     |
|    policy_gradient_loss | -0.0214   |
|    std                  | 3.62      |
|    value_loss           | 164       |
---------------------------------------
Iteration: 2270 | Episodes: 138400 | Median Reward: 36.18 | Max Reward: 49.29
Iteration: 2270 | Episodes: 138450 | Median Reward: 39.35 | Max Reward: 49.29
Iteration: 2271 | Episodes: 138500 | Median Reward: 42.73 | Max Reward: 49.29
Iteration: 2272 | Episodes: 138550 | Median Reward: 42.32 | Max Reward: 49.29
Iteration: 2273 | Episodes: 138600 | Median Reward: 42.76 | Max Reward: 49.29
Iteration: 2274 | Episodes: 138650 | Median Reward: 42.12 | Max Reward: 49.29
Iteration: 2274 | Episodes: 138700 | Median Reward: 38.17 | Max Reward: 49.29
Iteration: 2275 | Episodes: 138750 | Median Reward: 41.55 | Max Reward: 49.29
Iteration: 2276 | Episodes: 138800 | Median Reward: 40.00 | Max Reward: 49.29
Iteration: 2277 | Episodes: 138850 | Median Reward: 40.00 | Max Reward: 49.29
Iteration: 2278 | Episodes: 138900 | Median Reward: 40.98 | Max Reward: 49.29
Iteration: 2279 | Episodes: 138950 | Median Reward: 32.24 | Max Reward: 49.29
Iteration: 2279 | Episodes: 139000 | Median Reward: 42.69 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -71.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2280      |
|    time_elapsed         | 41463     |
|    total_timesteps      | 14008320  |
| train/                  |           |
|    approx_kl            | 19.846092 |
|    clip_fraction        | 0.311     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.682     |
|    learning_rate        | 0.0001    |
|    loss                 | 69.9      |
|    n_updates            | 22790     |
|    policy_gradient_loss | -0.0545   |
|    std                  | 3.65      |
|    value_loss           | 137       |
---------------------------------------
Iteration: 2280 | Episodes: 139050 | Median Reward: 39.25 | Max Reward: 49.29
Iteration: 2281 | Episodes: 139100 | Median Reward: 38.33 | Max Reward: 49.29
Iteration: 2282 | Episodes: 139150 | Median Reward: 39.63 | Max Reward: 49.29
Iteration: 2283 | Episodes: 139200 | Median Reward: 40.94 | Max Reward: 49.29
Iteration: 2284 | Episodes: 139250 | Median Reward: 44.01 | Max Reward: 49.29
Iteration: 2284 | Episodes: 139300 | Median Reward: 42.52 | Max Reward: 49.29
Iteration: 2285 | Episodes: 139350 | Median Reward: 40.44 | Max Reward: 49.29
Iteration: 2286 | Episodes: 139400 | Median Reward: 39.95 | Max Reward: 49.29
Iteration: 2287 | Episodes: 139450 | Median Reward: 38.03 | Max Reward: 49.29
Iteration: 2288 | Episodes: 139500 | Median Reward: 39.15 | Max Reward: 49.29
Iteration: 2288 | Episodes: 139550 | Median Reward: 36.43 | Max Reward: 49.29
Iteration: 2289 | Episodes: 139600 | Median Reward: 41.67 | Max Reward: 49.29
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.2       |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 2290        |
|    time_elapsed         | 41651       |
|    total_timesteps      | 14069760    |
| train/                  |             |
|    approx_kl            | 0.010841791 |
|    clip_fraction        | 0.01        |
|    clip_range           | 0.4         |
|    entropy_loss         | -112        |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0001      |
|    loss                 | 54.4        |
|    n_updates            | 22890       |
|    policy_gradient_loss | -0.00265    |
|    std                  | 3.67        |
|    value_loss           | 137         |
-----------------------------------------
Iteration: 2290 | Episodes: 139650 | Median Reward: 39.89 | Max Reward: 49.29
Iteration: 2291 | Episodes: 139700 | Median Reward: 41.56 | Max Reward: 49.29
Iteration: 2292 | Episodes: 139750 | Median Reward: 43.22 | Max Reward: 49.29
Iteration: 2293 | Episodes: 139800 | Median Reward: 40.55 | Max Reward: 49.29
Iteration: 2293 | Episodes: 139850 | Median Reward: 38.79 | Max Reward: 49.29
Iteration: 2294 | Episodes: 139900 | Median Reward: 40.97 | Max Reward: 49.29
Iteration: 2295 | Episodes: 139950 | Median Reward: 39.23 | Max Reward: 49.29
Iteration: 2296 | Episodes: 140000 | Median Reward: 38.87 | Max Reward: 49.29
Iteration: 2297 | Episodes: 140050 | Median Reward: 36.09 | Max Reward: 49.29
Iteration: 2297 | Episodes: 140100 | Median Reward: 43.90 | Max Reward: 49.29
Iteration: 2298 | Episodes: 140150 | Median Reward: 40.61 | Max Reward: 49.29
Iteration: 2299 | Episodes: 140200 | Median Reward: 43.15 | Max Reward: 49.29
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.5        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 2300         |
|    time_elapsed         | 41830        |
|    total_timesteps      | 14131200     |
| train/                  |              |
|    approx_kl            | 0.0039484305 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 0.688        |
|    learning_rate        | 0.0001       |
|    loss                 | 75.3         |
|    n_updates            | 22990        |
|    policy_gradient_loss | -0.000344    |
|    std                  | 3.69         |
|    value_loss           | 149          |
------------------------------------------
Iteration: 2300 | Episodes: 140250 | Median Reward: 41.32 | Max Reward: 49.29
Iteration: 2301 | Episodes: 140300 | Median Reward: 39.80 | Max Reward: 49.29
Iteration: 2302 | Episodes: 140350 | Median Reward: 42.20 | Max Reward: 49.29
Iteration: 2302 | Episodes: 140400 | Median Reward: 45.15 | Max Reward: 49.29
Iteration: 2303 | Episodes: 140450 | Median Reward: 42.78 | Max Reward: 49.29
Iteration: 2304 | Episodes: 140500 | Median Reward: 41.98 | Max Reward: 49.29
Iteration: 2305 | Episodes: 140550 | Median Reward: 43.52 | Max Reward: 49.29
Iteration: 2306 | Episodes: 140600 | Median Reward: 43.33 | Max Reward: 49.29
Iteration: 2307 | Episodes: 140650 | Median Reward: 42.91 | Max Reward: 49.29
Iteration: 2307 | Episodes: 140700 | Median Reward: 42.64 | Max Reward: 49.29
Iteration: 2308 | Episodes: 140750 | Median Reward: 39.36 | Max Reward: 49.29
Iteration: 2309 | Episodes: 140800 | Median Reward: 39.36 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -75.3    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2310     |
|    time_elapsed         | 42020    |
|    total_timesteps      | 14192640 |
| train/                  |          |
|    approx_kl            | 1.085918 |
|    clip_fraction        | 0.085    |
|    clip_range           | 0.4      |
|    entropy_loss         | -112     |
|    explained_variance   | 0.695    |
|    learning_rate        | 0.0001   |
|    loss                 | 74       |
|    n_updates            | 23090    |
|    policy_gradient_loss | -0.0284  |
|    std                  | 3.72     |
|    value_loss           | 136      |
--------------------------------------
Iteration: 2310 | Episodes: 140850 | Median Reward: 41.10 | Max Reward: 49.29
Iteration: 2311 | Episodes: 140900 | Median Reward: 41.24 | Max Reward: 49.29
Iteration: 2311 | Episodes: 140950 | Median Reward: 33.29 | Max Reward: 49.29
Iteration: 2312 | Episodes: 141000 | Median Reward: 35.19 | Max Reward: 49.29
Iteration: 2313 | Episodes: 141050 | Median Reward: 41.46 | Max Reward: 49.29
Iteration: 2314 | Episodes: 141100 | Median Reward: 40.54 | Max Reward: 49.29
Iteration: 2315 | Episodes: 141150 | Median Reward: 33.37 | Max Reward: 49.29
Iteration: 2316 | Episodes: 141200 | Median Reward: 36.71 | Max Reward: 49.29
Iteration: 2316 | Episodes: 141250 | Median Reward: -13.97 | Max Reward: 49.29
Iteration: 2317 | Episodes: 141300 | Median Reward: 32.14 | Max Reward: 49.29
Iteration: 2318 | Episodes: 141350 | Median Reward: -8.40 | Max Reward: 49.29
Iteration: 2319 | Episodes: 141400 | Median Reward: -15.22 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -112     |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2320     |
|    time_elapsed         | 42199    |
|    total_timesteps      | 14254080 |
| train/                  |          |
|    approx_kl            | 4.044628 |
|    clip_fraction        | 0.226    |
|    clip_range           | 0.4      |
|    entropy_loss         | -106     |
|    explained_variance   | 0.437    |
|    learning_rate        | 0.0001   |
|    loss                 | 76.1     |
|    n_updates            | 23190    |
|    policy_gradient_loss | 0.0437   |
|    std                  | 3.74     |
|    value_loss           | 118      |
--------------------------------------
Iteration: 2320 | Episodes: 141450 | Median Reward: -22.27 | Max Reward: 49.29
Iteration: 2320 | Episodes: 141500 | Median Reward: -10.72 | Max Reward: 49.29
Iteration: 2321 | Episodes: 141550 | Median Reward: -11.83 | Max Reward: 49.29
Iteration: 2322 | Episodes: 141600 | Median Reward: -13.01 | Max Reward: 49.29
Iteration: 2323 | Episodes: 141650 | Median Reward: -5.16 | Max Reward: 49.29
Iteration: 2324 | Episodes: 141700 | Median Reward: 8.38 | Max Reward: 49.29
Iteration: 2325 | Episodes: 141750 | Median Reward: -13.20 | Max Reward: 49.29
Iteration: 2325 | Episodes: 141800 | Median Reward: -18.63 | Max Reward: 49.29
Iteration: 2326 | Episodes: 141850 | Median Reward: -11.61 | Max Reward: 49.29
Iteration: 2327 | Episodes: 141900 | Median Reward: -11.56 | Max Reward: 49.29
Iteration: 2328 | Episodes: 141950 | Median Reward: -11.20 | Max Reward: 49.29
Iteration: 2329 | Episodes: 142000 | Median Reward: -5.10 | Max Reward: 49.29
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -98        |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2330       |
|    time_elapsed         | 42377      |
|    total_timesteps      | 14315520   |
| train/                  |            |
|    approx_kl            | 0.06604226 |
|    clip_fraction        | 0.0529     |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0001     |
|    loss                 | 29.6       |
|    n_updates            | 23290      |
|    policy_gradient_loss | -0.023     |
|    std                  | 3.74       |
|    value_loss           | 95.9       |
----------------------------------------
Iteration: 2330 | Episodes: 142050 | Median Reward: -11.01 | Max Reward: 49.29
Iteration: 2330 | Episodes: 142100 | Median Reward: -5.66 | Max Reward: 49.29
Iteration: 2331 | Episodes: 142150 | Median Reward: -6.85 | Max Reward: 49.29
Iteration: 2332 | Episodes: 142200 | Median Reward: -9.53 | Max Reward: 49.29
Iteration: 2333 | Episodes: 142250 | Median Reward: -11.68 | Max Reward: 49.29
Iteration: 2334 | Episodes: 142300 | Median Reward: -11.68 | Max Reward: 49.29
Iteration: 2334 | Episodes: 142350 | Median Reward: 0.40 | Max Reward: 49.29
Iteration: 2335 | Episodes: 142400 | Median Reward: 32.96 | Max Reward: 49.29
Iteration: 2336 | Episodes: 142450 | Median Reward: -8.78 | Max Reward: 49.29
Iteration: 2337 | Episodes: 142500 | Median Reward: -14.96 | Max Reward: 49.29
Iteration: 2338 | Episodes: 142550 | Median Reward: -14.96 | Max Reward: 49.29
Iteration: 2339 | Episodes: 142600 | Median Reward: -8.25 | Max Reward: 49.29
Iteration: 2339 | Episodes: 142650 | Median Reward: -9.33 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -98.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2340      |
|    time_elapsed         | 42562     |
|    total_timesteps      | 14376960  |
| train/                  |           |
|    approx_kl            | 17.792944 |
|    clip_fraction        | 0.409     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0001    |
|    loss                 | 31.8      |
|    n_updates            | 23390     |
|    policy_gradient_loss | 0.0727    |
|    std                  | 3.75      |
|    value_loss           | 75.5      |
---------------------------------------
Iteration: 2340 | Episodes: 142700 | Median Reward: -4.50 | Max Reward: 49.29
Iteration: 2341 | Episodes: 142750 | Median Reward: 0.45 | Max Reward: 49.29
Iteration: 2342 | Episodes: 142800 | Median Reward: -1.87 | Max Reward: 49.29
Iteration: 2343 | Episodes: 142850 | Median Reward: 15.68 | Max Reward: 49.29
Iteration: 2344 | Episodes: 142900 | Median Reward: -19.66 | Max Reward: 49.29
Iteration: 2344 | Episodes: 142950 | Median Reward: 28.18 | Max Reward: 49.29
Iteration: 2345 | Episodes: 143000 | Median Reward: 38.39 | Max Reward: 49.29
Iteration: 2346 | Episodes: 143050 | Median Reward: 38.39 | Max Reward: 49.29
Iteration: 2347 | Episodes: 143100 | Median Reward: 36.25 | Max Reward: 49.29
Iteration: 2348 | Episodes: 143150 | Median Reward: 29.64 | Max Reward: 49.29
Iteration: 2348 | Episodes: 143200 | Median Reward: -19.42 | Max Reward: 49.29
Iteration: 2349 | Episodes: 143250 | Median Reward: -12.04 | Max Reward: 49.29
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -98.1      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2350       |
|    time_elapsed         | 42740      |
|    total_timesteps      | 14438400   |
| train/                  |            |
|    approx_kl            | 0.82165384 |
|    clip_fraction        | 0.0561     |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.0001     |
|    loss                 | 61.6       |
|    n_updates            | 23490      |
|    policy_gradient_loss | -0.00509   |
|    std                  | 3.75       |
|    value_loss           | 109        |
----------------------------------------
Iteration: 2350 | Episodes: 143300 | Median Reward: 10.72 | Max Reward: 49.29
Iteration: 2351 | Episodes: 143350 | Median Reward: -9.42 | Max Reward: 49.29
Iteration: 2352 | Episodes: 143400 | Median Reward: -10.46 | Max Reward: 49.29
Iteration: 2352 | Episodes: 143450 | Median Reward: -9.57 | Max Reward: 49.29
Iteration: 2353 | Episodes: 143500 | Median Reward: 20.66 | Max Reward: 49.29
Iteration: 2354 | Episodes: 143550 | Median Reward: -16.19 | Max Reward: 49.29
Iteration: 2355 | Episodes: 143600 | Median Reward: -16.19 | Max Reward: 49.29
Iteration: 2356 | Episodes: 143650 | Median Reward: -14.00 | Max Reward: 49.29
Iteration: 2357 | Episodes: 143700 | Median Reward: -14.02 | Max Reward: 49.29
Iteration: 2357 | Episodes: 143750 | Median Reward: -2.40 | Max Reward: 49.29
Iteration: 2358 | Episodes: 143800 | Median Reward: -9.94 | Max Reward: 49.29
Iteration: 2359 | Episodes: 143850 | Median Reward: -9.32 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -107     |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2360     |
|    time_elapsed         | 42919    |
|    total_timesteps      | 14499840 |
| train/                  |          |
|    approx_kl            | 1.382611 |
|    clip_fraction        | 0.0915   |
|    clip_range           | 0.4      |
|    entropy_loss         | -110     |
|    explained_variance   | 0.779    |
|    learning_rate        | 0.0001   |
|    loss                 | 24.7     |
|    n_updates            | 23590    |
|    policy_gradient_loss | 0.00142  |
|    std                  | 3.75     |
|    value_loss           | 59.8     |
--------------------------------------
Iteration: 2360 | Episodes: 143900 | Median Reward: -10.65 | Max Reward: 49.29
Iteration: 2361 | Episodes: 143950 | Median Reward: -20.08 | Max Reward: 49.29
Iteration: 2361 | Episodes: 144000 | Median Reward: -18.37 | Max Reward: 49.29
Iteration: 2362 | Episodes: 144050 | Median Reward: -13.10 | Max Reward: 49.29
Iteration: 2363 | Episodes: 144100 | Median Reward: -17.32 | Max Reward: 49.29
Iteration: 2364 | Episodes: 144150 | Median Reward: -15.36 | Max Reward: 49.29
Iteration: 2365 | Episodes: 144200 | Median Reward: -15.10 | Max Reward: 49.29
Iteration: 2366 | Episodes: 144250 | Median Reward: -4.58 | Max Reward: 49.29
Iteration: 2366 | Episodes: 144300 | Median Reward: 15.70 | Max Reward: 49.29
Iteration: 2367 | Episodes: 144350 | Median Reward: -9.91 | Max Reward: 49.29
Iteration: 2368 | Episodes: 144400 | Median Reward: -22.06 | Max Reward: 49.29
Iteration: 2369 | Episodes: 144450 | Median Reward: -15.08 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -105      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2370      |
|    time_elapsed         | 43105     |
|    total_timesteps      | 14561280  |
| train/                  |           |
|    approx_kl            | 1.6905823 |
|    clip_fraction        | 0.139     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.756     |
|    learning_rate        | 0.0001    |
|    loss                 | 26.8      |
|    n_updates            | 23690     |
|    policy_gradient_loss | 0.0155    |
|    std                  | 3.76      |
|    value_loss           | 63.5      |
---------------------------------------
Iteration: 2370 | Episodes: 144500 | Median Reward: -13.67 | Max Reward: 49.29
Iteration: 2371 | Episodes: 144550 | Median Reward: -19.18 | Max Reward: 49.29
Iteration: 2371 | Episodes: 144600 | Median Reward: -10.80 | Max Reward: 49.29
Iteration: 2372 | Episodes: 144650 | Median Reward: -21.71 | Max Reward: 49.29
Iteration: 2373 | Episodes: 144700 | Median Reward: -21.30 | Max Reward: 49.29
Iteration: 2374 | Episodes: 144750 | Median Reward: -12.11 | Max Reward: 49.29
Iteration: 2375 | Episodes: 144800 | Median Reward: 39.82 | Max Reward: 49.29
Iteration: 2375 | Episodes: 144850 | Median Reward: -8.34 | Max Reward: 49.29
Iteration: 2376 | Episodes: 144900 | Median Reward: -6.63 | Max Reward: 49.29
Iteration: 2377 | Episodes: 144950 | Median Reward: 23.43 | Max Reward: 49.29
Iteration: 2378 | Episodes: 145000 | Median Reward: -8.28 | Max Reward: 49.29
Iteration: 2379 | Episodes: 145050 | Median Reward: -12.45 | Max Reward: 49.29
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -89.1        |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 2380         |
|    time_elapsed         | 43284        |
|    total_timesteps      | 14622720     |
| train/                  |              |
|    approx_kl            | 0.0071962527 |
|    clip_fraction        | 0.00375      |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 0.74         |
|    learning_rate        | 0.0001       |
|    loss                 | 17           |
|    n_updates            | 23790        |
|    policy_gradient_loss | -0.00465     |
|    std                  | 3.76         |
|    value_loss           | 63.8         |
------------------------------------------
Iteration: 2380 | Episodes: 145100 | Median Reward: 32.61 | Max Reward: 49.29
Iteration: 2380 | Episodes: 145150 | Median Reward: 43.59 | Max Reward: 49.29
Iteration: 2381 | Episodes: 145200 | Median Reward: -13.28 | Max Reward: 49.29
Iteration: 2382 | Episodes: 145250 | Median Reward: 17.60 | Max Reward: 49.29
Iteration: 2383 | Episodes: 145300 | Median Reward: 20.00 | Max Reward: 49.29
Iteration: 2384 | Episodes: 145350 | Median Reward: -23.37 | Max Reward: 49.29
Iteration: 2384 | Episodes: 145400 | Median Reward: -0.13 | Max Reward: 49.29
Iteration: 2385 | Episodes: 145450 | Median Reward: -10.18 | Max Reward: 49.29
Iteration: 2386 | Episodes: 145500 | Median Reward: -19.69 | Max Reward: 49.29
Iteration: 2387 | Episodes: 145550 | Median Reward: -24.56 | Max Reward: 49.29
Iteration: 2388 | Episodes: 145600 | Median Reward: -19.90 | Max Reward: 49.29
Iteration: 2389 | Episodes: 145650 | Median Reward: -19.77 | Max Reward: 49.29
Iteration: 2389 | Episodes: 145700 | Median Reward: -13.28 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -114      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2390      |
|    time_elapsed         | 43459     |
|    total_timesteps      | 14684160  |
| train/                  |           |
|    approx_kl            | 5.7041407 |
|    clip_fraction        | 0.144     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.749     |
|    learning_rate        | 0.0001    |
|    loss                 | 15.2      |
|    n_updates            | 23890     |
|    policy_gradient_loss | -0.0109   |
|    std                  | 3.77      |
|    value_loss           | 63.7      |
---------------------------------------
Iteration: 2390 | Episodes: 145750 | Median Reward: -18.20 | Max Reward: 49.29
Iteration: 2391 | Episodes: 145800 | Median Reward: -19.81 | Max Reward: 49.29
Iteration: 2392 | Episodes: 145850 | Median Reward: -20.90 | Max Reward: 49.29
Iteration: 2393 | Episodes: 145900 | Median Reward: 5.63 | Max Reward: 49.29
Iteration: 2394 | Episodes: 145950 | Median Reward: -6.11 | Max Reward: 49.29
Iteration: 2394 | Episodes: 146000 | Median Reward: 42.89 | Max Reward: 49.29
Iteration: 2395 | Episodes: 146050 | Median Reward: -8.92 | Max Reward: 49.29
Iteration: 2396 | Episodes: 146100 | Median Reward: -14.02 | Max Reward: 49.29
Iteration: 2397 | Episodes: 146150 | Median Reward: -13.12 | Max Reward: 49.29
Iteration: 2398 | Episodes: 146200 | Median Reward: -5.26 | Max Reward: 49.29
Iteration: 2398 | Episodes: 146250 | Median Reward: 7.24 | Max Reward: 49.29
Iteration: 2399 | Episodes: 146300 | Median Reward: -11.43 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2400      |
|    time_elapsed         | 43643     |
|    total_timesteps      | 14745600  |
| train/                  |           |
|    approx_kl            | 4.3999505 |
|    clip_fraction        | 0.277     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.674     |
|    learning_rate        | 0.0001    |
|    loss                 | 61.9      |
|    n_updates            | 23990     |
|    policy_gradient_loss | -0.0127   |
|    std                  | 3.77      |
|    value_loss           | 134       |
---------------------------------------
Iteration: 2400 | Episodes: 146350 | Median Reward: -18.66 | Max Reward: 49.29
Iteration: 2401 | Episodes: 146400 | Median Reward: -18.68 | Max Reward: 49.29
Iteration: 2402 | Episodes: 146450 | Median Reward: -15.80 | Max Reward: 49.29
Iteration: 2403 | Episodes: 146500 | Median Reward: -17.57 | Max Reward: 49.29
Iteration: 2403 | Episodes: 146550 | Median Reward: -23.05 | Max Reward: 49.29
Iteration: 2404 | Episodes: 146600 | Median Reward: -13.14 | Max Reward: 49.29
Iteration: 2405 | Episodes: 146650 | Median Reward: -8.98 | Max Reward: 49.29
Iteration: 2406 | Episodes: 146700 | Median Reward: -7.70 | Max Reward: 49.29
Iteration: 2407 | Episodes: 146750 | Median Reward: -3.24 | Max Reward: 49.29
Iteration: 2408 | Episodes: 146800 | Median Reward: 39.10 | Max Reward: 49.29
Iteration: 2408 | Episodes: 146850 | Median Reward: -10.96 | Max Reward: 49.29
Iteration: 2409 | Episodes: 146900 | Median Reward: 28.00 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -88.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2410      |
|    time_elapsed         | 43813     |
|    total_timesteps      | 14807040  |
| train/                  |           |
|    approx_kl            | 1.6297011 |
|    clip_fraction        | 0.0923    |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.668     |
|    learning_rate        | 0.0001    |
|    loss                 | 21.5      |
|    n_updates            | 24090     |
|    policy_gradient_loss | -0.0177   |
|    std                  | 3.78      |
|    value_loss           | 106       |
---------------------------------------
Iteration: 2410 | Episodes: 146950 | Median Reward: -8.30 | Max Reward: 49.29
Iteration: 2411 | Episodes: 147000 | Median Reward: -12.55 | Max Reward: 49.29
Iteration: 2412 | Episodes: 147050 | Median Reward: -18.14 | Max Reward: 49.29
Iteration: 2412 | Episodes: 147100 | Median Reward: -9.92 | Max Reward: 49.29
Iteration: 2413 | Episodes: 147150 | Median Reward: -7.38 | Max Reward: 49.29
Iteration: 2414 | Episodes: 147200 | Median Reward: -14.03 | Max Reward: 49.29
Iteration: 2415 | Episodes: 147250 | Median Reward: -14.54 | Max Reward: 49.29
Iteration: 2416 | Episodes: 147300 | Median Reward: -5.82 | Max Reward: 49.29
Iteration: 2417 | Episodes: 147350 | Median Reward: -15.57 | Max Reward: 49.29
Iteration: 2417 | Episodes: 147400 | Median Reward: -19.34 | Max Reward: 49.29
Iteration: 2418 | Episodes: 147450 | Median Reward: -17.85 | Max Reward: 49.29
Iteration: 2419 | Episodes: 147500 | Median Reward: 20.71 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -90.4     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2420      |
|    time_elapsed         | 43988     |
|    total_timesteps      | 14868480  |
| train/                  |           |
|    approx_kl            | 1.2310905 |
|    clip_fraction        | 0.0854    |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.706     |
|    learning_rate        | 0.0001    |
|    loss                 | 23.5      |
|    n_updates            | 24190     |
|    policy_gradient_loss | 0.00546   |
|    std                  | 3.79      |
|    value_loss           | 88.9      |
---------------------------------------
Iteration: 2420 | Episodes: 147550 | Median Reward: 35.13 | Max Reward: 49.29
Iteration: 2421 | Episodes: 147600 | Median Reward: -15.59 | Max Reward: 49.29
Iteration: 2421 | Episodes: 147650 | Median Reward: -10.63 | Max Reward: 49.29
Iteration: 2422 | Episodes: 147700 | Median Reward: -5.16 | Max Reward: 49.29
Iteration: 2423 | Episodes: 147750 | Median Reward: -16.25 | Max Reward: 49.29
Iteration: 2424 | Episodes: 147800 | Median Reward: -8.95 | Max Reward: 49.29
Iteration: 2425 | Episodes: 147850 | Median Reward: 17.74 | Max Reward: 49.29
Iteration: 2426 | Episodes: 147900 | Median Reward: 26.46 | Max Reward: 49.29
Iteration: 2426 | Episodes: 147950 | Median Reward: 9.60 | Max Reward: 49.29
Iteration: 2427 | Episodes: 148000 | Median Reward: -11.94 | Max Reward: 49.29
Iteration: 2428 | Episodes: 148050 | Median Reward: -10.58 | Max Reward: 49.29
Iteration: 2429 | Episodes: 148100 | Median Reward: -10.01 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -98.9    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2430     |
|    time_elapsed         | 44168    |
|    total_timesteps      | 14929920 |
| train/                  |          |
|    approx_kl            | 31.79374 |
|    clip_fraction        | 0.591    |
|    clip_range           | 0.4      |
|    entropy_loss         | -109     |
|    explained_variance   | 0.626    |
|    learning_rate        | 0.0001   |
|    loss                 | 18.8     |
|    n_updates            | 24290    |
|    policy_gradient_loss | 0.193    |
|    std                  | 3.79     |
|    value_loss           | 107      |
--------------------------------------
Iteration: 2430 | Episodes: 148150 | Median Reward: -9.43 | Max Reward: 49.29
Iteration: 2431 | Episodes: 148200 | Median Reward: 10.69 | Max Reward: 49.29
Iteration: 2431 | Episodes: 148250 | Median Reward: -15.32 | Max Reward: 49.29
Iteration: 2432 | Episodes: 148300 | Median Reward: -12.13 | Max Reward: 49.29
Iteration: 2433 | Episodes: 148350 | Median Reward: -10.60 | Max Reward: 49.29
Iteration: 2434 | Episodes: 148400 | Median Reward: -11.07 | Max Reward: 49.29
Iteration: 2435 | Episodes: 148450 | Median Reward: -9.92 | Max Reward: 49.29
Iteration: 2435 | Episodes: 148500 | Median Reward: -17.89 | Max Reward: 49.29
Iteration: 2436 | Episodes: 148550 | Median Reward: -16.19 | Max Reward: 49.29
Iteration: 2437 | Episodes: 148600 | Median Reward: -18.99 | Max Reward: 49.29
Iteration: 2438 | Episodes: 148650 | Median Reward: 35.63 | Max Reward: 49.29
Iteration: 2439 | Episodes: 148700 | Median Reward: 35.63 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -97       |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2440      |
|    time_elapsed         | 44343     |
|    total_timesteps      | 14991360  |
| train/                  |           |
|    approx_kl            | 38.102837 |
|    clip_fraction        | 0.48      |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.663     |
|    learning_rate        | 0.0001    |
|    loss                 | 94.4      |
|    n_updates            | 24390     |
|    policy_gradient_loss | 0.0411    |
|    std                  | 3.8       |
|    value_loss           | 180       |
---------------------------------------
Iteration: 2440 | Episodes: 148750 | Median Reward: -23.52 | Max Reward: 49.29
Iteration: 2440 | Episodes: 148800 | Median Reward: -0.41 | Max Reward: 49.29
Iteration: 2441 | Episodes: 148850 | Median Reward: -5.89 | Max Reward: 49.29
Iteration: 2442 | Episodes: 148900 | Median Reward: -8.71 | Max Reward: 49.29
Iteration: 2443 | Episodes: 148950 | Median Reward: -9.53 | Max Reward: 49.29
Iteration: 2444 | Episodes: 149000 | Median Reward: -15.90 | Max Reward: 49.29
Iteration: 2445 | Episodes: 149050 | Median Reward: 10.88 | Max Reward: 49.29
Iteration: 2445 | Episodes: 149100 | Median Reward: 31.22 | Max Reward: 49.29
Iteration: 2446 | Episodes: 149150 | Median Reward: 30.18 | Max Reward: 49.29
Iteration: 2447 | Episodes: 149200 | Median Reward: 19.37 | Max Reward: 49.29
Iteration: 2448 | Episodes: 149250 | Median Reward: 10.16 | Max Reward: 49.29
Iteration: 2449 | Episodes: 149300 | Median Reward: 32.91 | Max Reward: 49.29
Iteration: 2449 | Episodes: 149350 | Median Reward: 39.82 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -73.6    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2450     |
|    time_elapsed         | 44521    |
|    total_timesteps      | 15052800 |
| train/                  |          |
|    approx_kl            | 2.231093 |
|    clip_fraction        | 0.0729   |
|    clip_range           | 0.4      |
|    entropy_loss         | -109     |
|    explained_variance   | 0.694    |
|    learning_rate        | 0.0001   |
|    loss                 | 82.6     |
|    n_updates            | 24490    |
|    policy_gradient_loss | -0.00819 |
|    std                  | 3.81     |
|    value_loss           | 136      |
--------------------------------------
Iteration: 2450 | Episodes: 149400 | Median Reward: 28.49 | Max Reward: 49.29
Iteration: 2451 | Episodes: 149450 | Median Reward: 31.12 | Max Reward: 49.29
Iteration: 2452 | Episodes: 149500 | Median Reward: 31.12 | Max Reward: 49.29
Iteration: 2453 | Episodes: 149550 | Median Reward: 28.32 | Max Reward: 49.29
Iteration: 2454 | Episodes: 149600 | Median Reward: -23.15 | Max Reward: 49.29
Iteration: 2454 | Episodes: 149650 | Median Reward: 34.80 | Max Reward: 49.29
Iteration: 2455 | Episodes: 149700 | Median Reward: -13.58 | Max Reward: 49.29
Iteration: 2456 | Episodes: 149750 | Median Reward: -4.98 | Max Reward: 49.29
Iteration: 2457 | Episodes: 149800 | Median Reward: -11.60 | Max Reward: 49.29
Iteration: 2458 | Episodes: 149850 | Median Reward: -19.91 | Max Reward: 49.29
Iteration: 2458 | Episodes: 149900 | Median Reward: -20.63 | Max Reward: 49.29
Iteration: 2459 | Episodes: 149950 | Median Reward: -22.50 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -112      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2460      |
|    time_elapsed         | 44695     |
|    total_timesteps      | 15114240  |
| train/                  |           |
|    approx_kl            | 0.5989436 |
|    clip_fraction        | 0.0949    |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.696     |
|    learning_rate        | 0.0001    |
|    loss                 | 51.5      |
|    n_updates            | 24590     |
|    policy_gradient_loss | 0.00249   |
|    std                  | 3.81      |
|    value_loss           | 97.3      |
---------------------------------------
Iteration: 2460 | Episodes: 150000 | Median Reward: 6.77 | Max Reward: 49.29
Iteration: 2461 | Episodes: 150050 | Median Reward: -0.36 | Max Reward: 49.29
Iteration: 2462 | Episodes: 150100 | Median Reward: -7.48 | Max Reward: 49.29
Iteration: 2463 | Episodes: 150150 | Median Reward: -10.07 | Max Reward: 49.29
Iteration: 2463 | Episodes: 150200 | Median Reward: 40.92 | Max Reward: 49.29
Iteration: 2464 | Episodes: 150250 | Median Reward: -23.85 | Max Reward: 49.29
Iteration: 2465 | Episodes: 150300 | Median Reward: -19.47 | Max Reward: 49.29
Iteration: 2466 | Episodes: 150350 | Median Reward: -18.98 | Max Reward: 49.29
Iteration: 2467 | Episodes: 150400 | Median Reward: -19.84 | Max Reward: 49.29
Iteration: 2468 | Episodes: 150450 | Median Reward: -21.65 | Max Reward: 49.29
Iteration: 2468 | Episodes: 150500 | Median Reward: -18.37 | Max Reward: 49.29
Iteration: 2469 | Episodes: 150550 | Median Reward: -24.67 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -117     |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2470     |
|    time_elapsed         | 44878    |
|    total_timesteps      | 15175680 |
| train/                  |          |
|    approx_kl            | 2.081065 |
|    clip_fraction        | 0.106    |
|    clip_range           | 0.4      |
|    entropy_loss         | -113     |
|    explained_variance   | 0.676    |
|    learning_rate        | 0.0001   |
|    loss                 | 16.5     |
|    n_updates            | 24690    |
|    policy_gradient_loss | -0.00233 |
|    std                  | 3.82     |
|    value_loss           | 62.6     |
--------------------------------------
Iteration: 2470 | Episodes: 150600 | Median Reward: -21.54 | Max Reward: 49.29
Iteration: 2471 | Episodes: 150650 | Median Reward: -16.72 | Max Reward: 49.29
Iteration: 2472 | Episodes: 150700 | Median Reward: -23.33 | Max Reward: 49.29
Iteration: 2472 | Episodes: 150750 | Median Reward: -18.35 | Max Reward: 49.29
Iteration: 2473 | Episodes: 150800 | Median Reward: -10.87 | Max Reward: 49.29
Iteration: 2474 | Episodes: 150850 | Median Reward: -14.10 | Max Reward: 49.29
Iteration: 2475 | Episodes: 150900 | Median Reward: -16.26 | Max Reward: 49.29
Iteration: 2476 | Episodes: 150950 | Median Reward: -18.18 | Max Reward: 49.29
Iteration: 2477 | Episodes: 151000 | Median Reward: -2.58 | Max Reward: 49.29
Iteration: 2477 | Episodes: 151050 | Median Reward: 32.77 | Max Reward: 49.29
Iteration: 2478 | Episodes: 151100 | Median Reward: 17.01 | Max Reward: 49.29
Iteration: 2479 | Episodes: 151150 | Median Reward: 41.95 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78.8     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2480      |
|    time_elapsed         | 45052     |
|    total_timesteps      | 15237120  |
| train/                  |           |
|    approx_kl            | 18.634972 |
|    clip_fraction        | 0.332     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.682     |
|    learning_rate        | 0.0001    |
|    loss                 | 68.5      |
|    n_updates            | 24790     |
|    policy_gradient_loss | 0.0389    |
|    std                  | 3.83      |
|    value_loss           | 124       |
---------------------------------------
Iteration: 2480 | Episodes: 151200 | Median Reward: 41.60 | Max Reward: 49.29
Iteration: 2481 | Episodes: 151250 | Median Reward: -3.60 | Max Reward: 49.29
Iteration: 2481 | Episodes: 151300 | Median Reward: -2.33 | Max Reward: 49.29
Iteration: 2482 | Episodes: 151350 | Median Reward: 29.98 | Max Reward: 49.29
Iteration: 2483 | Episodes: 151400 | Median Reward: 18.15 | Max Reward: 49.29
Iteration: 2484 | Episodes: 151450 | Median Reward: 26.51 | Max Reward: 49.29
Iteration: 2485 | Episodes: 151500 | Median Reward: 26.67 | Max Reward: 49.29
Iteration: 2486 | Episodes: 151550 | Median Reward: 21.90 | Max Reward: 49.29
Iteration: 2486 | Episodes: 151600 | Median Reward: 24.80 | Max Reward: 49.29
Iteration: 2487 | Episodes: 151650 | Median Reward: 34.39 | Max Reward: 49.29
Iteration: 2488 | Episodes: 151700 | Median Reward: 32.36 | Max Reward: 49.29
Iteration: 2489 | Episodes: 151750 | Median Reward: 37.27 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -79       |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2490      |
|    time_elapsed         | 45229     |
|    total_timesteps      | 15298560  |
| train/                  |           |
|    approx_kl            | 3.9749746 |
|    clip_fraction        | 0.137     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.661     |
|    learning_rate        | 0.0001    |
|    loss                 | 79.1      |
|    n_updates            | 24890     |
|    policy_gradient_loss | 0.0311    |
|    std                  | 3.83      |
|    value_loss           | 175       |
---------------------------------------
Iteration: 2490 | Episodes: 151800 | Median Reward: 31.94 | Max Reward: 49.29
Iteration: 2491 | Episodes: 151850 | Median Reward: 26.61 | Max Reward: 49.29
Iteration: 2491 | Episodes: 151900 | Median Reward: 28.51 | Max Reward: 49.29
Iteration: 2492 | Episodes: 151950 | Median Reward: 35.29 | Max Reward: 49.29
Iteration: 2493 | Episodes: 152000 | Median Reward: 34.98 | Max Reward: 49.29
Iteration: 2494 | Episodes: 152050 | Median Reward: 12.24 | Max Reward: 49.29
Iteration: 2495 | Episodes: 152100 | Median Reward: 11.76 | Max Reward: 49.29
Iteration: 2495 | Episodes: 152150 | Median Reward: 20.03 | Max Reward: 49.29
Iteration: 2496 | Episodes: 152200 | Median Reward: -11.75 | Max Reward: 49.29
Iteration: 2497 | Episodes: 152250 | Median Reward: 0.49 | Max Reward: 49.29
Iteration: 2498 | Episodes: 152300 | Median Reward: -1.09 | Max Reward: 49.29
Iteration: 2499 | Episodes: 152350 | Median Reward: -10.26 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -101     |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2500     |
|    time_elapsed         | 45413    |
|    total_timesteps      | 15360000 |
| train/                  |          |
|    approx_kl            | 8.932184 |
|    clip_fraction        | 0.168    |
|    clip_range           | 0.4      |
|    entropy_loss         | -110     |
|    explained_variance   | 0.763    |
|    learning_rate        | 0.0001   |
|    loss                 | 17.3     |
|    n_updates            | 24990    |
|    policy_gradient_loss | 0.0454   |
|    std                  | 3.84     |
|    value_loss           | 61       |
--------------------------------------
Iteration: 2500 | Episodes: 152400 | Median Reward: -7.46 | Max Reward: 49.29
Iteration: 2500 | Episodes: 152450 | Median Reward: -4.09 | Max Reward: 49.29
Iteration: 2501 | Episodes: 152500 | Median Reward: -8.78 | Max Reward: 49.29
Iteration: 2502 | Episodes: 152550 | Median Reward: -12.48 | Max Reward: 49.29
Iteration: 2503 | Episodes: 152600 | Median Reward: -7.50 | Max Reward: 49.29
Iteration: 2504 | Episodes: 152650 | Median Reward: -12.51 | Max Reward: 49.29
Iteration: 2504 | Episodes: 152700 | Median Reward: 6.50 | Max Reward: 49.29
Iteration: 2505 | Episodes: 152750 | Median Reward: 32.25 | Max Reward: 49.29
Iteration: 2506 | Episodes: 152800 | Median Reward: 8.18 | Max Reward: 49.29
Iteration: 2507 | Episodes: 152850 | Median Reward: -19.23 | Max Reward: 49.29
Iteration: 2508 | Episodes: 152900 | Median Reward: -24.28 | Max Reward: 49.29
Iteration: 2509 | Episodes: 152950 | Median Reward: -23.70 | Max Reward: 49.29
Iteration: 2509 | Episodes: 153000 | Median Reward: -18.04 | Max Reward: 49.29
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -118        |
| time/                   |             |
|    fps                  | 338         |
|    iterations           | 2510        |
|    time_elapsed         | 45595       |
|    total_timesteps      | 15421440    |
| train/                  |             |
|    approx_kl            | 0.009614259 |
|    clip_fraction        | 0.00267     |
|    clip_range           | 0.4         |
|    entropy_loss         | -115        |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0001      |
|    loss                 | 11.2        |
|    n_updates            | 25090       |
|    policy_gradient_loss | -0.001      |
|    std                  | 3.84        |
|    value_loss           | 37.8        |
-----------------------------------------
Iteration: 2510 | Episodes: 153050 | Median Reward: -12.29 | Max Reward: 49.29
Iteration: 2511 | Episodes: 153100 | Median Reward: -8.51 | Max Reward: 49.29
Iteration: 2512 | Episodes: 153150 | Median Reward: -8.51 | Max Reward: 49.29
Iteration: 2513 | Episodes: 153200 | Median Reward: -18.30 | Max Reward: 49.29
Iteration: 2514 | Episodes: 153250 | Median Reward: -17.14 | Max Reward: 49.29
Iteration: 2514 | Episodes: 153300 | Median Reward: -31.88 | Max Reward: 49.29
Iteration: 2515 | Episodes: 153350 | Median Reward: -19.12 | Max Reward: 49.29
Iteration: 2516 | Episodes: 153400 | Median Reward: -20.91 | Max Reward: 49.29
Iteration: 2517 | Episodes: 153450 | Median Reward: -28.53 | Max Reward: 49.29
Iteration: 2518 | Episodes: 153500 | Median Reward: -26.59 | Max Reward: 49.29
Iteration: 2518 | Episodes: 153550 | Median Reward: -20.33 | Max Reward: 49.29
Iteration: 2519 | Episodes: 153600 | Median Reward: -13.60 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -106     |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2520     |
|    time_elapsed         | 45776    |
|    total_timesteps      | 15482880 |
| train/                  |          |
|    approx_kl            | 1.275136 |
|    clip_fraction        | 0.0246   |
|    clip_range           | 0.4      |
|    entropy_loss         | -114     |
|    explained_variance   | 0.763    |
|    learning_rate        | 0.0001   |
|    loss                 | 11.5     |
|    n_updates            | 25190    |
|    policy_gradient_loss | 0.0019   |
|    std                  | 3.85     |
|    value_loss           | 59.7     |
--------------------------------------
Iteration: 2520 | Episodes: 153650 | Median Reward: 6.53 | Max Reward: 49.29
Iteration: 2521 | Episodes: 153700 | Median Reward: 0.08 | Max Reward: 49.29
Iteration: 2522 | Episodes: 153750 | Median Reward: -19.03 | Max Reward: 49.29
Iteration: 2523 | Episodes: 153800 | Median Reward: -21.75 | Max Reward: 49.29
Iteration: 2523 | Episodes: 153850 | Median Reward: 8.56 | Max Reward: 49.29
Iteration: 2524 | Episodes: 153900 | Median Reward: 2.96 | Max Reward: 49.29
Iteration: 2525 | Episodes: 153950 | Median Reward: 34.78 | Max Reward: 49.29
Iteration: 2526 | Episodes: 154000 | Median Reward: 41.28 | Max Reward: 49.29
Iteration: 2527 | Episodes: 154050 | Median Reward: 41.35 | Max Reward: 49.29
Iteration: 2528 | Episodes: 154100 | Median Reward: 44.64 | Max Reward: 49.29
Iteration: 2528 | Episodes: 154150 | Median Reward: 24.29 | Max Reward: 49.29
Iteration: 2529 | Episodes: 154200 | Median Reward: -7.23 | Max Reward: 49.29
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -90      |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2530     |
|    time_elapsed         | 45964    |
|    total_timesteps      | 15544320 |
| train/                  |          |
|    approx_kl            | 4.203721 |
|    clip_fraction        | 0.102    |
|    clip_range           | 0.4      |
|    entropy_loss         | -113     |
|    explained_variance   | 0.68     |
|    learning_rate        | 0.0001   |
|    loss                 | 68.9     |
|    n_updates            | 25290    |
|    policy_gradient_loss | 0.0138   |
|    std                  | 3.86     |
|    value_loss           | 132      |
--------------------------------------
Iteration: 2530 | Episodes: 154250 | Median Reward: 43.87 | Max Reward: 49.29
Iteration: 2531 | Episodes: 154300 | Median Reward: 45.52 | Max Reward: 49.29
Iteration: 2532 | Episodes: 154350 | Median Reward: 25.40 | Max Reward: 49.29
Iteration: 2532 | Episodes: 154400 | Median Reward: 47.16 | Max Reward: 49.29
Iteration: 2533 | Episodes: 154450 | Median Reward: 46.78 | Max Reward: 49.29
Iteration: 2534 | Episodes: 154500 | Median Reward: 47.15 | Max Reward: 49.29
Iteration: 2535 | Episodes: 154550 | Median Reward: 47.59 | Max Reward: 49.29
Iteration: 2536 | Episodes: 154600 | Median Reward: 47.39 | Max Reward: 49.29
Iteration: 2536 | Episodes: 154650 | Median Reward: 47.21 | Max Reward: 49.29
Iteration: 2537 | Episodes: 154700 | Median Reward: 46.43 | Max Reward: 49.29
Iteration: 2538 | Episodes: 154750 | Median Reward: 45.85 | Max Reward: 49.29
Iteration: 2539 | Episodes: 154800 | Median Reward: 46.03 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 98.5      |
|    ep_rew_mean          | -51.3     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2540      |
|    time_elapsed         | 46145     |
|    total_timesteps      | 15605760  |
| train/                  |           |
|    approx_kl            | 2.6482797 |
|    clip_fraction        | 0.0679    |
|    clip_range           | 0.4       |
|    entropy_loss         | -113      |
|    explained_variance   | 0.629     |
|    learning_rate        | 0.0001    |
|    loss                 | 105       |
|    n_updates            | 25390     |
|    policy_gradient_loss | 0.00259   |
|    std                  | 3.88      |
|    value_loss           | 247       |
---------------------------------------
Iteration: 2540 | Episodes: 154850 | Median Reward: 47.24 | Max Reward: 49.29
Iteration: 2540 | Episodes: 154900 | Median Reward: 47.29 | Max Reward: 49.29
Iteration: 2541 | Episodes: 154950 | Median Reward: 47.78 | Max Reward: 49.29
Iteration: 2542 | Episodes: 155000 | Median Reward: 47.78 | Max Reward: 49.29
Iteration: 2543 | Episodes: 155050 | Median Reward: 47.24 | Max Reward: 49.29
Iteration: 2544 | Episodes: 155100 | Median Reward: 47.04 | Max Reward: 49.29
Iteration: 2545 | Episodes: 155150 | Median Reward: 47.38 | Max Reward: 49.29
Iteration: 2545 | Episodes: 155200 | Median Reward: 46.96 | Max Reward: 49.29
Iteration: 2546 | Episodes: 155250 | Median Reward: 47.49 | Max Reward: 49.29
Iteration: 2547 | Episodes: 155300 | Median Reward: 47.12 | Max Reward: 49.29
Iteration: 2548 | Episodes: 155350 | Median Reward: 47.01 | Max Reward: 49.29
Iteration: 2549 | Episodes: 155400 | Median Reward: 44.83 | Max Reward: 49.29
Iteration: 2549 | Episodes: 155450 | Median Reward: 14.44 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2550      |
|    time_elapsed         | 46324     |
|    total_timesteps      | 15667200  |
| train/                  |           |
|    approx_kl            | 6.3215733 |
|    clip_fraction        | 0.076     |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.647     |
|    learning_rate        | 0.0001    |
|    loss                 | 113       |
|    n_updates            | 25490     |
|    policy_gradient_loss | 0.00879   |
|    std                  | 3.9       |
|    value_loss           | 224       |
---------------------------------------
Iteration: 2550 | Episodes: 155500 | Median Reward: 21.20 | Max Reward: 49.29
Iteration: 2551 | Episodes: 155550 | Median Reward: 39.10 | Max Reward: 49.29
Iteration: 2552 | Episodes: 155600 | Median Reward: 42.95 | Max Reward: 49.29
Iteration: 2553 | Episodes: 155650 | Median Reward: 45.57 | Max Reward: 49.29
Iteration: 2554 | Episodes: 155700 | Median Reward: 36.89 | Max Reward: 49.29
Iteration: 2554 | Episodes: 155750 | Median Reward: 45.06 | Max Reward: 49.29
Iteration: 2555 | Episodes: 155800 | Median Reward: 47.27 | Max Reward: 49.29
Iteration: 2556 | Episodes: 155850 | Median Reward: 47.53 | Max Reward: 49.29
Iteration: 2557 | Episodes: 155900 | Median Reward: 47.52 | Max Reward: 49.29
Iteration: 2558 | Episodes: 155950 | Median Reward: 46.55 | Max Reward: 49.29
Iteration: 2558 | Episodes: 156000 | Median Reward: 47.10 | Max Reward: 49.29
Iteration: 2559 | Episodes: 156050 | Median Reward: 46.57 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -55.4     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2560      |
|    time_elapsed         | 46502     |
|    total_timesteps      | 15728640  |
| train/                  |           |
|    approx_kl            | 2.6923716 |
|    clip_fraction        | 0.103     |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.641     |
|    learning_rate        | 0.0001    |
|    loss                 | 99.4      |
|    n_updates            | 25590     |
|    policy_gradient_loss | 0.0172    |
|    std                  | 3.92      |
|    value_loss           | 201       |
---------------------------------------
Iteration: 2560 | Episodes: 156100 | Median Reward: 45.72 | Max Reward: 49.29
Iteration: 2561 | Episodes: 156150 | Median Reward: 33.78 | Max Reward: 49.29
Iteration: 2562 | Episodes: 156200 | Median Reward: 46.22 | Max Reward: 49.29
Iteration: 2562 | Episodes: 156250 | Median Reward: 46.97 | Max Reward: 49.29
Iteration: 2563 | Episodes: 156300 | Median Reward: 47.33 | Max Reward: 49.29
Iteration: 2564 | Episodes: 156350 | Median Reward: 46.58 | Max Reward: 49.29
Iteration: 2565 | Episodes: 156400 | Median Reward: 47.67 | Max Reward: 49.29
Iteration: 2566 | Episodes: 156450 | Median Reward: 47.89 | Max Reward: 49.29
Iteration: 2567 | Episodes: 156500 | Median Reward: 46.92 | Max Reward: 49.29
Iteration: 2567 | Episodes: 156550 | Median Reward: 47.52 | Max Reward: 49.29
Iteration: 2568 | Episodes: 156600 | Median Reward: 30.56 | Max Reward: 49.29
Iteration: 2569 | Episodes: 156650 | Median Reward: 33.54 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -64.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2570      |
|    time_elapsed         | 46694     |
|    total_timesteps      | 15790080  |
| train/                  |           |
|    approx_kl            | 1.4239948 |
|    clip_fraction        | 0.0705    |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.734     |
|    learning_rate        | 0.0001    |
|    loss                 | 48.5      |
|    n_updates            | 25690     |
|    policy_gradient_loss | 0.000994  |
|    std                  | 3.95      |
|    value_loss           | 124       |
---------------------------------------
Iteration: 2570 | Episodes: 156700 | Median Reward: 42.45 | Max Reward: 49.29
Iteration: 2571 | Episodes: 156750 | Median Reward: 46.87 | Max Reward: 49.29
Iteration: 2572 | Episodes: 156800 | Median Reward: 17.42 | Max Reward: 49.29
Iteration: 2572 | Episodes: 156850 | Median Reward: 30.10 | Max Reward: 49.29
Iteration: 2573 | Episodes: 156900 | Median Reward: 37.07 | Max Reward: 49.29
Iteration: 2574 | Episodes: 156950 | Median Reward: 35.88 | Max Reward: 49.29
Iteration: 2575 | Episodes: 157000 | Median Reward: 25.53 | Max Reward: 49.29
Iteration: 2576 | Episodes: 157050 | Median Reward: 19.60 | Max Reward: 49.29
Iteration: 2576 | Episodes: 157100 | Median Reward: 17.23 | Max Reward: 49.29
Iteration: 2577 | Episodes: 157150 | Median Reward: 27.90 | Max Reward: 49.29
Iteration: 2578 | Episodes: 157200 | Median Reward: 11.87 | Max Reward: 49.29
Iteration: 2579 | Episodes: 157250 | Median Reward: 5.81 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2580      |
|    time_elapsed         | 46876     |
|    total_timesteps      | 15851520  |
| train/                  |           |
|    approx_kl            | 2.2658553 |
|    clip_fraction        | 0.0987    |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.819     |
|    learning_rate        | 0.0001    |
|    loss                 | 17.6      |
|    n_updates            | 25790     |
|    policy_gradient_loss | 0.00167   |
|    std                  | 3.97      |
|    value_loss           | 58.9      |
---------------------------------------
Iteration: 2580 | Episodes: 157300 | Median Reward: 13.83 | Max Reward: 49.29
Iteration: 2580 | Episodes: 157350 | Median Reward: 47.35 | Max Reward: 49.29
Iteration: 2581 | Episodes: 157400 | Median Reward: 46.33 | Max Reward: 49.29
Iteration: 2582 | Episodes: 157450 | Median Reward: 46.44 | Max Reward: 49.29
Iteration: 2583 | Episodes: 157500 | Median Reward: 46.49 | Max Reward: 49.29
Iteration: 2584 | Episodes: 157550 | Median Reward: 44.15 | Max Reward: 49.29
Iteration: 2584 | Episodes: 157600 | Median Reward: 42.33 | Max Reward: 49.29
Iteration: 2585 | Episodes: 157650 | Median Reward: 30.96 | Max Reward: 49.29
Iteration: 2586 | Episodes: 157700 | Median Reward: 22.00 | Max Reward: 49.29
Iteration: 2587 | Episodes: 157750 | Median Reward: 18.44 | Max Reward: 49.29
Iteration: 2587 | Episodes: 157800 | Median Reward: 17.73 | Max Reward: 49.29
Iteration: 2588 | Episodes: 157850 | Median Reward: 47.55 | Max Reward: 49.29
Iteration: 2589 | Episodes: 157900 | Median Reward: 47.27 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -53.6     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2590      |
|    time_elapsed         | 47058     |
|    total_timesteps      | 15912960  |
| train/                  |           |
|    approx_kl            | 2.5441194 |
|    clip_fraction        | 0.0902    |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.681     |
|    learning_rate        | 0.0001    |
|    loss                 | 92.4      |
|    n_updates            | 25890     |
|    policy_gradient_loss | 0.0136    |
|    std                  | 3.99      |
|    value_loss           | 206       |
---------------------------------------
Iteration: 2590 | Episodes: 157950 | Median Reward: 47.31 | Max Reward: 49.29
Iteration: 2590 | Episodes: 158000 | Median Reward: 48.01 | Max Reward: 49.29
Iteration: 2591 | Episodes: 158050 | Median Reward: 47.77 | Max Reward: 49.29
Iteration: 2592 | Episodes: 158100 | Median Reward: 47.79 | Max Reward: 49.29
Iteration: 2593 | Episodes: 158150 | Median Reward: 47.18 | Max Reward: 49.29
Iteration: 2594 | Episodes: 158200 | Median Reward: 47.05 | Max Reward: 49.29
Iteration: 2594 | Episodes: 158250 | Median Reward: 48.34 | Max Reward: 49.29
Iteration: 2595 | Episodes: 158300 | Median Reward: 44.05 | Max Reward: 49.29
Iteration: 2596 | Episodes: 158350 | Median Reward: 27.67 | Max Reward: 49.29
Iteration: 2597 | Episodes: 158400 | Median Reward: 21.20 | Max Reward: 49.29
Iteration: 2598 | Episodes: 158450 | Median Reward: 44.18 | Max Reward: 49.29
Iteration: 2598 | Episodes: 158500 | Median Reward: 38.90 | Max Reward: 49.29
Iteration: 2599 | Episodes: 158550 | Median Reward: 44.95 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -58.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2600      |
|    time_elapsed         | 47250     |
|    total_timesteps      | 15974400  |
| train/                  |           |
|    approx_kl            | 1.9823447 |
|    clip_fraction        | 0.0659    |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.721     |
|    learning_rate        | 0.0001    |
|    loss                 | 34.8      |
|    n_updates            | 25990     |
|    policy_gradient_loss | 0.00293   |
|    std                  | 4.01      |
|    value_loss           | 141       |
---------------------------------------
Iteration: 2600 | Episodes: 158600 | Median Reward: 47.26 | Max Reward: 49.29
Iteration: 2601 | Episodes: 158650 | Median Reward: 47.05 | Max Reward: 49.29
Iteration: 2602 | Episodes: 158700 | Median Reward: 45.55 | Max Reward: 49.29
Iteration: 2602 | Episodes: 158750 | Median Reward: 25.05 | Max Reward: 49.29
Iteration: 2603 | Episodes: 158800 | Median Reward: 23.47 | Max Reward: 49.29
Iteration: 2604 | Episodes: 158850 | Median Reward: 43.96 | Max Reward: 49.29
Iteration: 2605 | Episodes: 158900 | Median Reward: 43.96 | Max Reward: 49.29
Iteration: 2606 | Episodes: 158950 | Median Reward: 31.56 | Max Reward: 49.29
Iteration: 2607 | Episodes: 159000 | Median Reward: 39.71 | Max Reward: 49.29
Iteration: 2607 | Episodes: 159050 | Median Reward: 46.60 | Max Reward: 49.29
Iteration: 2608 | Episodes: 159100 | Median Reward: 46.74 | Max Reward: 49.29
Iteration: 2609 | Episodes: 159150 | Median Reward: 46.71 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 98.2      |
|    ep_rew_mean          | -51.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2610      |
|    time_elapsed         | 47429     |
|    total_timesteps      | 16035840  |
| train/                  |           |
|    approx_kl            | 1.4765947 |
|    clip_fraction        | 0.0501    |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.677     |
|    learning_rate        | 0.0001    |
|    loss                 | 88        |
|    n_updates            | 26090     |
|    policy_gradient_loss | 0.00644   |
|    std                  | 4.03      |
|    value_loss           | 181       |
---------------------------------------
Iteration: 2610 | Episodes: 159200 | Median Reward: 46.77 | Max Reward: 49.29
Iteration: 2610 | Episodes: 159250 | Median Reward: 46.22 | Max Reward: 49.29
Iteration: 2611 | Episodes: 159300 | Median Reward: 46.07 | Max Reward: 49.29
Iteration: 2612 | Episodes: 159350 | Median Reward: 46.34 | Max Reward: 49.29
Iteration: 2613 | Episodes: 159400 | Median Reward: 46.34 | Max Reward: 49.29
Iteration: 2614 | Episodes: 159450 | Median Reward: 35.46 | Max Reward: 49.29
Iteration: 2614 | Episodes: 159500 | Median Reward: 44.11 | Max Reward: 49.29
Iteration: 2615 | Episodes: 159550 | Median Reward: 22.58 | Max Reward: 49.29
Iteration: 2616 | Episodes: 159600 | Median Reward: 20.16 | Max Reward: 49.29
Iteration: 2617 | Episodes: 159650 | Median Reward: 26.19 | Max Reward: 49.29
Iteration: 2618 | Episodes: 159700 | Median Reward: 29.44 | Max Reward: 49.29
Iteration: 2618 | Episodes: 159750 | Median Reward: 44.59 | Max Reward: 49.29
Iteration: 2619 | Episodes: 159800 | Median Reward: 44.85 | Max Reward: 49.29
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -58.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2620      |
|    time_elapsed         | 47604     |
|    total_timesteps      | 16097280  |
| train/                  |           |
|    approx_kl            | 1.4347774 |
|    clip_fraction        | 0.0687    |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.713     |
|    learning_rate        | 0.0001    |
|    loss                 | 78.4      |
|    n_updates            | 26190     |
|    policy_gradient_loss | 0.0211    |
|    std                  | 4.05      |
|    value_loss           | 166       |
---------------------------------------
Iteration: 2620 | Episodes: 159850 | Median Reward: 45.11 | Max Reward: 49.29
Iteration: 2621 | Episodes: 159900 | Median Reward: 37.91 | Max Reward: 49.29
Iteration: 2622 | Episodes: 159950 | Median Reward: 12.08 | Max Reward: 49.29
Iteration: 2623 | Episodes: 160000 | Median Reward: 9.14 | Max Reward: 49.29
Iteration: 2623 | Episodes: 160050 | Median Reward: -20.47 | Max Reward: 49.29
Iteration: 2624 | Episodes: 160100 | Median Reward: 32.75 | Max Reward: 49.29
Iteration: 2625 | Episodes: 160150 | Median Reward: 32.75 | Max Reward: 49.29
Iteration: 2626 | Episodes: 160200 | Median Reward: -13.21 | Max Reward: 49.29
Iteration: 2626 | Episodes: 160250 | Median Reward: -0.58 | Max Reward: 49.29
Iteration: 2627 | Episodes: 160300 | Median Reward: -14.80 | Max Reward: 49.29
Iteration: 2628 | Episodes: 160350 | Median Reward: 30.50 | Max Reward: 49.29
Iteration: 2629 | Episodes: 160400 | Median Reward: 41.36 | Max Reward: 49.29
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 91.1       |
|    ep_rew_mean          | -53.5      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2630       |
|    time_elapsed         | 47783      |
|    total_timesteps      | 16158720   |
| train/                  |            |
|    approx_kl            | 0.15362383 |
|    clip_fraction        | 0.0202     |
|    clip_range           | 0.4        |
|    entropy_loss         | -116       |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.0001     |
|    loss                 | 71.9       |
|    n_updates            | 26290      |
|    policy_gradient_loss | -0.00184   |
|    std                  | 4.07       |
|    value_loss           | 140        |
----------------------------------------
Iteration: 2630 | Episodes: 160450 | Median Reward: 43.03 | Max Reward: 49.29
Iteration: 2630 | Episodes: 160500 | Median Reward: 23.59 | Max Reward: 49.29
Iteration: 2631 | Episodes: 160550 | Median Reward: 43.28 | Max Reward: 49.29
Iteration: 2632 | Episodes: 160600 | Median Reward: 45.24 | Max Reward: 49.29
Iteration: 2633 | Episodes: 160650 | Median Reward: 38.38 | Max Reward: 49.29
Iteration: 2633 | Episodes: 160700 | Median Reward: 27.60 | Max Reward: 49.29
Iteration: 2634 | Episodes: 160750 | Median Reward: 4.36 | Max Reward: 49.29
Iteration: 2635 | Episodes: 160800 | Median Reward: -10.55 | Max Reward: 49.29
Iteration: 2636 | Episodes: 160850 | Median Reward: -11.96 | Max Reward: 49.29
Iteration: 2637 | Episodes: 160900 | Median Reward: -2.52 | Max Reward: 49.29
Iteration: 2637 | Episodes: 160950 | Median Reward: 24.28 | Max Reward: 49.29
Iteration: 2638 | Episodes: 161000 | Median Reward: -5.61 | Max Reward: 49.29
Iteration: 2639 | Episodes: 161050 | Median Reward: -13.34 | Max Reward: 49.29
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -99.9      |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2640       |
|    time_elapsed         | 47976      |
|    total_timesteps      | 16220160   |
| train/                  |            |
|    approx_kl            | 0.13488887 |
|    clip_fraction        | 0.0215     |
|    clip_range           | 0.4        |
|    entropy_loss         | -116       |
|    explained_variance   | 0.695      |
|    learning_rate        | 0.0001     |
|    loss                 | 33.3       |
|    n_updates            | 26390      |
|    policy_gradient_loss | -0.000945  |
|    std                  | 4.08       |
|    value_loss           | 115        |
----------------------------------------
Iteration: 2640 | Episodes: 161100 | Median Reward: -8.79 | Max Reward: 49.29
Iteration: 2641 | Episodes: 161150 | Median Reward: -4.45 | Max Reward: 49.45
Iteration: 2641 | Episodes: 161200 | Median Reward: 20.98 | Max Reward: 49.45
Iteration: 2642 | Episodes: 161250 | Median Reward: 28.30 | Max Reward: 49.45
Iteration: 2643 | Episodes: 161300 | Median Reward: 28.21 | Max Reward: 49.45
Iteration: 2644 | Episodes: 161350 | Median Reward: 33.68 | Max Reward: 49.45
Iteration: 2645 | Episodes: 161400 | Median Reward: -11.98 | Max Reward: 49.45
Iteration: 2645 | Episodes: 161450 | Median Reward: -17.04 | Max Reward: 49.45
Iteration: 2646 | Episodes: 161500 | Median Reward: -6.72 | Max Reward: 49.45
Iteration: 2647 | Episodes: 161550 | Median Reward: -0.39 | Max Reward: 49.45
Iteration: 2648 | Episodes: 161600 | Median Reward: 34.41 | Max Reward: 49.45
Iteration: 2649 | Episodes: 161650 | Median Reward: 14.87 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.5     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2650      |
|    time_elapsed         | 48156     |
|    total_timesteps      | 16281600  |
| train/                  |           |
|    approx_kl            | 1.1493876 |
|    clip_fraction        | 0.054     |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.663     |
|    learning_rate        | 0.0001    |
|    loss                 | 38.3      |
|    n_updates            | 26490     |
|    policy_gradient_loss | -0.000463 |
|    std                  | 4.09      |
|    value_loss           | 124       |
---------------------------------------
Iteration: 2650 | Episodes: 161700 | Median Reward: 46.03 | Max Reward: 49.45
Iteration: 2650 | Episodes: 161750 | Median Reward: 46.33 | Max Reward: 49.45
Iteration: 2651 | Episodes: 161800 | Median Reward: 46.65 | Max Reward: 49.45
Iteration: 2652 | Episodes: 161850 | Median Reward: 46.79 | Max Reward: 49.45
Iteration: 2653 | Episodes: 161900 | Median Reward: 46.94 | Max Reward: 49.45
Iteration: 2654 | Episodes: 161950 | Median Reward: 44.87 | Max Reward: 49.45
Iteration: 2654 | Episodes: 162000 | Median Reward: -0.48 | Max Reward: 49.45
Iteration: 2655 | Episodes: 162050 | Median Reward: 7.30 | Max Reward: 49.45
Iteration: 2656 | Episodes: 162100 | Median Reward: 4.29 | Max Reward: 49.45
Iteration: 2657 | Episodes: 162150 | Median Reward: -13.28 | Max Reward: 49.45
Iteration: 2657 | Episodes: 162200 | Median Reward: -8.80 | Max Reward: 49.45
Iteration: 2658 | Episodes: 162250 | Median Reward: -3.92 | Max Reward: 49.45
Iteration: 2659 | Episodes: 162300 | Median Reward: -2.77 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -96        |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2660       |
|    time_elapsed         | 48334      |
|    total_timesteps      | 16343040   |
| train/                  |            |
|    approx_kl            | 0.12072943 |
|    clip_fraction        | 0.0256     |
|    clip_range           | 0.4        |
|    entropy_loss         | -116       |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.0001     |
|    loss                 | 58.6       |
|    n_updates            | 26590      |
|    policy_gradient_loss | -0.00438   |
|    std                  | 4.1        |
|    value_loss           | 94.8       |
----------------------------------------
Iteration: 2660 | Episodes: 162350 | Median Reward: -5.74 | Max Reward: 49.45
Iteration: 2661 | Episodes: 162400 | Median Reward: -14.64 | Max Reward: 49.45
Iteration: 2661 | Episodes: 162450 | Median Reward: -15.62 | Max Reward: 49.45
Iteration: 2662 | Episodes: 162500 | Median Reward: -9.38 | Max Reward: 49.45
Iteration: 2663 | Episodes: 162550 | Median Reward: 34.76 | Max Reward: 49.45
Iteration: 2664 | Episodes: 162600 | Median Reward: 45.17 | Max Reward: 49.45
Iteration: 2665 | Episodes: 162650 | Median Reward: 45.17 | Max Reward: 49.45
Iteration: 2666 | Episodes: 162700 | Median Reward: 43.40 | Max Reward: 49.45
Iteration: 2666 | Episodes: 162750 | Median Reward: 46.27 | Max Reward: 49.45
Iteration: 2667 | Episodes: 162800 | Median Reward: 33.78 | Max Reward: 49.45
Iteration: 2668 | Episodes: 162850 | Median Reward: 41.17 | Max Reward: 49.45
Iteration: 2669 | Episodes: 162900 | Median Reward: 45.45 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 93.1      |
|    ep_rew_mean          | -51.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2670      |
|    time_elapsed         | 48525     |
|    total_timesteps      | 16404480  |
| train/                  |           |
|    approx_kl            | 5.0623994 |
|    clip_fraction        | 0.0969    |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.721     |
|    learning_rate        | 0.0001    |
|    loss                 | 19.6      |
|    n_updates            | 26690     |
|    policy_gradient_loss | 0.0161    |
|    std                  | 4.12      |
|    value_loss           | 138       |
---------------------------------------
Iteration: 2670 | Episodes: 162950 | Median Reward: 45.58 | Max Reward: 49.45
Iteration: 2670 | Episodes: 163000 | Median Reward: 41.89 | Max Reward: 49.45
Iteration: 2671 | Episodes: 163050 | Median Reward: 44.57 | Max Reward: 49.45
Iteration: 2672 | Episodes: 163100 | Median Reward: 45.29 | Max Reward: 49.45
Iteration: 2673 | Episodes: 163150 | Median Reward: 45.59 | Max Reward: 49.45
Iteration: 2674 | Episodes: 163200 | Median Reward: 45.59 | Max Reward: 49.45
Iteration: 2674 | Episodes: 163250 | Median Reward: 24.66 | Max Reward: 49.45
Iteration: 2675 | Episodes: 163300 | Median Reward: 44.86 | Max Reward: 49.45
Iteration: 2676 | Episodes: 163350 | Median Reward: 29.05 | Max Reward: 49.45
Iteration: 2677 | Episodes: 163400 | Median Reward: 36.16 | Max Reward: 49.45
Iteration: 2678 | Episodes: 163450 | Median Reward: 45.18 | Max Reward: 49.45
Iteration: 2679 | Episodes: 163500 | Median Reward: 42.10 | Max Reward: 49.45
Iteration: 2679 | Episodes: 163550 | Median Reward: 46.05 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -57       |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2680      |
|    time_elapsed         | 48706     |
|    total_timesteps      | 16465920  |
| train/                  |           |
|    approx_kl            | 3.9784853 |
|    clip_fraction        | 0.0955    |
|    clip_range           | 0.4       |
|    entropy_loss         | -116      |
|    explained_variance   | 0.742     |
|    learning_rate        | 0.0001    |
|    loss                 | 69.4      |
|    n_updates            | 26790     |
|    policy_gradient_loss | 0.00245   |
|    std                  | 4.14      |
|    value_loss           | 131       |
---------------------------------------
Iteration: 2680 | Episodes: 163600 | Median Reward: 47.83 | Max Reward: 49.45
Iteration: 2681 | Episodes: 163650 | Median Reward: 47.16 | Max Reward: 49.45
Iteration: 2681 | Episodes: 163700 | Median Reward: 45.83 | Max Reward: 49.45
Iteration: 2682 | Episodes: 163750 | Median Reward: 45.97 | Max Reward: 49.45
Iteration: 2683 | Episodes: 163800 | Median Reward: 44.17 | Max Reward: 49.45
Iteration: 2684 | Episodes: 163850 | Median Reward: 42.99 | Max Reward: 49.45
Iteration: 2685 | Episodes: 163900 | Median Reward: 36.25 | Max Reward: 49.45
Iteration: 2685 | Episodes: 163950 | Median Reward: 43.51 | Max Reward: 49.45
Iteration: 2686 | Episodes: 164000 | Median Reward: 31.16 | Max Reward: 49.45
Iteration: 2687 | Episodes: 164050 | Median Reward: 30.04 | Max Reward: 49.45
Iteration: 2688 | Episodes: 164100 | Median Reward: 32.95 | Max Reward: 49.45
Iteration: 2689 | Episodes: 164150 | Median Reward: 40.70 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -57.2    |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2690     |
|    time_elapsed         | 48886    |
|    total_timesteps      | 16527360 |
| train/                  |          |
|    approx_kl            | 7.019125 |
|    clip_fraction        | 0.0913   |
|    clip_range           | 0.4      |
|    entropy_loss         | -116     |
|    explained_variance   | 0.755    |
|    learning_rate        | 0.0001   |
|    loss                 | 27.7     |
|    n_updates            | 26890    |
|    policy_gradient_loss | 0.0176   |
|    std                  | 4.17     |
|    value_loss           | 112      |
--------------------------------------
Iteration: 2690 | Episodes: 164200 | Median Reward: 47.56 | Max Reward: 49.45
Iteration: 2690 | Episodes: 164250 | Median Reward: 47.14 | Max Reward: 49.45
Iteration: 2691 | Episodes: 164300 | Median Reward: 45.28 | Max Reward: 49.45
Iteration: 2692 | Episodes: 164350 | Median Reward: 46.79 | Max Reward: 49.45
Iteration: 2693 | Episodes: 164400 | Median Reward: 46.92 | Max Reward: 49.45
Iteration: 2694 | Episodes: 164450 | Median Reward: 46.38 | Max Reward: 49.45
Iteration: 2694 | Episodes: 164500 | Median Reward: 47.70 | Max Reward: 49.45
Iteration: 2695 | Episodes: 164550 | Median Reward: 46.97 | Max Reward: 49.45
Iteration: 2696 | Episodes: 164600 | Median Reward: 47.38 | Max Reward: 49.45
Iteration: 2697 | Episodes: 164650 | Median Reward: 47.23 | Max Reward: 49.45
Iteration: 2698 | Episodes: 164700 | Median Reward: 46.01 | Max Reward: 49.45
Iteration: 2699 | Episodes: 164750 | Median Reward: 45.36 | Max Reward: 49.45
Iteration: 2699 | Episodes: 164800 | Median Reward: 46.72 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -53.9     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2700      |
|    time_elapsed         | 49064     |
|    total_timesteps      | 16588800  |
| train/                  |           |
|    approx_kl            | 12.757297 |
|    clip_fraction        | 0.308     |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.727     |
|    learning_rate        | 0.0001    |
|    loss                 | 72.5      |
|    n_updates            | 26990     |
|    policy_gradient_loss | 0.094     |
|    std                  | 4.21      |
|    value_loss           | 153       |
---------------------------------------
Iteration: 2700 | Episodes: 164850 | Median Reward: 45.03 | Max Reward: 49.45
Iteration: 2701 | Episodes: 164900 | Median Reward: 43.24 | Max Reward: 49.45
Iteration: 2702 | Episodes: 164950 | Median Reward: 17.57 | Max Reward: 49.45
Iteration: 2703 | Episodes: 165000 | Median Reward: -8.87 | Max Reward: 49.45
Iteration: 2703 | Episodes: 165050 | Median Reward: -19.06 | Max Reward: 49.45
Iteration: 2704 | Episodes: 165100 | Median Reward: 42.31 | Max Reward: 49.45
Iteration: 2705 | Episodes: 165150 | Median Reward: 42.54 | Max Reward: 49.45
Iteration: 2706 | Episodes: 165200 | Median Reward: 44.42 | Max Reward: 49.45
Iteration: 2707 | Episodes: 165250 | Median Reward: 32.64 | Max Reward: 49.45
Iteration: 2708 | Episodes: 165300 | Median Reward: -9.39 | Max Reward: 49.45
Iteration: 2708 | Episodes: 165350 | Median Reward: -9.51 | Max Reward: 49.45
Iteration: 2709 | Episodes: 165400 | Median Reward: -24.78 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -115      |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 2710      |
|    time_elapsed         | 49257     |
|    total_timesteps      | 16650240  |
| train/                  |           |
|    approx_kl            | 7.4441276 |
|    clip_fraction        | 0.311     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.86      |
|    learning_rate        | 0.0001    |
|    loss                 | 15.4      |
|    n_updates            | 27090     |
|    policy_gradient_loss | 0.137     |
|    std                  | 4.23      |
|    value_loss           | 52.2      |
---------------------------------------
Iteration: 2710 | Episodes: 165450 | Median Reward: -27.93 | Max Reward: 49.45
Iteration: 2711 | Episodes: 165500 | Median Reward: -29.81 | Max Reward: 49.45
Iteration: 2712 | Episodes: 165550 | Median Reward: -31.75 | Max Reward: 49.45
Iteration: 2713 | Episodes: 165600 | Median Reward: -32.43 | Max Reward: 49.45
Iteration: 2713 | Episodes: 165650 | Median Reward: -9.80 | Max Reward: 49.45
Iteration: 2714 | Episodes: 165700 | Median Reward: -13.64 | Max Reward: 49.45
Iteration: 2715 | Episodes: 165750 | Median Reward: -19.77 | Max Reward: 49.45
Iteration: 2716 | Episodes: 165800 | Median Reward: -25.69 | Max Reward: 49.45
Iteration: 2717 | Episodes: 165850 | Median Reward: -32.75 | Max Reward: 49.45
Iteration: 2717 | Episodes: 165900 | Median Reward: -15.13 | Max Reward: 49.45
Iteration: 2718 | Episodes: 165950 | Median Reward: -10.13 | Max Reward: 49.45
Iteration: 2719 | Episodes: 166000 | Median Reward: -9.43 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -97      |
| time/                   |          |
|    fps                  | 338      |
|    iterations           | 2720     |
|    time_elapsed         | 49435    |
|    total_timesteps      | 16711680 |
| train/                  |          |
|    approx_kl            | 14.71198 |
|    clip_fraction        | 0.639    |
|    clip_range           | 0.4      |
|    entropy_loss         | -98.4    |
|    explained_variance   | 0.755    |
|    learning_rate        | 0.0001   |
|    loss                 | 22.9     |
|    n_updates            | 27190    |
|    policy_gradient_loss | 0.195    |
|    std                  | 4.24     |
|    value_loss           | 65.7     |
--------------------------------------
Iteration: 2720 | Episodes: 166050 | Median Reward: 36.02 | Max Reward: 49.45
Iteration: 2721 | Episodes: 166100 | Median Reward: 9.23 | Max Reward: 49.45
Iteration: 2722 | Episodes: 166150 | Median Reward: -13.10 | Max Reward: 49.45
Iteration: 2722 | Episodes: 166200 | Median Reward: -7.33 | Max Reward: 49.45
Iteration: 2723 | Episodes: 166250 | Median Reward: 41.13 | Max Reward: 49.45
Iteration: 2724 | Episodes: 166300 | Median Reward: -7.66 | Max Reward: 49.45
Iteration: 2725 | Episodes: 166350 | Median Reward: -12.12 | Max Reward: 49.45
Iteration: 2726 | Episodes: 166400 | Median Reward: -12.12 | Max Reward: 49.45
Iteration: 2727 | Episodes: 166450 | Median Reward: -10.42 | Max Reward: 49.45
Iteration: 2727 | Episodes: 166500 | Median Reward: -18.74 | Max Reward: 49.45
Iteration: 2728 | Episodes: 166550 | Median Reward: -17.88 | Max Reward: 49.45
Iteration: 2729 | Episodes: 166600 | Median Reward: -7.41 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -106       |
| time/                   |            |
|    fps                  | 338        |
|    iterations           | 2730       |
|    time_elapsed         | 49615      |
|    total_timesteps      | 16773120   |
| train/                  |            |
|    approx_kl            | 0.14120588 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.4        |
|    entropy_loss         | -99.7      |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0001     |
|    loss                 | 19.3       |
|    n_updates            | 27290      |
|    policy_gradient_loss | -0.0312    |
|    std                  | 4.25       |
|    value_loss           | 54.2       |
----------------------------------------
Iteration: 2730 | Episodes: 166650 | Median Reward: -5.87 | Max Reward: 49.45
Iteration: 2731 | Episodes: 166700 | Median Reward: -13.58 | Max Reward: 49.45
Iteration: 2731 | Episodes: 166750 | Median Reward: 22.32 | Max Reward: 49.45
Iteration: 2732 | Episodes: 166800 | Median Reward: 4.12 | Max Reward: 49.45
Iteration: 2733 | Episodes: 166850 | Median Reward: 5.86 | Max Reward: 49.45
Iteration: 2734 | Episodes: 166900 | Median Reward: -7.04 | Max Reward: 49.45
Iteration: 2735 | Episodes: 166950 | Median Reward: -14.90 | Max Reward: 49.45
Iteration: 2736 | Episodes: 167000 | Median Reward: -6.79 | Max Reward: 49.45
Iteration: 2736 | Episodes: 167050 | Median Reward: 12.45 | Max Reward: 49.45
Iteration: 2737 | Episodes: 167100 | Median Reward: 32.90 | Max Reward: 49.45
Iteration: 2738 | Episodes: 167150 | Median Reward: 32.90 | Max Reward: 49.45
Iteration: 2739 | Episodes: 167200 | Median Reward: 20.61 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -90.7      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2740       |
|    time_elapsed         | 49810      |
|    total_timesteps      | 16834560   |
| train/                  |            |
|    approx_kl            | 0.65513575 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.4        |
|    entropy_loss         | -102       |
|    explained_variance   | 0.773      |
|    learning_rate        | 0.0001     |
|    loss                 | 43.8       |
|    n_updates            | 27390      |
|    policy_gradient_loss | 0.000127   |
|    std                  | 4.27       |
|    value_loss           | 91.4       |
----------------------------------------
Iteration: 2740 | Episodes: 167250 | Median Reward: 7.02 | Max Reward: 49.45
Iteration: 2740 | Episodes: 167300 | Median Reward: 7.74 | Max Reward: 49.45
Iteration: 2741 | Episodes: 167350 | Median Reward: -1.14 | Max Reward: 49.45
Iteration: 2742 | Episodes: 167400 | Median Reward: 2.97 | Max Reward: 49.45
Iteration: 2743 | Episodes: 167450 | Median Reward: 7.86 | Max Reward: 49.45
Iteration: 2744 | Episodes: 167500 | Median Reward: -1.88 | Max Reward: 49.45
Iteration: 2745 | Episodes: 167550 | Median Reward: 5.15 | Max Reward: 49.45
Iteration: 2745 | Episodes: 167600 | Median Reward: -17.18 | Max Reward: 49.45
Iteration: 2746 | Episodes: 167650 | Median Reward: 29.37 | Max Reward: 49.45
Iteration: 2747 | Episodes: 167700 | Median Reward: 29.37 | Max Reward: 49.45
Iteration: 2748 | Episodes: 167750 | Median Reward: 31.39 | Max Reward: 49.45
Iteration: 2749 | Episodes: 167800 | Median Reward: 29.59 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -88      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2750     |
|    time_elapsed         | 49992    |
|    total_timesteps      | 16896000 |
| train/                  |          |
|    approx_kl            | 8.156477 |
|    clip_fraction        | 0.432    |
|    clip_range           | 0.4      |
|    entropy_loss         | -105     |
|    explained_variance   | 0.756    |
|    learning_rate        | 0.0001   |
|    loss                 | 59.3     |
|    n_updates            | 27490    |
|    policy_gradient_loss | 0.0354   |
|    std                  | 4.28     |
|    value_loss           | 106      |
--------------------------------------
Iteration: 2750 | Episodes: 167850 | Median Reward: 4.13 | Max Reward: 49.45
Iteration: 2750 | Episodes: 167900 | Median Reward: 17.84 | Max Reward: 49.45
Iteration: 2751 | Episodes: 167950 | Median Reward: 22.66 | Max Reward: 49.45
Iteration: 2752 | Episodes: 168000 | Median Reward: 40.55 | Max Reward: 49.45
Iteration: 2753 | Episodes: 168050 | Median Reward: 44.60 | Max Reward: 49.45
Iteration: 2754 | Episodes: 168100 | Median Reward: -23.22 | Max Reward: 49.45
Iteration: 2754 | Episodes: 168150 | Median Reward: -7.05 | Max Reward: 49.45
Iteration: 2755 | Episodes: 168200 | Median Reward: 5.37 | Max Reward: 49.45
Iteration: 2756 | Episodes: 168250 | Median Reward: 44.10 | Max Reward: 49.45
Iteration: 2757 | Episodes: 168300 | Median Reward: 41.66 | Max Reward: 49.45
Iteration: 2758 | Episodes: 168350 | Median Reward: -3.10 | Max Reward: 49.45
Iteration: 2759 | Episodes: 168400 | Median Reward: 16.26 | Max Reward: 49.45
Iteration: 2759 | Episodes: 168450 | Median Reward: 17.41 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2760      |
|    time_elapsed         | 50173     |
|    total_timesteps      | 16957440  |
| train/                  |           |
|    approx_kl            | 1.5931503 |
|    clip_fraction        | 0.184     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0001    |
|    loss                 | 32.5      |
|    n_updates            | 27590     |
|    policy_gradient_loss | -0.0108   |
|    std                  | 4.3       |
|    value_loss           | 91.5      |
---------------------------------------
Iteration: 2760 | Episodes: 168500 | Median Reward: 21.06 | Max Reward: 49.45
Iteration: 2761 | Episodes: 168550 | Median Reward: -4.55 | Max Reward: 49.45
Iteration: 2762 | Episodes: 168600 | Median Reward: -9.68 | Max Reward: 49.45
Iteration: 2763 | Episodes: 168650 | Median Reward: -12.19 | Max Reward: 49.45
Iteration: 2764 | Episodes: 168700 | Median Reward: 0.61 | Max Reward: 49.45
Iteration: 2764 | Episodes: 168750 | Median Reward: 39.25 | Max Reward: 49.45
Iteration: 2765 | Episodes: 168800 | Median Reward: -15.88 | Max Reward: 49.45
Iteration: 2766 | Episodes: 168850 | Median Reward: -14.45 | Max Reward: 49.45
Iteration: 2767 | Episodes: 168900 | Median Reward: -10.31 | Max Reward: 49.45
Iteration: 2768 | Episodes: 168950 | Median Reward: -13.85 | Max Reward: 49.45
Iteration: 2768 | Episodes: 169000 | Median Reward: -10.69 | Max Reward: 49.45
Iteration: 2769 | Episodes: 169050 | Median Reward: -21.52 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -112     |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2770     |
|    time_elapsed         | 50354    |
|    total_timesteps      | 17018880 |
| train/                  |          |
|    approx_kl            | 2.995302 |
|    clip_fraction        | 0.315    |
|    clip_range           | 0.4      |
|    entropy_loss         | -109     |
|    explained_variance   | 0.765    |
|    learning_rate        | 0.0001   |
|    loss                 | 36.3     |
|    n_updates            | 27690    |
|    policy_gradient_loss | -0.0218  |
|    std                  | 4.31     |
|    value_loss           | 71.5     |
--------------------------------------
Iteration: 2770 | Episodes: 169100 | Median Reward: -22.85 | Max Reward: 49.45
Iteration: 2771 | Episodes: 169150 | Median Reward: -19.21 | Max Reward: 49.45
Iteration: 2772 | Episodes: 169200 | Median Reward: -17.52 | Max Reward: 49.45
Iteration: 2773 | Episodes: 169250 | Median Reward: -18.03 | Max Reward: 49.45
Iteration: 2773 | Episodes: 169300 | Median Reward: -22.95 | Max Reward: 49.45
Iteration: 2774 | Episodes: 169350 | Median Reward: -0.58 | Max Reward: 49.45
Iteration: 2775 | Episodes: 169400 | Median Reward: -14.17 | Max Reward: 49.45
Iteration: 2776 | Episodes: 169450 | Median Reward: -15.05 | Max Reward: 49.45
Iteration: 2777 | Episodes: 169500 | Median Reward: 25.48 | Max Reward: 49.45
Iteration: 2777 | Episodes: 169550 | Median Reward: -10.86 | Max Reward: 49.45
Iteration: 2778 | Episodes: 169600 | Median Reward: 39.44 | Max Reward: 49.45
Iteration: 2779 | Episodes: 169650 | Median Reward: 42.11 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2780      |
|    time_elapsed         | 50539     |
|    total_timesteps      | 17080320  |
| train/                  |           |
|    approx_kl            | 5.7252383 |
|    clip_fraction        | 0.182     |
|    clip_range           | 0.4       |
|    entropy_loss         | -111      |
|    explained_variance   | 0.736     |
|    learning_rate        | 0.0001    |
|    loss                 | 66.2      |
|    n_updates            | 27790     |
|    policy_gradient_loss | 0.0414    |
|    std                  | 4.32      |
|    value_loss           | 111       |
---------------------------------------
Iteration: 2780 | Episodes: 169700 | Median Reward: 42.79 | Max Reward: 49.45
Iteration: 2781 | Episodes: 169750 | Median Reward: 1.97 | Max Reward: 49.45
Iteration: 2782 | Episodes: 169800 | Median Reward: -14.41 | Max Reward: 49.45
Iteration: 2782 | Episodes: 169850 | Median Reward: -7.99 | Max Reward: 49.45
Iteration: 2783 | Episodes: 169900 | Median Reward: 41.56 | Max Reward: 49.45
Iteration: 2784 | Episodes: 169950 | Median Reward: 41.71 | Max Reward: 49.45
Iteration: 2785 | Episodes: 170000 | Median Reward: 41.35 | Max Reward: 49.45
Iteration: 2786 | Episodes: 170050 | Median Reward: 16.22 | Max Reward: 49.45
Iteration: 2787 | Episodes: 170100 | Median Reward: 22.99 | Max Reward: 49.45
Iteration: 2787 | Episodes: 170150 | Median Reward: 40.78 | Max Reward: 49.45
Iteration: 2788 | Episodes: 170200 | Median Reward: 44.51 | Max Reward: 49.45
Iteration: 2789 | Episodes: 170250 | Median Reward: -10.86 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2790      |
|    time_elapsed         | 50719     |
|    total_timesteps      | 17141760  |
| train/                  |           |
|    approx_kl            | 19.023891 |
|    clip_fraction        | 0.396     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.717     |
|    learning_rate        | 0.0001    |
|    loss                 | 89.6      |
|    n_updates            | 27890     |
|    policy_gradient_loss | 0.0573    |
|    std                  | 4.34      |
|    value_loss           | 127       |
---------------------------------------
Iteration: 2790 | Episodes: 170300 | Median Reward: -7.28 | Max Reward: 49.45
Iteration: 2791 | Episodes: 170350 | Median Reward: -4.18 | Max Reward: 49.45
Iteration: 2791 | Episodes: 170400 | Median Reward: 10.58 | Max Reward: 49.45
Iteration: 2792 | Episodes: 170450 | Median Reward: 23.03 | Max Reward: 49.45
Iteration: 2793 | Episodes: 170500 | Median Reward: 17.46 | Max Reward: 49.45
Iteration: 2794 | Episodes: 170550 | Median Reward: 6.08 | Max Reward: 49.45
Iteration: 2795 | Episodes: 170600 | Median Reward: -1.25 | Max Reward: 49.45
Iteration: 2796 | Episodes: 170650 | Median Reward: 6.69 | Max Reward: 49.45
Iteration: 2796 | Episodes: 170700 | Median Reward: -11.20 | Max Reward: 49.45
Iteration: 2797 | Episodes: 170750 | Median Reward: -5.76 | Max Reward: 49.45
Iteration: 2798 | Episodes: 170800 | Median Reward: 0.92 | Max Reward: 49.45
Iteration: 2799 | Episodes: 170850 | Median Reward: 13.39 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2800      |
|    time_elapsed         | 50901     |
|    total_timesteps      | 17203200  |
| train/                  |           |
|    approx_kl            | 3.4075997 |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.801     |
|    learning_rate        | 0.0001    |
|    loss                 | 34.1      |
|    n_updates            | 27990     |
|    policy_gradient_loss | 0.0703    |
|    std                  | 4.34      |
|    value_loss           | 76.6      |
---------------------------------------
Iteration: 2800 | Episodes: 170900 | Median Reward: -4.29 | Max Reward: 49.45
Iteration: 2800 | Episodes: 170950 | Median Reward: -8.76 | Max Reward: 49.45
Iteration: 2801 | Episodes: 171000 | Median Reward: -21.82 | Max Reward: 49.45
Iteration: 2802 | Episodes: 171050 | Median Reward: -16.98 | Max Reward: 49.45
Iteration: 2803 | Episodes: 171100 | Median Reward: -9.24 | Max Reward: 49.45
Iteration: 2804 | Episodes: 171150 | Median Reward: -9.24 | Max Reward: 49.45
exit()
Iteration: 2805 | Episodes: 171200 | Median Reward: -18.79 | Max Reward: 49.45
Iteration: 2805 | Episodes: 171250 | Median Reward: -20.07 | Max Reward: 49.45
Iteration: 2806 | Episodes: 171300 | Median Reward: -25.27 | Max Reward: 49.45
Iteration: 2807 | Episodes: 171350 | Median Reward: -23.94 | Max Reward: 49.45
Iteration: 2808 | Episodes: 171400 | Median Reward: -13.00 | Max Reward: 49.45
Iteration: 2809 | Episodes: 171450 | Median Reward: -9.67 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -104      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2810      |
|    time_elapsed         | 51092     |
|    total_timesteps      | 17264640  |
| train/                  |           |
|    approx_kl            | 7.7621074 |
|    clip_fraction        | 0.246     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.758     |
|    learning_rate        | 0.0001    |
|    loss                 | 36.7      |
|    n_updates            | 28090     |
|    policy_gradient_loss | 0.0244    |
|    std                  | 4.36      |
|    value_loss           | 82.4      |
---------------------------------------
Iteration: 2810 | Episodes: 171500 | Median Reward: -15.86 | Max Reward: 49.45
Iteration: 2810 | Episodes: 171550 | Median Reward: -14.53 | Max Reward: 49.45
Iteration: 2811 | Episodes: 171600 | Median Reward: -12.74 | Max Reward: 49.45
Iteration: 2812 | Episodes: 171650 | Median Reward: -18.44 | Max Reward: 49.45
Iteration: 2813 | Episodes: 171700 | Median Reward: -12.89 | Max Reward: 49.45
Iteration: 2814 | Episodes: 171750 | Median Reward: -8.55 | Max Reward: 49.45
Iteration: 2814 | Episodes: 171800 | Median Reward: -6.52 | Max Reward: 49.45
Iteration: 2815 | Episodes: 171850 | Median Reward: -5.92 | Max Reward: 49.45
Iteration: 2816 | Episodes: 171900 | Median Reward: -5.21 | Max Reward: 49.45
Iteration: 2817 | Episodes: 171950 | Median Reward: -5.21 | Max Reward: 49.45
Iteration: 2818 | Episodes: 172000 | Median Reward: -19.86 | Max Reward: 49.45
Iteration: 2819 | Episodes: 172050 | Median Reward: -2.57 | Max Reward: 49.45
Iteration: 2819 | Episodes: 172100 | Median Reward: -14.16 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -110       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2820       |
|    time_elapsed         | 51274      |
|    total_timesteps      | 17326080   |
| train/                  |            |
|    approx_kl            | 0.15979964 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0001     |
|    loss                 | 26.2       |
|    n_updates            | 28190      |
|    policy_gradient_loss | -0.0131    |
|    std                  | 4.36       |
|    value_loss           | 60.1       |
----------------------------------------
Iteration: 2820 | Episodes: 172150 | Median Reward: -10.86 | Max Reward: 49.45
Iteration: 2821 | Episodes: 172200 | Median Reward: -12.11 | Max Reward: 49.45
Iteration: 2822 | Episodes: 172250 | Median Reward: -12.11 | Max Reward: 49.45
Iteration: 2823 | Episodes: 172300 | Median Reward: -14.96 | Max Reward: 49.45
Iteration: 2824 | Episodes: 172350 | Median Reward: -9.99 | Max Reward: 49.45
Iteration: 2824 | Episodes: 172400 | Median Reward: -8.61 | Max Reward: 49.45
Iteration: 2825 | Episodes: 172450 | Median Reward: -9.91 | Max Reward: 49.45
Iteration: 2826 | Episodes: 172500 | Median Reward: -15.86 | Max Reward: 49.45
Iteration: 2827 | Episodes: 172550 | Median Reward: -16.64 | Max Reward: 49.45
Iteration: 2828 | Episodes: 172600 | Median Reward: -8.52 | Max Reward: 49.45
Iteration: 2828 | Episodes: 172650 | Median Reward: -12.91 | Max Reward: 49.45
Iteration: 2829 | Episodes: 172700 | Median Reward: -18.71 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -103       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2830       |
|    time_elapsed         | 51455      |
|    total_timesteps      | 17387520   |
| train/                  |            |
|    approx_kl            | 0.04116583 |
|    clip_fraction        | 0.0399     |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.843      |
|    learning_rate        | 0.0001     |
|    loss                 | 9.57       |
|    n_updates            | 28290      |
|    policy_gradient_loss | 0.0107     |
|    std                  | 4.37       |
|    value_loss           | 43.4       |
----------------------------------------
Iteration: 2830 | Episodes: 172750 | Median Reward: 11.27 | Max Reward: 49.45
Iteration: 2831 | Episodes: 172800 | Median Reward: 11.11 | Max Reward: 49.45
Iteration: 2832 | Episodes: 172850 | Median Reward: -8.20 | Max Reward: 49.45
Iteration: 2833 | Episodes: 172900 | Median Reward: -12.06 | Max Reward: 49.45
Iteration: 2833 | Episodes: 172950 | Median Reward: -8.33 | Max Reward: 49.45
Iteration: 2834 | Episodes: 173000 | Median Reward: -14.53 | Max Reward: 49.45
Iteration: 2835 | Episodes: 173050 | Median Reward: -15.84 | Max Reward: 49.45
Iteration: 2836 | Episodes: 173100 | Median Reward: -16.92 | Max Reward: 49.45
Iteration: 2837 | Episodes: 173150 | Median Reward: -17.94 | Max Reward: 49.45
Iteration: 2837 | Episodes: 173200 | Median Reward: -18.93 | Max Reward: 49.45
Iteration: 2838 | Episodes: 173250 | Median Reward: -12.34 | Max Reward: 49.45
Iteration: 2839 | Episodes: 173300 | Median Reward: -12.05 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -111       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2840       |
|    time_elapsed         | 51646      |
|    total_timesteps      | 17448960   |
| train/                  |            |
|    approx_kl            | 0.04250362 |
|    clip_fraction        | 0.0886     |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0001     |
|    loss                 | 8.78       |
|    n_updates            | 28390      |
|    policy_gradient_loss | -2.73e-06  |
|    std                  | 4.37       |
|    value_loss           | 33.5       |
----------------------------------------
Iteration: 2840 | Episodes: 173350 | Median Reward: -8.19 | Max Reward: 49.45
Iteration: 2841 | Episodes: 173400 | Median Reward: -0.94 | Max Reward: 49.45
Iteration: 2842 | Episodes: 173450 | Median Reward: -18.76 | Max Reward: 49.45
Iteration: 2842 | Episodes: 173500 | Median Reward: -20.80 | Max Reward: 49.45
Iteration: 2843 | Episodes: 173550 | Median Reward: 15.67 | Max Reward: 49.45
Iteration: 2844 | Episodes: 173600 | Median Reward: 22.66 | Max Reward: 49.45
Iteration: 2845 | Episodes: 173650 | Median Reward: 27.04 | Max Reward: 49.45
Iteration: 2846 | Episodes: 173700 | Median Reward: 33.36 | Max Reward: 49.45
Iteration: 2847 | Episodes: 173750 | Median Reward: 33.38 | Max Reward: 49.45
Iteration: 2847 | Episodes: 173800 | Median Reward: -7.80 | Max Reward: 49.45
Iteration: 2848 | Episodes: 173850 | Median Reward: -1.68 | Max Reward: 49.45
Iteration: 2849 | Episodes: 173900 | Median Reward: -8.88 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -100     |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2850     |
|    time_elapsed         | 51828    |
|    total_timesteps      | 17510400 |
| train/                  |          |
|    approx_kl            | 5.651329 |
|    clip_fraction        | 0.306    |
|    clip_range           | 0.4      |
|    entropy_loss         | -113     |
|    explained_variance   | 0.734    |
|    learning_rate        | 0.0001   |
|    loss                 | 40.1     |
|    n_updates            | 28490    |
|    policy_gradient_loss | 0.0536   |
|    std                  | 4.38     |
|    value_loss           | 115      |
--------------------------------------
Iteration: 2850 | Episodes: 173950 | Median Reward: -16.16 | Max Reward: 49.45
Iteration: 2851 | Episodes: 174000 | Median Reward: -15.48 | Max Reward: 49.45
Iteration: 2851 | Episodes: 174050 | Median Reward: 35.13 | Max Reward: 49.45
Iteration: 2852 | Episodes: 174100 | Median Reward: -24.25 | Max Reward: 49.45
Iteration: 2853 | Episodes: 174150 | Median Reward: -13.83 | Max Reward: 49.45
Iteration: 2854 | Episodes: 174200 | Median Reward: -13.33 | Max Reward: 49.45
Iteration: 2855 | Episodes: 174250 | Median Reward: -13.33 | Max Reward: 49.45
Iteration: 2856 | Episodes: 174300 | Median Reward: 34.68 | Max Reward: 49.45
Iteration: 2856 | Episodes: 174350 | Median Reward: -9.54 | Max Reward: 49.45
Iteration: 2857 | Episodes: 174400 | Median Reward: -18.22 | Max Reward: 49.45
Iteration: 2858 | Episodes: 174450 | Median Reward: -11.19 | Max Reward: 49.45
Iteration: 2859 | Episodes: 174500 | Median Reward: -11.19 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -96.7    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2860     |
|    time_elapsed         | 52011    |
|    total_timesteps      | 17571840 |
| train/                  |          |
|    approx_kl            | 2.421301 |
|    clip_fraction        | 0.268    |
|    clip_range           | 0.4      |
|    entropy_loss         | -113     |
|    explained_variance   | 0.775    |
|    learning_rate        | 0.0001   |
|    loss                 | 41.5     |
|    n_updates            | 28590    |
|    policy_gradient_loss | 0.0257   |
|    std                  | 4.39     |
|    value_loss           | 73.4     |
--------------------------------------
Iteration: 2860 | Episodes: 174550 | Median Reward: -21.70 | Max Reward: 49.45
Iteration: 2861 | Episodes: 174600 | Median Reward: -13.44 | Max Reward: 49.45
Iteration: 2861 | Episodes: 174650 | Median Reward: -11.21 | Max Reward: 49.45
Iteration: 2862 | Episodes: 174700 | Median Reward: -11.21 | Max Reward: 49.45
Iteration: 2863 | Episodes: 174750 | Median Reward: 35.93 | Max Reward: 49.45
Iteration: 2864 | Episodes: 174800 | Median Reward: 36.17 | Max Reward: 49.45
Iteration: 2865 | Episodes: 174850 | Median Reward: 38.11 | Max Reward: 49.45
Iteration: 2865 | Episodes: 174900 | Median Reward: 15.42 | Max Reward: 49.45
Iteration: 2866 | Episodes: 174950 | Median Reward: -19.94 | Max Reward: 49.45
Iteration: 2867 | Episodes: 175000 | Median Reward: -19.94 | Max Reward: 49.45
Iteration: 2868 | Episodes: 175050 | Median Reward: -19.76 | Max Reward: 49.45
Iteration: 2869 | Episodes: 175100 | Median Reward: -19.79 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -124     |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2870     |
|    time_elapsed         | 52190    |
|    total_timesteps      | 17633280 |
| train/                  |          |
|    approx_kl            | 30.72462 |
|    clip_fraction        | 0.2      |
|    clip_range           | 0.4      |
|    entropy_loss         | -112     |
|    explained_variance   | 0.862    |
|    learning_rate        | 0.0001   |
|    loss                 | 9.87     |
|    n_updates            | 28690    |
|    policy_gradient_loss | 0.0669   |
|    std                  | 4.4      |
|    value_loss           | 32.6     |
--------------------------------------
Iteration: 2870 | Episodes: 175150 | Median Reward: -26.98 | Max Reward: 49.45
Iteration: 2870 | Episodes: 175200 | Median Reward: -19.63 | Max Reward: 49.45
Iteration: 2871 | Episodes: 175250 | Median Reward: -13.51 | Max Reward: 49.45
Iteration: 2872 | Episodes: 175300 | Median Reward: -6.62 | Max Reward: 49.45
Iteration: 2873 | Episodes: 175350 | Median Reward: -10.23 | Max Reward: 49.45
Iteration: 2874 | Episodes: 175400 | Median Reward: -24.59 | Max Reward: 49.45
Iteration: 2874 | Episodes: 175450 | Median Reward: 13.80 | Max Reward: 49.45
Iteration: 2875 | Episodes: 175500 | Median Reward: -12.08 | Max Reward: 49.45
Iteration: 2876 | Episodes: 175550 | Median Reward: 1.35 | Max Reward: 49.45
Iteration: 2877 | Episodes: 175600 | Median Reward: 22.72 | Max Reward: 49.45
Iteration: 2878 | Episodes: 175650 | Median Reward: 44.87 | Max Reward: 49.45
Iteration: 2879 | Episodes: 175700 | Median Reward: 45.84 | Max Reward: 49.45
Iteration: 2879 | Episodes: 175750 | Median Reward: 44.94 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -59       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2880      |
|    time_elapsed         | 52382     |
|    total_timesteps      | 17694720  |
| train/                  |           |
|    approx_kl            | 0.7868189 |
|    clip_fraction        | 0.131     |
|    clip_range           | 0.4       |
|    entropy_loss         | -112      |
|    explained_variance   | 0.698     |
|    learning_rate        | 0.0001    |
|    loss                 | 106       |
|    n_updates            | 28790     |
|    policy_gradient_loss | -0.0139   |
|    std                  | 4.41      |
|    value_loss           | 192       |
---------------------------------------
Iteration: 2880 | Episodes: 175800 | Median Reward: 45.73 | Max Reward: 49.45
Iteration: 2881 | Episodes: 175850 | Median Reward: 45.71 | Max Reward: 49.45
Iteration: 2882 | Episodes: 175900 | Median Reward: 45.54 | Max Reward: 49.45
Iteration: 2883 | Episodes: 175950 | Median Reward: 45.41 | Max Reward: 49.45
Iteration: 2883 | Episodes: 176000 | Median Reward: 47.58 | Max Reward: 49.45
Iteration: 2884 | Episodes: 176050 | Median Reward: 45.86 | Max Reward: 49.45
Iteration: 2885 | Episodes: 176100 | Median Reward: 45.84 | Max Reward: 49.45
Iteration: 2886 | Episodes: 176150 | Median Reward: 45.56 | Max Reward: 49.45
Iteration: 2887 | Episodes: 176200 | Median Reward: 45.93 | Max Reward: 49.45
Iteration: 2888 | Episodes: 176250 | Median Reward: 47.27 | Max Reward: 49.45
Iteration: 2888 | Episodes: 176300 | Median Reward: 47.49 | Max Reward: 49.45
Iteration: 2889 | Episodes: 176350 | Median Reward: 47.41 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -52.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2890      |
|    time_elapsed         | 52560     |
|    total_timesteps      | 17756160  |
| train/                  |           |
|    approx_kl            | 7.6151724 |
|    clip_fraction        | 0.157     |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 0.688     |
|    learning_rate        | 0.0001    |
|    loss                 | 105       |
|    n_updates            | 28890     |
|    policy_gradient_loss | 0.0298    |
|    std                  | 4.43      |
|    value_loss           | 221       |
---------------------------------------
Iteration: 2890 | Episodes: 176400 | Median Reward: 47.34 | Max Reward: 49.45
Iteration: 2891 | Episodes: 176450 | Median Reward: 47.55 | Max Reward: 49.45
Iteration: 2892 | Episodes: 176500 | Median Reward: 47.49 | Max Reward: 49.45
Iteration: 2893 | Episodes: 176550 | Median Reward: 44.16 | Max Reward: 49.45
Iteration: 2893 | Episodes: 176600 | Median Reward: 45.75 | Max Reward: 49.45
Iteration: 2894 | Episodes: 176650 | Median Reward: 46.31 | Max Reward: 49.45
Iteration: 2895 | Episodes: 176700 | Median Reward: 47.04 | Max Reward: 49.45
Iteration: 2896 | Episodes: 176750 | Median Reward: 46.71 | Max Reward: 49.45
Iteration: 2897 | Episodes: 176800 | Median Reward: 43.45 | Max Reward: 49.45
Iteration: 2897 | Episodes: 176850 | Median Reward: -16.16 | Max Reward: 49.45
Iteration: 2898 | Episodes: 176900 | Median Reward: 43.88 | Max Reward: 49.45
Iteration: 2899 | Episodes: 176950 | Median Reward: 43.91 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2900      |
|    time_elapsed         | 52739     |
|    total_timesteps      | 17817600  |
| train/                  |           |
|    approx_kl            | 19.472153 |
|    clip_fraction        | 0.338     |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.728     |
|    learning_rate        | 0.0001    |
|    loss                 | 88.8      |
|    n_updates            | 28990     |
|    policy_gradient_loss | 0.0533    |
|    std                  | 4.45      |
|    value_loss           | 161       |
---------------------------------------
Iteration: 2900 | Episodes: 177000 | Median Reward: -9.03 | Max Reward: 49.45
Iteration: 2901 | Episodes: 177050 | Median Reward: -15.78 | Max Reward: 49.45
Iteration: 2902 | Episodes: 177100 | Median Reward: -0.03 | Max Reward: 49.45
Iteration: 2902 | Episodes: 177150 | Median Reward: 45.77 | Max Reward: 49.45
Iteration: 2903 | Episodes: 177200 | Median Reward: 8.08 | Max Reward: 49.45
Iteration: 2904 | Episodes: 177250 | Median Reward: -6.89 | Max Reward: 49.45
Iteration: 2905 | Episodes: 177300 | Median Reward: -8.16 | Max Reward: 49.45
Iteration: 2906 | Episodes: 177350 | Median Reward: -11.12 | Max Reward: 49.45
Iteration: 2907 | Episodes: 177400 | Median Reward: 45.46 | Max Reward: 49.45
Iteration: 2907 | Episodes: 177450 | Median Reward: 45.98 | Max Reward: 49.45
Iteration: 2908 | Episodes: 177500 | Median Reward: 45.72 | Max Reward: 49.45
Iteration: 2909 | Episodes: 177550 | Median Reward: 44.96 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -74.4    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 2910     |
|    time_elapsed         | 52934    |
|    total_timesteps      | 17879040 |
| train/                  |          |
|    approx_kl            | 4.81569  |
|    clip_fraction        | 0.0961   |
|    clip_range           | 0.4      |
|    entropy_loss         | -116     |
|    explained_variance   | 0.71     |
|    learning_rate        | 0.0001   |
|    loss                 | 94.3     |
|    n_updates            | 29090    |
|    policy_gradient_loss | 0.0221   |
|    std                  | 4.47     |
|    value_loss           | 169      |
--------------------------------------
Iteration: 2910 | Episodes: 177600 | Median Reward: 43.79 | Max Reward: 49.45
Iteration: 2911 | Episodes: 177650 | Median Reward: 41.51 | Max Reward: 49.45
Iteration: 2911 | Episodes: 177700 | Median Reward: 13.05 | Max Reward: 49.45
Iteration: 2912 | Episodes: 177750 | Median Reward: 41.59 | Max Reward: 49.45
Iteration: 2913 | Episodes: 177800 | Median Reward: 40.61 | Max Reward: 49.45
Iteration: 2914 | Episodes: 177850 | Median Reward: 38.03 | Max Reward: 49.45
Iteration: 2915 | Episodes: 177900 | Median Reward: -9.95 | Max Reward: 49.45
Iteration: 2916 | Episodes: 177950 | Median Reward: -16.32 | Max Reward: 49.45
Iteration: 2916 | Episodes: 178000 | Median Reward: -18.74 | Max Reward: 49.45
Iteration: 2917 | Episodes: 178050 | Median Reward: -12.62 | Max Reward: 49.45
Iteration: 2918 | Episodes: 178100 | Median Reward: -0.05 | Max Reward: 49.45
Iteration: 2919 | Episodes: 178150 | Median Reward: 9.90 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -85.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2920      |
|    time_elapsed         | 53113     |
|    total_timesteps      | 17940480  |
| train/                  |           |
|    approx_kl            | 1.6580461 |
|    clip_fraction        | 0.201     |
|    clip_range           | 0.4       |
|    entropy_loss         | -110      |
|    explained_variance   | 0.775     |
|    learning_rate        | 0.0001    |
|    loss                 | 58.4      |
|    n_updates            | 29190     |
|    policy_gradient_loss | 0.00647   |
|    std                  | 4.48      |
|    value_loss           | 74.8      |
---------------------------------------
Iteration: 2920 | Episodes: 178200 | Median Reward: 27.77 | Max Reward: 49.45
Iteration: 2920 | Episodes: 178250 | Median Reward: -16.81 | Max Reward: 49.45
Iteration: 2921 | Episodes: 178300 | Median Reward: 32.24 | Max Reward: 49.45
Iteration: 2922 | Episodes: 178350 | Median Reward: 1.79 | Max Reward: 49.45
Iteration: 2923 | Episodes: 178400 | Median Reward: 14.70 | Max Reward: 49.45
Iteration: 2924 | Episodes: 178450 | Median Reward: 14.04 | Max Reward: 49.45
Iteration: 2925 | Episodes: 178500 | Median Reward: 3.58 | Max Reward: 49.45
Iteration: 2925 | Episodes: 178550 | Median Reward: -16.52 | Max Reward: 49.45
Iteration: 2926 | Episodes: 178600 | Median Reward: -12.11 | Max Reward: 49.45
Iteration: 2927 | Episodes: 178650 | Median Reward: -10.99 | Max Reward: 49.45
Iteration: 2928 | Episodes: 178700 | Median Reward: 38.86 | Max Reward: 49.45
Iteration: 2929 | Episodes: 178750 | Median Reward: 38.53 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -81.8     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2930      |
|    time_elapsed         | 53295     |
|    total_timesteps      | 18001920  |
| train/                  |           |
|    approx_kl            | 40.343933 |
|    clip_fraction        | 0.313     |
|    clip_range           | 0.4       |
|    entropy_loss         | -114      |
|    explained_variance   | 0.735     |
|    learning_rate        | 0.0001    |
|    loss                 | 88.3      |
|    n_updates            | 29290     |
|    policy_gradient_loss | 0.185     |
|    std                  | 4.49      |
|    value_loss           | 134       |
---------------------------------------
Iteration: 2930 | Episodes: 178800 | Median Reward: 17.88 | Max Reward: 49.45
Iteration: 2930 | Episodes: 178850 | Median Reward: 11.71 | Max Reward: 49.45
Iteration: 2931 | Episodes: 178900 | Median Reward: -0.34 | Max Reward: 49.45
Iteration: 2932 | Episodes: 178950 | Median Reward: -1.38 | Max Reward: 49.45
Iteration: 2933 | Episodes: 179000 | Median Reward: -3.08 | Max Reward: 49.45
Iteration: 2934 | Episodes: 179050 | Median Reward: -2.80 | Max Reward: 49.45
Iteration: 2934 | Episodes: 179100 | Median Reward: -5.85 | Max Reward: 49.45
Iteration: 2935 | Episodes: 179150 | Median Reward: -7.44 | Max Reward: 49.45
Iteration: 2936 | Episodes: 179200 | Median Reward: -14.58 | Max Reward: 49.45
Iteration: 2937 | Episodes: 179250 | Median Reward: -19.92 | Max Reward: 49.45
Iteration: 2938 | Episodes: 179300 | Median Reward: -19.92 | Max Reward: 49.45
Iteration: 2939 | Episodes: 179350 | Median Reward: -13.11 | Max Reward: 49.45
Iteration: 2939 | Episodes: 179400 | Median Reward: 2.40 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -99.6      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2940       |
|    time_elapsed         | 53487      |
|    total_timesteps      | 18063360   |
| train/                  |            |
|    approx_kl            | 0.20987397 |
|    clip_fraction        | 0.037      |
|    clip_range           | 0.4        |
|    entropy_loss         | -114       |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.0001     |
|    loss                 | 16.9       |
|    n_updates            | 29390      |
|    policy_gradient_loss | 0.00339    |
|    std                  | 4.5        |
|    value_loss           | 47.5       |
----------------------------------------
Iteration: 2940 | Episodes: 179450 | Median Reward: -7.91 | Max Reward: 49.45
Iteration: 2941 | Episodes: 179500 | Median Reward: -9.82 | Max Reward: 49.45
Iteration: 2942 | Episodes: 179550 | Median Reward: -12.34 | Max Reward: 49.45
Iteration: 2943 | Episodes: 179600 | Median Reward: -18.36 | Max Reward: 49.45
Iteration: 2944 | Episodes: 179650 | Median Reward: -18.54 | Max Reward: 49.45
Iteration: 2944 | Episodes: 179700 | Median Reward: -17.61 | Max Reward: 49.45
Iteration: 2945 | Episodes: 179750 | Median Reward: -12.88 | Max Reward: 49.45
Iteration: 2946 | Episodes: 179800 | Median Reward: -15.09 | Max Reward: 49.45
Iteration: 2947 | Episodes: 179850 | Median Reward: -13.90 | Max Reward: 49.45
Iteration: 2948 | Episodes: 179900 | Median Reward: -10.25 | Max Reward: 49.45
Iteration: 2948 | Episodes: 179950 | Median Reward: -11.58 | Max Reward: 49.45
Iteration: 2949 | Episodes: 180000 | Median Reward: -12.41 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -108      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2950      |
|    time_elapsed         | 53670     |
|    total_timesteps      | 18124800  |
| train/                  |           |
|    approx_kl            | 2.1220334 |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.875     |
|    learning_rate        | 0.0001    |
|    loss                 | 11.2      |
|    n_updates            | 29490     |
|    policy_gradient_loss | 0.0865    |
|    std                  | 4.51      |
|    value_loss           | 34.6      |
---------------------------------------
Iteration: 2950 | Episodes: 180050 | Median Reward: -7.08 | Max Reward: 49.45
Iteration: 2951 | Episodes: 180100 | Median Reward: -0.81 | Max Reward: 49.45
Iteration: 2952 | Episodes: 180150 | Median Reward: -4.23 | Max Reward: 49.45
Iteration: 2953 | Episodes: 180200 | Median Reward: -4.93 | Max Reward: 49.45
Iteration: 2953 | Episodes: 180250 | Median Reward: -16.62 | Max Reward: 49.45
Iteration: 2954 | Episodes: 180300 | Median Reward: -15.50 | Max Reward: 49.45
Iteration: 2955 | Episodes: 180350 | Median Reward: -14.38 | Max Reward: 49.45
Iteration: 2956 | Episodes: 180400 | Median Reward: -14.38 | Max Reward: 49.45
Iteration: 2957 | Episodes: 180450 | Median Reward: -15.49 | Max Reward: 49.45
Iteration: 2957 | Episodes: 180500 | Median Reward: -6.12 | Max Reward: 49.45
Iteration: 2958 | Episodes: 180550 | Median Reward: 2.21 | Max Reward: 49.45
Iteration: 2959 | Episodes: 180600 | Median Reward: 4.33 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -99.8      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2960       |
|    time_elapsed         | 53851      |
|    total_timesteps      | 18186240   |
| train/                  |            |
|    approx_kl            | 0.11998831 |
|    clip_fraction        | 0.0671     |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.0001     |
|    loss                 | 17.9       |
|    n_updates            | 29590      |
|    policy_gradient_loss | 0.00359    |
|    std                  | 4.52       |
|    value_loss           | 53.8       |
----------------------------------------
Iteration: 2960 | Episodes: 180650 | Median Reward: -0.99 | Max Reward: 49.45
Iteration: 2961 | Episodes: 180700 | Median Reward: -2.60 | Max Reward: 49.45
Iteration: 2962 | Episodes: 180750 | Median Reward: -7.13 | Max Reward: 49.45
Iteration: 2962 | Episodes: 180800 | Median Reward: -5.62 | Max Reward: 49.45
Iteration: 2963 | Episodes: 180850 | Median Reward: -11.53 | Max Reward: 49.45
Iteration: 2964 | Episodes: 180900 | Median Reward: -17.44 | Max Reward: 49.45
Iteration: 2965 | Episodes: 180950 | Median Reward: -15.72 | Max Reward: 49.45
Iteration: 2966 | Episodes: 181000 | Median Reward: -12.27 | Max Reward: 49.45
Iteration: 2967 | Episodes: 181050 | Median Reward: 4.73 | Max Reward: 49.45
Iteration: 2967 | Episodes: 181100 | Median Reward: 1.62 | Max Reward: 49.45
Iteration: 2968 | Episodes: 181150 | Median Reward: 4.29 | Max Reward: 49.45
Iteration: 2969 | Episodes: 181200 | Median Reward: -11.35 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -108      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2970      |
|    time_elapsed         | 54030     |
|    total_timesteps      | 18247680  |
| train/                  |           |
|    approx_kl            | 3.8875117 |
|    clip_fraction        | 0.193     |
|    clip_range           | 0.4       |
|    entropy_loss         | -108      |
|    explained_variance   | 0.875     |
|    learning_rate        | 0.0001    |
|    loss                 | 15.6      |
|    n_updates            | 29690     |
|    policy_gradient_loss | -0.0223   |
|    std                  | 4.53      |
|    value_loss           | 35.9      |
---------------------------------------
Iteration: 2970 | Episodes: 181250 | Median Reward: -13.49 | Max Reward: 49.45
Iteration: 2971 | Episodes: 181300 | Median Reward: -6.72 | Max Reward: 49.45
Iteration: 2971 | Episodes: 181350 | Median Reward: -4.10 | Max Reward: 49.45
Iteration: 2972 | Episodes: 181400 | Median Reward: -15.42 | Max Reward: 49.45
Iteration: 2973 | Episodes: 181450 | Median Reward: 3.68 | Max Reward: 49.45
Iteration: 2974 | Episodes: 181500 | Median Reward: 5.93 | Max Reward: 49.45
Iteration: 2975 | Episodes: 181550 | Median Reward: -9.16 | Max Reward: 49.45
Iteration: 2976 | Episodes: 181600 | Median Reward: 9.36 | Max Reward: 49.45
Iteration: 2976 | Episodes: 181650 | Median Reward: -12.93 | Max Reward: 49.45
Iteration: 2977 | Episodes: 181700 | Median Reward: -12.93 | Max Reward: 49.45
Iteration: 2978 | Episodes: 181750 | Median Reward: -14.51 | Max Reward: 49.45
Iteration: 2979 | Episodes: 181800 | Median Reward: -18.44 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -112       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 2980       |
|    time_elapsed         | 54223      |
|    total_timesteps      | 18309120   |
| train/                  |            |
|    approx_kl            | 0.06881739 |
|    clip_fraction        | 0.0661     |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0001     |
|    loss                 | 7.42       |
|    n_updates            | 29790      |
|    policy_gradient_loss | -0.0241    |
|    std                  | 4.54       |
|    value_loss           | 31.3       |
----------------------------------------
Iteration: 2980 | Episodes: 181850 | Median Reward: -17.68 | Max Reward: 49.45
Iteration: 2980 | Episodes: 181900 | Median Reward: -16.81 | Max Reward: 49.45
Iteration: 2981 | Episodes: 181950 | Median Reward: -13.30 | Max Reward: 49.45
Iteration: 2982 | Episodes: 182000 | Median Reward: -12.64 | Max Reward: 49.45
Iteration: 2983 | Episodes: 182050 | Median Reward: -6.92 | Max Reward: 49.45
Iteration: 2984 | Episodes: 182100 | Median Reward: -4.95 | Max Reward: 49.45
Iteration: 2985 | Episodes: 182150 | Median Reward: -7.07 | Max Reward: 49.45
Iteration: 2985 | Episodes: 182200 | Median Reward: -8.66 | Max Reward: 49.45
Iteration: 2986 | Episodes: 182250 | Median Reward: -12.53 | Max Reward: 49.45
Iteration: 2987 | Episodes: 182300 | Median Reward: -11.63 | Max Reward: 49.45
Iteration: 2988 | Episodes: 182350 | Median Reward: -11.78 | Max Reward: 49.45
Iteration: 2989 | Episodes: 182400 | Median Reward: -7.04 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -96.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 2990      |
|    time_elapsed         | 54403     |
|    total_timesteps      | 18370560  |
| train/                  |           |
|    approx_kl            | 0.4683677 |
|    clip_fraction        | 0.119     |
|    clip_range           | 0.4       |
|    entropy_loss         | -109      |
|    explained_variance   | 0.873     |
|    learning_rate        | 0.0001    |
|    loss                 | 5.46      |
|    n_updates            | 29890     |
|    policy_gradient_loss | 0.0102    |
|    std                  | 4.54      |
|    value_loss           | 28.8      |
---------------------------------------
Iteration: 2990 | Episodes: 182450 | Median Reward: 12.55 | Max Reward: 49.45
Iteration: 2990 | Episodes: 182500 | Median Reward: -4.25 | Max Reward: 49.45
Iteration: 2991 | Episodes: 182550 | Median Reward: -3.88 | Max Reward: 49.45
Iteration: 2992 | Episodes: 182600 | Median Reward: -3.52 | Max Reward: 49.45
Iteration: 2993 | Episodes: 182650 | Median Reward: -3.53 | Max Reward: 49.45
Iteration: 2994 | Episodes: 182700 | Median Reward: 7.04 | Max Reward: 49.45
Iteration: 2994 | Episodes: 182750 | Median Reward: 13.18 | Max Reward: 49.45
Iteration: 2995 | Episodes: 182800 | Median Reward: -18.63 | Max Reward: 49.45
Iteration: 2996 | Episodes: 182850 | Median Reward: -13.51 | Max Reward: 49.45
Iteration: 2997 | Episodes: 182900 | Median Reward: -13.51 | Max Reward: 49.45
Iteration: 2998 | Episodes: 182950 | Median Reward: -10.50 | Max Reward: 49.45
Iteration: 2999 | Episodes: 183000 | Median Reward: -11.83 | Max Reward: 49.45
Iteration: 2999 | Episodes: 183050 | Median Reward: 11.23 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -95.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3000      |
|    time_elapsed         | 54584     |
|    total_timesteps      | 18432000  |
| train/                  |           |
|    approx_kl            | 1.7126333 |
|    clip_fraction        | 0.147     |
|    clip_range           | 0.4       |
|    entropy_loss         | -113      |
|    explained_variance   | 0.782     |
|    learning_rate        | 0.0001    |
|    loss                 | 7.02      |
|    n_updates            | 29990     |
|    policy_gradient_loss | 0.0324    |
|    std                  | 4.55      |
|    value_loss           | 63.6      |
---------------------------------------
Iteration: 3000 | Episodes: 183100 | Median Reward: -1.06 | Max Reward: 49.45
Iteration: 3001 | Episodes: 183150 | Median Reward: 4.25 | Max Reward: 49.45
Iteration: 3002 | Episodes: 183200 | Median Reward: 4.25 | Max Reward: 49.45
Iteration: 3003 | Episodes: 183250 | Median Reward: -10.17 | Max Reward: 49.45
Iteration: 3004 | Episodes: 183300 | Median Reward: -12.85 | Max Reward: 49.45
Iteration: 3004 | Episodes: 183350 | Median Reward: 6.02 | Max Reward: 49.45
Iteration: 3005 | Episodes: 183400 | Median Reward: 11.76 | Max Reward: 49.45
Iteration: 3006 | Episodes: 183450 | Median Reward: -9.09 | Max Reward: 49.45
Iteration: 3007 | Episodes: 183500 | Median Reward: -9.18 | Max Reward: 49.45
Iteration: 3008 | Episodes: 183550 | Median Reward: -0.10 | Max Reward: 49.45
Iteration: 3008 | Episodes: 183600 | Median Reward: -1.62 | Max Reward: 49.45
Iteration: 3009 | Episodes: 183650 | Median Reward: -13.32 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -109       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3010       |
|    time_elapsed         | 54776      |
|    total_timesteps      | 18493440   |
| train/                  |            |
|    approx_kl            | 0.34752506 |
|    clip_fraction        | 0.099      |
|    clip_range           | 0.4        |
|    entropy_loss         | -111       |
|    explained_variance   | 0.852      |
|    learning_rate        | 0.0001     |
|    loss                 | 16.4       |
|    n_updates            | 30090      |
|    policy_gradient_loss | 0.00277    |
|    std                  | 4.55       |
|    value_loss           | 41.9       |
----------------------------------------
Iteration: 3010 | Episodes: 183700 | Median Reward: -7.84 | Max Reward: 49.45
Iteration: 3011 | Episodes: 183750 | Median Reward: -7.84 | Max Reward: 49.45
Iteration: 3012 | Episodes: 183800 | Median Reward: -11.35 | Max Reward: 49.45
Iteration: 3013 | Episodes: 183850 | Median Reward: -14.20 | Max Reward: 49.45
Iteration: 3013 | Episodes: 183900 | Median Reward: 33.70 | Max Reward: 49.45
Iteration: 3014 | Episodes: 183950 | Median Reward: -19.62 | Max Reward: 49.45
Iteration: 3015 | Episodes: 184000 | Median Reward: -19.62 | Max Reward: 49.45
Iteration: 3016 | Episodes: 184050 | Median Reward: -20.55 | Max Reward: 49.45
Iteration: 3017 | Episodes: 184100 | Median Reward: -19.07 | Max Reward: 49.45
Iteration: 3017 | Episodes: 184150 | Median Reward: -12.20 | Max Reward: 49.45
Iteration: 3018 | Episodes: 184200 | Median Reward: -17.06 | Max Reward: 49.45
Iteration: 3019 | Episodes: 184250 | Median Reward: -15.82 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -117       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3020       |
|    time_elapsed         | 54954      |
|    total_timesteps      | 18554880   |
| train/                  |            |
|    approx_kl            | 0.71712965 |
|    clip_fraction        | 0.0743     |
|    clip_range           | 0.4        |
|    entropy_loss         | -115       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0001     |
|    loss                 | 5.5        |
|    n_updates            | 30190      |
|    policy_gradient_loss | 0.00202    |
|    std                  | 4.57       |
|    value_loss           | 22.7       |
----------------------------------------
Iteration: 3020 | Episodes: 184300 | Median Reward: -16.83 | Max Reward: 49.45
Iteration: 3021 | Episodes: 184350 | Median Reward: -12.38 | Max Reward: 49.45
Iteration: 3022 | Episodes: 184400 | Median Reward: -7.54 | Max Reward: 49.45
Iteration: 3022 | Episodes: 184450 | Median Reward: -16.31 | Max Reward: 49.45
Iteration: 3023 | Episodes: 184500 | Median Reward: -10.28 | Max Reward: 49.45
Iteration: 3024 | Episodes: 184550 | Median Reward: -9.77 | Max Reward: 49.45
Iteration: 3025 | Episodes: 184600 | Median Reward: -9.77 | Max Reward: 49.45
Iteration: 3026 | Episodes: 184650 | Median Reward: -7.92 | Max Reward: 49.45
Iteration: 3027 | Episodes: 184700 | Median Reward: -11.14 | Max Reward: 49.45
Iteration: 3027 | Episodes: 184750 | Median Reward: -0.98 | Max Reward: 49.45
Iteration: 3028 | Episodes: 184800 | Median Reward: -16.00 | Max Reward: 49.45
Iteration: 3029 | Episodes: 184850 | Median Reward: 15.26 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -88.8    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3030     |
|    time_elapsed         | 55133    |
|    total_timesteps      | 18616320 |
| train/                  |          |
|    approx_kl            | 4.851703 |
|    clip_fraction        | 0.0696   |
|    clip_range           | 0.4      |
|    entropy_loss         | -118     |
|    explained_variance   | 0.791    |
|    learning_rate        | 0.0001   |
|    loss                 | 57.5     |
|    n_updates            | 30290    |
|    policy_gradient_loss | 0.0381   |
|    std                  | 4.58     |
|    value_loss           | 61.6     |
--------------------------------------
Iteration: 3030 | Episodes: 184900 | Median Reward: 31.26 | Max Reward: 49.45
Iteration: 3031 | Episodes: 184950 | Median Reward: -10.44 | Max Reward: 49.45
Iteration: 3031 | Episodes: 185000 | Median Reward: 18.16 | Max Reward: 49.45
Iteration: 3032 | Episodes: 185050 | Median Reward: -11.36 | Max Reward: 49.45
Iteration: 3033 | Episodes: 185100 | Median Reward: -11.73 | Max Reward: 49.45
Iteration: 3034 | Episodes: 185150 | Median Reward: 10.30 | Max Reward: 49.45
Iteration: 3035 | Episodes: 185200 | Median Reward: 11.42 | Max Reward: 49.45
Iteration: 3036 | Episodes: 185250 | Median Reward: 33.08 | Max Reward: 49.45
Iteration: 3036 | Episodes: 185300 | Median Reward: 36.19 | Max Reward: 49.45
Iteration: 3037 | Episodes: 185350 | Median Reward: -19.59 | Max Reward: 49.45
Iteration: 3038 | Episodes: 185400 | Median Reward: -4.36 | Max Reward: 49.45
Iteration: 3039 | Episodes: 185450 | Median Reward: 31.56 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3040      |
|    time_elapsed         | 55315     |
|    total_timesteps      | 18677760  |
| train/                  |           |
|    approx_kl            | 0.7509448 |
|    clip_fraction        | 0.107     |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 0.76      |
|    learning_rate        | 0.0001    |
|    loss                 | 40.5      |
|    n_updates            | 30390     |
|    policy_gradient_loss | 0.0471    |
|    std                  | 4.58      |
|    value_loss           | 102       |
---------------------------------------
Iteration: 3040 | Episodes: 185500 | Median Reward: 29.25 | Max Reward: 49.45
Iteration: 3041 | Episodes: 185550 | Median Reward: -15.94 | Max Reward: 49.45
Iteration: 3041 | Episodes: 185600 | Median Reward: 36.79 | Max Reward: 49.45
Iteration: 3042 | Episodes: 185650 | Median Reward: 36.08 | Max Reward: 49.45
Iteration: 3043 | Episodes: 185700 | Median Reward: 38.48 | Max Reward: 49.45
Iteration: 3044 | Episodes: 185750 | Median Reward: 38.80 | Max Reward: 49.45
Iteration: 3045 | Episodes: 185800 | Median Reward: 41.63 | Max Reward: 49.45
Iteration: 3045 | Episodes: 185850 | Median Reward: 41.33 | Max Reward: 49.45
Iteration: 3046 | Episodes: 185900 | Median Reward: 43.93 | Max Reward: 49.45
Iteration: 3047 | Episodes: 185950 | Median Reward: 43.23 | Max Reward: 49.45
Iteration: 3048 | Episodes: 186000 | Median Reward: 43.23 | Max Reward: 49.45
Iteration: 3049 | Episodes: 186050 | Median Reward: 44.72 | Max Reward: 49.45
Iteration: 3049 | Episodes: 186100 | Median Reward: 41.81 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 82.5      |
|    ep_rew_mean          | -38       |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3050      |
|    time_elapsed         | 55506     |
|    total_timesteps      | 18739200  |
| train/                  |           |
|    approx_kl            | 16.449875 |
|    clip_fraction        | 0.264     |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0.694     |
|    learning_rate        | 0.0001    |
|    loss                 | 100       |
|    n_updates            | 30490     |
|    policy_gradient_loss | 0.125     |
|    std                  | 4.59      |
|    value_loss           | 214       |
---------------------------------------
Iteration: 3050 | Episodes: 186150 | Median Reward: 44.14 | Max Reward: 49.45
Iteration: 3051 | Episodes: 186200 | Median Reward: 44.31 | Max Reward: 49.45
Iteration: 3052 | Episodes: 186250 | Median Reward: 42.69 | Max Reward: 49.45
Iteration: 3053 | Episodes: 186300 | Median Reward: 38.94 | Max Reward: 49.45
Iteration: 3053 | Episodes: 186350 | Median Reward: 41.13 | Max Reward: 49.45
Iteration: 3054 | Episodes: 186400 | Median Reward: 42.82 | Max Reward: 49.45
Iteration: 3055 | Episodes: 186450 | Median Reward: 40.27 | Max Reward: 49.45
Iteration: 3056 | Episodes: 186500 | Median Reward: 39.63 | Max Reward: 49.45
Iteration: 3057 | Episodes: 186550 | Median Reward: 38.09 | Max Reward: 49.45
Iteration: 3057 | Episodes: 186600 | Median Reward: 37.48 | Max Reward: 49.45
Iteration: 3058 | Episodes: 186650 | Median Reward: 37.24 | Max Reward: 49.45
Iteration: 3059 | Episodes: 186700 | Median Reward: 39.53 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -60.8    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3060     |
|    time_elapsed         | 55690    |
|    total_timesteps      | 18800640 |
| train/                  |          |
|    approx_kl            | 8.119186 |
|    clip_fraction        | 0.119    |
|    clip_range           | 0.4      |
|    entropy_loss         | -118     |
|    explained_variance   | 0.755    |
|    learning_rate        | 0.0001   |
|    loss                 | 66.1     |
|    n_updates            | 30590    |
|    policy_gradient_loss | 0.0219   |
|    std                  | 4.62     |
|    value_loss           | 146      |
--------------------------------------
Iteration: 3060 | Episodes: 186750 | Median Reward: 44.07 | Max Reward: 49.45
Iteration: 3061 | Episodes: 186800 | Median Reward: 44.60 | Max Reward: 49.45
Iteration: 3062 | Episodes: 186850 | Median Reward: 44.57 | Max Reward: 49.45
Iteration: 3062 | Episodes: 186900 | Median Reward: 45.41 | Max Reward: 49.45
Iteration: 3063 | Episodes: 186950 | Median Reward: 44.45 | Max Reward: 49.45
Iteration: 3064 | Episodes: 187000 | Median Reward: 43.35 | Max Reward: 49.45
Iteration: 3065 | Episodes: 187050 | Median Reward: 42.74 | Max Reward: 49.45
Iteration: 3066 | Episodes: 187100 | Median Reward: 41.64 | Max Reward: 49.45
Iteration: 3067 | Episodes: 187150 | Median Reward: 43.78 | Max Reward: 49.45
Iteration: 3067 | Episodes: 187200 | Median Reward: 39.87 | Max Reward: 49.45
Iteration: 3068 | Episodes: 187250 | Median Reward: 41.61 | Max Reward: 49.45
Iteration: 3069 | Episodes: 187300 | Median Reward: 41.50 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -65.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3070      |
|    time_elapsed         | 55870     |
|    total_timesteps      | 18862080  |
| train/                  |           |
|    approx_kl            | 11.592043 |
|    clip_fraction        | 0.117     |
|    clip_range           | 0.4       |
|    entropy_loss         | -118      |
|    explained_variance   | 0.745     |
|    learning_rate        | 0.0001    |
|    loss                 | 75.1      |
|    n_updates            | 30690     |
|    policy_gradient_loss | 0.0151    |
|    std                  | 4.64      |
|    value_loss           | 159       |
---------------------------------------
Iteration: 3070 | Episodes: 187350 | Median Reward: 41.50 | Max Reward: 49.45
Iteration: 3071 | Episodes: 187400 | Median Reward: 41.40 | Max Reward: 49.45
Iteration: 3071 | Episodes: 187450 | Median Reward: 29.49 | Max Reward: 49.45
Iteration: 3072 | Episodes: 187500 | Median Reward: 32.14 | Max Reward: 49.45
Iteration: 3073 | Episodes: 187550 | Median Reward: 22.98 | Max Reward: 49.45
Iteration: 3074 | Episodes: 187600 | Median Reward: 27.95 | Max Reward: 49.45
Iteration: 3075 | Episodes: 187650 | Median Reward: 26.14 | Max Reward: 49.45
Iteration: 3076 | Episodes: 187700 | Median Reward: 32.73 | Max Reward: 49.45
Iteration: 3076 | Episodes: 187750 | Median Reward: 13.51 | Max Reward: 49.45
Iteration: 3077 | Episodes: 187800 | Median Reward: -16.29 | Max Reward: 49.45
Iteration: 3078 | Episodes: 187850 | Median Reward: -5.20 | Max Reward: 49.45
Iteration: 3079 | Episodes: 187900 | Median Reward: -3.82 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -89.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3080      |
|    time_elapsed         | 56062     |
|    total_timesteps      | 18923520  |
| train/                  |           |
|    approx_kl            | 0.5003723 |
|    clip_fraction        | 0.0824    |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 0.845     |
|    learning_rate        | 0.0001    |
|    loss                 | 25.2      |
|    n_updates            | 30790     |
|    policy_gradient_loss | 0.0117    |
|    std                  | 4.65      |
|    value_loss           | 47.3      |
---------------------------------------
Iteration: 3080 | Episodes: 187950 | Median Reward: 29.00 | Max Reward: 49.45
Iteration: 3080 | Episodes: 188000 | Median Reward: 29.82 | Max Reward: 49.45
Iteration: 3081 | Episodes: 188050 | Median Reward: -4.26 | Max Reward: 49.45
Iteration: 3082 | Episodes: 188100 | Median Reward: -8.18 | Max Reward: 49.45
Iteration: 3083 | Episodes: 188150 | Median Reward: -0.21 | Max Reward: 49.45
Iteration: 3084 | Episodes: 188200 | Median Reward: 4.59 | Max Reward: 49.45
Iteration: 3085 | Episodes: 188250 | Median Reward: -1.00 | Max Reward: 49.45
Iteration: 3085 | Episodes: 188300 | Median Reward: 0.92 | Max Reward: 49.45
Iteration: 3086 | Episodes: 188350 | Median Reward: -11.17 | Max Reward: 49.45
Iteration: 3087 | Episodes: 188400 | Median Reward: -8.55 | Max Reward: 49.45
Iteration: 3088 | Episodes: 188450 | Median Reward: -8.55 | Max Reward: 49.45
Iteration: 3089 | Episodes: 188500 | Median Reward: -15.44 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -109       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3090       |
|    time_elapsed         | 56241      |
|    total_timesteps      | 18984960   |
| train/                  |            |
|    approx_kl            | 0.12215175 |
|    clip_fraction        | 0.0494     |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.0001     |
|    loss                 | 19.7       |
|    n_updates            | 30890      |
|    policy_gradient_loss | 0.0162     |
|    std                  | 4.66       |
|    value_loss           | 44.5       |
----------------------------------------
Iteration: 3090 | Episodes: 188550 | Median Reward: -15.68 | Max Reward: 49.45
Iteration: 3090 | Episodes: 188600 | Median Reward: -13.81 | Max Reward: 49.45
Iteration: 3091 | Episodes: 188650 | Median Reward: 32.82 | Max Reward: 49.45
Iteration: 3092 | Episodes: 188700 | Median Reward: -10.07 | Max Reward: 49.45
Iteration: 3093 | Episodes: 188750 | Median Reward: -10.07 | Max Reward: 49.45
Iteration: 3094 | Episodes: 188800 | Median Reward: 35.04 | Max Reward: 49.45
Iteration: 3094 | Episodes: 188850 | Median Reward: 13.84 | Max Reward: 49.45
Iteration: 3095 | Episodes: 188900 | Median Reward: 28.12 | Max Reward: 49.45
Iteration: 3096 | Episodes: 188950 | Median Reward: 12.26 | Max Reward: 49.45
Iteration: 3097 | Episodes: 189000 | Median Reward: 22.19 | Max Reward: 49.45
Iteration: 3098 | Episodes: 189050 | Median Reward: 35.49 | Max Reward: 49.45
Iteration: 3099 | Episodes: 189100 | Median Reward: -14.80 | Max Reward: 49.45
Iteration: 3099 | Episodes: 189150 | Median Reward: -16.70 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -109       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3100       |
|    time_elapsed         | 56420      |
|    total_timesteps      | 19046400   |
| train/                  |            |
|    approx_kl            | 0.06131887 |
|    clip_fraction        | 0.032      |
|    clip_range           | 0.4        |
|    entropy_loss         | -118       |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0001     |
|    loss                 | 6.15       |
|    n_updates            | 30990      |
|    policy_gradient_loss | 0.000628   |
|    std                  | 4.66       |
|    value_loss           | 37.7       |
----------------------------------------
Iteration: 3100 | Episodes: 189200 | Median Reward: -11.51 | Max Reward: 49.45
Iteration: 3101 | Episodes: 189250 | Median Reward: 30.45 | Max Reward: 49.45
Iteration: 3102 | Episodes: 189300 | Median Reward: 25.14 | Max Reward: 49.45
Iteration: 3103 | Episodes: 189350 | Median Reward: -1.96 | Max Reward: 49.45
Iteration: 3103 | Episodes: 189400 | Median Reward: -20.44 | Max Reward: 49.45
Iteration: 3104 | Episodes: 189450 | Median Reward: 36.04 | Max Reward: 49.45
Iteration: 3105 | Episodes: 189500 | Median Reward: 33.74 | Max Reward: 49.45
Iteration: 3106 | Episodes: 189550 | Median Reward: 10.84 | Max Reward: 49.45
Iteration: 3107 | Episodes: 189600 | Median Reward: -8.89 | Max Reward: 49.45
Iteration: 3108 | Episodes: 189650 | Median Reward: -18.45 | Max Reward: 49.45
Iteration: 3108 | Episodes: 189700 | Median Reward: -19.69 | Max Reward: 49.45
Iteration: 3109 | Episodes: 189750 | Median Reward: 32.69 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3110      |
|    time_elapsed         | 56609     |
|    total_timesteps      | 19107840  |
| train/                  |           |
|    approx_kl            | 3.9602585 |
|    clip_fraction        | 0.158     |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 0.819     |
|    learning_rate        | 0.0001    |
|    loss                 | 9.56      |
|    n_updates            | 31090     |
|    policy_gradient_loss | 0.0194    |
|    std                  | 4.67      |
|    value_loss           | 45.5      |
---------------------------------------
Iteration: 3110 | Episodes: 189800 | Median Reward: 20.36 | Max Reward: 49.45
Iteration: 3111 | Episodes: 189850 | Median Reward: -5.92 | Max Reward: 49.45
Iteration: 3112 | Episodes: 189900 | Median Reward: 1.41 | Max Reward: 49.45
Iteration: 3113 | Episodes: 189950 | Median Reward: 35.62 | Max Reward: 49.45
Iteration: 3113 | Episodes: 190000 | Median Reward: 37.37 | Max Reward: 49.45
Iteration: 3114 | Episodes: 190050 | Median Reward: -7.36 | Max Reward: 49.45
Iteration: 3115 | Episodes: 190100 | Median Reward: -15.41 | Max Reward: 49.45
Iteration: 3116 | Episodes: 190150 | Median Reward: -17.44 | Max Reward: 49.45
Iteration: 3117 | Episodes: 190200 | Median Reward: -15.99 | Max Reward: 49.45
Iteration: 3117 | Episodes: 190250 | Median Reward: -19.38 | Max Reward: 49.45
Iteration: 3118 | Episodes: 190300 | Median Reward: 2.20 | Max Reward: 49.45
Iteration: 3119 | Episodes: 190350 | Median Reward: -3.12 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -100       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3120       |
|    time_elapsed         | 56791      |
|    total_timesteps      | 19169280   |
| train/                  |            |
|    approx_kl            | 0.49864405 |
|    clip_fraction        | 0.0886     |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0.828      |
|    learning_rate        | 0.0001     |
|    loss                 | 46.9       |
|    n_updates            | 31190      |
|    policy_gradient_loss | -0.00918   |
|    std                  | 4.68       |
|    value_loss           | 64.8       |
----------------------------------------
Iteration: 3120 | Episodes: 190400 | Median Reward: -13.68 | Max Reward: 49.45
Iteration: 3121 | Episodes: 190450 | Median Reward: -14.84 | Max Reward: 49.45
Iteration: 3122 | Episodes: 190500 | Median Reward: -17.37 | Max Reward: 49.45
Iteration: 3122 | Episodes: 190550 | Median Reward: 10.65 | Max Reward: 49.45
Iteration: 3123 | Episodes: 190600 | Median Reward: 19.34 | Max Reward: 49.45
Iteration: 3124 | Episodes: 190650 | Median Reward: 20.11 | Max Reward: 49.45
Iteration: 3125 | Episodes: 190700 | Median Reward: 33.71 | Max Reward: 49.45
Iteration: 3126 | Episodes: 190750 | Median Reward: 36.68 | Max Reward: 49.45
Iteration: 3127 | Episodes: 190800 | Median Reward: 37.46 | Max Reward: 49.45
Iteration: 3127 | Episodes: 190850 | Median Reward: 41.78 | Max Reward: 49.45
Iteration: 3128 | Episodes: 190900 | Median Reward: 38.49 | Max Reward: 49.45
Iteration: 3129 | Episodes: 190950 | Median Reward: 38.63 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -71.5    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3130     |
|    time_elapsed         | 56973    |
|    total_timesteps      | 19230720 |
| train/                  |          |
|    approx_kl            | 10.16729 |
|    clip_fraction        | 0.142    |
|    clip_range           | 0.4      |
|    entropy_loss         | -118     |
|    explained_variance   | 0.754    |
|    learning_rate        | 0.0001   |
|    loss                 | 39.3     |
|    n_updates            | 31290    |
|    policy_gradient_loss | 0.0524   |
|    std                  | 4.69     |
|    value_loss           | 101      |
--------------------------------------
Iteration: 3130 | Episodes: 191000 | Median Reward: 40.97 | Max Reward: 49.45
Iteration: 3131 | Episodes: 191050 | Median Reward: 43.68 | Max Reward: 49.45
Iteration: 3131 | Episodes: 191100 | Median Reward: 44.39 | Max Reward: 49.45
Iteration: 3132 | Episodes: 191150 | Median Reward: 44.49 | Max Reward: 49.45
Iteration: 3133 | Episodes: 191200 | Median Reward: 42.51 | Max Reward: 49.45
Iteration: 3134 | Episodes: 191250 | Median Reward: 44.82 | Max Reward: 49.45
Iteration: 3135 | Episodes: 191300 | Median Reward: 45.03 | Max Reward: 49.45
Iteration: 3136 | Episodes: 191350 | Median Reward: 42.08 | Max Reward: 49.45
Iteration: 3136 | Episodes: 191400 | Median Reward: 44.41 | Max Reward: 49.45
Iteration: 3137 | Episodes: 191450 | Median Reward: 44.90 | Max Reward: 49.45
Iteration: 3138 | Episodes: 191500 | Median Reward: 44.31 | Max Reward: 49.45
Iteration: 3139 | Episodes: 191550 | Median Reward: 44.28 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -56.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3140      |
|    time_elapsed         | 57151     |
|    total_timesteps      | 19292160  |
| train/                  |           |
|    approx_kl            | 7.1802144 |
|    clip_fraction        | 0.166     |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0.752     |
|    learning_rate        | 0.0001    |
|    loss                 | 73.2      |
|    n_updates            | 31390     |
|    policy_gradient_loss | 0.0341    |
|    std                  | 4.71      |
|    value_loss           | 163       |
---------------------------------------
Iteration: 3140 | Episodes: 191600 | Median Reward: 43.10 | Max Reward: 49.45
Iteration: 3140 | Episodes: 191650 | Median Reward: 43.58 | Max Reward: 49.45
Iteration: 3141 | Episodes: 191700 | Median Reward: 45.62 | Max Reward: 49.45
Iteration: 3142 | Episodes: 191750 | Median Reward: 42.59 | Max Reward: 49.45
Iteration: 3143 | Episodes: 191800 | Median Reward: 43.63 | Max Reward: 49.45
Iteration: 3144 | Episodes: 191850 | Median Reward: 44.09 | Max Reward: 49.45
Iteration: 3145 | Episodes: 191900 | Median Reward: 43.84 | Max Reward: 49.45
Iteration: 3145 | Episodes: 191950 | Median Reward: 45.17 | Max Reward: 49.45
Iteration: 3146 | Episodes: 192000 | Median Reward: 43.32 | Max Reward: 49.45
Iteration: 3147 | Episodes: 192050 | Median Reward: 44.12 | Max Reward: 49.45
Iteration: 3148 | Episodes: 192100 | Median Reward: 43.63 | Max Reward: 49.45
Iteration: 3149 | Episodes: 192150 | Median Reward: 42.95 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -57.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3150      |
|    time_elapsed         | 57341     |
|    total_timesteps      | 19353600  |
| train/                  |           |
|    approx_kl            | 2.0864658 |
|    clip_fraction        | 0.0656    |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0.768     |
|    learning_rate        | 0.0001    |
|    loss                 | 72.4      |
|    n_updates            | 31490     |
|    policy_gradient_loss | 0.0137    |
|    std                  | 4.75      |
|    value_loss           | 142       |
---------------------------------------
Iteration: 3150 | Episodes: 192200 | Median Reward: 42.80 | Max Reward: 49.45
Iteration: 3150 | Episodes: 192250 | Median Reward: 42.81 | Max Reward: 49.45
Iteration: 3151 | Episodes: 192300 | Median Reward: 44.42 | Max Reward: 49.45
Iteration: 3152 | Episodes: 192350 | Median Reward: 44.41 | Max Reward: 49.45
Iteration: 3153 | Episodes: 192400 | Median Reward: 44.26 | Max Reward: 49.45
Iteration: 3154 | Episodes: 192450 | Median Reward: 43.36 | Max Reward: 49.45
Iteration: 3154 | Episodes: 192500 | Median Reward: 41.46 | Max Reward: 49.45
Iteration: 3155 | Episodes: 192550 | Median Reward: 43.28 | Max Reward: 49.45
Iteration: 3156 | Episodes: 192600 | Median Reward: 44.58 | Max Reward: 49.45
Iteration: 3157 | Episodes: 192650 | Median Reward: 46.52 | Max Reward: 49.45
Iteration: 3158 | Episodes: 192700 | Median Reward: 46.40 | Max Reward: 49.45
Iteration: 3159 | Episodes: 192750 | Median Reward: 42.00 | Max Reward: 49.45
Iteration: 3159 | Episodes: 192800 | Median Reward: 41.01 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3160      |
|    time_elapsed         | 57522     |
|    total_timesteps      | 19415040  |
| train/                  |           |
|    approx_kl            | 24.132095 |
|    clip_fraction        | 0.433     |
|    clip_range           | 0.4       |
|    entropy_loss         | -118      |
|    explained_variance   | 0.781     |
|    learning_rate        | 0.0001    |
|    loss                 | 54.3      |
|    n_updates            | 31590     |
|    policy_gradient_loss | 0.114     |
|    std                  | 4.79      |
|    value_loss           | 125       |
---------------------------------------
Iteration: 3160 | Episodes: 192850 | Median Reward: 40.90 | Max Reward: 49.45
Iteration: 3161 | Episodes: 192900 | Median Reward: 39.40 | Max Reward: 49.45
Iteration: 3162 | Episodes: 192950 | Median Reward: 38.68 | Max Reward: 49.45
Iteration: 3163 | Episodes: 193000 | Median Reward: 38.47 | Max Reward: 49.45
Iteration: 3163 | Episodes: 193050 | Median Reward: -15.79 | Max Reward: 49.45
Iteration: 3164 | Episodes: 193100 | Median Reward: 40.66 | Max Reward: 49.45
Iteration: 3165 | Episodes: 193150 | Median Reward: 40.42 | Max Reward: 49.45
Iteration: 3166 | Episodes: 193200 | Median Reward: 39.82 | Max Reward: 49.45
Iteration: 3167 | Episodes: 193250 | Median Reward: 39.32 | Max Reward: 49.45
Iteration: 3168 | Episodes: 193300 | Median Reward: 42.04 | Max Reward: 49.45
Iteration: 3168 | Episodes: 193350 | Median Reward: 41.51 | Max Reward: 49.45
Iteration: 3169 | Episodes: 193400 | Median Reward: 43.76 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -81      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3170     |
|    time_elapsed         | 57702    |
|    total_timesteps      | 19476480 |
| train/                  |          |
|    approx_kl            | 5.005562 |
|    clip_fraction        | 0.145    |
|    clip_range           | 0.4      |
|    entropy_loss         | -118     |
|    explained_variance   | 0.79     |
|    learning_rate        | 0.0001   |
|    loss                 | 31       |
|    n_updates            | 31690    |
|    policy_gradient_loss | 0.0469   |
|    std                  | 4.81     |
|    value_loss           | 87.2     |
--------------------------------------
Iteration: 3170 | Episodes: 193450 | Median Reward: 36.98 | Max Reward: 49.45
Iteration: 3171 | Episodes: 193500 | Median Reward: 37.32 | Max Reward: 49.45
Iteration: 3172 | Episodes: 193550 | Median Reward: 39.08 | Max Reward: 49.45
Iteration: 3173 | Episodes: 193600 | Median Reward: 40.19 | Max Reward: 49.45
Iteration: 3173 | Episodes: 193650 | Median Reward: 39.54 | Max Reward: 49.45
Iteration: 3174 | Episodes: 193700 | Median Reward: 39.89 | Max Reward: 49.45
Iteration: 3175 | Episodes: 193750 | Median Reward: 41.11 | Max Reward: 49.45
Iteration: 3176 | Episodes: 193800 | Median Reward: 41.02 | Max Reward: 49.45
Iteration: 3177 | Episodes: 193850 | Median Reward: 41.22 | Max Reward: 49.45
Iteration: 3177 | Episodes: 193900 | Median Reward: -19.87 | Max Reward: 49.45
Iteration: 3178 | Episodes: 193950 | Median Reward: 42.73 | Max Reward: 49.45
Iteration: 3179 | Episodes: 194000 | Median Reward: 41.06 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -66.3     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3180      |
|    time_elapsed         | 57893     |
|    total_timesteps      | 19537920  |
| train/                  |           |
|    approx_kl            | 1.9161873 |
|    clip_fraction        | 0.0971    |
|    clip_range           | 0.4       |
|    entropy_loss         | -118      |
|    explained_variance   | 0.785     |
|    learning_rate        | 0.0001    |
|    loss                 | 57.7      |
|    n_updates            | 31790     |
|    policy_gradient_loss | 0.0311    |
|    std                  | 4.82      |
|    value_loss           | 115       |
---------------------------------------
Iteration: 3180 | Episodes: 194050 | Median Reward: 40.49 | Max Reward: 49.45
Iteration: 3181 | Episodes: 194100 | Median Reward: -17.09 | Max Reward: 49.45
Iteration: 3182 | Episodes: 194150 | Median Reward: 34.50 | Max Reward: 49.45
Iteration: 3182 | Episodes: 194200 | Median Reward: 41.27 | Max Reward: 49.45
Iteration: 3183 | Episodes: 194250 | Median Reward: 41.09 | Max Reward: 49.45
Iteration: 3184 | Episodes: 194300 | Median Reward: 15.36 | Max Reward: 49.45
Iteration: 3185 | Episodes: 194350 | Median Reward: -8.92 | Max Reward: 49.45
Iteration: 3186 | Episodes: 194400 | Median Reward: -7.99 | Max Reward: 49.45
Iteration: 3186 | Episodes: 194450 | Median Reward: 41.51 | Max Reward: 49.45
Iteration: 3187 | Episodes: 194500 | Median Reward: 39.75 | Max Reward: 49.45
Iteration: 3188 | Episodes: 194550 | Median Reward: 40.98 | Max Reward: 49.45
Iteration: 3189 | Episodes: 194600 | Median Reward: -7.12 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91.9     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3190      |
|    time_elapsed         | 58075     |
|    total_timesteps      | 19599360  |
| train/                  |           |
|    approx_kl            | 15.689639 |
|    clip_fraction        | 0.144     |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0.797     |
|    learning_rate        | 0.0001    |
|    loss                 | 55.7      |
|    n_updates            | 31890     |
|    policy_gradient_loss | 0.00495   |
|    std                  | 4.83      |
|    value_loss           | 108       |
---------------------------------------
Iteration: 3190 | Episodes: 194650 | Median Reward: -16.55 | Max Reward: 49.45
Iteration: 3191 | Episodes: 194700 | Median Reward: -16.96 | Max Reward: 49.45
Iteration: 3191 | Episodes: 194750 | Median Reward: -17.61 | Max Reward: 49.45
Iteration: 3192 | Episodes: 194800 | Median Reward: 39.02 | Max Reward: 49.45
Iteration: 3193 | Episodes: 194850 | Median Reward: -7.15 | Max Reward: 49.45
Iteration: 3194 | Episodes: 194900 | Median Reward: -7.30 | Max Reward: 49.45
Iteration: 3195 | Episodes: 194950 | Median Reward: 40.42 | Max Reward: 49.45
Iteration: 3196 | Episodes: 195000 | Median Reward: 40.19 | Max Reward: 49.45
Iteration: 3196 | Episodes: 195050 | Median Reward: 25.52 | Max Reward: 49.45
Iteration: 3197 | Episodes: 195100 | Median Reward: 16.27 | Max Reward: 49.45
Iteration: 3198 | Episodes: 195150 | Median Reward: -5.96 | Max Reward: 49.45
Iteration: 3199 | Episodes: 195200 | Median Reward: -2.18 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -97.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3200      |
|    time_elapsed         | 58252     |
|    total_timesteps      | 19660800  |
| train/                  |           |
|    approx_kl            | 2.2713149 |
|    clip_fraction        | 0.0939    |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 0.797     |
|    learning_rate        | 0.0001    |
|    loss                 | 42.6      |
|    n_updates            | 31990     |
|    policy_gradient_loss | 0.00236   |
|    std                  | 4.84      |
|    value_loss           | 77.3      |
---------------------------------------
Iteration: 3200 | Episodes: 195250 | Median Reward: -18.56 | Max Reward: 49.45
Iteration: 3200 | Episodes: 195300 | Median Reward: 30.41 | Max Reward: 49.45
Iteration: 3201 | Episodes: 195350 | Median Reward: 40.67 | Max Reward: 49.45
Iteration: 3202 | Episodes: 195400 | Median Reward: 39.10 | Max Reward: 49.45
Iteration: 3203 | Episodes: 195450 | Median Reward: 36.57 | Max Reward: 49.45
Iteration: 3204 | Episodes: 195500 | Median Reward: 19.65 | Max Reward: 49.45
Iteration: 3205 | Episodes: 195550 | Median Reward: 1.72 | Max Reward: 49.45
Iteration: 3205 | Episodes: 195600 | Median Reward: 13.91 | Max Reward: 49.45
Iteration: 3206 | Episodes: 195650 | Median Reward: -13.09 | Max Reward: 49.45
Iteration: 3207 | Episodes: 195700 | Median Reward: -13.53 | Max Reward: 49.45
Iteration: 3208 | Episodes: 195750 | Median Reward: -13.53 | Max Reward: 49.45
Iteration: 3209 | Episodes: 195800 | Median Reward: 2.58 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -88.2     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3210      |
|    time_elapsed         | 58438     |
|    total_timesteps      | 19722240  |
| train/                  |           |
|    approx_kl            | 2.7728758 |
|    clip_fraction        | 0.188     |
|    clip_range           | 0.4       |
|    entropy_loss         | -116      |
|    explained_variance   | 0.787     |
|    learning_rate        | 0.0001    |
|    loss                 | 53.9      |
|    n_updates            | 32090     |
|    policy_gradient_loss | 0.048     |
|    std                  | 4.85      |
|    value_loss           | 88.7      |
---------------------------------------
Iteration: 3210 | Episodes: 195850 | Median Reward: 14.60 | Max Reward: 49.45
Iteration: 3210 | Episodes: 195900 | Median Reward: -20.21 | Max Reward: 49.45
Iteration: 3211 | Episodes: 195950 | Median Reward: -13.36 | Max Reward: 49.45
Iteration: 3212 | Episodes: 196000 | Median Reward: 37.79 | Max Reward: 49.45
Iteration: 3213 | Episodes: 196050 | Median Reward: 32.43 | Max Reward: 49.45
Iteration: 3214 | Episodes: 196100 | Median Reward: -11.50 | Max Reward: 49.45
Iteration: 3214 | Episodes: 196150 | Median Reward: -16.28 | Max Reward: 49.45
Iteration: 3215 | Episodes: 196200 | Median Reward: 2.13 | Max Reward: 49.45
Iteration: 3216 | Episodes: 196250 | Median Reward: -12.41 | Max Reward: 49.45
Iteration: 3217 | Episodes: 196300 | Median Reward: -7.11 | Max Reward: 49.45
Iteration: 3218 | Episodes: 196350 | Median Reward: 1.39 | Max Reward: 49.45
Iteration: 3219 | Episodes: 196400 | Median Reward: 26.09 | Max Reward: 49.45
Iteration: 3219 | Episodes: 196450 | Median Reward: 40.12 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.7     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3220      |
|    time_elapsed         | 58629     |
|    total_timesteps      | 19783680  |
| train/                  |           |
|    approx_kl            | 2.6522875 |
|    clip_fraction        | 0.0966    |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.811     |
|    learning_rate        | 0.0001    |
|    loss                 | 17.2      |
|    n_updates            | 32190     |
|    policy_gradient_loss | 0.0184    |
|    std                  | 4.86      |
|    value_loss           | 73        |
---------------------------------------
Iteration: 3220 | Episodes: 196500 | Median Reward: 22.62 | Max Reward: 49.45
Iteration: 3221 | Episodes: 196550 | Median Reward: -5.81 | Max Reward: 49.45
Iteration: 3222 | Episodes: 196600 | Median Reward: -1.01 | Max Reward: 49.45
Iteration: 3223 | Episodes: 196650 | Median Reward: 33.37 | Max Reward: 49.45
Iteration: 3223 | Episodes: 196700 | Median Reward: 14.00 | Max Reward: 49.45
Iteration: 3224 | Episodes: 196750 | Median Reward: 42.02 | Max Reward: 49.45
Iteration: 3225 | Episodes: 196800 | Median Reward: 14.84 | Max Reward: 49.45
Iteration: 3226 | Episodes: 196850 | Median Reward: 19.43 | Max Reward: 49.45
Iteration: 3227 | Episodes: 196900 | Median Reward: 19.43 | Max Reward: 49.45
Iteration: 3228 | Episodes: 196950 | Median Reward: -9.34 | Max Reward: 49.45
Iteration: 3228 | Episodes: 197000 | Median Reward: 15.94 | Max Reward: 49.45
Iteration: 3229 | Episodes: 197050 | Median Reward: -0.07 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -92.2    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3230     |
|    time_elapsed         | 58811    |
|    total_timesteps      | 19845120 |
| train/                  |          |
|    approx_kl            | 9.16044  |
|    clip_fraction        | 0.241    |
|    clip_range           | 0.4      |
|    entropy_loss         | -116     |
|    explained_variance   | 0.777    |
|    learning_rate        | 0.0001   |
|    loss                 | 60.9     |
|    n_updates            | 32290    |
|    policy_gradient_loss | 0.0502   |
|    std                  | 4.87     |
|    value_loss           | 93.1     |
--------------------------------------
Iteration: 3230 | Episodes: 197100 | Median Reward: -2.61 | Max Reward: 49.45
Iteration: 3231 | Episodes: 197150 | Median Reward: 16.17 | Max Reward: 49.45
Iteration: 3232 | Episodes: 197200 | Median Reward: 35.92 | Max Reward: 49.45
Iteration: 3233 | Episodes: 197250 | Median Reward: 23.84 | Max Reward: 49.45
Iteration: 3233 | Episodes: 197300 | Median Reward: 45.07 | Max Reward: 49.45
Iteration: 3234 | Episodes: 197350 | Median Reward: 42.65 | Max Reward: 49.45
Iteration: 3235 | Episodes: 197400 | Median Reward: -2.64 | Max Reward: 49.45
Iteration: 3236 | Episodes: 197450 | Median Reward: -8.15 | Max Reward: 49.45
Iteration: 3237 | Episodes: 197500 | Median Reward: -6.28 | Max Reward: 49.45
Iteration: 3237 | Episodes: 197550 | Median Reward: -9.89 | Max Reward: 49.45
Iteration: 3238 | Episodes: 197600 | Median Reward: -4.79 | Max Reward: 49.45
Iteration: 3239 | Episodes: 197650 | Median Reward: -10.70 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -101      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3240      |
|    time_elapsed         | 58992     |
|    total_timesteps      | 19906560  |
| train/                  |           |
|    approx_kl            | 0.8327991 |
|    clip_fraction        | 0.176     |
|    clip_range           | 0.4       |
|    entropy_loss         | -115      |
|    explained_variance   | 0.804     |
|    learning_rate        | 0.0001    |
|    loss                 | 36.7      |
|    n_updates            | 32390     |
|    policy_gradient_loss | -0.00127  |
|    std                  | 4.88      |
|    value_loss           | 66.4      |
---------------------------------------
Iteration: 3240 | Episodes: 197700 | Median Reward: -5.01 | Max Reward: 49.45
Iteration: 3241 | Episodes: 197750 | Median Reward: 11.99 | Max Reward: 49.45
Iteration: 3242 | Episodes: 197800 | Median Reward: 3.83 | Max Reward: 49.45
Iteration: 3242 | Episodes: 197850 | Median Reward: -8.74 | Max Reward: 49.45
Iteration: 3243 | Episodes: 197900 | Median Reward: -5.80 | Max Reward: 49.45
Iteration: 3244 | Episodes: 197950 | Median Reward: -15.17 | Max Reward: 49.45
Iteration: 3245 | Episodes: 198000 | Median Reward: -13.32 | Max Reward: 49.45
Iteration: 3246 | Episodes: 198050 | Median Reward: 7.77 | Max Reward: 49.45
Iteration: 3247 | Episodes: 198100 | Median Reward: 2.60 | Max Reward: 49.45
Iteration: 3247 | Episodes: 198150 | Median Reward: 24.58 | Max Reward: 49.45
Iteration: 3248 | Episodes: 198200 | Median Reward: -9.97 | Max Reward: 49.45
Iteration: 3249 | Episodes: 198250 | Median Reward: -9.97 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -99.7      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3250       |
|    time_elapsed         | 59181      |
|    total_timesteps      | 19968000   |
| train/                  |            |
|    approx_kl            | 0.14828491 |
|    clip_fraction        | 0.0636     |
|    clip_range           | 0.4        |
|    entropy_loss         | -116       |
|    explained_variance   | 0.903      |
|    learning_rate        | 0.0001     |
|    loss                 | 13.3       |
|    n_updates            | 32490      |
|    policy_gradient_loss | 0.0116     |
|    std                  | 4.89       |
|    value_loss           | 36.1       |
----------------------------------------
Iteration: 3250 | Episodes: 198300 | Median Reward: -11.86 | Max Reward: 49.45
Iteration: 3251 | Episodes: 198350 | Median Reward: -10.44 | Max Reward: 49.45
Iteration: 3251 | Episodes: 198400 | Median Reward: -9.86 | Max Reward: 49.45
Iteration: 3252 | Episodes: 198450 | Median Reward: -17.04 | Max Reward: 49.45
Iteration: 3253 | Episodes: 198500 | Median Reward: -17.04 | Max Reward: 49.45
Iteration: 3254 | Episodes: 198550 | Median Reward: -16.59 | Max Reward: 49.45
Iteration: 3255 | Episodes: 198600 | Median Reward: -16.04 | Max Reward: 49.45
Iteration: 3256 | Episodes: 198650 | Median Reward: -11.83 | Max Reward: 49.45
Iteration: 3256 | Episodes: 198700 | Median Reward: -14.69 | Max Reward: 49.45
Iteration: 3257 | Episodes: 198750 | Median Reward: -17.99 | Max Reward: 49.45
Iteration: 3258 | Episodes: 198800 | Median Reward: -15.99 | Max Reward: 49.45
Iteration: 3259 | Episodes: 198850 | Median Reward: -15.86 | Max Reward: 49.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -110        |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 3260        |
|    time_elapsed         | 59361       |
|    total_timesteps      | 20029440    |
| train/                  |             |
|    approx_kl            | 0.010126991 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.4         |
|    entropy_loss         | -118        |
|    explained_variance   | 0.883       |
|    learning_rate        | 0.0001      |
|    loss                 | 12.9        |
|    n_updates            | 32590       |
|    policy_gradient_loss | -0.0042     |
|    std                  | 4.89        |
|    value_loss           | 37.7        |
-----------------------------------------
Iteration: 3260 | Episodes: 198900 | Median Reward: -17.30 | Max Reward: 49.45
Iteration: 3260 | Episodes: 198950 | Median Reward: -19.71 | Max Reward: 49.45
Iteration: 3261 | Episodes: 199000 | Median Reward: -23.00 | Max Reward: 49.45
Iteration: 3262 | Episodes: 199050 | Median Reward: -21.19 | Max Reward: 49.45
Iteration: 3263 | Episodes: 199100 | Median Reward: -16.90 | Max Reward: 49.45
Iteration: 3264 | Episodes: 199150 | Median Reward: -12.72 | Max Reward: 49.45
Iteration: 3265 | Episodes: 199200 | Median Reward: -15.36 | Max Reward: 49.45
Iteration: 3265 | Episodes: 199250 | Median Reward: -19.29 | Max Reward: 49.45
Iteration: 3266 | Episodes: 199300 | Median Reward: -11.68 | Max Reward: 49.45
Iteration: 3267 | Episodes: 199350 | Median Reward: 5.45 | Max Reward: 49.45
Iteration: 3268 | Episodes: 199400 | Median Reward: -7.17 | Max Reward: 49.45
Iteration: 3269 | Episodes: 199450 | Median Reward: -14.48 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -115       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3270       |
|    time_elapsed         | 59541      |
|    total_timesteps      | 20090880   |
| train/                  |            |
|    approx_kl            | 0.59620893 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.4        |
|    entropy_loss         | -116       |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0001     |
|    loss                 | 12.4       |
|    n_updates            | 32690      |
|    policy_gradient_loss | 0.0306     |
|    std                  | 4.91       |
|    value_loss           | 39.1       |
----------------------------------------
Iteration: 3270 | Episodes: 199500 | Median Reward: -17.33 | Max Reward: 49.45
Iteration: 3270 | Episodes: 199550 | Median Reward: -16.49 | Max Reward: 49.45
Iteration: 3271 | Episodes: 199600 | Median Reward: -21.56 | Max Reward: 49.45
Iteration: 3272 | Episodes: 199650 | Median Reward: -20.90 | Max Reward: 49.45
Iteration: 3273 | Episodes: 199700 | Median Reward: -15.63 | Max Reward: 49.45
Iteration: 3274 | Episodes: 199750 | Median Reward: -15.63 | Max Reward: 49.45
Iteration: 3274 | Episodes: 199800 | Median Reward: -19.59 | Max Reward: 49.45
Iteration: 3275 | Episodes: 199850 | Median Reward: -10.17 | Max Reward: 49.45
Iteration: 3276 | Episodes: 199900 | Median Reward: -9.31 | Max Reward: 49.45
Iteration: 3277 | Episodes: 199950 | Median Reward: -11.94 | Max Reward: 49.45
Iteration: 3278 | Episodes: 200000 | Median Reward: -18.06 | Max Reward: 49.45
Iteration: 3279 | Episodes: 200050 | Median Reward: -17.90 | Max Reward: 49.45
Iteration: 3279 | Episodes: 200100 | Median Reward: -6.30 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -112      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3280      |
|    time_elapsed         | 59733     |
|    total_timesteps      | 20152320  |
| train/                  |           |
|    approx_kl            | 1.9937992 |
|    clip_fraction        | 0.152     |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0001    |
|    loss                 | 5.87      |
|    n_updates            | 32790     |
|    policy_gradient_loss | 0.00284   |
|    std                  | 4.92      |
|    value_loss           | 26.4      |
---------------------------------------
Iteration: 3280 | Episodes: 200150 | Median Reward: -17.75 | Max Reward: 49.45
Iteration: 3281 | Episodes: 200200 | Median Reward: -9.35 | Max Reward: 49.45
Iteration: 3282 | Episodes: 200250 | Median Reward: -11.81 | Max Reward: 49.45
Iteration: 3283 | Episodes: 200300 | Median Reward: -15.42 | Max Reward: 49.45
Iteration: 3283 | Episodes: 200350 | Median Reward: -6.62 | Max Reward: 49.45
Iteration: 3284 | Episodes: 200400 | Median Reward: -8.95 | Max Reward: 49.45
Iteration: 3285 | Episodes: 200450 | Median Reward: -8.90 | Max Reward: 49.45
Iteration: 3286 | Episodes: 200500 | Median Reward: -5.83 | Max Reward: 49.45
Iteration: 3287 | Episodes: 200550 | Median Reward: -0.07 | Max Reward: 49.45
Iteration: 3288 | Episodes: 200600 | Median Reward: -21.91 | Max Reward: 49.45
Iteration: 3288 | Episodes: 200650 | Median Reward: -14.50 | Max Reward: 49.45
Iteration: 3289 | Episodes: 200700 | Median Reward: -19.26 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -117       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3290       |
|    time_elapsed         | 59913      |
|    total_timesteps      | 20213760   |
| train/                  |            |
|    approx_kl            | 0.87369186 |
|    clip_fraction        | 0.0578     |
|    clip_range           | 0.4        |
|    entropy_loss         | -118       |
|    explained_variance   | 0.912      |
|    learning_rate        | 0.0001     |
|    loss                 | 6.94       |
|    n_updates            | 32890      |
|    policy_gradient_loss | 0.00716    |
|    std                  | 4.93       |
|    value_loss           | 27.2       |
----------------------------------------
Iteration: 3290 | Episodes: 200750 | Median Reward: -6.39 | Max Reward: 49.45
Iteration: 3291 | Episodes: 200800 | Median Reward: -2.50 | Max Reward: 49.45
Iteration: 3292 | Episodes: 200850 | Median Reward: -6.04 | Max Reward: 49.45
Iteration: 3293 | Episodes: 200900 | Median Reward: -11.45 | Max Reward: 49.45
Iteration: 3293 | Episodes: 200950 | Median Reward: -20.35 | Max Reward: 49.45
Iteration: 3294 | Episodes: 201000 | Median Reward: -18.49 | Max Reward: 49.45
Iteration: 3295 | Episodes: 201050 | Median Reward: -5.46 | Max Reward: 49.45
Iteration: 3296 | Episodes: 201100 | Median Reward: -2.28 | Max Reward: 49.45
Iteration: 3297 | Episodes: 201150 | Median Reward: -15.69 | Max Reward: 49.45
Iteration: 3297 | Episodes: 201200 | Median Reward: -17.80 | Max Reward: 49.45
Iteration: 3298 | Episodes: 201250 | Median Reward: -14.87 | Max Reward: 49.45
Iteration: 3299 | Episodes: 201300 | Median Reward: -14.86 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -109       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3300       |
|    time_elapsed         | 60088      |
|    total_timesteps      | 20275200   |
| train/                  |            |
|    approx_kl            | 0.16548514 |
|    clip_fraction        | 0.0262     |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | 0.842      |
|    learning_rate        | 0.0001     |
|    loss                 | 10         |
|    n_updates            | 32990      |
|    policy_gradient_loss | 0.000397   |
|    std                  | 4.94       |
|    value_loss           | 34.6       |
----------------------------------------
Iteration: 3300 | Episodes: 201350 | Median Reward: -8.12 | Max Reward: 49.45
Iteration: 3301 | Episodes: 201400 | Median Reward: -20.84 | Max Reward: 49.45
Iteration: 3302 | Episodes: 201450 | Median Reward: -20.29 | Max Reward: 49.45
Iteration: 3302 | Episodes: 201500 | Median Reward: -12.27 | Max Reward: 49.45
Iteration: 3303 | Episodes: 201550 | Median Reward: -12.46 | Max Reward: 49.45
Iteration: 3304 | Episodes: 201600 | Median Reward: -3.58 | Max Reward: 49.45
Iteration: 3305 | Episodes: 201650 | Median Reward: -6.11 | Max Reward: 49.45
Iteration: 3306 | Episodes: 201700 | Median Reward: -16.02 | Max Reward: 49.45
Iteration: 3307 | Episodes: 201750 | Median Reward: 18.16 | Max Reward: 49.45
Iteration: 3307 | Episodes: 201800 | Median Reward: -11.71 | Max Reward: 49.45
Iteration: 3308 | Episodes: 201850 | Median Reward: -11.61 | Max Reward: 49.45
Iteration: 3309 | Episodes: 201900 | Median Reward: -11.92 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -110      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3310      |
|    time_elapsed         | 60268     |
|    total_timesteps      | 20336640  |
| train/                  |           |
|    approx_kl            | 1.5811408 |
|    clip_fraction        | 0.0927    |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0.854     |
|    learning_rate        | 0.0001    |
|    loss                 | 4.41      |
|    n_updates            | 33090     |
|    policy_gradient_loss | 0.0498    |
|    std                  | 4.95      |
|    value_loss           | 35.4      |
---------------------------------------
Iteration: 3310 | Episodes: 201950 | Median Reward: -9.76 | Max Reward: 49.45
Iteration: 3311 | Episodes: 202000 | Median Reward: -16.66 | Max Reward: 49.45
Iteration: 3311 | Episodes: 202050 | Median Reward: -25.62 | Max Reward: 49.45
Iteration: 3312 | Episodes: 202100 | Median Reward: -20.41 | Max Reward: 49.45
Iteration: 3313 | Episodes: 202150 | Median Reward: -12.68 | Max Reward: 49.45
Iteration: 3314 | Episodes: 202200 | Median Reward: -10.05 | Max Reward: 49.45
Iteration: 3315 | Episodes: 202250 | Median Reward: -15.76 | Max Reward: 49.45
Iteration: 3316 | Episodes: 202300 | Median Reward: -21.19 | Max Reward: 49.45
Iteration: 3316 | Episodes: 202350 | Median Reward: -20.46 | Max Reward: 49.45
Iteration: 3317 | Episodes: 202400 | Median Reward: -6.48 | Max Reward: 49.45
Iteration: 3318 | Episodes: 202450 | Median Reward: -6.48 | Max Reward: 49.45
Iteration: 3319 | Episodes: 202500 | Median Reward: -26.23 | Max Reward: 49.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -113        |
| time/                   |             |
|    fps                  | 337         |
|    iterations           | 3320        |
|    time_elapsed         | 60463       |
|    total_timesteps      | 20398080    |
| train/                  |             |
|    approx_kl            | 0.081745535 |
|    clip_fraction        | 0.0616      |
|    clip_range           | 0.4         |
|    entropy_loss         | -118        |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0001      |
|    loss                 | 7.11        |
|    n_updates            | 33190       |
|    policy_gradient_loss | -0.0101     |
|    std                  | 4.97        |
|    value_loss           | 59.7        |
-----------------------------------------
Iteration: 3320 | Episodes: 202550 | Median Reward: -26.23 | Max Reward: 49.45
Iteration: 3320 | Episodes: 202600 | Median Reward: 0.56 | Max Reward: 49.45
Iteration: 3321 | Episodes: 202650 | Median Reward: -7.40 | Max Reward: 49.45
Iteration: 3322 | Episodes: 202700 | Median Reward: -16.34 | Max Reward: 49.45
Iteration: 3323 | Episodes: 202750 | Median Reward: -19.09 | Max Reward: 49.45
Iteration: 3324 | Episodes: 202800 | Median Reward: -23.24 | Max Reward: 49.45
Iteration: 3325 | Episodes: 202850 | Median Reward: -13.77 | Max Reward: 49.45
Iteration: 3325 | Episodes: 202900 | Median Reward: -21.61 | Max Reward: 49.45
Iteration: 3326 | Episodes: 202950 | Median Reward: -10.98 | Max Reward: 49.45
Iteration: 3327 | Episodes: 203000 | Median Reward: 13.84 | Max Reward: 49.45
Iteration: 3328 | Episodes: 203050 | Median Reward: -0.67 | Max Reward: 49.45
Iteration: 3329 | Episodes: 203100 | Median Reward: -25.78 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -102     |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3330     |
|    time_elapsed         | 60643    |
|    total_timesteps      | 20459520 |
| train/                  |          |
|    approx_kl            | 2.05628  |
|    clip_fraction        | 0.184    |
|    clip_range           | 0.4      |
|    entropy_loss         | -119     |
|    explained_variance   | 0.886    |
|    learning_rate        | 0.0001   |
|    loss                 | 6.14     |
|    n_updates            | 33290    |
|    policy_gradient_loss | 0.045    |
|    std                  | 4.98     |
|    value_loss           | 30.7     |
--------------------------------------
Iteration: 3330 | Episodes: 203150 | Median Reward: 12.97 | Max Reward: 49.45
Iteration: 3330 | Episodes: 203200 | Median Reward: -17.43 | Max Reward: 49.45
Iteration: 3331 | Episodes: 203250 | Median Reward: -11.18 | Max Reward: 49.45
Iteration: 3332 | Episodes: 203300 | Median Reward: -9.03 | Max Reward: 49.45
Iteration: 3333 | Episodes: 203350 | Median Reward: -7.56 | Max Reward: 49.45
Iteration: 3334 | Episodes: 203400 | Median Reward: -11.92 | Max Reward: 49.45
Iteration: 3334 | Episodes: 203450 | Median Reward: -18.33 | Max Reward: 49.45
Iteration: 3335 | Episodes: 203500 | Median Reward: -13.86 | Max Reward: 49.45
Iteration: 3336 | Episodes: 203550 | Median Reward: -10.81 | Max Reward: 49.45
Iteration: 3337 | Episodes: 203600 | Median Reward: -10.81 | Max Reward: 49.45
Iteration: 3338 | Episodes: 203650 | Median Reward: -18.30 | Max Reward: 49.45
Iteration: 3339 | Episodes: 203700 | Median Reward: -19.67 | Max Reward: 49.45
Iteration: 3339 | Episodes: 203750 | Median Reward: -18.27 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -113      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3340      |
|    time_elapsed         | 60826     |
|    total_timesteps      | 20520960  |
| train/                  |           |
|    approx_kl            | 0.4022265 |
|    clip_fraction        | 0.0248    |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0.811     |
|    learning_rate        | 0.0001    |
|    loss                 | 28.5      |
|    n_updates            | 33390     |
|    policy_gradient_loss | 0.00782   |
|    std                  | 4.99      |
|    value_loss           | 48.9      |
---------------------------------------
Iteration: 3340 | Episodes: 203800 | Median Reward: -19.06 | Max Reward: 49.45
Iteration: 3341 | Episodes: 203850 | Median Reward: -14.47 | Max Reward: 49.45
Iteration: 3342 | Episodes: 203900 | Median Reward: -12.05 | Max Reward: 49.45
Iteration: 3343 | Episodes: 203950 | Median Reward: -12.05 | Max Reward: 49.45
Iteration: 3344 | Episodes: 204000 | Median Reward: -6.11 | Max Reward: 49.45
Iteration: 3344 | Episodes: 204050 | Median Reward: -18.55 | Max Reward: 49.45
Iteration: 3345 | Episodes: 204100 | Median Reward: -12.84 | Max Reward: 49.45
Iteration: 3346 | Episodes: 204150 | Median Reward: -13.78 | Max Reward: 49.45
Iteration: 3347 | Episodes: 204200 | Median Reward: -13.22 | Max Reward: 49.45
Iteration: 3348 | Episodes: 204250 | Median Reward: -16.34 | Max Reward: 49.45
Iteration: 3348 | Episodes: 204300 | Median Reward: -10.36 | Max Reward: 49.45
Iteration: 3349 | Episodes: 204350 | Median Reward: 3.34 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -97.8    |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3350     |
|    time_elapsed         | 61015    |
|    total_timesteps      | 20582400 |
| train/                  |          |
|    approx_kl            | 4.385494 |
|    clip_fraction        | 0.0755   |
|    clip_range           | 0.4      |
|    entropy_loss         | -120     |
|    explained_variance   | 0.844    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.81     |
|    n_updates            | 33490    |
|    policy_gradient_loss | 0.0378   |
|    std                  | 5        |
|    value_loss           | 42.4     |
--------------------------------------
Iteration: 3350 | Episodes: 204400 | Median Reward: -9.14 | Max Reward: 49.45
Iteration: 3351 | Episodes: 204450 | Median Reward: -14.94 | Max Reward: 49.45
Iteration: 3352 | Episodes: 204500 | Median Reward: -13.90 | Max Reward: 49.45
Iteration: 3353 | Episodes: 204550 | Median Reward: -3.62 | Max Reward: 49.45
Iteration: 3353 | Episodes: 204600 | Median Reward: -26.58 | Max Reward: 49.45
Iteration: 3354 | Episodes: 204650 | Median Reward: -20.70 | Max Reward: 49.45
Iteration: 3355 | Episodes: 204700 | Median Reward: -18.55 | Max Reward: 49.45
Iteration: 3356 | Episodes: 204750 | Median Reward: -15.75 | Max Reward: 49.45
Iteration: 3357 | Episodes: 204800 | Median Reward: -19.32 | Max Reward: 49.45
Iteration: 3357 | Episodes: 204850 | Median Reward: -3.86 | Max Reward: 49.45
Iteration: 3358 | Episodes: 204900 | Median Reward: -16.49 | Max Reward: 49.45
Iteration: 3359 | Episodes: 204950 | Median Reward: -16.21 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -106       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3360       |
|    time_elapsed         | 61200      |
|    total_timesteps      | 20643840   |
| train/                  |            |
|    approx_kl            | 0.03176394 |
|    clip_fraction        | 0.0172     |
|    clip_range           | 0.4        |
|    entropy_loss         | -121       |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0001     |
|    loss                 | 6.94       |
|    n_updates            | 33590      |
|    policy_gradient_loss | -0.00207   |
|    std                  | 5.01       |
|    value_loss           | 37.7       |
----------------------------------------
Iteration: 3360 | Episodes: 205000 | Median Reward: -12.63 | Max Reward: 49.45
Iteration: 3361 | Episodes: 205050 | Median Reward: -12.63 | Max Reward: 49.45
Iteration: 3362 | Episodes: 205100 | Median Reward: -14.49 | Max Reward: 49.45
Iteration: 3362 | Episodes: 205150 | Median Reward: -21.41 | Max Reward: 49.45
Iteration: 3363 | Episodes: 205200 | Median Reward: -22.20 | Max Reward: 49.45
Iteration: 3364 | Episodes: 205250 | Median Reward: -14.95 | Max Reward: 49.45
Iteration: 3365 | Episodes: 205300 | Median Reward: -14.95 | Max Reward: 49.45
Iteration: 3366 | Episodes: 205350 | Median Reward: -19.10 | Max Reward: 49.45
Iteration: 3366 | Episodes: 205400 | Median Reward: -18.04 | Max Reward: 49.45
Iteration: 3367 | Episodes: 205450 | Median Reward: 4.30 | Max Reward: 49.45
Iteration: 3368 | Episodes: 205500 | Median Reward: -5.65 | Max Reward: 49.45
Iteration: 3369 | Episodes: 205550 | Median Reward: -16.54 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -112       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3370       |
|    time_elapsed         | 61382      |
|    total_timesteps      | 20705280   |
| train/                  |            |
|    approx_kl            | 0.05369433 |
|    clip_fraction        | 0.0352     |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.03       |
|    n_updates            | 33690      |
|    policy_gradient_loss | -0.00443   |
|    std                  | 5.02       |
|    value_loss           | 31.1       |
----------------------------------------
Iteration: 3370 | Episodes: 205600 | Median Reward: -16.54 | Max Reward: 49.45
Iteration: 3371 | Episodes: 205650 | Median Reward: -16.09 | Max Reward: 49.45
Iteration: 3371 | Episodes: 205700 | Median Reward: -11.49 | Max Reward: 49.45
Iteration: 3372 | Episodes: 205750 | Median Reward: -27.16 | Max Reward: 49.45
Iteration: 3373 | Episodes: 205800 | Median Reward: -13.83 | Max Reward: 49.45
Iteration: 3374 | Episodes: 205850 | Median Reward: -13.66 | Max Reward: 49.45
Iteration: 3375 | Episodes: 205900 | Median Reward: -15.95 | Max Reward: 49.45
Iteration: 3376 | Episodes: 205950 | Median Reward: -13.78 | Max Reward: 49.45
Iteration: 3376 | Episodes: 206000 | Median Reward: -4.91 | Max Reward: 49.45
Iteration: 3377 | Episodes: 206050 | Median Reward: 10.64 | Max Reward: 49.45
Iteration: 3378 | Episodes: 206100 | Median Reward: 5.20 | Max Reward: 49.45
Iteration: 3379 | Episodes: 206150 | Median Reward: -6.63 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -103      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3380      |
|    time_elapsed         | 61573     |
|    total_timesteps      | 20766720  |
| train/                  |           |
|    approx_kl            | 1.2550952 |
|    clip_fraction        | 0.0786    |
|    clip_range           | 0.4       |
|    entropy_loss         | -119      |
|    explained_variance   | 0.901     |
|    learning_rate        | 0.0001    |
|    loss                 | 15.1      |
|    n_updates            | 33790     |
|    policy_gradient_loss | -0.00729  |
|    std                  | 5.03      |
|    value_loss           | 30.4      |
---------------------------------------
Iteration: 3380 | Episodes: 206200 | Median Reward: -5.85 | Max Reward: 49.45
Iteration: 3380 | Episodes: 206250 | Median Reward: -16.20 | Max Reward: 49.45
Iteration: 3381 | Episodes: 206300 | Median Reward: -25.20 | Max Reward: 49.45
Iteration: 3382 | Episodes: 206350 | Median Reward: -15.74 | Max Reward: 49.45
Iteration: 3383 | Episodes: 206400 | Median Reward: -6.70 | Max Reward: 49.45
Iteration: 3384 | Episodes: 206450 | Median Reward: 0.82 | Max Reward: 49.45
Iteration: 3385 | Episodes: 206500 | Median Reward: -18.41 | Max Reward: 49.45
Iteration: 3385 | Episodes: 206550 | Median Reward: -16.09 | Max Reward: 49.45
Iteration: 3386 | Episodes: 206600 | Median Reward: -6.88 | Max Reward: 49.45
Iteration: 3387 | Episodes: 206650 | Median Reward: -10.61 | Max Reward: 49.45
Iteration: 3388 | Episodes: 206700 | Median Reward: -18.54 | Max Reward: 49.45
Iteration: 3389 | Episodes: 206750 | Median Reward: -18.63 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -111       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3390       |
|    time_elapsed         | 61750      |
|    total_timesteps      | 20828160   |
| train/                  |            |
|    approx_kl            | 0.11792202 |
|    clip_fraction        | 0.0122     |
|    clip_range           | 0.4        |
|    entropy_loss         | -121       |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.42       |
|    n_updates            | 33890      |
|    policy_gradient_loss | -0.00546   |
|    std                  | 5.05       |
|    value_loss           | 17.4       |
----------------------------------------
Iteration: 3390 | Episodes: 206800 | Median Reward: -25.72 | Max Reward: 49.45
Iteration: 3390 | Episodes: 206850 | Median Reward: -24.58 | Max Reward: 49.45
Iteration: 3391 | Episodes: 206900 | Median Reward: -13.50 | Max Reward: 49.45
Iteration: 3392 | Episodes: 206950 | Median Reward: -18.06 | Max Reward: 49.45
Iteration: 3393 | Episodes: 207000 | Median Reward: -22.22 | Max Reward: 49.45
Iteration: 3394 | Episodes: 207050 | Median Reward: -18.06 | Max Reward: 49.45
Iteration: 3394 | Episodes: 207100 | Median Reward: -3.26 | Max Reward: 49.45
Iteration: 3395 | Episodes: 207150 | Median Reward: -19.79 | Max Reward: 49.45
Iteration: 3396 | Episodes: 207200 | Median Reward: -15.77 | Max Reward: 49.45
Iteration: 3397 | Episodes: 207250 | Median Reward: -16.68 | Max Reward: 49.45
Iteration: 3398 | Episodes: 207300 | Median Reward: -21.96 | Max Reward: 49.45
Iteration: 3399 | Episodes: 207350 | Median Reward: -14.51 | Max Reward: 49.45
Iteration: 3399 | Episodes: 207400 | Median Reward: -19.44 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -110       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3400       |
|    time_elapsed         | 61932      |
|    total_timesteps      | 20889600   |
| train/                  |            |
|    approx_kl            | 0.13698024 |
|    clip_fraction        | 0.0697     |
|    clip_range           | 0.4        |
|    entropy_loss         | -119       |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.0001     |
|    loss                 | 24.8       |
|    n_updates            | 33990      |
|    policy_gradient_loss | -0.00708   |
|    std                  | 5.06       |
|    value_loss           | 31.6       |
----------------------------------------
Iteration: 3400 | Episodes: 207450 | Median Reward: -18.38 | Max Reward: 49.45
Iteration: 3401 | Episodes: 207500 | Median Reward: -19.11 | Max Reward: 49.45
Iteration: 3402 | Episodes: 207550 | Median Reward: -19.46 | Max Reward: 49.45
Iteration: 3403 | Episodes: 207600 | Median Reward: -8.45 | Max Reward: 49.45
Iteration: 3403 | Episodes: 207650 | Median Reward: -9.13 | Max Reward: 49.45
Iteration: 3404 | Episodes: 207700 | Median Reward: -18.99 | Max Reward: 49.45
Iteration: 3405 | Episodes: 207750 | Median Reward: -10.06 | Max Reward: 49.45
Iteration: 3406 | Episodes: 207800 | Median Reward: 8.47 | Max Reward: 49.45
Iteration: 3407 | Episodes: 207850 | Median Reward: 20.67 | Max Reward: 49.45
Iteration: 3408 | Episodes: 207900 | Median Reward: -17.46 | Max Reward: 49.45
Iteration: 3408 | Episodes: 207950 | Median Reward: -16.70 | Max Reward: 49.45
Iteration: 3409 | Episodes: 208000 | Median Reward: -3.95 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -104     |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3410     |
|    time_elapsed         | 62110    |
|    total_timesteps      | 20951040 |
| train/                  |          |
|    approx_kl            | 7.185728 |
|    clip_fraction        | 0.182    |
|    clip_range           | 0.4      |
|    entropy_loss         | -118     |
|    explained_variance   | 0.883    |
|    learning_rate        | 0.0001   |
|    loss                 | 7.62     |
|    n_updates            | 34090    |
|    policy_gradient_loss | 0.00652  |
|    std                  | 5.07     |
|    value_loss           | 24.3     |
--------------------------------------
Iteration: 3410 | Episodes: 208050 | Median Reward: -2.27 | Max Reward: 49.45
Iteration: 3411 | Episodes: 208100 | Median Reward: -15.56 | Max Reward: 49.45
Iteration: 3412 | Episodes: 208150 | Median Reward: -11.94 | Max Reward: 49.45
Iteration: 3413 | Episodes: 208200 | Median Reward: 0.52 | Max Reward: 49.45
Iteration: 3413 | Episodes: 208250 | Median Reward: -10.39 | Max Reward: 49.45
Iteration: 3414 | Episodes: 208300 | Median Reward: 21.38 | Max Reward: 49.45
Iteration: 3415 | Episodes: 208350 | Median Reward: 21.38 | Max Reward: 49.45
Iteration: 3416 | Episodes: 208400 | Median Reward: -12.24 | Max Reward: 49.45
Iteration: 3417 | Episodes: 208450 | Median Reward: -10.15 | Max Reward: 49.45
Iteration: 3417 | Episodes: 208500 | Median Reward: -1.38 | Max Reward: 49.45
Iteration: 3418 | Episodes: 208550 | Median Reward: 18.17 | Max Reward: 49.45
Iteration: 3419 | Episodes: 208600 | Median Reward: 14.80 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -90.5      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3420       |
|    time_elapsed         | 62301      |
|    total_timesteps      | 21012480   |
| train/                  |            |
|    approx_kl            | 0.16177699 |
|    clip_fraction        | 0.0502     |
|    clip_range           | 0.4        |
|    entropy_loss         | -119       |
|    explained_variance   | 0.856      |
|    learning_rate        | 0.0001     |
|    loss                 | 13.5       |
|    n_updates            | 34190      |
|    policy_gradient_loss | 0.00402    |
|    std                  | 5.08       |
|    value_loss           | 59.8       |
----------------------------------------
Iteration: 3420 | Episodes: 208650 | Median Reward: 5.17 | Max Reward: 49.45
Iteration: 3421 | Episodes: 208700 | Median Reward: 9.91 | Max Reward: 49.45
Iteration: 3422 | Episodes: 208750 | Median Reward: -7.83 | Max Reward: 49.45
Iteration: 3422 | Episodes: 208800 | Median Reward: -14.16 | Max Reward: 49.45
Iteration: 3423 | Episodes: 208850 | Median Reward: -11.27 | Max Reward: 49.45
Iteration: 3424 | Episodes: 208900 | Median Reward: -6.36 | Max Reward: 49.45
Iteration: 3425 | Episodes: 208950 | Median Reward: 5.03 | Max Reward: 49.45
Iteration: 3426 | Episodes: 209000 | Median Reward: 1.07 | Max Reward: 49.45
Iteration: 3427 | Episodes: 209050 | Median Reward: -9.69 | Max Reward: 49.45
Iteration: 3427 | Episodes: 209100 | Median Reward: -6.77 | Max Reward: 49.45
Iteration: 3428 | Episodes: 209150 | Median Reward: 9.64 | Max Reward: 49.45
Iteration: 3429 | Episodes: 209200 | Median Reward: -5.40 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -103      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3430      |
|    time_elapsed         | 62480     |
|    total_timesteps      | 21073920  |
| train/                  |           |
|    approx_kl            | 1.1240152 |
|    clip_fraction        | 0.0722    |
|    clip_range           | 0.4       |
|    entropy_loss         | -118      |
|    explained_variance   | 0.856     |
|    learning_rate        | 0.0001    |
|    loss                 | 46.1      |
|    n_updates            | 34290     |
|    policy_gradient_loss | 0.00771   |
|    std                  | 5.09      |
|    value_loss           | 63.7      |
---------------------------------------
Iteration: 3430 | Episodes: 209250 | Median Reward: -6.77 | Max Reward: 49.45
Iteration: 3431 | Episodes: 209300 | Median Reward: 6.39 | Max Reward: 49.45
Iteration: 3431 | Episodes: 209350 | Median Reward: 0.27 | Max Reward: 49.45
Iteration: 3432 | Episodes: 209400 | Median Reward: 2.68 | Max Reward: 49.45
Iteration: 3433 | Episodes: 209450 | Median Reward: -13.95 | Max Reward: 49.45
Iteration: 3434 | Episodes: 209500 | Median Reward: -9.25 | Max Reward: 49.45
Iteration: 3435 | Episodes: 209550 | Median Reward: -6.00 | Max Reward: 49.45
Iteration: 3436 | Episodes: 209600 | Median Reward: -9.19 | Max Reward: 49.45
Iteration: 3436 | Episodes: 209650 | Median Reward: -21.11 | Max Reward: 49.45
Iteration: 3437 | Episodes: 209700 | Median Reward: -2.99 | Max Reward: 49.45
Iteration: 3438 | Episodes: 209750 | Median Reward: -11.48 | Max Reward: 49.45
Iteration: 3439 | Episodes: 209800 | Median Reward: -6.87 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -97.8      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3440       |
|    time_elapsed         | 62663      |
|    total_timesteps      | 21135360   |
| train/                  |            |
|    approx_kl            | 0.34468707 |
|    clip_fraction        | 0.0619     |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | 0.874      |
|    learning_rate        | 0.0001     |
|    loss                 | 19.7       |
|    n_updates            | 34390      |
|    policy_gradient_loss | 0.00731    |
|    std                  | 5.1        |
|    value_loss           | 36.4       |
----------------------------------------
Iteration: 3440 | Episodes: 209850 | Median Reward: -3.10 | Max Reward: 49.45
Iteration: 3440 | Episodes: 209900 | Median Reward: -9.96 | Max Reward: 49.45
Iteration: 3441 | Episodes: 209950 | Median Reward: 21.25 | Max Reward: 49.45
Iteration: 3442 | Episodes: 210000 | Median Reward: 11.78 | Max Reward: 49.45
Iteration: 3443 | Episodes: 210050 | Median Reward: -0.29 | Max Reward: 49.45
Iteration: 3444 | Episodes: 210100 | Median Reward: -6.23 | Max Reward: 49.45
Iteration: 3445 | Episodes: 210150 | Median Reward: -4.51 | Max Reward: 49.45
Iteration: 3445 | Episodes: 210200 | Median Reward: 33.40 | Max Reward: 49.45
Iteration: 3446 | Episodes: 210250 | Median Reward: -3.06 | Max Reward: 49.45
Iteration: 3447 | Episodes: 210300 | Median Reward: -0.50 | Max Reward: 49.45
Iteration: 3448 | Episodes: 210350 | Median Reward: 6.21 | Max Reward: 49.45
Iteration: 3449 | Episodes: 210400 | Median Reward: 2.58 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -103      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3450      |
|    time_elapsed         | 62854     |
|    total_timesteps      | 21196800  |
| train/                  |           |
|    approx_kl            | 11.051592 |
|    clip_fraction        | 0.171     |
|    clip_range           | 0.4       |
|    entropy_loss         | -117      |
|    explained_variance   | 0.883     |
|    learning_rate        | 0.0001    |
|    loss                 | 5.39      |
|    n_updates            | 34490     |
|    policy_gradient_loss | 0.023     |
|    std                  | 5.11      |
|    value_loss           | 47        |
---------------------------------------
Iteration: 3450 | Episodes: 210450 | Median Reward: -2.65 | Max Reward: 49.45
Iteration: 3450 | Episodes: 210500 | Median Reward: -9.73 | Max Reward: 49.45
Iteration: 3451 | Episodes: 210550 | Median Reward: -9.30 | Max Reward: 49.45
Iteration: 3452 | Episodes: 210600 | Median Reward: -16.26 | Max Reward: 49.45
Iteration: 3453 | Episodes: 210650 | Median Reward: -12.67 | Max Reward: 49.45
Iteration: 3454 | Episodes: 210700 | Median Reward: 9.70 | Max Reward: 49.45
Iteration: 3454 | Episodes: 210750 | Median Reward: 40.99 | Max Reward: 49.45
Iteration: 3455 | Episodes: 210800 | Median Reward: 39.97 | Max Reward: 49.45
Iteration: 3456 | Episodes: 210850 | Median Reward: 39.72 | Max Reward: 49.45
Iteration: 3457 | Episodes: 210900 | Median Reward: 40.52 | Max Reward: 49.45
Iteration: 3458 | Episodes: 210950 | Median Reward: 41.07 | Max Reward: 49.45
Iteration: 3459 | Episodes: 211000 | Median Reward: 40.41 | Max Reward: 49.45
Iteration: 3459 | Episodes: 211050 | Median Reward: 41.21 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -67.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3460      |
|    time_elapsed         | 63037     |
|    total_timesteps      | 21258240  |
| train/                  |           |
|    approx_kl            | 2.5790124 |
|    clip_fraction        | 0.0535    |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.751     |
|    learning_rate        | 0.0001    |
|    loss                 | 8.73      |
|    n_updates            | 34590     |
|    policy_gradient_loss | -0.00248  |
|    std                  | 5.12      |
|    value_loss           | 127       |
---------------------------------------
Iteration: 3460 | Episodes: 211100 | Median Reward: 40.43 | Max Reward: 49.45
Iteration: 3461 | Episodes: 211150 | Median Reward: 41.26 | Max Reward: 49.45
Iteration: 3462 | Episodes: 211200 | Median Reward: 42.29 | Max Reward: 49.45
Iteration: 3463 | Episodes: 211250 | Median Reward: 42.32 | Max Reward: 49.45
Iteration: 3464 | Episodes: 211300 | Median Reward: 42.59 | Max Reward: 49.45
Iteration: 3464 | Episodes: 211350 | Median Reward: 43.66 | Max Reward: 49.45
Iteration: 3465 | Episodes: 211400 | Median Reward: 43.55 | Max Reward: 49.45
Iteration: 3466 | Episodes: 211450 | Median Reward: 44.48 | Max Reward: 49.45
Iteration: 3467 | Episodes: 211500 | Median Reward: 44.94 | Max Reward: 49.45
Iteration: 3468 | Episodes: 211550 | Median Reward: 46.25 | Max Reward: 49.45
Iteration: 3468 | Episodes: 211600 | Median Reward: 46.30 | Max Reward: 49.45
Iteration: 3469 | Episodes: 211650 | Median Reward: 42.67 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -56      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3470     |
|    time_elapsed         | 63221    |
|    total_timesteps      | 21319680 |
| train/                  |          |
|    approx_kl            | 23.20641 |
|    clip_fraction        | 0.169    |
|    clip_range           | 0.4      |
|    entropy_loss         | -121     |
|    explained_variance   | 0.739    |
|    learning_rate        | 0.0001   |
|    loss                 | 95.5     |
|    n_updates            | 34690    |
|    policy_gradient_loss | 0.0738   |
|    std                  | 5.14     |
|    value_loss           | 199      |
--------------------------------------
Iteration: 3470 | Episodes: 211700 | Median Reward: 42.90 | Max Reward: 49.45
Iteration: 3471 | Episodes: 211750 | Median Reward: 39.36 | Max Reward: 49.45
Iteration: 3472 | Episodes: 211800 | Median Reward: 35.82 | Max Reward: 49.45
Iteration: 3473 | Episodes: 211850 | Median Reward: 11.28 | Max Reward: 49.45
Iteration: 3473 | Episodes: 211900 | Median Reward: 37.88 | Max Reward: 49.45
Iteration: 3474 | Episodes: 211950 | Median Reward: 35.61 | Max Reward: 49.45
Iteration: 3475 | Episodes: 212000 | Median Reward: 35.38 | Max Reward: 49.45
Iteration: 3476 | Episodes: 212050 | Median Reward: 36.34 | Max Reward: 49.45
Iteration: 3477 | Episodes: 212100 | Median Reward: 37.29 | Max Reward: 49.45
Iteration: 3477 | Episodes: 212150 | Median Reward: 40.46 | Max Reward: 49.45
Iteration: 3478 | Episodes: 212200 | Median Reward: -19.25 | Max Reward: 49.45
Iteration: 3479 | Episodes: 212250 | Median Reward: 18.20 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -88.4      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3480       |
|    time_elapsed         | 63414      |
|    total_timesteps      | 21381120   |
| train/                  |            |
|    approx_kl            | 0.39975542 |
|    clip_fraction        | 0.0407     |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0001     |
|    loss                 | 55.6       |
|    n_updates            | 34790      |
|    policy_gradient_loss | -0.000306  |
|    std                  | 5.16       |
|    value_loss           | 72.8       |
----------------------------------------
Iteration: 3480 | Episodes: 212300 | Median Reward: 35.66 | Max Reward: 49.45
Iteration: 3481 | Episodes: 212350 | Median Reward: 40.60 | Max Reward: 49.45
Iteration: 3482 | Episodes: 212400 | Median Reward: 40.48 | Max Reward: 49.45
Iteration: 3482 | Episodes: 212450 | Median Reward: 40.75 | Max Reward: 49.45
Iteration: 3483 | Episodes: 212500 | Median Reward: 41.26 | Max Reward: 49.45
Iteration: 3484 | Episodes: 212550 | Median Reward: 41.20 | Max Reward: 49.45
Iteration: 3485 | Episodes: 212600 | Median Reward: 40.71 | Max Reward: 49.45
Iteration: 3486 | Episodes: 212650 | Median Reward: 41.02 | Max Reward: 49.45
Iteration: 3487 | Episodes: 212700 | Median Reward: 25.25 | Max Reward: 49.45
Iteration: 3487 | Episodes: 212750 | Median Reward: -6.79 | Max Reward: 49.45
Iteration: 3488 | Episodes: 212800 | Median Reward: -10.83 | Max Reward: 49.45
Iteration: 3489 | Episodes: 212850 | Median Reward: -13.07 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -102       |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3490       |
|    time_elapsed         | 63592      |
|    total_timesteps      | 21442560   |
| train/                  |            |
|    approx_kl            | 0.93782794 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.4        |
|    entropy_loss         | -116       |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.01       |
|    n_updates            | 34890      |
|    policy_gradient_loss | -0.00769   |
|    std                  | 5.17       |
|    value_loss           | 30.3       |
----------------------------------------
Iteration: 3490 | Episodes: 212900 | Median Reward: -22.98 | Max Reward: 49.45
Iteration: 3491 | Episodes: 212950 | Median Reward: -24.17 | Max Reward: 49.45
Iteration: 3491 | Episodes: 213000 | Median Reward: 23.57 | Max Reward: 49.45
Iteration: 3492 | Episodes: 213050 | Median Reward: 11.52 | Max Reward: 49.45
Iteration: 3493 | Episodes: 213100 | Median Reward: 10.87 | Max Reward: 49.45
Iteration: 3494 | Episodes: 213150 | Median Reward: -0.38 | Max Reward: 49.45
Iteration: 3495 | Episodes: 213200 | Median Reward: -0.71 | Max Reward: 49.45
Iteration: 3496 | Episodes: 213250 | Median Reward: -6.35 | Max Reward: 49.45
Iteration: 3496 | Episodes: 213300 | Median Reward: -6.56 | Max Reward: 49.45
Iteration: 3497 | Episodes: 213350 | Median Reward: -4.07 | Max Reward: 49.45
Iteration: 3498 | Episodes: 213400 | Median Reward: 17.22 | Max Reward: 49.45
Iteration: 3499 | Episodes: 213450 | Median Reward: 21.62 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -87.4     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3500      |
|    time_elapsed         | 63764     |
|    total_timesteps      | 21504000  |
| train/                  |           |
|    approx_kl            | 0.9320764 |
|    clip_fraction        | 0.0845    |
|    clip_range           | 0.4       |
|    entropy_loss         | -118      |
|    explained_variance   | 0.861     |
|    learning_rate        | 0.0001    |
|    loss                 | 30.5      |
|    n_updates            | 34990     |
|    policy_gradient_loss | 0.00316   |
|    std                  | 5.18      |
|    value_loss           | 65.2      |
---------------------------------------
Iteration: 3500 | Episodes: 213500 | Median Reward: 11.48 | Max Reward: 49.45
Iteration: 3500 | Episodes: 213550 | Median Reward: -13.39 | Max Reward: 49.45
Iteration: 3501 | Episodes: 213600 | Median Reward: -8.69 | Max Reward: 49.45
Iteration: 3502 | Episodes: 213650 | Median Reward: -16.40 | Max Reward: 49.45
Iteration: 3503 | Episodes: 213700 | Median Reward: -13.77 | Max Reward: 49.45
Iteration: 3504 | Episodes: 213750 | Median Reward: -10.92 | Max Reward: 49.45
Iteration: 3505 | Episodes: 213800 | Median Reward: -3.71 | Max Reward: 49.45
Iteration: 3505 | Episodes: 213850 | Median Reward: -10.84 | Max Reward: 49.45
Iteration: 3506 | Episodes: 213900 | Median Reward: -13.01 | Max Reward: 49.45
Iteration: 3507 | Episodes: 213950 | Median Reward: -4.38 | Max Reward: 49.45
Iteration: 3508 | Episodes: 214000 | Median Reward: -3.98 | Max Reward: 49.45
Iteration: 3509 | Episodes: 214050 | Median Reward: -3.98 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -97.4      |
| time/                   |            |
|    fps                  | 337        |
|    iterations           | 3510       |
|    time_elapsed         | 63939      |
|    total_timesteps      | 21565440   |
| train/                  |            |
|    approx_kl            | 0.17773671 |
|    clip_fraction        | 0.0635     |
|    clip_range           | 0.4        |
|    entropy_loss         | -120       |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.71       |
|    n_updates            | 35090      |
|    policy_gradient_loss | -0.00772   |
|    std                  | 5.19       |
|    value_loss           | 24.7       |
----------------------------------------
Iteration: 3510 | Episodes: 214100 | Median Reward: 1.75 | Max Reward: 49.45
Iteration: 3510 | Episodes: 214150 | Median Reward: 5.59 | Max Reward: 49.45
Iteration: 3511 | Episodes: 214200 | Median Reward: 12.28 | Max Reward: 49.45
Iteration: 3512 | Episodes: 214250 | Median Reward: -5.37 | Max Reward: 49.45
Iteration: 3513 | Episodes: 214300 | Median Reward: -6.22 | Max Reward: 49.45
Iteration: 3514 | Episodes: 214350 | Median Reward: -7.78 | Max Reward: 49.45
Iteration: 3514 | Episodes: 214400 | Median Reward: -12.90 | Max Reward: 49.45
Iteration: 3515 | Episodes: 214450 | Median Reward: 1.79 | Max Reward: 49.45
Iteration: 3516 | Episodes: 214500 | Median Reward: -9.90 | Max Reward: 49.45
Iteration: 3517 | Episodes: 214550 | Median Reward: -12.87 | Max Reward: 49.45
Iteration: 3518 | Episodes: 214600 | Median Reward: -15.70 | Max Reward: 49.45
Iteration: 3519 | Episodes: 214650 | Median Reward: -12.64 | Max Reward: 49.45
Iteration: 3519 | Episodes: 214700 | Median Reward: 10.06 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -100      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3520      |
|    time_elapsed         | 64127     |
|    total_timesteps      | 21626880  |
| train/                  |           |
|    approx_kl            | 0.2743937 |
|    clip_fraction        | 0.0193    |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.904     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.94      |
|    n_updates            | 35190     |
|    policy_gradient_loss | -0.00954  |
|    std                  | 5.19      |
|    value_loss           | 22.8      |
---------------------------------------
Iteration: 3520 | Episodes: 214750 | Median Reward: 8.43 | Max Reward: 49.45
Iteration: 3521 | Episodes: 214800 | Median Reward: 14.47 | Max Reward: 49.45
Iteration: 3522 | Episodes: 214850 | Median Reward: -6.23 | Max Reward: 49.45
Iteration: 3523 | Episodes: 214900 | Median Reward: -14.11 | Max Reward: 49.45
Iteration: 3524 | Episodes: 214950 | Median Reward: 10.23 | Max Reward: 49.45
Iteration: 3524 | Episodes: 215000 | Median Reward: -10.07 | Max Reward: 49.45
Iteration: 3525 | Episodes: 215050 | Median Reward: -9.52 | Max Reward: 49.45
Iteration: 3526 | Episodes: 215100 | Median Reward: -10.30 | Max Reward: 49.45
Iteration: 3527 | Episodes: 215150 | Median Reward: -10.54 | Max Reward: 49.45
Iteration: 3528 | Episodes: 215200 | Median Reward: -11.65 | Max Reward: 49.45
Iteration: 3528 | Episodes: 215250 | Median Reward: -17.63 | Max Reward: 49.45
Iteration: 3529 | Episodes: 215300 | Median Reward: -10.62 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -101      |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3530      |
|    time_elapsed         | 64300     |
|    total_timesteps      | 21688320  |
| train/                  |           |
|    approx_kl            | 0.4728953 |
|    clip_fraction        | 0.0448    |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.863     |
|    learning_rate        | 0.0001    |
|    loss                 | 4.04      |
|    n_updates            | 35290     |
|    policy_gradient_loss | -0.00878  |
|    std                  | 5.2       |
|    value_loss           | 40.1      |
---------------------------------------
Iteration: 3530 | Episodes: 215350 | Median Reward: -14.64 | Max Reward: 49.45
Iteration: 3531 | Episodes: 215400 | Median Reward: -15.38 | Max Reward: 49.45
Iteration: 3532 | Episodes: 215450 | Median Reward: -8.58 | Max Reward: 49.45
Iteration: 3533 | Episodes: 215500 | Median Reward: -5.60 | Max Reward: 49.45
Iteration: 3533 | Episodes: 215550 | Median Reward: -12.63 | Max Reward: 49.45
Iteration: 3534 | Episodes: 215600 | Median Reward: 2.72 | Max Reward: 49.45
Iteration: 3535 | Episodes: 215650 | Median Reward: -0.47 | Max Reward: 49.45
Iteration: 3536 | Episodes: 215700 | Median Reward: -8.25 | Max Reward: 49.45
Iteration: 3537 | Episodes: 215750 | Median Reward: -7.47 | Max Reward: 49.45
Iteration: 3537 | Episodes: 215800 | Median Reward: 28.84 | Max Reward: 49.45
Iteration: 3538 | Episodes: 215850 | Median Reward: 7.41 | Max Reward: 49.45
Iteration: 3539 | Episodes: 215900 | Median Reward: 7.41 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -93.5     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3540      |
|    time_elapsed         | 64472     |
|    total_timesteps      | 21749760  |
| train/                  |           |
|    approx_kl            | 0.9321196 |
|    clip_fraction        | 0.184     |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0.869     |
|    learning_rate        | 0.0001    |
|    loss                 | 12.3      |
|    n_updates            | 35390     |
|    policy_gradient_loss | 0.0279    |
|    std                  | 5.21      |
|    value_loss           | 49.5      |
---------------------------------------
Iteration: 3540 | Episodes: 215950 | Median Reward: 9.38 | Max Reward: 49.45
Iteration: 3541 | Episodes: 216000 | Median Reward: 25.64 | Max Reward: 49.45
Iteration: 3542 | Episodes: 216050 | Median Reward: 24.17 | Max Reward: 49.45
Iteration: 3542 | Episodes: 216100 | Median Reward: -7.88 | Max Reward: 49.45
Iteration: 3543 | Episodes: 216150 | Median Reward: 16.45 | Max Reward: 49.45
Iteration: 3544 | Episodes: 216200 | Median Reward: 0.26 | Max Reward: 49.45
Iteration: 3545 | Episodes: 216250 | Median Reward: 18.72 | Max Reward: 49.45
Iteration: 3546 | Episodes: 216300 | Median Reward: 21.46 | Max Reward: 49.45
Iteration: 3547 | Episodes: 216350 | Median Reward: -19.20 | Max Reward: 49.45
Iteration: 3547 | Episodes: 216400 | Median Reward: -1.69 | Max Reward: 49.45
Iteration: 3548 | Episodes: 216450 | Median Reward: 8.01 | Max Reward: 49.45
Iteration: 3549 | Episodes: 216500 | Median Reward: 3.08 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -93      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3550     |
|    time_elapsed         | 64654    |
|    total_timesteps      | 21811200 |
| train/                  |          |
|    approx_kl            | 7.924361 |
|    clip_fraction        | 0.133    |
|    clip_range           | 0.4      |
|    entropy_loss         | -120     |
|    explained_variance   | 0.835    |
|    learning_rate        | 0.0001   |
|    loss                 | 29       |
|    n_updates            | 35490    |
|    policy_gradient_loss | 0.0664   |
|    std                  | 5.22     |
|    value_loss           | 69.9     |
--------------------------------------
Iteration: 3550 | Episodes: 216550 | Median Reward: 0.79 | Max Reward: 49.45
Iteration: 3551 | Episodes: 216600 | Median Reward: 3.67 | Max Reward: 49.45
Iteration: 3551 | Episodes: 216650 | Median Reward: 0.58 | Max Reward: 49.45
Iteration: 3552 | Episodes: 216700 | Median Reward: 23.27 | Max Reward: 49.45
Iteration: 3553 | Episodes: 216750 | Median Reward: 16.35 | Max Reward: 49.45
Iteration: 3554 | Episodes: 216800 | Median Reward: 7.86 | Max Reward: 49.45
Iteration: 3555 | Episodes: 216850 | Median Reward: 7.91 | Max Reward: 49.45
Iteration: 3556 | Episodes: 216900 | Median Reward: 11.32 | Max Reward: 49.45
Iteration: 3556 | Episodes: 216950 | Median Reward: -3.18 | Max Reward: 49.45
Iteration: 3557 | Episodes: 217000 | Median Reward: 21.23 | Max Reward: 49.45
Iteration: 3558 | Episodes: 217050 | Median Reward: 21.86 | Max Reward: 49.45
Iteration: 3559 | Episodes: 217100 | Median Reward: 21.86 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -85.6     |
| time/                   |           |
|    fps                  | 337       |
|    iterations           | 3560      |
|    time_elapsed         | 64825     |
|    total_timesteps      | 21872640  |
| train/                  |           |
|    approx_kl            | 1.4752964 |
|    clip_fraction        | 0.0981    |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0.86      |
|    learning_rate        | 0.0001    |
|    loss                 | 23.7      |
|    n_updates            | 35590     |
|    policy_gradient_loss | -0.00161  |
|    std                  | 5.23      |
|    value_loss           | 58.5      |
---------------------------------------
Iteration: 3560 | Episodes: 217150 | Median Reward: 18.21 | Max Reward: 49.45
Iteration: 3560 | Episodes: 217200 | Median Reward: 15.96 | Max Reward: 49.45
Iteration: 3561 | Episodes: 217250 | Median Reward: 14.96 | Max Reward: 49.45
Iteration: 3562 | Episodes: 217300 | Median Reward: 5.96 | Max Reward: 49.45
Iteration: 3563 | Episodes: 217350 | Median Reward: -13.31 | Max Reward: 49.45
Iteration: 3564 | Episodes: 217400 | Median Reward: -10.69 | Max Reward: 49.45
Iteration: 3565 | Episodes: 217450 | Median Reward: -3.45 | Max Reward: 49.45
Iteration: 3565 | Episodes: 217500 | Median Reward: 22.23 | Max Reward: 49.45
Iteration: 3566 | Episodes: 217550 | Median Reward: 18.92 | Max Reward: 49.45
Iteration: 3567 | Episodes: 217600 | Median Reward: 6.97 | Max Reward: 49.45
Iteration: 3568 | Episodes: 217650 | Median Reward: -7.03 | Max Reward: 49.45
Iteration: 3569 | Episodes: 217700 | Median Reward: -7.03 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -98      |
| time/                   |          |
|    fps                  | 337      |
|    iterations           | 3570     |
|    time_elapsed         | 64953    |
|    total_timesteps      | 21934080 |
| train/                  |          |
|    approx_kl            | 4.370107 |
|    clip_fraction        | 0.0742   |
|    clip_range           | 0.4      |
|    entropy_loss         | -121     |
|    explained_variance   | 0.921    |
|    learning_rate        | 0.0001   |
|    loss                 | -0.0229  |
|    n_updates            | 35690    |
|    policy_gradient_loss | 0.0313   |
|    std                  | 5.23     |
|    value_loss           | 21.2     |
--------------------------------------
Iteration: 3570 | Episodes: 217750 | Median Reward: 17.45 | Max Reward: 49.45
Iteration: 3570 | Episodes: 217800 | Median Reward: 29.82 | Max Reward: 49.45
Iteration: 3571 | Episodes: 217850 | Median Reward: 9.92 | Max Reward: 49.45
Iteration: 3572 | Episodes: 217900 | Median Reward: 17.02 | Max Reward: 49.45
Iteration: 3573 | Episodes: 217950 | Median Reward: 4.99 | Max Reward: 49.45
Iteration: 3574 | Episodes: 218000 | Median Reward: -11.72 | Max Reward: 49.45
Iteration: 3574 | Episodes: 218050 | Median Reward: -1.67 | Max Reward: 49.45
Iteration: 3575 | Episodes: 218100 | Median Reward: 14.50 | Max Reward: 49.45
Iteration: 3576 | Episodes: 218150 | Median Reward: -0.63 | Max Reward: 49.45
Iteration: 3577 | Episodes: 218200 | Median Reward: -0.63 | Max Reward: 49.45
Iteration: 3578 | Episodes: 218250 | Median Reward: 28.27 | Max Reward: 49.45
Iteration: 3579 | Episodes: 218300 | Median Reward: 27.29 | Max Reward: 49.45
Iteration: 3579 | Episodes: 218350 | Median Reward: 37.40 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -73.2     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3580      |
|    time_elapsed         | 65009     |
|    total_timesteps      | 21995520  |
| train/                  |           |
|    approx_kl            | 3.5967863 |
|    clip_fraction        | 0.139     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0001    |
|    loss                 | 34        |
|    n_updates            | 35790     |
|    policy_gradient_loss | 0.0209    |
|    std                  | 5.25      |
|    value_loss           | 87.5      |
---------------------------------------
Iteration: 3580 | Episodes: 218400 | Median Reward: 36.51 | Max Reward: 49.45
Iteration: 3581 | Episodes: 218450 | Median Reward: 36.51 | Max Reward: 49.45
Iteration: 3582 | Episodes: 218500 | Median Reward: 39.04 | Max Reward: 49.45
Iteration: 3583 | Episodes: 218550 | Median Reward: 38.81 | Max Reward: 49.45
Iteration: 3583 | Episodes: 218600 | Median Reward: 36.55 | Max Reward: 49.45
Iteration: 3584 | Episodes: 218650 | Median Reward: 36.85 | Max Reward: 49.45
Iteration: 3585 | Episodes: 218700 | Median Reward: 41.29 | Max Reward: 49.45
Iteration: 3586 | Episodes: 218750 | Median Reward: 43.03 | Max Reward: 49.45
Iteration: 3587 | Episodes: 218800 | Median Reward: 44.32 | Max Reward: 49.45
Iteration: 3588 | Episodes: 218850 | Median Reward: 44.51 | Max Reward: 49.45
Iteration: 3588 | Episodes: 218900 | Median Reward: 45.00 | Max Reward: 49.45
Iteration: 3589 | Episodes: 218950 | Median Reward: 44.80 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -55.8     |
| time/                   |           |
|    fps                  | 338       |
|    iterations           | 3590      |
|    time_elapsed         | 65065     |
|    total_timesteps      | 22056960  |
| train/                  |           |
|    approx_kl            | 3.3008842 |
|    clip_fraction        | 0.0784    |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.777     |
|    learning_rate        | 0.0001    |
|    loss                 | 75.2      |
|    n_updates            | 35890     |
|    policy_gradient_loss | 0.0138    |
|    std                  | 5.26      |
|    value_loss           | 164       |
---------------------------------------
Iteration: 3590 | Episodes: 219000 | Median Reward: 42.21 | Max Reward: 49.45
Iteration: 3591 | Episodes: 219050 | Median Reward: 42.10 | Max Reward: 49.45
Iteration: 3592 | Episodes: 219100 | Median Reward: 41.10 | Max Reward: 49.45
Iteration: 3593 | Episodes: 219150 | Median Reward: 42.71 | Max Reward: 49.45
Iteration: 3593 | Episodes: 219200 | Median Reward: 37.52 | Max Reward: 49.45
Iteration: 3594 | Episodes: 219250 | Median Reward: 42.59 | Max Reward: 49.45
Iteration: 3595 | Episodes: 219300 | Median Reward: 41.77 | Max Reward: 49.45
Iteration: 3596 | Episodes: 219350 | Median Reward: 31.46 | Max Reward: 49.45
Iteration: 3597 | Episodes: 219400 | Median Reward: 30.27 | Max Reward: 49.45
Iteration: 3597 | Episodes: 219450 | Median Reward: 28.64 | Max Reward: 49.45
Iteration: 3598 | Episodes: 219500 | Median Reward: 32.08 | Max Reward: 49.45
Iteration: 3599 | Episodes: 219550 | Median Reward: 34.09 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -75.9      |
| time/                   |            |
|    fps                  | 339        |
|    iterations           | 3600       |
|    time_elapsed         | 65121      |
|    total_timesteps      | 22118400   |
| train/                  |            |
|    approx_kl            | 14.3788595 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.4        |
|    entropy_loss         | -122       |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.0001     |
|    loss                 | 16.1       |
|    n_updates            | 35990      |
|    policy_gradient_loss | 0.0401     |
|    std                  | 5.28       |
|    value_loss           | 76.8       |
----------------------------------------
Iteration: 3600 | Episodes: 219600 | Median Reward: 36.80 | Max Reward: 49.45
Iteration: 3601 | Episodes: 219650 | Median Reward: 38.54 | Max Reward: 49.45
Iteration: 3602 | Episodes: 219700 | Median Reward: 38.69 | Max Reward: 49.45
Iteration: 3602 | Episodes: 219750 | Median Reward: 39.17 | Max Reward: 49.45
Iteration: 3603 | Episodes: 219800 | Median Reward: 38.19 | Max Reward: 49.45
Iteration: 3604 | Episodes: 219850 | Median Reward: 38.58 | Max Reward: 49.45
Iteration: 3605 | Episodes: 219900 | Median Reward: 38.58 | Max Reward: 49.45
Iteration: 3606 | Episodes: 219950 | Median Reward: 41.27 | Max Reward: 49.45
Iteration: 3607 | Episodes: 220000 | Median Reward: 34.71 | Max Reward: 49.45
Iteration: 3607 | Episodes: 220050 | Median Reward: 38.68 | Max Reward: 49.45
Iteration: 3608 | Episodes: 220100 | Median Reward: 38.72 | Max Reward: 49.45
Iteration: 3609 | Episodes: 220150 | Median Reward: 38.72 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -62.3     |
| time/                   |           |
|    fps                  | 340       |
|    iterations           | 3610      |
|    time_elapsed         | 65177     |
|    total_timesteps      | 22179840  |
| train/                  |           |
|    approx_kl            | 10.734745 |
|    clip_fraction        | 0.0814    |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.823     |
|    learning_rate        | 0.0001    |
|    loss                 | 47.8      |
|    n_updates            | 36090     |
|    policy_gradient_loss | 0.0105    |
|    std                  | 5.31      |
|    value_loss           | 108       |
---------------------------------------
Iteration: 3610 | Episodes: 220200 | Median Reward: 37.28 | Max Reward: 49.45
Iteration: 3611 | Episodes: 220250 | Median Reward: 37.29 | Max Reward: 49.45
Iteration: 3611 | Episodes: 220300 | Median Reward: 38.81 | Max Reward: 49.45
Iteration: 3612 | Episodes: 220350 | Median Reward: 34.00 | Max Reward: 49.45
Iteration: 3613 | Episodes: 220400 | Median Reward: 34.01 | Max Reward: 49.45
Iteration: 3614 | Episodes: 220450 | Median Reward: 34.01 | Max Reward: 49.45
Iteration: 3615 | Episodes: 220500 | Median Reward: 29.40 | Max Reward: 49.45
Iteration: 3616 | Episodes: 220550 | Median Reward: 28.20 | Max Reward: 49.45
Iteration: 3616 | Episodes: 220600 | Median Reward: 32.99 | Max Reward: 49.45
Iteration: 3617 | Episodes: 220650 | Median Reward: 36.80 | Max Reward: 49.45
Iteration: 3618 | Episodes: 220700 | Median Reward: 36.42 | Max Reward: 49.45
Iteration: 3619 | Episodes: 220750 | Median Reward: 33.76 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -84.7     |
| time/                   |           |
|    fps                  | 340       |
|    iterations           | 3620      |
|    time_elapsed         | 65232     |
|    total_timesteps      | 22241280  |
| train/                  |           |
|    approx_kl            | 18.026716 |
|    clip_fraction        | 0.177     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.856     |
|    learning_rate        | 0.0001    |
|    loss                 | 42.9      |
|    n_updates            | 36190     |
|    policy_gradient_loss | 0.0557    |
|    std                  | 5.32      |
|    value_loss           | 73.9      |
---------------------------------------
Iteration: 3620 | Episodes: 220800 | Median Reward: 8.00 | Max Reward: 49.45
Iteration: 3620 | Episodes: 220850 | Median Reward: 32.77 | Max Reward: 49.45
Iteration: 3621 | Episodes: 220900 | Median Reward: 30.94 | Max Reward: 49.45
Iteration: 3622 | Episodes: 220950 | Median Reward: 34.61 | Max Reward: 49.45
Iteration: 3623 | Episodes: 221000 | Median Reward: 36.47 | Max Reward: 49.45
Iteration: 3624 | Episodes: 221050 | Median Reward: 37.28 | Max Reward: 49.45
Iteration: 3625 | Episodes: 221100 | Median Reward: 37.83 | Max Reward: 49.45
Iteration: 3625 | Episodes: 221150 | Median Reward: 32.47 | Max Reward: 49.45
Iteration: 3626 | Episodes: 221200 | Median Reward: 34.99 | Max Reward: 49.45
Iteration: 3627 | Episodes: 221250 | Median Reward: 32.74 | Max Reward: 49.45
Iteration: 3628 | Episodes: 221300 | Median Reward: 22.55 | Max Reward: 49.45
Iteration: 3629 | Episodes: 221350 | Median Reward: 2.92 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -92.4    |
| time/                   |          |
|    fps                  | 341      |
|    iterations           | 3630     |
|    time_elapsed         | 65288    |
|    total_timesteps      | 22302720 |
| train/                  |          |
|    approx_kl            | 4.734903 |
|    clip_fraction        | 0.0882   |
|    clip_range           | 0.4      |
|    entropy_loss         | -120     |
|    explained_variance   | 0.888    |
|    learning_rate        | 0.0001   |
|    loss                 | 6.21     |
|    n_updates            | 36290    |
|    policy_gradient_loss | 0.0157   |
|    std                  | 5.33     |
|    value_loss           | 37.4     |
--------------------------------------
Iteration: 3630 | Episodes: 221400 | Median Reward: 12.59 | Max Reward: 49.45
Iteration: 3630 | Episodes: 221450 | Median Reward: -13.00 | Max Reward: 49.45
Iteration: 3631 | Episodes: 221500 | Median Reward: 23.60 | Max Reward: 49.45
Iteration: 3632 | Episodes: 221550 | Median Reward: 0.88 | Max Reward: 49.45
Iteration: 3633 | Episodes: 221600 | Median Reward: -6.31 | Max Reward: 49.45
Iteration: 3634 | Episodes: 221650 | Median Reward: 27.44 | Max Reward: 49.45
Iteration: 3634 | Episodes: 221700 | Median Reward: -2.38 | Max Reward: 49.45
Iteration: 3635 | Episodes: 221750 | Median Reward: 38.59 | Max Reward: 49.45
Iteration: 3636 | Episodes: 221800 | Median Reward: 32.62 | Max Reward: 49.45
Iteration: 3637 | Episodes: 221850 | Median Reward: 24.14 | Max Reward: 49.45
Iteration: 3638 | Episodes: 221900 | Median Reward: 37.08 | Max Reward: 49.45
Iteration: 3639 | Episodes: 221950 | Median Reward: -9.64 | Max Reward: 49.45
Iteration: 3639 | Episodes: 222000 | Median Reward: 34.10 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -83.5     |
| time/                   |           |
|    fps                  | 342       |
|    iterations           | 3640      |
|    time_elapsed         | 65344     |
|    total_timesteps      | 22364160  |
| train/                  |           |
|    approx_kl            | 2.1522174 |
|    clip_fraction        | 0.0457    |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.857     |
|    learning_rate        | 0.0001    |
|    loss                 | 23.7      |
|    n_updates            | 36390     |
|    policy_gradient_loss | 0.0117    |
|    std                  | 5.34      |
|    value_loss           | 55.9      |
---------------------------------------
Iteration: 3640 | Episodes: 222050 | Median Reward: 22.55 | Max Reward: 49.45
Iteration: 3641 | Episodes: 222100 | Median Reward: 9.63 | Max Reward: 49.45
Iteration: 3642 | Episodes: 222150 | Median Reward: 21.28 | Max Reward: 49.45
Iteration: 3643 | Episodes: 222200 | Median Reward: 36.24 | Max Reward: 49.45
Iteration: 3644 | Episodes: 222250 | Median Reward: 36.06 | Max Reward: 49.45
Iteration: 3644 | Episodes: 222300 | Median Reward: -3.87 | Max Reward: 49.45
Iteration: 3645 | Episodes: 222350 | Median Reward: 37.20 | Max Reward: 49.45
Iteration: 3646 | Episodes: 222400 | Median Reward: 37.97 | Max Reward: 49.45
Iteration: 3647 | Episodes: 222450 | Median Reward: 34.75 | Max Reward: 49.45
Iteration: 3648 | Episodes: 222500 | Median Reward: 35.46 | Max Reward: 49.45
Iteration: 3648 | Episodes: 222550 | Median Reward: 37.35 | Max Reward: 49.45
Iteration: 3649 | Episodes: 222600 | Median Reward: 29.18 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -87.8     |
| time/                   |           |
|    fps                  | 342       |
|    iterations           | 3650      |
|    time_elapsed         | 65400     |
|    total_timesteps      | 22425600  |
| train/                  |           |
|    approx_kl            | 12.320829 |
|    clip_fraction        | 0.0629    |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.838     |
|    learning_rate        | 0.0001    |
|    loss                 | 21.1      |
|    n_updates            | 36490     |
|    policy_gradient_loss | 0.00959   |
|    std                  | 5.35      |
|    value_loss           | 72.4      |
---------------------------------------
Iteration: 3650 | Episodes: 222650 | Median Reward: 36.37 | Max Reward: 49.45
Iteration: 3651 | Episodes: 222700 | Median Reward: 37.81 | Max Reward: 49.45
Iteration: 3652 | Episodes: 222750 | Median Reward: 37.81 | Max Reward: 49.45
Iteration: 3653 | Episodes: 222800 | Median Reward: 27.25 | Max Reward: 49.45
Iteration: 3653 | Episodes: 222850 | Median Reward: 36.17 | Max Reward: 49.45
Iteration: 3654 | Episodes: 222900 | Median Reward: 12.64 | Max Reward: 49.45
Iteration: 3655 | Episodes: 222950 | Median Reward: 4.11 | Max Reward: 49.45
Iteration: 3656 | Episodes: 223000 | Median Reward: 9.04 | Max Reward: 49.45
Iteration: 3657 | Episodes: 223050 | Median Reward: -4.68 | Max Reward: 49.45
Iteration: 3657 | Episodes: 223100 | Median Reward: 2.77 | Max Reward: 49.45
Iteration: 3658 | Episodes: 223150 | Median Reward: 36.25 | Max Reward: 49.45
Iteration: 3659 | Episodes: 223200 | Median Reward: 35.92 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.3     |
| time/                   |           |
|    fps                  | 343       |
|    iterations           | 3660      |
|    time_elapsed         | 65455     |
|    total_timesteps      | 22487040  |
| train/                  |           |
|    approx_kl            | 5.7337484 |
|    clip_fraction        | 0.0942    |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.854     |
|    learning_rate        | 0.0001    |
|    loss                 | 31.2      |
|    n_updates            | 36590     |
|    policy_gradient_loss | 0.0185    |
|    std                  | 5.37      |
|    value_loss           | 68.8      |
---------------------------------------
Iteration: 3660 | Episodes: 223250 | Median Reward: 25.18 | Max Reward: 49.45
Iteration: 3661 | Episodes: 223300 | Median Reward: 24.47 | Max Reward: 49.45
Iteration: 3662 | Episodes: 223350 | Median Reward: 25.50 | Max Reward: 49.45
Iteration: 3662 | Episodes: 223400 | Median Reward: 33.53 | Max Reward: 49.45
Iteration: 3663 | Episodes: 223450 | Median Reward: -4.38 | Max Reward: 49.45
Iteration: 3664 | Episodes: 223500 | Median Reward: -8.84 | Max Reward: 49.45
Iteration: 3665 | Episodes: 223550 | Median Reward: 23.54 | Max Reward: 49.45
Iteration: 3666 | Episodes: 223600 | Median Reward: 19.24 | Max Reward: 49.45
Iteration: 3667 | Episodes: 223650 | Median Reward: 7.68 | Max Reward: 49.45
Iteration: 3667 | Episodes: 223700 | Median Reward: 30.15 | Max Reward: 49.45
Iteration: 3668 | Episodes: 223750 | Median Reward: 32.86 | Max Reward: 49.45
Iteration: 3669 | Episodes: 223800 | Median Reward: 29.61 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78.2     |
| time/                   |           |
|    fps                  | 344       |
|    iterations           | 3670      |
|    time_elapsed         | 65511     |
|    total_timesteps      | 22548480  |
| train/                  |           |
|    approx_kl            | 13.923288 |
|    clip_fraction        | 0.069     |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.85      |
|    learning_rate        | 0.0001    |
|    loss                 | 38.5      |
|    n_updates            | 36690     |
|    policy_gradient_loss | 0.0153    |
|    std                  | 5.37      |
|    value_loss           | 74.6      |
---------------------------------------
Iteration: 3670 | Episodes: 223850 | Median Reward: 27.00 | Max Reward: 49.45
Iteration: 3671 | Episodes: 223900 | Median Reward: 18.30 | Max Reward: 49.45
Iteration: 3671 | Episodes: 223950 | Median Reward: 17.30 | Max Reward: 49.45
Iteration: 3672 | Episodes: 224000 | Median Reward: 28.42 | Max Reward: 49.45
Iteration: 3673 | Episodes: 224050 | Median Reward: 3.32 | Max Reward: 49.45
Iteration: 3674 | Episodes: 224100 | Median Reward: 14.14 | Max Reward: 49.45
Iteration: 3675 | Episodes: 224150 | Median Reward: 22.10 | Max Reward: 49.45
Iteration: 3676 | Episodes: 224200 | Median Reward: 17.43 | Max Reward: 49.45
Iteration: 3676 | Episodes: 224250 | Median Reward: 29.87 | Max Reward: 49.45
Iteration: 3677 | Episodes: 224300 | Median Reward: 32.34 | Max Reward: 49.45
Iteration: 3678 | Episodes: 224350 | Median Reward: 22.56 | Max Reward: 49.45
Iteration: 3679 | Episodes: 224400 | Median Reward: 22.56 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -79.1     |
| time/                   |           |
|    fps                  | 344       |
|    iterations           | 3680      |
|    time_elapsed         | 65566     |
|    total_timesteps      | 22609920  |
| train/                  |           |
|    approx_kl            | 5.2891827 |
|    clip_fraction        | 0.0909    |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.878     |
|    learning_rate        | 0.0001    |
|    loss                 | 25.7      |
|    n_updates            | 36790     |
|    policy_gradient_loss | 0.0305    |
|    std                  | 5.38      |
|    value_loss           | 50.9      |
---------------------------------------
Iteration: 3680 | Episodes: 224450 | Median Reward: 28.76 | Max Reward: 49.45
Iteration: 3680 | Episodes: 224500 | Median Reward: 27.22 | Max Reward: 49.45
Iteration: 3681 | Episodes: 224550 | Median Reward: -13.18 | Max Reward: 49.45
Iteration: 3682 | Episodes: 224600 | Median Reward: 25.16 | Max Reward: 49.45
Iteration: 3683 | Episodes: 224650 | Median Reward: 26.08 | Max Reward: 49.45
Iteration: 3684 | Episodes: 224700 | Median Reward: 25.25 | Max Reward: 49.45
Iteration: 3685 | Episodes: 224750 | Median Reward: 15.04 | Max Reward: 49.45
Iteration: 3685 | Episodes: 224800 | Median Reward: 12.06 | Max Reward: 49.45
Iteration: 3686 | Episodes: 224850 | Median Reward: 7.35 | Max Reward: 49.45
Iteration: 3687 | Episodes: 224900 | Median Reward: 0.12 | Max Reward: 49.45
Iteration: 3688 | Episodes: 224950 | Median Reward: 5.31 | Max Reward: 49.45
Iteration: 3689 | Episodes: 225000 | Median Reward: 24.12 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -88.8     |
| time/                   |           |
|    fps                  | 345       |
|    iterations           | 3690      |
|    time_elapsed         | 65622     |
|    total_timesteps      | 22671360  |
| train/                  |           |
|    approx_kl            | 5.3995686 |
|    clip_fraction        | 0.119     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.872     |
|    learning_rate        | 0.0001    |
|    loss                 | 34.3      |
|    n_updates            | 36890     |
|    policy_gradient_loss | 0.00677   |
|    std                  | 5.39      |
|    value_loss           | 59.6      |
---------------------------------------
Iteration: 3690 | Episodes: 225050 | Median Reward: 20.97 | Max Reward: 49.45
Iteration: 3690 | Episodes: 225100 | Median Reward: 3.02 | Max Reward: 49.45
Iteration: 3691 | Episodes: 225150 | Median Reward: 23.94 | Max Reward: 49.45
Iteration: 3692 | Episodes: 225200 | Median Reward: 34.52 | Max Reward: 49.45
Iteration: 3693 | Episodes: 225250 | Median Reward: 36.26 | Max Reward: 49.45
Iteration: 3694 | Episodes: 225300 | Median Reward: 28.27 | Max Reward: 49.45
Iteration: 3694 | Episodes: 225350 | Median Reward: 31.26 | Max Reward: 49.45
Iteration: 3695 | Episodes: 225400 | Median Reward: 32.96 | Max Reward: 49.45
Iteration: 3696 | Episodes: 225450 | Median Reward: 39.92 | Max Reward: 49.45
Iteration: 3697 | Episodes: 225500 | Median Reward: 39.92 | Max Reward: 49.45
Iteration: 3698 | Episodes: 225550 | Median Reward: 37.92 | Max Reward: 49.45
Iteration: 3699 | Episodes: 225600 | Median Reward: 16.64 | Max Reward: 49.45
Iteration: 3699 | Episodes: 225650 | Median Reward: 35.14 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -79.6     |
| time/                   |           |
|    fps                  | 346       |
|    iterations           | 3700      |
|    time_elapsed         | 65678     |
|    total_timesteps      | 22732800  |
| train/                  |           |
|    approx_kl            | 2.9645834 |
|    clip_fraction        | 0.048     |
|    clip_range           | 0.4       |
|    entropy_loss         | -123      |
|    explained_variance   | 0.877     |
|    learning_rate        | 0.0001    |
|    loss                 | 25.1      |
|    n_updates            | 36990     |
|    policy_gradient_loss | 0.0104    |
|    std                  | 5.41      |
|    value_loss           | 48.7      |
---------------------------------------
Iteration: 3700 | Episodes: 225700 | Median Reward: 26.76 | Max Reward: 49.45
Iteration: 3701 | Episodes: 225750 | Median Reward: 28.30 | Max Reward: 49.45
Iteration: 3702 | Episodes: 225800 | Median Reward: 34.54 | Max Reward: 49.45
Iteration: 3703 | Episodes: 225850 | Median Reward: 38.90 | Max Reward: 49.45
Iteration: 3704 | Episodes: 225900 | Median Reward: 36.61 | Max Reward: 49.45
Iteration: 3704 | Episodes: 225950 | Median Reward: 37.00 | Max Reward: 49.45
Iteration: 3705 | Episodes: 226000 | Median Reward: 33.75 | Max Reward: 49.45
Iteration: 3706 | Episodes: 226050 | Median Reward: 32.75 | Max Reward: 49.45
Iteration: 3707 | Episodes: 226100 | Median Reward: 25.34 | Max Reward: 49.45
Iteration: 3708 | Episodes: 226150 | Median Reward: -11.33 | Max Reward: 49.45
Iteration: 3708 | Episodes: 226200 | Median Reward: 38.98 | Max Reward: 49.45
Iteration: 3709 | Episodes: 226250 | Median Reward: 35.65 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -80.7    |
| time/                   |          |
|    fps                  | 346      |
|    iterations           | 3710     |
|    time_elapsed         | 65734    |
|    total_timesteps      | 22794240 |
| train/                  |          |
|    approx_kl            | 3.814609 |
|    clip_fraction        | 0.0698   |
|    clip_range           | 0.4      |
|    entropy_loss         | -122     |
|    explained_variance   | 0.854    |
|    learning_rate        | 0.0001   |
|    loss                 | 39.7     |
|    n_updates            | 37090    |
|    policy_gradient_loss | -0.00377 |
|    std                  | 5.42     |
|    value_loss           | 72       |
--------------------------------------
Iteration: 3710 | Episodes: 226300 | Median Reward: 19.69 | Max Reward: 49.45
Iteration: 3711 | Episodes: 226350 | Median Reward: 29.98 | Max Reward: 49.45
Iteration: 3712 | Episodes: 226400 | Median Reward: 35.49 | Max Reward: 49.45
Iteration: 3713 | Episodes: 226450 | Median Reward: 37.41 | Max Reward: 49.45
Iteration: 3713 | Episodes: 226500 | Median Reward: 1.87 | Max Reward: 49.45
Iteration: 3714 | Episodes: 226550 | Median Reward: -3.93 | Max Reward: 49.45
Iteration: 3715 | Episodes: 226600 | Median Reward: 3.64 | Max Reward: 49.45
Iteration: 3716 | Episodes: 226650 | Median Reward: 15.17 | Max Reward: 49.45
Iteration: 3717 | Episodes: 226700 | Median Reward: 4.38 | Max Reward: 49.45
Iteration: 3717 | Episodes: 226750 | Median Reward: -12.13 | Max Reward: 49.45
Iteration: 3718 | Episodes: 226800 | Median Reward: 12.12 | Max Reward: 49.45
Iteration: 3719 | Episodes: 226850 | Median Reward: 23.34 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -89.8     |
| time/                   |           |
|    fps                  | 347       |
|    iterations           | 3720      |
|    time_elapsed         | 65791     |
|    total_timesteps      | 22855680  |
| train/                  |           |
|    approx_kl            | 2.3054533 |
|    clip_fraction        | 0.087     |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.874     |
|    learning_rate        | 0.0001    |
|    loss                 | 13.8      |
|    n_updates            | 37190     |
|    policy_gradient_loss | -0.00466  |
|    std                  | 5.43      |
|    value_loss           | 52.7      |
---------------------------------------
Iteration: 3720 | Episodes: 226900 | Median Reward: 14.67 | Max Reward: 49.45
Iteration: 3721 | Episodes: 226950 | Median Reward: 0.78 | Max Reward: 49.45
Iteration: 3722 | Episodes: 227000 | Median Reward: -13.19 | Max Reward: 49.45
Iteration: 3722 | Episodes: 227050 | Median Reward: 30.99 | Max Reward: 49.45
Iteration: 3723 | Episodes: 227100 | Median Reward: 26.92 | Max Reward: 49.45
Iteration: 3724 | Episodes: 227150 | Median Reward: 15.18 | Max Reward: 49.45
Iteration: 3725 | Episodes: 227200 | Median Reward: 1.49 | Max Reward: 49.45
Iteration: 3726 | Episodes: 227250 | Median Reward: 17.12 | Max Reward: 49.45
Iteration: 3727 | Episodes: 227300 | Median Reward: 28.09 | Max Reward: 49.45
Iteration: 3727 | Episodes: 227350 | Median Reward: 12.18 | Max Reward: 49.45
Iteration: 3728 | Episodes: 227400 | Median Reward: 12.18 | Max Reward: 49.45
Iteration: 3729 | Episodes: 227450 | Median Reward: 2.41 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -92.5     |
| time/                   |           |
|    fps                  | 348       |
|    iterations           | 3730      |
|    time_elapsed         | 65847     |
|    total_timesteps      | 22917120  |
| train/                  |           |
|    approx_kl            | 2.2529745 |
|    clip_fraction        | 0.123     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.9       |
|    learning_rate        | 0.0001    |
|    loss                 | 12.1      |
|    n_updates            | 37290     |
|    policy_gradient_loss | -0.00177  |
|    std                  | 5.44      |
|    value_loss           | 36.8      |
---------------------------------------
Iteration: 3730 | Episodes: 227500 | Median Reward: 7.04 | Max Reward: 49.45
Iteration: 3731 | Episodes: 227550 | Median Reward: -0.23 | Max Reward: 49.45
Iteration: 3731 | Episodes: 227600 | Median Reward: 3.59 | Max Reward: 49.45
Iteration: 3732 | Episodes: 227650 | Median Reward: -6.59 | Max Reward: 49.45
Iteration: 3733 | Episodes: 227700 | Median Reward: -4.62 | Max Reward: 49.45
Iteration: 3734 | Episodes: 227750 | Median Reward: 20.61 | Max Reward: 49.45
Iteration: 3735 | Episodes: 227800 | Median Reward: 20.61 | Max Reward: 49.45
Iteration: 3736 | Episodes: 227850 | Median Reward: -8.61 | Max Reward: 49.45
Iteration: 3736 | Episodes: 227900 | Median Reward: -15.02 | Max Reward: 49.45
Iteration: 3737 | Episodes: 227950 | Median Reward: -2.63 | Max Reward: 49.45
Iteration: 3738 | Episodes: 228000 | Median Reward: -2.63 | Max Reward: 49.45
Iteration: 3739 | Episodes: 228050 | Median Reward: -3.06 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -101      |
| time/                   |           |
|    fps                  | 348       |
|    iterations           | 3740      |
|    time_elapsed         | 65903     |
|    total_timesteps      | 22978560  |
| train/                  |           |
|    approx_kl            | 1.6265223 |
|    clip_fraction        | 0.0687    |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.885     |
|    learning_rate        | 0.0001    |
|    loss                 | 8.94      |
|    n_updates            | 37390     |
|    policy_gradient_loss | -0.014    |
|    std                  | 5.44      |
|    value_loss           | 32.9      |
---------------------------------------
Iteration: 3740 | Episodes: 228100 | Median Reward: -4.63 | Max Reward: 49.45
Iteration: 3740 | Episodes: 228150 | Median Reward: -12.15 | Max Reward: 49.45
Iteration: 3741 | Episodes: 228200 | Median Reward: 11.63 | Max Reward: 49.45
Iteration: 3742 | Episodes: 228250 | Median Reward: -9.62 | Max Reward: 49.45
Iteration: 3743 | Episodes: 228300 | Median Reward: -10.81 | Max Reward: 49.45
Iteration: 3744 | Episodes: 228350 | Median Reward: -3.40 | Max Reward: 49.45
Iteration: 3745 | Episodes: 228400 | Median Reward: -9.20 | Max Reward: 49.45
Iteration: 3745 | Episodes: 228450 | Median Reward: 19.36 | Max Reward: 49.45
Iteration: 3746 | Episodes: 228500 | Median Reward: 3.70 | Max Reward: 49.45
Iteration: 3747 | Episodes: 228550 | Median Reward: 3.49 | Max Reward: 49.45
Iteration: 3748 | Episodes: 228600 | Median Reward: -14.76 | Max Reward: 49.45
Iteration: 3749 | Episodes: 228650 | Median Reward: -14.80 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -104      |
| time/                   |           |
|    fps                  | 349       |
|    iterations           | 3750      |
|    time_elapsed         | 65959     |
|    total_timesteps      | 23040000  |
| train/                  |           |
|    approx_kl            | 0.3271284 |
|    clip_fraction        | 0.056     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.836     |
|    learning_rate        | 0.0001    |
|    loss                 | 24.3      |
|    n_updates            | 37490     |
|    policy_gradient_loss | 0.00987   |
|    std                  | 5.45      |
|    value_loss           | 47.6      |
---------------------------------------
Iteration: 3750 | Episodes: 228700 | Median Reward: -6.62 | Max Reward: 49.45
Iteration: 3750 | Episodes: 228750 | Median Reward: 7.46 | Max Reward: 49.45
Iteration: 3751 | Episodes: 228800 | Median Reward: 1.16 | Max Reward: 49.45
Iteration: 3752 | Episodes: 228850 | Median Reward: -4.81 | Max Reward: 49.45
Iteration: 3753 | Episodes: 228900 | Median Reward: 5.45 | Max Reward: 49.45
Iteration: 3754 | Episodes: 228950 | Median Reward: 15.02 | Max Reward: 49.45
Iteration: 3754 | Episodes: 229000 | Median Reward: -20.53 | Max Reward: 49.45
Iteration: 3755 | Episodes: 229050 | Median Reward: -3.96 | Max Reward: 49.45
Iteration: 3756 | Episodes: 229100 | Median Reward: 17.04 | Max Reward: 49.45
Iteration: 3757 | Episodes: 229150 | Median Reward: 15.14 | Max Reward: 49.45
Iteration: 3758 | Episodes: 229200 | Median Reward: 6.23 | Max Reward: 49.45
Iteration: 3759 | Episodes: 229250 | Median Reward: 21.16 | Max Reward: 49.45
Iteration: 3759 | Episodes: 229300 | Median Reward: 13.17 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -90.6      |
| time/                   |            |
|    fps                  | 349        |
|    iterations           | 3760       |
|    time_elapsed         | 66016      |
|    total_timesteps      | 23101440   |
| train/                  |            |
|    approx_kl            | 0.31394082 |
|    clip_fraction        | 0.0289     |
|    clip_range           | 0.4        |
|    entropy_loss         | -122       |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0001     |
|    loss                 | 21         |
|    n_updates            | 37590      |
|    policy_gradient_loss | 0.000746   |
|    std                  | 5.46       |
|    value_loss           | 42.7       |
----------------------------------------
Iteration: 3760 | Episodes: 229350 | Median Reward: 27.14 | Max Reward: 49.45
Iteration: 3761 | Episodes: 229400 | Median Reward: -12.70 | Max Reward: 49.45
Iteration: 3762 | Episodes: 229450 | Median Reward: -17.69 | Max Reward: 49.45
Iteration: 3763 | Episodes: 229500 | Median Reward: -1.21 | Max Reward: 49.45
Iteration: 3763 | Episodes: 229550 | Median Reward: -1.21 | Max Reward: 49.45
Iteration: 3764 | Episodes: 229600 | Median Reward: -2.80 | Max Reward: 49.45
Iteration: 3765 | Episodes: 229650 | Median Reward: 5.32 | Max Reward: 49.45
Iteration: 3766 | Episodes: 229700 | Median Reward: -15.20 | Max Reward: 49.45
Iteration: 3767 | Episodes: 229750 | Median Reward: -17.21 | Max Reward: 49.45
Iteration: 3768 | Episodes: 229800 | Median Reward: -11.33 | Max Reward: 49.45
Iteration: 3768 | Episodes: 229850 | Median Reward: -23.66 | Max Reward: 49.45
Iteration: 3769 | Episodes: 229900 | Median Reward: -10.19 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -110       |
| time/                   |            |
|    fps                  | 350        |
|    iterations           | 3770       |
|    time_elapsed         | 66072      |
|    total_timesteps      | 23162880   |
| train/                  |            |
|    approx_kl            | 0.47197887 |
|    clip_fraction        | 0.0357     |
|    clip_range           | 0.4        |
|    entropy_loss         | -123       |
|    explained_variance   | 0.915      |
|    learning_rate        | 0.0001     |
|    loss                 | 6.39       |
|    n_updates            | 37690      |
|    policy_gradient_loss | -0.0109    |
|    std                  | 5.46       |
|    value_loss           | 29.3       |
----------------------------------------
Iteration: 3770 | Episodes: 229950 | Median Reward: -17.22 | Max Reward: 49.45
Iteration: 3771 | Episodes: 230000 | Median Reward: -19.97 | Max Reward: 49.45
Iteration: 3772 | Episodes: 230050 | Median Reward: -13.66 | Max Reward: 49.45
Iteration: 3773 | Episodes: 230100 | Median Reward: -13.66 | Max Reward: 49.45
Iteration: 3773 | Episodes: 230150 | Median Reward: -14.98 | Max Reward: 49.45
Iteration: 3774 | Episodes: 230200 | Median Reward: -10.55 | Max Reward: 49.45
Iteration: 3775 | Episodes: 230250 | Median Reward: -2.55 | Max Reward: 49.45
Iteration: 3776 | Episodes: 230300 | Median Reward: 1.59 | Max Reward: 49.45
Iteration: 3777 | Episodes: 230350 | Median Reward: -13.73 | Max Reward: 49.45
Iteration: 3777 | Episodes: 230400 | Median Reward: -11.67 | Max Reward: 49.45
Iteration: 3778 | Episodes: 230450 | Median Reward: -0.54 | Max Reward: 49.45
Iteration: 3779 | Episodes: 230500 | Median Reward: 0.88 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -96.6      |
| time/                   |            |
|    fps                  | 351        |
|    iterations           | 3780       |
|    time_elapsed         | 66128      |
|    total_timesteps      | 23224320   |
| train/                  |            |
|    approx_kl            | 0.36880523 |
|    clip_fraction        | 0.046      |
|    clip_range           | 0.4        |
|    entropy_loss         | -122       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0001     |
|    loss                 | 4.37       |
|    n_updates            | 37790      |
|    policy_gradient_loss | -0.00303   |
|    std                  | 5.47       |
|    value_loss           | 29.2       |
----------------------------------------
Iteration: 3780 | Episodes: 230550 | Median Reward: 16.02 | Max Reward: 49.45
Iteration: 3781 | Episodes: 230600 | Median Reward: 18.22 | Max Reward: 49.45
Iteration: 3782 | Episodes: 230650 | Median Reward: 20.51 | Max Reward: 49.45
Iteration: 3782 | Episodes: 230700 | Median Reward: -20.92 | Max Reward: 49.45
Iteration: 3783 | Episodes: 230750 | Median Reward: -20.71 | Max Reward: 49.45
Iteration: 3784 | Episodes: 230800 | Median Reward: -14.70 | Max Reward: 49.45
Iteration: 3785 | Episodes: 230850 | Median Reward: -11.13 | Max Reward: 49.45
Iteration: 3786 | Episodes: 230900 | Median Reward: -15.51 | Max Reward: 49.45
Iteration: 3787 | Episodes: 230950 | Median Reward: -2.88 | Max Reward: 49.45
Iteration: 3787 | Episodes: 231000 | Median Reward: -15.67 | Max Reward: 49.45
Iteration: 3788 | Episodes: 231050 | Median Reward: -10.17 | Max Reward: 49.45
Iteration: 3789 | Episodes: 231100 | Median Reward: 10.76 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -92.7      |
| time/                   |            |
|    fps                  | 351        |
|    iterations           | 3790       |
|    time_elapsed         | 66184      |
|    total_timesteps      | 23285760   |
| train/                  |            |
|    approx_kl            | 0.25070262 |
|    clip_fraction        | 0.0324     |
|    clip_range           | 0.4        |
|    entropy_loss         | -123       |
|    explained_variance   | 0.868      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.14       |
|    n_updates            | 37890      |
|    policy_gradient_loss | 0.00402    |
|    std                  | 5.48       |
|    value_loss           | 37.9       |
----------------------------------------
Iteration: 3790 | Episodes: 231150 | Median Reward: 10.76 | Max Reward: 49.45
Iteration: 3791 | Episodes: 231200 | Median Reward: -11.70 | Max Reward: 49.45
Iteration: 3791 | Episodes: 231250 | Median Reward: -16.88 | Max Reward: 49.45
Iteration: 3792 | Episodes: 231300 | Median Reward: -9.94 | Max Reward: 49.45
Iteration: 3793 | Episodes: 231350 | Median Reward: -15.62 | Max Reward: 49.45
Iteration: 3794 | Episodes: 231400 | Median Reward: -15.49 | Max Reward: 49.45
Iteration: 3795 | Episodes: 231450 | Median Reward: -10.92 | Max Reward: 49.45
Iteration: 3796 | Episodes: 231500 | Median Reward: 23.37 | Max Reward: 49.45
Iteration: 3796 | Episodes: 231550 | Median Reward: 21.62 | Max Reward: 49.45
Iteration: 3797 | Episodes: 231600 | Median Reward: 11.46 | Max Reward: 49.45
Iteration: 3798 | Episodes: 231650 | Median Reward: -8.45 | Max Reward: 49.45
Iteration: 3799 | Episodes: 231700 | Median Reward: -8.58 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 352        |
|    iterations           | 3800       |
|    time_elapsed         | 66240      |
|    total_timesteps      | 23347200   |
| train/                  |            |
|    approx_kl            | 0.35345802 |
|    clip_fraction        | 0.0241     |
|    clip_range           | 0.4        |
|    entropy_loss         | -123       |
|    explained_variance   | 0.914      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.79       |
|    n_updates            | 37990      |
|    policy_gradient_loss | -0.00472   |
|    std                  | 5.49       |
|    value_loss           | 23.7       |
----------------------------------------
Iteration: 3800 | Episodes: 231750 | Median Reward: 3.09 | Max Reward: 49.45
Iteration: 3800 | Episodes: 231800 | Median Reward: 26.23 | Max Reward: 49.45
Iteration: 3801 | Episodes: 231850 | Median Reward: 19.15 | Max Reward: 49.45
Iteration: 3802 | Episodes: 231900 | Median Reward: -6.46 | Max Reward: 49.45
Iteration: 3803 | Episodes: 231950 | Median Reward: -6.68 | Max Reward: 49.45
Iteration: 3804 | Episodes: 232000 | Median Reward: -6.90 | Max Reward: 49.45
Iteration: 3805 | Episodes: 232050 | Median Reward: -7.65 | Max Reward: 49.45
Iteration: 3805 | Episodes: 232100 | Median Reward: -23.92 | Max Reward: 49.45
Iteration: 3806 | Episodes: 232150 | Median Reward: -20.94 | Max Reward: 49.45
Iteration: 3807 | Episodes: 232200 | Median Reward: -12.92 | Max Reward: 49.45
Iteration: 3808 | Episodes: 232250 | Median Reward: -12.92 | Max Reward: 49.45
Iteration: 3809 | Episodes: 232300 | Median Reward: -17.70 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -106      |
| time/                   |           |
|    fps                  | 353       |
|    iterations           | 3810      |
|    time_elapsed         | 66297     |
|    total_timesteps      | 23408640  |
| train/                  |           |
|    approx_kl            | 1.0179979 |
|    clip_fraction        | 0.0563    |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.891     |
|    learning_rate        | 0.0001    |
|    loss                 | 9.72      |
|    n_updates            | 38090     |
|    policy_gradient_loss | 0.00225   |
|    std                  | 5.5       |
|    value_loss           | 25.5      |
---------------------------------------
Iteration: 3810 | Episodes: 232350 | Median Reward: -11.85 | Max Reward: 49.45
Iteration: 3810 | Episodes: 232400 | Median Reward: 13.30 | Max Reward: 49.45
Iteration: 3811 | Episodes: 232450 | Median Reward: -3.35 | Max Reward: 49.45
Iteration: 3812 | Episodes: 232500 | Median Reward: 3.61 | Max Reward: 49.45
Iteration: 3813 | Episodes: 232550 | Median Reward: 11.95 | Max Reward: 49.45
Iteration: 3814 | Episodes: 232600 | Median Reward: 2.47 | Max Reward: 49.45
Iteration: 3814 | Episodes: 232650 | Median Reward: 9.56 | Max Reward: 49.45
Iteration: 3815 | Episodes: 232700 | Median Reward: -12.48 | Max Reward: 49.45
Iteration: 3816 | Episodes: 232750 | Median Reward: -14.47 | Max Reward: 49.45
Iteration: 3817 | Episodes: 232800 | Median Reward: -14.10 | Max Reward: 49.45
Iteration: 3818 | Episodes: 232850 | Median Reward: 5.40 | Max Reward: 49.45
Iteration: 3819 | Episodes: 232900 | Median Reward: -19.51 | Max Reward: 49.45
Iteration: 3819 | Episodes: 232950 | Median Reward: -2.60 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -102       |
| time/                   |            |
|    fps                  | 353        |
|    iterations           | 3820       |
|    time_elapsed         | 66352      |
|    total_timesteps      | 23470080   |
| train/                  |            |
|    approx_kl            | 0.13951406 |
|    clip_fraction        | 0.0336     |
|    clip_range           | 0.4        |
|    entropy_loss         | -123       |
|    explained_variance   | 0.882      |
|    learning_rate        | 0.0001     |
|    loss                 | 4.35       |
|    n_updates            | 38190      |
|    policy_gradient_loss | -0.0112    |
|    std                  | 5.51       |
|    value_loss           | 32.6       |
----------------------------------------
Iteration: 3820 | Episodes: 233000 | Median Reward: -6.03 | Max Reward: 49.45
Iteration: 3821 | Episodes: 233050 | Median Reward: 15.07 | Max Reward: 49.45
Iteration: 3822 | Episodes: 233100 | Median Reward: -5.83 | Max Reward: 49.45
Iteration: 3823 | Episodes: 233150 | Median Reward: -9.84 | Max Reward: 49.45
Iteration: 3824 | Episodes: 233200 | Median Reward: 16.42 | Max Reward: 49.45
Iteration: 3824 | Episodes: 233250 | Median Reward: -17.47 | Max Reward: 49.45
Iteration: 3825 | Episodes: 233300 | Median Reward: -10.09 | Max Reward: 49.45
Iteration: 3826 | Episodes: 233350 | Median Reward: 7.81 | Max Reward: 49.45
Iteration: 3827 | Episodes: 233400 | Median Reward: 14.11 | Max Reward: 49.45
Iteration: 3828 | Episodes: 233450 | Median Reward: -12.22 | Max Reward: 49.45
Iteration: 3828 | Episodes: 233500 | Median Reward: -13.03 | Max Reward: 49.45
Iteration: 3829 | Episodes: 233550 | Median Reward: -14.32 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -99.1      |
| time/                   |            |
|    fps                  | 354        |
|    iterations           | 3830       |
|    time_elapsed         | 66408      |
|    total_timesteps      | 23531520   |
| train/                  |            |
|    approx_kl            | 0.21522145 |
|    clip_fraction        | 0.0236     |
|    clip_range           | 0.4        |
|    entropy_loss         | -123       |
|    explained_variance   | 0.89       |
|    learning_rate        | 0.0001     |
|    loss                 | 1.68       |
|    n_updates            | 38290      |
|    policy_gradient_loss | -0.00257   |
|    std                  | 5.52       |
|    value_loss           | 36.1       |
----------------------------------------
Iteration: 3830 | Episodes: 233600 | Median Reward: -7.72 | Max Reward: 49.45
Iteration: 3831 | Episodes: 233650 | Median Reward: 0.39 | Max Reward: 49.45
Iteration: 3832 | Episodes: 233700 | Median Reward: 28.36 | Max Reward: 49.45
Iteration: 3833 | Episodes: 233750 | Median Reward: 13.21 | Max Reward: 49.45
Iteration: 3833 | Episodes: 233800 | Median Reward: -1.71 | Max Reward: 49.45
Iteration: 3834 | Episodes: 233850 | Median Reward: 25.34 | Max Reward: 49.45
Iteration: 3835 | Episodes: 233900 | Median Reward: 23.67 | Max Reward: 49.45
Iteration: 3836 | Episodes: 233950 | Median Reward: 15.81 | Max Reward: 49.45
Iteration: 3837 | Episodes: 234000 | Median Reward: 0.20 | Max Reward: 49.45
Iteration: 3837 | Episodes: 234050 | Median Reward: -3.98 | Max Reward: 49.45
Iteration: 3838 | Episodes: 234100 | Median Reward: -10.25 | Max Reward: 49.45
Iteration: 3839 | Episodes: 234150 | Median Reward: -13.72 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -106      |
| time/                   |           |
|    fps                  | 354       |
|    iterations           | 3840      |
|    time_elapsed         | 66464     |
|    total_timesteps      | 23592960  |
| train/                  |           |
|    approx_kl            | 2.0422344 |
|    clip_fraction        | 0.0529    |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.938     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0295    |
|    n_updates            | 38390     |
|    policy_gradient_loss | 0.00701   |
|    std                  | 5.53      |
|    value_loss           | 16.4      |
---------------------------------------
Iteration: 3840 | Episodes: 234200 | Median Reward: -13.72 | Max Reward: 49.45
Iteration: 3841 | Episodes: 234250 | Median Reward: -2.12 | Max Reward: 49.45
Iteration: 3842 | Episodes: 234300 | Median Reward: 36.83 | Max Reward: 49.45
Iteration: 3842 | Episodes: 234350 | Median Reward: 34.97 | Max Reward: 49.45
Iteration: 3843 | Episodes: 234400 | Median Reward: 38.61 | Max Reward: 49.45
Iteration: 3844 | Episodes: 234450 | Median Reward: 39.11 | Max Reward: 49.45
Iteration: 3845 | Episodes: 234500 | Median Reward: 35.93 | Max Reward: 49.45
Iteration: 3846 | Episodes: 234550 | Median Reward: 29.74 | Max Reward: 49.45
Iteration: 3847 | Episodes: 234600 | Median Reward: 16.72 | Max Reward: 49.45
Iteration: 3847 | Episodes: 234650 | Median Reward: -12.65 | Max Reward: 49.45
Iteration: 3848 | Episodes: 234700 | Median Reward: 29.03 | Max Reward: 49.45
Iteration: 3849 | Episodes: 234750 | Median Reward: 29.17 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -81       |
| time/                   |           |
|    fps                  | 355       |
|    iterations           | 3850      |
|    time_elapsed         | 66521     |
|    total_timesteps      | 23654400  |
| train/                  |           |
|    approx_kl            | 3.3679194 |
|    clip_fraction        | 0.0899    |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.863     |
|    learning_rate        | 0.0001    |
|    loss                 | 25.7      |
|    n_updates            | 38490     |
|    policy_gradient_loss | 0.00574   |
|    std                  | 5.55      |
|    value_loss           | 64.8      |
---------------------------------------
Iteration: 3850 | Episodes: 234800 | Median Reward: 33.79 | Max Reward: 49.45
Iteration: 3851 | Episodes: 234850 | Median Reward: 40.19 | Max Reward: 49.45
Iteration: 3851 | Episodes: 234900 | Median Reward: 40.80 | Max Reward: 49.45
Iteration: 3852 | Episodes: 234950 | Median Reward: 39.20 | Max Reward: 49.45
Iteration: 3853 | Episodes: 235000 | Median Reward: 36.82 | Max Reward: 49.45
Iteration: 3854 | Episodes: 235050 | Median Reward: 34.81 | Max Reward: 49.45
Iteration: 3855 | Episodes: 235100 | Median Reward: 31.50 | Max Reward: 49.45
Iteration: 3856 | Episodes: 235150 | Median Reward: 35.72 | Max Reward: 49.45
Iteration: 3856 | Episodes: 235200 | Median Reward: 35.77 | Max Reward: 49.45
Iteration: 3857 | Episodes: 235250 | Median Reward: 26.94 | Max Reward: 49.45
Iteration: 3858 | Episodes: 235300 | Median Reward: 31.62 | Max Reward: 49.45
Iteration: 3859 | Episodes: 235350 | Median Reward: 38.97 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -72.4     |
| time/                   |           |
|    fps                  | 356       |
|    iterations           | 3860      |
|    time_elapsed         | 66577     |
|    total_timesteps      | 23715840  |
| train/                  |           |
|    approx_kl            | 10.080959 |
|    clip_fraction        | 0.107     |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.846     |
|    learning_rate        | 0.0001    |
|    loss                 | 38.5      |
|    n_updates            | 38590     |
|    policy_gradient_loss | 0.00916   |
|    std                  | 5.57      |
|    value_loss           | 99.5      |
---------------------------------------
Iteration: 3860 | Episodes: 235400 | Median Reward: 35.30 | Max Reward: 49.45
Iteration: 3860 | Episodes: 235450 | Median Reward: 19.83 | Max Reward: 49.45
Iteration: 3861 | Episodes: 235500 | Median Reward: 41.30 | Max Reward: 49.45
Iteration: 3862 | Episodes: 235550 | Median Reward: 40.54 | Max Reward: 49.45
Iteration: 3863 | Episodes: 235600 | Median Reward: 40.13 | Max Reward: 49.45
Iteration: 3864 | Episodes: 235650 | Median Reward: 40.10 | Max Reward: 49.45
Iteration: 3865 | Episodes: 235700 | Median Reward: 38.89 | Max Reward: 49.45
Iteration: 3865 | Episodes: 235750 | Median Reward: 29.06 | Max Reward: 49.45
Iteration: 3866 | Episodes: 235800 | Median Reward: 39.03 | Max Reward: 49.45
Iteration: 3867 | Episodes: 235850 | Median Reward: 39.96 | Max Reward: 49.45
Iteration: 3868 | Episodes: 235900 | Median Reward: 40.11 | Max Reward: 49.45
Iteration: 3869 | Episodes: 235950 | Median Reward: 36.76 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.4     |
| time/                   |           |
|    fps                  | 356       |
|    iterations           | 3870      |
|    time_elapsed         | 66632     |
|    total_timesteps      | 23777280  |
| train/                  |           |
|    approx_kl            | 14.523719 |
|    clip_fraction        | 0.286     |
|    clip_range           | 0.4       |
|    entropy_loss         | -123      |
|    explained_variance   | 0.844     |
|    learning_rate        | 0.0001    |
|    loss                 | 48.2      |
|    n_updates            | 38690     |
|    policy_gradient_loss | 0.0725    |
|    std                  | 5.58      |
|    value_loss           | 95        |
---------------------------------------
Iteration: 3870 | Episodes: 236000 | Median Reward: 13.22 | Max Reward: 49.45
Iteration: 3870 | Episodes: 236050 | Median Reward: -3.14 | Max Reward: 49.45
Iteration: 3871 | Episodes: 236100 | Median Reward: -7.84 | Max Reward: 49.45
Iteration: 3872 | Episodes: 236150 | Median Reward: -7.84 | Max Reward: 49.45
Iteration: 3873 | Episodes: 236200 | Median Reward: -9.05 | Max Reward: 49.45
Iteration: 3874 | Episodes: 236250 | Median Reward: 14.00 | Max Reward: 49.45
Iteration: 3874 | Episodes: 236300 | Median Reward: 25.30 | Max Reward: 49.45
Iteration: 3875 | Episodes: 236350 | Median Reward: 14.32 | Max Reward: 49.45
Iteration: 3876 | Episodes: 236400 | Median Reward: 17.09 | Max Reward: 49.45
Iteration: 3877 | Episodes: 236450 | Median Reward: 19.68 | Max Reward: 49.45
Iteration: 3878 | Episodes: 236500 | Median Reward: 0.06 | Max Reward: 49.45
Iteration: 3879 | Episodes: 236550 | Median Reward: 20.68 | Max Reward: 49.45
Iteration: 3879 | Episodes: 236600 | Median Reward: 3.10 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -93.2     |
| time/                   |           |
|    fps                  | 357       |
|    iterations           | 3880      |
|    time_elapsed         | 66688     |
|    total_timesteps      | 23838720  |
| train/                  |           |
|    approx_kl            | 1.1752938 |
|    clip_fraction        | 0.0669    |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.884     |
|    learning_rate        | 0.0001    |
|    loss                 | 9.99      |
|    n_updates            | 38790     |
|    policy_gradient_loss | 0.00291   |
|    std                  | 5.59      |
|    value_loss           | 36.7      |
---------------------------------------
Iteration: 3880 | Episodes: 236650 | Median Reward: 8.39 | Max Reward: 49.45
Iteration: 3881 | Episodes: 236700 | Median Reward: -4.63 | Max Reward: 49.45
Iteration: 3882 | Episodes: 236750 | Median Reward: -10.30 | Max Reward: 49.45
Iteration: 3883 | Episodes: 236800 | Median Reward: -3.47 | Max Reward: 49.45
Iteration: 3884 | Episodes: 236850 | Median Reward: -16.90 | Max Reward: 49.45
Iteration: 3884 | Episodes: 236900 | Median Reward: -15.15 | Max Reward: 49.45
Iteration: 3885 | Episodes: 236950 | Median Reward: -12.61 | Max Reward: 49.45
Iteration: 3886 | Episodes: 237000 | Median Reward: -14.69 | Max Reward: 49.45
Iteration: 3887 | Episodes: 237050 | Median Reward: -15.93 | Max Reward: 49.45
Iteration: 3888 | Episodes: 237100 | Median Reward: -3.96 | Max Reward: 49.45
Iteration: 3888 | Episodes: 237150 | Median Reward: 1.89 | Max Reward: 49.45
Iteration: 3889 | Episodes: 237200 | Median Reward: -16.23 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -106      |
| time/                   |           |
|    fps                  | 358       |
|    iterations           | 3890      |
|    time_elapsed         | 66744     |
|    total_timesteps      | 23900160  |
| train/                  |           |
|    approx_kl            | 0.9392801 |
|    clip_fraction        | 0.0519    |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.918     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.19      |
|    n_updates            | 38890     |
|    policy_gradient_loss | 0.00187   |
|    std                  | 5.6       |
|    value_loss           | 25.4      |
---------------------------------------
Iteration: 3890 | Episodes: 237250 | Median Reward: -0.94 | Max Reward: 49.45
Iteration: 3891 | Episodes: 237300 | Median Reward: 1.18 | Max Reward: 49.45
Iteration: 3892 | Episodes: 237350 | Median Reward: -9.00 | Max Reward: 49.45
Iteration: 3893 | Episodes: 237400 | Median Reward: -4.88 | Max Reward: 49.45
Iteration: 3893 | Episodes: 237450 | Median Reward: -1.76 | Max Reward: 49.45
Iteration: 3894 | Episodes: 237500 | Median Reward: 17.51 | Max Reward: 49.45
Iteration: 3895 | Episodes: 237550 | Median Reward: -3.32 | Max Reward: 49.45
Iteration: 3896 | Episodes: 237600 | Median Reward: -3.32 | Max Reward: 49.45
Iteration: 3897 | Episodes: 237650 | Median Reward: 10.36 | Max Reward: 49.45
Iteration: 3897 | Episodes: 237700 | Median Reward: -10.49 | Max Reward: 49.45
Iteration: 3898 | Episodes: 237750 | Median Reward: -11.08 | Max Reward: 49.45
Iteration: 3899 | Episodes: 237800 | Median Reward: 16.31 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -93.9     |
| time/                   |           |
|    fps                  | 358       |
|    iterations           | 3900      |
|    time_elapsed         | 66800     |
|    total_timesteps      | 23961600  |
| train/                  |           |
|    approx_kl            | 1.9533522 |
|    clip_fraction        | 0.0664    |
|    clip_range           | 0.4       |
|    entropy_loss         | -120      |
|    explained_variance   | 0.916     |
|    learning_rate        | 0.0001    |
|    loss                 | 5.36      |
|    n_updates            | 38990     |
|    policy_gradient_loss | -0.00122  |
|    std                  | 5.6       |
|    value_loss           | 21.6      |
---------------------------------------
Iteration: 3900 | Episodes: 237850 | Median Reward: -3.97 | Max Reward: 49.45
Iteration: 3901 | Episodes: 237900 | Median Reward: -11.09 | Max Reward: 49.45
Iteration: 3902 | Episodes: 237950 | Median Reward: -13.26 | Max Reward: 49.45
Iteration: 3902 | Episodes: 238000 | Median Reward: -12.32 | Max Reward: 49.45
Iteration: 3903 | Episodes: 238050 | Median Reward: 2.41 | Max Reward: 49.45
Iteration: 3904 | Episodes: 238100 | Median Reward: 4.60 | Max Reward: 49.45
Iteration: 3905 | Episodes: 238150 | Median Reward: 12.23 | Max Reward: 49.45
Iteration: 3906 | Episodes: 238200 | Median Reward: 12.23 | Max Reward: 49.45
Iteration: 3907 | Episodes: 238250 | Median Reward: -3.96 | Max Reward: 49.45
Iteration: 3907 | Episodes: 238300 | Median Reward: -22.21 | Max Reward: 49.45
Iteration: 3908 | Episodes: 238350 | Median Reward: -2.29 | Max Reward: 49.45
Iteration: 3909 | Episodes: 238400 | Median Reward: -2.29 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -99.2     |
| time/                   |           |
|    fps                  | 359       |
|    iterations           | 3910      |
|    time_elapsed         | 66856     |
|    total_timesteps      | 24023040  |
| train/                  |           |
|    approx_kl            | 7.0543575 |
|    clip_fraction        | 0.122     |
|    clip_range           | 0.4       |
|    entropy_loss         | -121      |
|    explained_variance   | 0.933     |
|    learning_rate        | 0.0001    |
|    loss                 | 9.06      |
|    n_updates            | 39090     |
|    policy_gradient_loss | 0.0266    |
|    std                  | 5.61      |
|    value_loss           | 22.6      |
---------------------------------------
Iteration: 3910 | Episodes: 238450 | Median Reward: 3.17 | Max Reward: 49.45
Iteration: 3911 | Episodes: 238500 | Median Reward: 12.09 | Max Reward: 49.45
Iteration: 3911 | Episodes: 238550 | Median Reward: -18.85 | Max Reward: 49.45
Iteration: 3912 | Episodes: 238600 | Median Reward: 19.87 | Max Reward: 49.45
Iteration: 3913 | Episodes: 238650 | Median Reward: 16.20 | Max Reward: 49.45
Iteration: 3914 | Episodes: 238700 | Median Reward: 5.41 | Max Reward: 49.45
Iteration: 3915 | Episodes: 238750 | Median Reward: -8.18 | Max Reward: 49.45
Iteration: 3916 | Episodes: 238800 | Median Reward: -8.35 | Max Reward: 49.45
Iteration: 3916 | Episodes: 238850 | Median Reward: 19.61 | Max Reward: 49.45
Iteration: 3917 | Episodes: 238900 | Median Reward: -21.89 | Max Reward: 49.45
Iteration: 3918 | Episodes: 238950 | Median Reward: -15.47 | Max Reward: 49.45
Iteration: 3919 | Episodes: 239000 | Median Reward: 10.72 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -93.2      |
| time/                   |            |
|    fps                  | 359        |
|    iterations           | 3920       |
|    time_elapsed         | 66912      |
|    total_timesteps      | 24084480   |
| train/                  |            |
|    approx_kl            | 0.44490653 |
|    clip_fraction        | 0.0473     |
|    clip_range           | 0.4        |
|    entropy_loss         | -123       |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.0001     |
|    loss                 | 5.8        |
|    n_updates            | 39190      |
|    policy_gradient_loss | -0.00676   |
|    std                  | 5.62       |
|    value_loss           | 33.5       |
----------------------------------------
Iteration: 3920 | Episodes: 239050 | Median Reward: 10.72 | Max Reward: 49.45
Iteration: 3921 | Episodes: 239100 | Median Reward: 6.75 | Max Reward: 49.45
Iteration: 3921 | Episodes: 239150 | Median Reward: -13.89 | Max Reward: 49.45
Iteration: 3922 | Episodes: 239200 | Median Reward: -14.21 | Max Reward: 49.45
Iteration: 3923 | Episodes: 239250 | Median Reward: 7.29 | Max Reward: 49.45
Iteration: 3924 | Episodes: 239300 | Median Reward: 14.52 | Max Reward: 49.45
Iteration: 3925 | Episodes: 239350 | Median Reward: -3.58 | Max Reward: 49.45
Iteration: 3925 | Episodes: 239400 | Median Reward: -15.21 | Max Reward: 49.45
Iteration: 3926 | Episodes: 239450 | Median Reward: 7.58 | Max Reward: 49.45
Iteration: 3927 | Episodes: 239500 | Median Reward: 8.77 | Max Reward: 49.45
Iteration: 3928 | Episodes: 239550 | Median Reward: 5.79 | Max Reward: 49.45
Iteration: 3929 | Episodes: 239600 | Median Reward: 17.56 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -86.2      |
| time/                   |            |
|    fps                  | 360        |
|    iterations           | 3930       |
|    time_elapsed         | 66968      |
|    total_timesteps      | 24145920   |
| train/                  |            |
|    approx_kl            | 0.37843144 |
|    clip_fraction        | 0.0624     |
|    clip_range           | 0.4        |
|    entropy_loss         | -122       |
|    explained_variance   | 0.91       |
|    learning_rate        | 0.0001     |
|    loss                 | 1.7        |
|    n_updates            | 39290      |
|    policy_gradient_loss | 0.00575    |
|    std                  | 5.63       |
|    value_loss           | 31.3       |
----------------------------------------
Iteration: 3930 | Episodes: 239650 | Median Reward: 24.36 | Max Reward: 49.45
Iteration: 3930 | Episodes: 239700 | Median Reward: 6.33 | Max Reward: 49.45
Iteration: 3931 | Episodes: 239750 | Median Reward: 15.07 | Max Reward: 49.45
Iteration: 3932 | Episodes: 239800 | Median Reward: -20.51 | Max Reward: 49.45
Iteration: 3933 | Episodes: 239850 | Median Reward: -14.60 | Max Reward: 49.45
Iteration: 3934 | Episodes: 239900 | Median Reward: 9.19 | Max Reward: 49.45
Iteration: 3934 | Episodes: 239950 | Median Reward: 6.14 | Max Reward: 49.45
Iteration: 3935 | Episodes: 240000 | Median Reward: 41.95 | Max Reward: 49.45
Iteration: 3936 | Episodes: 240050 | Median Reward: 41.96 | Max Reward: 49.45
Iteration: 3937 | Episodes: 240100 | Median Reward: 42.28 | Max Reward: 49.45
Iteration: 3938 | Episodes: 240150 | Median Reward: 42.58 | Max Reward: 49.45
Iteration: 3939 | Episodes: 240200 | Median Reward: 43.54 | Max Reward: 49.45
Iteration: 3939 | Episodes: 240250 | Median Reward: 42.75 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -57      |
| time/                   |          |
|    fps                  | 361      |
|    iterations           | 3940     |
|    time_elapsed         | 67024    |
|    total_timesteps      | 24207360 |
| train/                  |          |
|    approx_kl            | 4.881793 |
|    clip_fraction        | 0.0683   |
|    clip_range           | 0.4      |
|    entropy_loss         | -124     |
|    explained_variance   | 0.828    |
|    learning_rate        | 0.0001   |
|    loss                 | 61.5     |
|    n_updates            | 39390    |
|    policy_gradient_loss | 0.0155   |
|    std                  | 5.64     |
|    value_loss           | 138      |
--------------------------------------
Iteration: 3940 | Episodes: 240300 | Median Reward: 42.60 | Max Reward: 49.45
Iteration: 3941 | Episodes: 240350 | Median Reward: 43.08 | Max Reward: 49.45
Iteration: 3942 | Episodes: 240400 | Median Reward: 41.61 | Max Reward: 49.45
Iteration: 3943 | Episodes: 240450 | Median Reward: 42.62 | Max Reward: 49.45
Iteration: 3944 | Episodes: 240500 | Median Reward: 41.97 | Max Reward: 49.45
Iteration: 3944 | Episodes: 240550 | Median Reward: 43.36 | Max Reward: 49.45
Iteration: 3945 | Episodes: 240600 | Median Reward: 45.97 | Max Reward: 49.45
Iteration: 3946 | Episodes: 240650 | Median Reward: 46.72 | Max Reward: 49.45
Iteration: 3947 | Episodes: 240700 | Median Reward: 45.75 | Max Reward: 49.45
Iteration: 3947 | Episodes: 240750 | Median Reward: 46.04 | Max Reward: 49.45
Iteration: 3948 | Episodes: 240800 | Median Reward: 45.68 | Max Reward: 49.45
Iteration: 3949 | Episodes: 240850 | Median Reward: 45.62 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -55      |
| time/                   |          |
|    fps                  | 361      |
|    iterations           | 3950     |
|    time_elapsed         | 67080    |
|    total_timesteps      | 24268800 |
| train/                  |          |
|    approx_kl            | 9.931256 |
|    clip_fraction        | 0.118    |
|    clip_range           | 0.4      |
|    entropy_loss         | -124     |
|    explained_variance   | 0.821    |
|    learning_rate        | 0.0001   |
|    loss                 | 58.7     |
|    n_updates            | 39490    |
|    policy_gradient_loss | 0.0257   |
|    std                  | 5.66     |
|    value_loss           | 127      |
--------------------------------------
Iteration: 3950 | Episodes: 240900 | Median Reward: 45.42 | Max Reward: 49.45
Iteration: 3951 | Episodes: 240950 | Median Reward: 45.86 | Max Reward: 49.45
Iteration: 3951 | Episodes: 241000 | Median Reward: 47.34 | Max Reward: 49.45
Iteration: 3952 | Episodes: 241050 | Median Reward: 41.41 | Max Reward: 49.45
Iteration: 3953 | Episodes: 241100 | Median Reward: 44.79 | Max Reward: 49.45
Iteration: 3954 | Episodes: 241150 | Median Reward: 46.39 | Max Reward: 49.45
Iteration: 3955 | Episodes: 241200 | Median Reward: 46.54 | Max Reward: 49.45
Iteration: 3955 | Episodes: 241250 | Median Reward: 49.00 | Max Reward: 49.45
Iteration: 3956 | Episodes: 241300 | Median Reward: 47.49 | Max Reward: 49.45
Iteration: 3957 | Episodes: 241350 | Median Reward: 47.09 | Max Reward: 49.45
Iteration: 3957 | Episodes: 241400 | Median Reward: 43.02 | Max Reward: 49.45
Iteration: 3958 | Episodes: 241450 | Median Reward: 46.81 | Max Reward: 49.45
Iteration: 3959 | Episodes: 241500 | Median Reward: 46.96 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -54.1    |
| time/                   |          |
|    fps                  | 362      |
|    iterations           | 3960     |
|    time_elapsed         | 67135    |
|    total_timesteps      | 24330240 |
| train/                  |          |
|    approx_kl            | 2.421237 |
|    clip_fraction        | 0.06     |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0.808    |
|    learning_rate        | 0.0001   |
|    loss                 | 49.2     |
|    n_updates            | 39590    |
|    policy_gradient_loss | 0.0142   |
|    std                  | 5.69     |
|    value_loss           | 120      |
--------------------------------------
Iteration: 3960 | Episodes: 241550 | Median Reward: 46.97 | Max Reward: 49.45
Iteration: 3961 | Episodes: 241600 | Median Reward: 47.27 | Max Reward: 49.45
Iteration: 3961 | Episodes: 241650 | Median Reward: 45.40 | Max Reward: 49.45
Iteration: 3962 | Episodes: 241700 | Median Reward: 45.45 | Max Reward: 49.45
Iteration: 3963 | Episodes: 241750 | Median Reward: 46.91 | Max Reward: 49.45
Iteration: 3964 | Episodes: 241800 | Median Reward: 47.09 | Max Reward: 49.45
Iteration: 3965 | Episodes: 241850 | Median Reward: 47.31 | Max Reward: 49.45
Iteration: 3965 | Episodes: 241900 | Median Reward: 43.53 | Max Reward: 49.45
Iteration: 3966 | Episodes: 241950 | Median Reward: 44.96 | Max Reward: 49.45
Iteration: 3967 | Episodes: 242000 | Median Reward: 44.75 | Max Reward: 49.45
Iteration: 3968 | Episodes: 242050 | Median Reward: 44.75 | Max Reward: 49.45
Iteration: 3969 | Episodes: 242100 | Median Reward: 46.32 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -54.2     |
| time/                   |           |
|    fps                  | 363       |
|    iterations           | 3970      |
|    time_elapsed         | 67191     |
|    total_timesteps      | 24391680  |
| train/                  |           |
|    approx_kl            | 2.2393558 |
|    clip_fraction        | 0.0298    |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.846     |
|    learning_rate        | 0.0001    |
|    loss                 | 42.3      |
|    n_updates            | 39690     |
|    policy_gradient_loss | 0.00076   |
|    std                  | 5.73      |
|    value_loss           | 108       |
---------------------------------------
Iteration: 3970 | Episodes: 242150 | Median Reward: 47.12 | Max Reward: 49.45
Iteration: 3970 | Episodes: 242200 | Median Reward: 43.48 | Max Reward: 49.45
Iteration: 3971 | Episodes: 242250 | Median Reward: 46.21 | Max Reward: 49.45
Iteration: 3972 | Episodes: 242300 | Median Reward: 46.28 | Max Reward: 49.45
Iteration: 3973 | Episodes: 242350 | Median Reward: 46.29 | Max Reward: 49.45
Iteration: 3974 | Episodes: 242400 | Median Reward: 46.30 | Max Reward: 49.45
Iteration: 3974 | Episodes: 242450 | Median Reward: 45.89 | Max Reward: 49.45
Iteration: 3975 | Episodes: 242500 | Median Reward: 46.64 | Max Reward: 49.45
Iteration: 3976 | Episodes: 242550 | Median Reward: 46.29 | Max Reward: 49.45
Iteration: 3977 | Episodes: 242600 | Median Reward: 46.21 | Max Reward: 49.45
Iteration: 3978 | Episodes: 242650 | Median Reward: 46.27 | Max Reward: 49.45
Iteration: 3979 | Episodes: 242700 | Median Reward: 46.37 | Max Reward: 49.45
Iteration: 3979 | Episodes: 242750 | Median Reward: 46.15 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -53.7    |
| time/                   |          |
|    fps                  | 363      |
|    iterations           | 3980     |
|    time_elapsed         | 67247    |
|    total_timesteps      | 24453120 |
| train/                  |          |
|    approx_kl            | 5.797035 |
|    clip_fraction        | 0.0778   |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0.854    |
|    learning_rate        | 0.0001   |
|    loss                 | 43.6     |
|    n_updates            | 39790    |
|    policy_gradient_loss | -0.00564 |
|    std                  | 5.77     |
|    value_loss           | 100      |
--------------------------------------
Iteration: 3980 | Episodes: 242800 | Median Reward: 45.05 | Max Reward: 49.45
Iteration: 3981 | Episodes: 242850 | Median Reward: 46.45 | Max Reward: 49.45
Iteration: 3982 | Episodes: 242900 | Median Reward: 46.79 | Max Reward: 49.45
Iteration: 3983 | Episodes: 242950 | Median Reward: 46.07 | Max Reward: 49.45
Iteration: 3984 | Episodes: 243000 | Median Reward: 44.65 | Max Reward: 49.45
Iteration: 3984 | Episodes: 243050 | Median Reward: 43.48 | Max Reward: 49.45
Iteration: 3985 | Episodes: 243100 | Median Reward: 42.99 | Max Reward: 49.45
Iteration: 3986 | Episodes: 243150 | Median Reward: 44.32 | Max Reward: 49.45
Iteration: 3987 | Episodes: 243200 | Median Reward: 44.89 | Max Reward: 49.45
Iteration: 3988 | Episodes: 243250 | Median Reward: 44.96 | Max Reward: 49.45
Iteration: 3988 | Episodes: 243300 | Median Reward: 45.41 | Max Reward: 49.45
Iteration: 3989 | Episodes: 243350 | Median Reward: 45.30 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -55      |
| time/                   |          |
|    fps                  | 364      |
|    iterations           | 3990     |
|    time_elapsed         | 67303    |
|    total_timesteps      | 24514560 |
| train/                  |          |
|    approx_kl            | 6.187831 |
|    clip_fraction        | 0.0481   |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0.868    |
|    learning_rate        | 0.0001   |
|    loss                 | 36.7     |
|    n_updates            | 39890    |
|    policy_gradient_loss | 0.0151   |
|    std                  | 5.82     |
|    value_loss           | 88.5     |
--------------------------------------
Iteration: 3990 | Episodes: 243400 | Median Reward: 44.56 | Max Reward: 49.45
Iteration: 3991 | Episodes: 243450 | Median Reward: 43.76 | Max Reward: 49.45
Iteration: 3992 | Episodes: 243500 | Median Reward: 43.74 | Max Reward: 49.45
Iteration: 3993 | Episodes: 243550 | Median Reward: 44.94 | Max Reward: 49.45
Iteration: 3993 | Episodes: 243600 | Median Reward: 45.15 | Max Reward: 49.45
Iteration: 3994 | Episodes: 243650 | Median Reward: 45.15 | Max Reward: 49.45
Iteration: 3995 | Episodes: 243700 | Median Reward: 44.98 | Max Reward: 49.45
Iteration: 3996 | Episodes: 243750 | Median Reward: 43.70 | Max Reward: 49.45
Iteration: 3997 | Episodes: 243800 | Median Reward: 44.01 | Max Reward: 49.45
Iteration: 3997 | Episodes: 243850 | Median Reward: 45.36 | Max Reward: 49.45
Iteration: 3998 | Episodes: 243900 | Median Reward: 44.57 | Max Reward: 49.45
Iteration: 3999 | Episodes: 243950 | Median Reward: 44.82 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -55.1     |
| time/                   |           |
|    fps                  | 364       |
|    iterations           | 4000      |
|    time_elapsed         | 67358     |
|    total_timesteps      | 24576000  |
| train/                  |           |
|    approx_kl            | 23.723783 |
|    clip_fraction        | 0.244     |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | 0.877     |
|    learning_rate        | 0.0001    |
|    loss                 | 34.1      |
|    n_updates            | 39990     |
|    policy_gradient_loss | 0.0932    |
|    std                  | 5.89      |
|    value_loss           | 79        |
---------------------------------------
Iteration: 4000 | Episodes: 244000 | Median Reward: 46.22 | Max Reward: 49.45
Iteration: 4001 | Episodes: 244050 | Median Reward: 46.38 | Max Reward: 49.45
Iteration: 4002 | Episodes: 244100 | Median Reward: 43.49 | Max Reward: 49.45
Iteration: 4002 | Episodes: 244150 | Median Reward: 43.45 | Max Reward: 49.45
Iteration: 4003 | Episodes: 244200 | Median Reward: 43.27 | Max Reward: 49.45
Iteration: 4004 | Episodes: 244250 | Median Reward: 43.27 | Max Reward: 49.45
Iteration: 4005 | Episodes: 244300 | Median Reward: 45.25 | Max Reward: 49.45
Iteration: 4005 | Episodes: 244350 | Median Reward: 43.82 | Max Reward: 49.45
Iteration: 4006 | Episodes: 244400 | Median Reward: 44.65 | Max Reward: 49.45
Iteration: 4007 | Episodes: 244450 | Median Reward: 44.26 | Max Reward: 49.45
Iteration: 4008 | Episodes: 244500 | Median Reward: 44.25 | Max Reward: 49.45
Iteration: 4009 | Episodes: 244550 | Median Reward: 45.14 | Max Reward: 49.45
Iteration: 4009 | Episodes: 244600 | Median Reward: 46.70 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -54.7      |
| time/                   |            |
|    fps                  | 365        |
|    iterations           | 4010       |
|    time_elapsed         | 67414      |
|    total_timesteps      | 24637440   |
| train/                  |            |
|    approx_kl            | 15.9471445 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | 0.881      |
|    learning_rate        | 0.0001     |
|    loss                 | 35.2       |
|    n_updates            | 40090      |
|    policy_gradient_loss | 0.0627     |
|    std                  | 5.95       |
|    value_loss           | 74.9       |
----------------------------------------
Iteration: 4010 | Episodes: 244650 | Median Reward: 46.66 | Max Reward: 49.45
Iteration: 4011 | Episodes: 244700 | Median Reward: 46.55 | Max Reward: 49.45
Iteration: 4012 | Episodes: 244750 | Median Reward: 46.55 | Max Reward: 49.45
Iteration: 4013 | Episodes: 244800 | Median Reward: 46.60 | Max Reward: 49.45
Iteration: 4014 | Episodes: 244850 | Median Reward: 46.89 | Max Reward: 49.45
Iteration: 4014 | Episodes: 244900 | Median Reward: 45.99 | Max Reward: 49.45
Iteration: 4015 | Episodes: 244950 | Median Reward: 46.00 | Max Reward: 49.45
Iteration: 4016 | Episodes: 245000 | Median Reward: 46.24 | Max Reward: 49.45
Iteration: 4017 | Episodes: 245050 | Median Reward: 45.03 | Max Reward: 49.45
Iteration: 4018 | Episodes: 245100 | Median Reward: 42.24 | Max Reward: 49.45
Iteration: 4018 | Episodes: 245150 | Median Reward: 41.48 | Max Reward: 49.45
Iteration: 4019 | Episodes: 245200 | Median Reward: 43.87 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -57.1    |
| time/                   |          |
|    fps                  | 366      |
|    iterations           | 4020     |
|    time_elapsed         | 67469    |
|    total_timesteps      | 24698880 |
| train/                  |          |
|    approx_kl            | 28.6109  |
|    clip_fraction        | 0.353    |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0.898    |
|    learning_rate        | 0.0001   |
|    loss                 | 22.8     |
|    n_updates            | 40190    |
|    policy_gradient_loss | 0.118    |
|    std                  | 6.02     |
|    value_loss           | 61.3     |
--------------------------------------
Iteration: 4020 | Episodes: 245250 | Median Reward: 43.47 | Max Reward: 49.45
Iteration: 4021 | Episodes: 245300 | Median Reward: 43.81 | Max Reward: 49.45
Iteration: 4022 | Episodes: 245350 | Median Reward: 43.96 | Max Reward: 49.45
Iteration: 4023 | Episodes: 245400 | Median Reward: 43.96 | Max Reward: 49.45
Iteration: 4023 | Episodes: 245450 | Median Reward: 44.65 | Max Reward: 49.45
Iteration: 4024 | Episodes: 245500 | Median Reward: 37.14 | Max Reward: 49.45
Iteration: 4025 | Episodes: 245550 | Median Reward: 37.78 | Max Reward: 49.45
Iteration: 4026 | Episodes: 245600 | Median Reward: 42.83 | Max Reward: 49.45
Iteration: 4027 | Episodes: 245650 | Median Reward: 39.29 | Max Reward: 49.45
Iteration: 4028 | Episodes: 245700 | Median Reward: -5.34 | Max Reward: 49.45
Iteration: 4028 | Episodes: 245750 | Median Reward: 44.42 | Max Reward: 49.45
Iteration: 4029 | Episodes: 245800 | Median Reward: 36.67 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -77.6     |
| time/                   |           |
|    fps                  | 366       |
|    iterations           | 4030      |
|    time_elapsed         | 67525     |
|    total_timesteps      | 24760320  |
| train/                  |           |
|    approx_kl            | 11.534625 |
|    clip_fraction        | 0.0719    |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | 0.88      |
|    learning_rate        | 0.0001    |
|    loss                 | 23.3      |
|    n_updates            | 40290     |
|    policy_gradient_loss | 0.0167    |
|    std                  | 6.04      |
|    value_loss           | 63.9      |
---------------------------------------
Iteration: 4030 | Episodes: 245850 | Median Reward: 42.99 | Max Reward: 49.45
Iteration: 4031 | Episodes: 245900 | Median Reward: 44.07 | Max Reward: 49.45
Iteration: 4032 | Episodes: 245950 | Median Reward: 44.33 | Max Reward: 49.45
Iteration: 4032 | Episodes: 246000 | Median Reward: 44.38 | Max Reward: 49.45
Iteration: 4033 | Episodes: 246050 | Median Reward: 43.92 | Max Reward: 49.45
Iteration: 4034 | Episodes: 246100 | Median Reward: 43.55 | Max Reward: 49.45
Iteration: 4035 | Episodes: 246150 | Median Reward: 44.29 | Max Reward: 49.45
Iteration: 4036 | Episodes: 246200 | Median Reward: 44.29 | Max Reward: 49.45
Iteration: 4037 | Episodes: 246250 | Median Reward: -10.19 | Max Reward: 49.45
Iteration: 4037 | Episodes: 246300 | Median Reward: -10.05 | Max Reward: 49.45
Iteration: 4038 | Episodes: 246350 | Median Reward: 41.27 | Max Reward: 49.45
Iteration: 4039 | Episodes: 246400 | Median Reward: 40.75 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -64.1    |
| time/                   |          |
|    fps                  | 367      |
|    iterations           | 4040     |
|    time_elapsed         | 67581    |
|    total_timesteps      | 24821760 |
| train/                  |          |
|    approx_kl            | 11.7285  |
|    clip_fraction        | 0.0714   |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0.892    |
|    learning_rate        | 0.0001   |
|    loss                 | 25.4     |
|    n_updates            | 40390    |
|    policy_gradient_loss | 0.0152   |
|    std                  | 6.07     |
|    value_loss           | 60.8     |
--------------------------------------
Iteration: 4040 | Episodes: 246450 | Median Reward: 40.14 | Max Reward: 49.45
Iteration: 4041 | Episodes: 246500 | Median Reward: 37.60 | Max Reward: 49.45
Iteration: 4041 | Episodes: 246550 | Median Reward: 36.27 | Max Reward: 49.45
Iteration: 4042 | Episodes: 246600 | Median Reward: 41.02 | Max Reward: 49.45
Iteration: 4043 | Episodes: 246650 | Median Reward: 35.07 | Max Reward: 49.45
Iteration: 4044 | Episodes: 246700 | Median Reward: 19.47 | Max Reward: 49.45
Iteration: 4045 | Episodes: 246750 | Median Reward: 8.16 | Max Reward: 49.45
Iteration: 4046 | Episodes: 246800 | Median Reward: 0.86 | Max Reward: 49.45
Iteration: 4046 | Episodes: 246850 | Median Reward: 34.54 | Max Reward: 49.45
Iteration: 4047 | Episodes: 246900 | Median Reward: 34.01 | Max Reward: 49.45
Iteration: 4048 | Episodes: 246950 | Median Reward: 26.15 | Max Reward: 49.45
Iteration: 4049 | Episodes: 247000 | Median Reward: 34.49 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -74.7     |
| time/                   |           |
|    fps                  | 367       |
|    iterations           | 4050      |
|    time_elapsed         | 67636     |
|    total_timesteps      | 24883200  |
| train/                  |           |
|    approx_kl            | 3.4223504 |
|    clip_fraction        | 0.0519    |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.912     |
|    learning_rate        | 0.0001    |
|    loss                 | 10.7      |
|    n_updates            | 40490     |
|    policy_gradient_loss | 0.0264    |
|    std                  | 6.09      |
|    value_loss           | 37.8      |
---------------------------------------
Iteration: 4050 | Episodes: 247050 | Median Reward: 37.13 | Max Reward: 49.45
Iteration: 4051 | Episodes: 247100 | Median Reward: 23.97 | Max Reward: 49.45
Iteration: 4051 | Episodes: 247150 | Median Reward: 33.39 | Max Reward: 49.45
Iteration: 4052 | Episodes: 247200 | Median Reward: 33.47 | Max Reward: 49.45
Iteration: 4053 | Episodes: 247250 | Median Reward: -9.16 | Max Reward: 49.45
Iteration: 4054 | Episodes: 247300 | Median Reward: -10.00 | Max Reward: 49.45
Iteration: 4055 | Episodes: 247350 | Median Reward: -12.70 | Max Reward: 49.45
Iteration: 4055 | Episodes: 247400 | Median Reward: -1.16 | Max Reward: 49.45
Iteration: 4056 | Episodes: 247450 | Median Reward: -8.99 | Max Reward: 49.45
Iteration: 4057 | Episodes: 247500 | Median Reward: -11.26 | Max Reward: 49.45
Iteration: 4058 | Episodes: 247550 | Median Reward: -11.26 | Max Reward: 49.45
Iteration: 4059 | Episodes: 247600 | Median Reward: -4.10 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -94.9      |
| time/                   |            |
|    fps                  | 368        |
|    iterations           | 4060       |
|    time_elapsed         | 67692      |
|    total_timesteps      | 24944640   |
| train/                  |            |
|    approx_kl            | 0.48896265 |
|    clip_fraction        | 0.0959     |
|    clip_range           | 0.4        |
|    entropy_loss         | -124       |
|    explained_variance   | 0.892      |
|    learning_rate        | 0.0001     |
|    loss                 | 16.1       |
|    n_updates            | 40590      |
|    policy_gradient_loss | 0.00071    |
|    std                  | 6.09       |
|    value_loss           | 46.3       |
----------------------------------------
Iteration: 4060 | Episodes: 247650 | Median Reward: 24.51 | Max Reward: 49.45
Iteration: 4060 | Episodes: 247700 | Median Reward: 18.11 | Max Reward: 49.45
Iteration: 4061 | Episodes: 247750 | Median Reward: -6.14 | Max Reward: 49.45
Iteration: 4062 | Episodes: 247800 | Median Reward: 24.06 | Max Reward: 49.45
Iteration: 4063 | Episodes: 247850 | Median Reward: 27.17 | Max Reward: 49.45
Iteration: 4064 | Episodes: 247900 | Median Reward: 16.20 | Max Reward: 49.45
Iteration: 4065 | Episodes: 247950 | Median Reward: -9.13 | Max Reward: 49.45
Iteration: 4065 | Episodes: 248000 | Median Reward: -9.71 | Max Reward: 49.45
Iteration: 4066 | Episodes: 248050 | Median Reward: 3.21 | Max Reward: 49.45
Iteration: 4067 | Episodes: 248100 | Median Reward: -2.53 | Max Reward: 49.45
Iteration: 4068 | Episodes: 248150 | Median Reward: 16.79 | Max Reward: 49.45
Iteration: 4069 | Episodes: 248200 | Median Reward: 27.72 | Max Reward: 49.45
Iteration: 4069 | Episodes: 248250 | Median Reward: 33.31 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.5     |
| time/                   |           |
|    fps                  | 369       |
|    iterations           | 4070      |
|    time_elapsed         | 67748     |
|    total_timesteps      | 25006080  |
| train/                  |           |
|    approx_kl            | 4.8529096 |
|    clip_fraction        | 0.0636    |
|    clip_range           | 0.4       |
|    entropy_loss         | -124      |
|    explained_variance   | 0.893     |
|    learning_rate        | 0.0001    |
|    loss                 | 20.5      |
|    n_updates            | 40690     |
|    policy_gradient_loss | 0.00554   |
|    std                  | 6.1       |
|    value_loss           | 48.8      |
---------------------------------------
Iteration: 4070 | Episodes: 248300 | Median Reward: 24.24 | Max Reward: 49.45
Iteration: 4071 | Episodes: 248350 | Median Reward: 11.62 | Max Reward: 49.45
Iteration: 4072 | Episodes: 248400 | Median Reward: 6.91 | Max Reward: 49.45
Iteration: 4073 | Episodes: 248450 | Median Reward: 3.77 | Max Reward: 49.45
Iteration: 4074 | Episodes: 248500 | Median Reward: -8.04 | Max Reward: 49.45
Iteration: 4074 | Episodes: 248550 | Median Reward: 25.48 | Max Reward: 49.45
Iteration: 4075 | Episodes: 248600 | Median Reward: 24.35 | Max Reward: 49.45
Iteration: 4076 | Episodes: 248650 | Median Reward: 27.38 | Max Reward: 49.45
Iteration: 4077 | Episodes: 248700 | Median Reward: 23.94 | Max Reward: 49.45
Iteration: 4078 | Episodes: 248750 | Median Reward: 23.94 | Max Reward: 49.45
Iteration: 4078 | Episodes: 248800 | Median Reward: 34.79 | Max Reward: 49.45
Iteration: 4079 | Episodes: 248850 | Median Reward: 36.83 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -75      |
| time/                   |          |
|    fps                  | 369      |
|    iterations           | 4080     |
|    time_elapsed         | 67831    |
|    total_timesteps      | 25067520 |
| train/                  |          |
|    approx_kl            | 3.59928  |
|    clip_fraction        | 0.0837   |
|    clip_range           | 0.4      |
|    entropy_loss         | -124     |
|    explained_variance   | 0.902    |
|    learning_rate        | 0.0001   |
|    loss                 | 15.2     |
|    n_updates            | 40790    |
|    policy_gradient_loss | 0.015    |
|    std                  | 6.11     |
|    value_loss           | 46.9     |
--------------------------------------
Iteration: 4080 | Episodes: 248900 | Median Reward: -4.87 | Max Reward: 49.45
Iteration: 4081 | Episodes: 248950 | Median Reward: 27.47 | Max Reward: 49.45
Iteration: 4082 | Episodes: 249000 | Median Reward: 33.75 | Max Reward: 49.45
Iteration: 4083 | Episodes: 249050 | Median Reward: 34.62 | Max Reward: 49.45
Iteration: 4083 | Episodes: 249100 | Median Reward: 34.33 | Max Reward: 49.45
Iteration: 4084 | Episodes: 249150 | Median Reward: 32.52 | Max Reward: 49.45
Iteration: 4085 | Episodes: 249200 | Median Reward: 38.01 | Max Reward: 49.45
Iteration: 4086 | Episodes: 249250 | Median Reward: 40.49 | Max Reward: 49.45
Iteration: 4087 | Episodes: 249300 | Median Reward: 42.39 | Max Reward: 49.45
Iteration: 4088 | Episodes: 249350 | Median Reward: 43.00 | Max Reward: 49.45
Iteration: 4088 | Episodes: 249400 | Median Reward: 40.91 | Max Reward: 49.45
Iteration: 4089 | Episodes: 249450 | Median Reward: 41.97 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.1     |
| time/                   |           |
|    fps                  | 369       |
|    iterations           | 4090      |
|    time_elapsed         | 68008     |
|    total_timesteps      | 25128960  |
| train/                  |           |
|    approx_kl            | 15.761517 |
|    clip_fraction        | 0.0896    |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.898     |
|    learning_rate        | 0.0001    |
|    loss                 | 24.2      |
|    n_updates            | 40890     |
|    policy_gradient_loss | 0.0109    |
|    std                  | 6.12      |
|    value_loss           | 57.4      |
---------------------------------------
Iteration: 4090 | Episodes: 249500 | Median Reward: 41.97 | Max Reward: 49.45
Iteration: 4091 | Episodes: 249550 | Median Reward: 37.90 | Max Reward: 49.45
Iteration: 4092 | Episodes: 249600 | Median Reward: 30.17 | Max Reward: 49.45
Iteration: 4092 | Episodes: 249650 | Median Reward: 37.22 | Max Reward: 49.45
Iteration: 4093 | Episodes: 249700 | Median Reward: 42.20 | Max Reward: 49.45
Iteration: 4094 | Episodes: 249750 | Median Reward: 41.76 | Max Reward: 49.45
Iteration: 4095 | Episodes: 249800 | Median Reward: 41.17 | Max Reward: 49.45
Iteration: 4096 | Episodes: 249850 | Median Reward: 39.87 | Max Reward: 49.45
Iteration: 4097 | Episodes: 249900 | Median Reward: 38.57 | Max Reward: 49.45
Iteration: 4097 | Episodes: 249950 | Median Reward: 39.47 | Max Reward: 49.45
Iteration: 4098 | Episodes: 250000 | Median Reward: 28.54 | Max Reward: 49.45
Iteration: 4099 | Episodes: 250050 | Median Reward: 41.76 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -72.9     |
| time/                   |           |
|    fps                  | 369       |
|    iterations           | 4100      |
|    time_elapsed         | 68181     |
|    total_timesteps      | 25190400  |
| train/                  |           |
|    approx_kl            | 29.956276 |
|    clip_fraction        | 0.182     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.906     |
|    learning_rate        | 0.0001    |
|    loss                 | 19.2      |
|    n_updates            | 40990     |
|    policy_gradient_loss | 0.0847    |
|    std                  | 6.14      |
|    value_loss           | 48        |
---------------------------------------
Iteration: 4100 | Episodes: 250100 | Median Reward: 41.76 | Max Reward: 49.45
Iteration: 4101 | Episodes: 250150 | Median Reward: 43.00 | Max Reward: 49.45
Iteration: 4102 | Episodes: 250200 | Median Reward: 42.63 | Max Reward: 49.45
Iteration: 4102 | Episodes: 250250 | Median Reward: 46.14 | Max Reward: 49.45
Iteration: 4103 | Episodes: 250300 | Median Reward: 44.58 | Max Reward: 49.45
Iteration: 4104 | Episodes: 250350 | Median Reward: 44.07 | Max Reward: 49.45
Iteration: 4105 | Episodes: 250400 | Median Reward: 42.29 | Max Reward: 49.45
Iteration: 4106 | Episodes: 250450 | Median Reward: 42.35 | Max Reward: 49.45
Iteration: 4106 | Episodes: 250500 | Median Reward: 43.31 | Max Reward: 49.45
Iteration: 4107 | Episodes: 250550 | Median Reward: 44.80 | Max Reward: 49.45
Iteration: 4108 | Episodes: 250600 | Median Reward: 45.14 | Max Reward: 49.45
Iteration: 4109 | Episodes: 250650 | Median Reward: 42.61 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -56.6    |
| time/                   |          |
|    fps                  | 369      |
|    iterations           | 4110     |
|    time_elapsed         | 68364    |
|    total_timesteps      | 25251840 |
| train/                  |          |
|    approx_kl            | 4.784232 |
|    clip_fraction        | 0.0617   |
|    clip_range           | 0.4      |
|    entropy_loss         | -126     |
|    explained_variance   | 0.891    |
|    learning_rate        | 0.0001   |
|    loss                 | 26.6     |
|    n_updates            | 41090    |
|    policy_gradient_loss | 0.00865  |
|    std                  | 6.18     |
|    value_loss           | 67.7     |
--------------------------------------
Iteration: 4110 | Episodes: 250700 | Median Reward: 42.34 | Max Reward: 49.45
Iteration: 4111 | Episodes: 250750 | Median Reward: 45.65 | Max Reward: 49.45
Iteration: 4111 | Episodes: 250800 | Median Reward: 43.25 | Max Reward: 49.45
Iteration: 4112 | Episodes: 250850 | Median Reward: 42.88 | Max Reward: 49.45
Iteration: 4113 | Episodes: 250900 | Median Reward: 42.58 | Max Reward: 49.45
Iteration: 4114 | Episodes: 250950 | Median Reward: 42.58 | Max Reward: 49.45
Iteration: 4115 | Episodes: 251000 | Median Reward: 42.78 | Max Reward: 49.45
Iteration: 4115 | Episodes: 251050 | Median Reward: 44.36 | Max Reward: 49.45
Iteration: 4116 | Episodes: 251100 | Median Reward: 41.89 | Max Reward: 49.45
Iteration: 4117 | Episodes: 251150 | Median Reward: 43.26 | Max Reward: 49.45
Iteration: 4118 | Episodes: 251200 | Median Reward: 43.17 | Max Reward: 49.45
Iteration: 4119 | Episodes: 251250 | Median Reward: 40.88 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -70.2    |
| time/                   |          |
|    fps                  | 369      |
|    iterations           | 4120     |
|    time_elapsed         | 68550    |
|    total_timesteps      | 25313280 |
| train/                  |          |
|    approx_kl            | 9.793165 |
|    clip_fraction        | 0.128    |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0.897    |
|    learning_rate        | 0.0001   |
|    loss                 | 21.4     |
|    n_updates            | 41190    |
|    policy_gradient_loss | 0.0276   |
|    std                  | 6.22     |
|    value_loss           | 56.6     |
--------------------------------------
Iteration: 4120 | Episodes: 251300 | Median Reward: 36.98 | Max Reward: 49.45
Iteration: 4120 | Episodes: 251350 | Median Reward: 33.05 | Max Reward: 49.45
Iteration: 4121 | Episodes: 251400 | Median Reward: 35.65 | Max Reward: 49.45
Iteration: 4122 | Episodes: 251450 | Median Reward: 39.28 | Max Reward: 49.45
Iteration: 4123 | Episodes: 251500 | Median Reward: 38.42 | Max Reward: 49.45
Iteration: 4124 | Episodes: 251550 | Median Reward: 24.20 | Max Reward: 49.45
Iteration: 4125 | Episodes: 251600 | Median Reward: 41.49 | Max Reward: 49.45
Iteration: 4125 | Episodes: 251650 | Median Reward: 35.98 | Max Reward: 49.45
Iteration: 4126 | Episodes: 251700 | Median Reward: 35.93 | Max Reward: 49.45
Iteration: 4127 | Episodes: 251750 | Median Reward: 34.18 | Max Reward: 49.45
Iteration: 4128 | Episodes: 251800 | Median Reward: 32.06 | Max Reward: 49.45
Iteration: 4129 | Episodes: 251850 | Median Reward: 31.29 | Max Reward: 49.45
Iteration: 4129 | Episodes: 251900 | Median Reward: 28.95 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -86.9      |
| time/                   |            |
|    fps                  | 369        |
|    iterations           | 4130       |
|    time_elapsed         | 68727      |
|    total_timesteps      | 25374720   |
| train/                  |            |
|    approx_kl            | 0.48386288 |
|    clip_fraction        | 0.0274     |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.911      |
|    learning_rate        | 0.0001     |
|    loss                 | 13.5       |
|    n_updates            | 41290      |
|    policy_gradient_loss | 0.00203    |
|    std                  | 6.25       |
|    value_loss           | 39.8       |
----------------------------------------
Iteration: 4130 | Episodes: 251950 | Median Reward: 28.41 | Max Reward: 49.45
Iteration: 4131 | Episodes: 252000 | Median Reward: 18.98 | Max Reward: 49.45
Iteration: 4132 | Episodes: 252050 | Median Reward: 29.75 | Max Reward: 49.45
Iteration: 4133 | Episodes: 252100 | Median Reward: 35.87 | Max Reward: 49.45
Iteration: 4134 | Episodes: 252150 | Median Reward: 36.28 | Max Reward: 49.45
Iteration: 4134 | Episodes: 252200 | Median Reward: 5.35 | Max Reward: 49.45
Iteration: 4135 | Episodes: 252250 | Median Reward: -3.40 | Max Reward: 49.45
Iteration: 4136 | Episodes: 252300 | Median Reward: -8.19 | Max Reward: 49.45
Iteration: 4137 | Episodes: 252350 | Median Reward: -11.19 | Max Reward: 49.45
Iteration: 4138 | Episodes: 252400 | Median Reward: -12.79 | Max Reward: 49.45
Iteration: 4139 | Episodes: 252450 | Median Reward: 25.28 | Max Reward: 49.45
Iteration: 4139 | Episodes: 252500 | Median Reward: 24.91 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -83.3    |
| time/                   |          |
|    fps                  | 369      |
|    iterations           | 4140     |
|    time_elapsed         | 68908    |
|    total_timesteps      | 25436160 |
| train/                  |          |
|    approx_kl            | 1.76069  |
|    clip_fraction        | 0.102    |
|    clip_range           | 0.4      |
|    entropy_loss         | -125     |
|    explained_variance   | 0.909    |
|    learning_rate        | 0.0001   |
|    loss                 | 13.3     |
|    n_updates            | 41390    |
|    policy_gradient_loss | 0.0257   |
|    std                  | 6.26     |
|    value_loss           | 42       |
--------------------------------------
Iteration: 4140 | Episodes: 252550 | Median Reward: 29.13 | Max Reward: 49.45
Iteration: 4141 | Episodes: 252600 | Median Reward: 35.26 | Max Reward: 49.45
Iteration: 4142 | Episodes: 252650 | Median Reward: 25.05 | Max Reward: 49.45
Iteration: 4143 | Episodes: 252700 | Median Reward: 31.42 | Max Reward: 49.45
Iteration: 4143 | Episodes: 252750 | Median Reward: 34.72 | Max Reward: 49.45
Iteration: 4144 | Episodes: 252800 | Median Reward: 13.84 | Max Reward: 49.45
Iteration: 4145 | Episodes: 252850 | Median Reward: -8.31 | Max Reward: 49.45
Iteration: 4146 | Episodes: 252900 | Median Reward: 29.03 | Max Reward: 49.45
Iteration: 4147 | Episodes: 252950 | Median Reward: 29.36 | Max Reward: 49.45
Iteration: 4148 | Episodes: 253000 | Median Reward: 11.20 | Max Reward: 49.45
Iteration: 4148 | Episodes: 253050 | Median Reward: 7.20 | Max Reward: 49.45
Iteration: 4149 | Episodes: 253100 | Median Reward: 10.10 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -92.6     |
| time/                   |           |
|    fps                  | 368       |
|    iterations           | 4150      |
|    time_elapsed         | 69105     |
|    total_timesteps      | 25497600  |
| train/                  |           |
|    approx_kl            | 0.4913339 |
|    clip_fraction        | 0.0448    |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.927     |
|    learning_rate        | 0.0001    |
|    loss                 | 10.9      |
|    n_updates            | 41490     |
|    policy_gradient_loss | 0.000872  |
|    std                  | 6.27      |
|    value_loss           | 33.1      |
---------------------------------------
Iteration: 4150 | Episodes: 253150 | Median Reward: -8.87 | Max Reward: 49.45
Iteration: 4151 | Episodes: 253200 | Median Reward: -14.67 | Max Reward: 49.45
Iteration: 4152 | Episodes: 253250 | Median Reward: -15.84 | Max Reward: 49.45
Iteration: 4152 | Episodes: 253300 | Median Reward: -10.73 | Max Reward: 49.45
Iteration: 4153 | Episodes: 253350 | Median Reward: 20.69 | Max Reward: 49.45
Iteration: 4154 | Episodes: 253400 | Median Reward: 22.59 | Max Reward: 49.45
Iteration: 4155 | Episodes: 253450 | Median Reward: 5.40 | Max Reward: 49.45
Iteration: 4156 | Episodes: 253500 | Median Reward: -0.56 | Max Reward: 49.45
Iteration: 4157 | Episodes: 253550 | Median Reward: 3.00 | Max Reward: 49.45
Iteration: 4157 | Episodes: 253600 | Median Reward: 25.34 | Max Reward: 49.45
Iteration: 4158 | Episodes: 253650 | Median Reward: -3.13 | Max Reward: 49.45
Iteration: 4159 | Episodes: 253700 | Median Reward: -19.66 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -109       |
| time/                   |            |
|    fps                  | 368        |
|    iterations           | 4160       |
|    time_elapsed         | 69285      |
|    total_timesteps      | 25559040   |
| train/                  |            |
|    approx_kl            | 0.27431452 |
|    clip_fraction        | 0.0407     |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.934      |
|    learning_rate        | 0.0001     |
|    loss                 | 5.12       |
|    n_updates            | 41590      |
|    policy_gradient_loss | 0.00465    |
|    std                  | 6.28       |
|    value_loss           | 32.7       |
----------------------------------------
Iteration: 4160 | Episodes: 253750 | Median Reward: -18.79 | Max Reward: 49.45
Iteration: 4161 | Episodes: 253800 | Median Reward: -18.42 | Max Reward: 49.45
Iteration: 4162 | Episodes: 253850 | Median Reward: -12.66 | Max Reward: 49.45
Iteration: 4162 | Episodes: 253900 | Median Reward: -10.65 | Max Reward: 49.45
Iteration: 4163 | Episodes: 253950 | Median Reward: -9.49 | Max Reward: 49.45
Iteration: 4164 | Episodes: 254000 | Median Reward: -9.49 | Max Reward: 49.45
Iteration: 4165 | Episodes: 254050 | Median Reward: -0.90 | Max Reward: 49.45
Iteration: 4166 | Episodes: 254100 | Median Reward: -3.83 | Max Reward: 49.45
Iteration: 4166 | Episodes: 254150 | Median Reward: -8.82 | Max Reward: 49.45
Iteration: 4167 | Episodes: 254200 | Median Reward: 9.60 | Max Reward: 49.45
Iteration: 4168 | Episodes: 254250 | Median Reward: -5.88 | Max Reward: 49.45
Iteration: 4169 | Episodes: 254300 | Median Reward: -14.15 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -108      |
| time/                   |           |
|    fps                  | 368       |
|    iterations           | 4170      |
|    time_elapsed         | 69469     |
|    total_timesteps      | 25620480  |
| train/                  |           |
|    approx_kl            | 1.0747902 |
|    clip_fraction        | 0.0718    |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.902     |
|    learning_rate        | 0.0001    |
|    loss                 | 10.6      |
|    n_updates            | 41690     |
|    policy_gradient_loss | 0.0156    |
|    std                  | 6.28      |
|    value_loss           | 38.1      |
---------------------------------------
Iteration: 4170 | Episodes: 254350 | Median Reward: -14.15 | Max Reward: 49.45
Iteration: 4171 | Episodes: 254400 | Median Reward: 1.72 | Max Reward: 49.45
Iteration: 4171 | Episodes: 254450 | Median Reward: -6.59 | Max Reward: 49.45
Iteration: 4172 | Episodes: 254500 | Median Reward: 10.02 | Max Reward: 49.45
Iteration: 4173 | Episodes: 254550 | Median Reward: -8.91 | Max Reward: 49.45
Iteration: 4174 | Episodes: 254600 | Median Reward: -17.39 | Max Reward: 49.45
Iteration: 4175 | Episodes: 254650 | Median Reward: -24.47 | Max Reward: 49.45
Iteration: 4175 | Episodes: 254700 | Median Reward: -26.43 | Max Reward: 49.45
Iteration: 4176 | Episodes: 254750 | Median Reward: -10.20 | Max Reward: 49.45
Iteration: 4177 | Episodes: 254800 | Median Reward: -1.62 | Max Reward: 49.45
Iteration: 4178 | Episodes: 254850 | Median Reward: 3.81 | Max Reward: 49.45
Iteration: 4179 | Episodes: 254900 | Median Reward: 3.81 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -103      |
| time/                   |           |
|    fps                  | 368       |
|    iterations           | 4180      |
|    time_elapsed         | 69661     |
|    total_timesteps      | 25681920  |
| train/                  |           |
|    approx_kl            | 2.4819381 |
|    clip_fraction        | 0.0956    |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.91      |
|    learning_rate        | 0.0001    |
|    loss                 | 15.1      |
|    n_updates            | 41790     |
|    policy_gradient_loss | 0.0381    |
|    std                  | 6.29      |
|    value_loss           | 38        |
---------------------------------------
Iteration: 4180 | Episodes: 254950 | Median Reward: -11.39 | Max Reward: 49.45
Iteration: 4180 | Episodes: 255000 | Median Reward: -0.89 | Max Reward: 49.45
Iteration: 4181 | Episodes: 255050 | Median Reward: -7.56 | Max Reward: 49.45
Iteration: 4182 | Episodes: 255100 | Median Reward: -15.65 | Max Reward: 49.45
Iteration: 4183 | Episodes: 255150 | Median Reward: -16.83 | Max Reward: 49.45
Iteration: 4184 | Episodes: 255200 | Median Reward: -6.71 | Max Reward: 49.45
Iteration: 4185 | Episodes: 255250 | Median Reward: -13.84 | Max Reward: 49.45
Iteration: 4185 | Episodes: 255300 | Median Reward: -20.34 | Max Reward: 49.45
Iteration: 4186 | Episodes: 255350 | Median Reward: -20.27 | Max Reward: 49.45
Iteration: 4187 | Episodes: 255400 | Median Reward: -22.77 | Max Reward: 49.45
Iteration: 4188 | Episodes: 255450 | Median Reward: -21.56 | Max Reward: 49.45
Iteration: 4189 | Episodes: 255500 | Median Reward: -14.97 | Max Reward: 49.45
Iteration: 4189 | Episodes: 255550 | Median Reward: -26.32 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -115      |
| time/                   |           |
|    fps                  | 368       |
|    iterations           | 4190      |
|    time_elapsed         | 69841     |
|    total_timesteps      | 25743360  |
| train/                  |           |
|    approx_kl            | 1.1323742 |
|    clip_fraction        | 0.0479    |
|    clip_range           | 0.4       |
|    entropy_loss         | -123      |
|    explained_variance   | 0.894     |
|    learning_rate        | 0.0001    |
|    loss                 | 10.1      |
|    n_updates            | 41890     |
|    policy_gradient_loss | -0.00099  |
|    std                  | 6.31      |
|    value_loss           | 35.6      |
---------------------------------------
Iteration: 4190 | Episodes: 255600 | Median Reward: -7.15 | Max Reward: 49.45
Iteration: 4191 | Episodes: 255650 | Median Reward: -21.94 | Max Reward: 49.45
Iteration: 4192 | Episodes: 255700 | Median Reward: -21.64 | Max Reward: 49.45
Iteration: 4193 | Episodes: 255750 | Median Reward: -16.42 | Max Reward: 49.45
Iteration: 4194 | Episodes: 255800 | Median Reward: -16.70 | Max Reward: 49.45
Iteration: 4194 | Episodes: 255850 | Median Reward: -21.06 | Max Reward: 49.45
Iteration: 4195 | Episodes: 255900 | Median Reward: -10.67 | Max Reward: 49.45
Iteration: 4196 | Episodes: 255950 | Median Reward: -11.71 | Max Reward: 49.45
Iteration: 4197 | Episodes: 256000 | Median Reward: -14.67 | Max Reward: 49.45
Iteration: 4198 | Episodes: 256050 | Median Reward: -11.40 | Max Reward: 49.45
Iteration: 4198 | Episodes: 256100 | Median Reward: 0.32 | Max Reward: 49.45
Iteration: 4199 | Episodes: 256150 | Median Reward: -17.31 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -108       |
| time/                   |            |
|    fps                  | 368        |
|    iterations           | 4200       |
|    time_elapsed         | 70022      |
|    total_timesteps      | 25804800   |
| train/                  |            |
|    approx_kl            | 0.15006106 |
|    clip_fraction        | 0.0396     |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | 0.908      |
|    learning_rate        | 0.0001     |
|    loss                 | 7.06       |
|    n_updates            | 41990      |
|    policy_gradient_loss | -0.00594   |
|    std                  | 6.32       |
|    value_loss           | 28.3       |
----------------------------------------
Iteration: 4200 | Episodes: 256200 | Median Reward: -16.93 | Max Reward: 49.45
Iteration: 4201 | Episodes: 256250 | Median Reward: -18.16 | Max Reward: 49.45
Iteration: 4202 | Episodes: 256300 | Median Reward: -20.44 | Max Reward: 49.45
Iteration: 4203 | Episodes: 256350 | Median Reward: -10.40 | Max Reward: 49.45
Iteration: 4203 | Episodes: 256400 | Median Reward: -13.69 | Max Reward: 49.45
Iteration: 4204 | Episodes: 256450 | Median Reward: -16.87 | Max Reward: 49.45
Iteration: 4205 | Episodes: 256500 | Median Reward: -13.10 | Max Reward: 49.45
Iteration: 4206 | Episodes: 256550 | Median Reward: -4.34 | Max Reward: 49.45
Iteration: 4207 | Episodes: 256600 | Median Reward: -6.60 | Max Reward: 49.45
Iteration: 4208 | Episodes: 256650 | Median Reward: -13.78 | Max Reward: 49.45
Iteration: 4208 | Episodes: 256700 | Median Reward: -0.55 | Max Reward: 49.45
Iteration: 4209 | Episodes: 256750 | Median Reward: -1.00 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -106       |
| time/                   |            |
|    fps                  | 368        |
|    iterations           | 4210       |
|    time_elapsed         | 70200      |
|    total_timesteps      | 25866240   |
| train/                  |            |
|    approx_kl            | 0.10703318 |
|    clip_fraction        | 0.0161     |
|    clip_range           | 0.4        |
|    entropy_loss         | -124       |
|    explained_variance   | 0.844      |
|    learning_rate        | 0.0001     |
|    loss                 | 12.2       |
|    n_updates            | 42090      |
|    policy_gradient_loss | 0.00285    |
|    std                  | 6.33       |
|    value_loss           | 39.9       |
----------------------------------------
Iteration: 4210 | Episodes: 256800 | Median Reward: -17.50 | Max Reward: 49.45
Iteration: 4211 | Episodes: 256850 | Median Reward: -15.82 | Max Reward: 49.45
Iteration: 4212 | Episodes: 256900 | Median Reward: -8.21 | Max Reward: 49.45
Iteration: 4212 | Episodes: 256950 | Median Reward: -5.59 | Max Reward: 49.45
Iteration: 4213 | Episodes: 257000 | Median Reward: 4.86 | Max Reward: 49.45
Iteration: 4214 | Episodes: 257050 | Median Reward: 2.30 | Max Reward: 49.45
Iteration: 4215 | Episodes: 257100 | Median Reward: -15.35 | Max Reward: 49.45
Iteration: 4216 | Episodes: 257150 | Median Reward: -21.91 | Max Reward: 49.45
Iteration: 4217 | Episodes: 257200 | Median Reward: -13.54 | Max Reward: 49.45
Iteration: 4217 | Episodes: 257250 | Median Reward: -14.68 | Max Reward: 49.45
Iteration: 4218 | Episodes: 257300 | Median Reward: -14.35 | Max Reward: 49.45
Iteration: 4219 | Episodes: 257350 | Median Reward: -20.97 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -119       |
| time/                   |            |
|    fps                  | 368        |
|    iterations           | 4220       |
|    time_elapsed         | 70391      |
|    total_timesteps      | 25927680   |
| train/                  |            |
|    approx_kl            | 0.12914185 |
|    clip_fraction        | 0.0219     |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.905      |
|    learning_rate        | 0.0001     |
|    loss                 | 4.47       |
|    n_updates            | 42190      |
|    policy_gradient_loss | -0.00726   |
|    std                  | 6.34       |
|    value_loss           | 26.9       |
----------------------------------------
Iteration: 4220 | Episodes: 257400 | Median Reward: -13.26 | Max Reward: 49.45
Iteration: 4221 | Episodes: 257450 | Median Reward: -7.77 | Max Reward: 49.45
Iteration: 4222 | Episodes: 257500 | Median Reward: -0.45 | Max Reward: 49.45
Iteration: 4222 | Episodes: 257550 | Median Reward: -5.12 | Max Reward: 49.45
Iteration: 4223 | Episodes: 257600 | Median Reward: -20.43 | Max Reward: 49.45
Iteration: 4224 | Episodes: 257650 | Median Reward: -9.69 | Max Reward: 49.45
Iteration: 4225 | Episodes: 257700 | Median Reward: -2.10 | Max Reward: 49.45
Iteration: 4226 | Episodes: 257750 | Median Reward: -14.16 | Max Reward: 49.45
Iteration: 4226 | Episodes: 257800 | Median Reward: -17.77 | Max Reward: 49.45
Iteration: 4227 | Episodes: 257850 | Median Reward: -18.39 | Max Reward: 49.45
Iteration: 4228 | Episodes: 257900 | Median Reward: -18.19 | Max Reward: 49.45
Iteration: 4229 | Episodes: 257950 | Median Reward: -18.91 | Max Reward: 49.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -115        |
| time/                   |             |
|    fps                  | 368         |
|    iterations           | 4230        |
|    time_elapsed         | 70571       |
|    total_timesteps      | 25989120    |
| train/                  |             |
|    approx_kl            | 0.057911877 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.4         |
|    entropy_loss         | -126        |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.78        |
|    n_updates            | 42290       |
|    policy_gradient_loss | -0.000245   |
|    std                  | 6.36        |
|    value_loss           | 20          |
-----------------------------------------
Iteration: 4230 | Episodes: 258000 | Median Reward: -17.72 | Max Reward: 49.45
Iteration: 4231 | Episodes: 258050 | Median Reward: -21.22 | Max Reward: 49.45
Iteration: 4231 | Episodes: 258100 | Median Reward: -12.98 | Max Reward: 49.45
Iteration: 4232 | Episodes: 258150 | Median Reward: -12.46 | Max Reward: 49.45
Iteration: 4233 | Episodes: 258200 | Median Reward: -11.95 | Max Reward: 49.45
Iteration: 4234 | Episodes: 258250 | Median Reward: -9.07 | Max Reward: 49.45
Iteration: 4235 | Episodes: 258300 | Median Reward: -10.48 | Max Reward: 49.45
Iteration: 4235 | Episodes: 258350 | Median Reward: -20.00 | Max Reward: 49.45
Iteration: 4236 | Episodes: 258400 | Median Reward: -19.85 | Max Reward: 49.45
Iteration: 4237 | Episodes: 258450 | Median Reward: -23.94 | Max Reward: 49.45
Iteration: 4238 | Episodes: 258500 | Median Reward: -23.05 | Max Reward: 49.45
Iteration: 4239 | Episodes: 258550 | Median Reward: -8.80 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -107       |
| time/                   |            |
|    fps                  | 368        |
|    iterations           | 4240       |
|    time_elapsed         | 70752      |
|    total_timesteps      | 26050560   |
| train/                  |            |
|    approx_kl            | 0.49071878 |
|    clip_fraction        | 0.0345     |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.71       |
|    n_updates            | 42390      |
|    policy_gradient_loss | -0.0116    |
|    std                  | 6.37       |
|    value_loss           | 23.6       |
----------------------------------------
Iteration: 4240 | Episodes: 258600 | Median Reward: -12.57 | Max Reward: 49.45
Iteration: 4240 | Episodes: 258650 | Median Reward: -14.66 | Max Reward: 49.45
Iteration: 4241 | Episodes: 258700 | Median Reward: -9.79 | Max Reward: 49.45
Iteration: 4242 | Episodes: 258750 | Median Reward: -13.80 | Max Reward: 49.45
Iteration: 4243 | Episodes: 258800 | Median Reward: -15.26 | Max Reward: 49.45
Iteration: 4244 | Episodes: 258850 | Median Reward: -15.37 | Max Reward: 49.45
Iteration: 4245 | Episodes: 258900 | Median Reward: -15.48 | Max Reward: 49.45
Iteration: 4245 | Episodes: 258950 | Median Reward: -18.69 | Max Reward: 49.45
Iteration: 4246 | Episodes: 259000 | Median Reward: -13.05 | Max Reward: 49.45
Iteration: 4247 | Episodes: 259050 | Median Reward: -16.98 | Max Reward: 49.45
Iteration: 4248 | Episodes: 259100 | Median Reward: -18.81 | Max Reward: 49.45
Iteration: 4249 | Episodes: 259150 | Median Reward: -10.33 | Max Reward: 49.45
Iteration: 4249 | Episodes: 259200 | Median Reward: -9.01 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -108       |
| time/                   |            |
|    fps                  | 368        |
|    iterations           | 4250       |
|    time_elapsed         | 70941      |
|    total_timesteps      | 26112000   |
| train/                  |            |
|    approx_kl            | 0.39080292 |
|    clip_fraction        | 0.056      |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.919      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.592      |
|    n_updates            | 42490      |
|    policy_gradient_loss | 0.00948    |
|    std                  | 6.39       |
|    value_loss           | 22.4       |
----------------------------------------
Iteration: 4250 | Episodes: 259250 | Median Reward: -11.99 | Max Reward: 49.45
Iteration: 4251 | Episodes: 259300 | Median Reward: -12.98 | Max Reward: 49.45
Iteration: 4252 | Episodes: 259350 | Median Reward: -4.71 | Max Reward: 49.45
Iteration: 4253 | Episodes: 259400 | Median Reward: -6.49 | Max Reward: 49.45
Iteration: 4254 | Episodes: 259450 | Median Reward: -10.50 | Max Reward: 49.45
Iteration: 4254 | Episodes: 259500 | Median Reward: 28.20 | Max Reward: 49.45
Iteration: 4255 | Episodes: 259550 | Median Reward: -6.79 | Max Reward: 49.45
Iteration: 4256 | Episodes: 259600 | Median Reward: -17.54 | Max Reward: 49.45
Iteration: 4257 | Episodes: 259650 | Median Reward: -16.05 | Max Reward: 49.45
Iteration: 4258 | Episodes: 259700 | Median Reward: -10.53 | Max Reward: 49.45
Iteration: 4258 | Episodes: 259750 | Median Reward: -15.89 | Max Reward: 49.45
Iteration: 4259 | Episodes: 259800 | Median Reward: -3.79 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -104      |
| time/                   |           |
|    fps                  | 368       |
|    iterations           | 4260      |
|    time_elapsed         | 71122     |
|    total_timesteps      | 26173440  |
| train/                  |           |
|    approx_kl            | 16.295517 |
|    clip_fraction        | 0.17      |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.938     |
|    learning_rate        | 0.0001    |
|    loss                 | 8.43      |
|    n_updates            | 42590     |
|    policy_gradient_loss | -0.0131   |
|    std                  | 6.4       |
|    value_loss           | 23.1      |
---------------------------------------
Iteration: 4260 | Episodes: 259850 | Median Reward: -7.03 | Max Reward: 49.45
Iteration: 4261 | Episodes: 259900 | Median Reward: -3.50 | Max Reward: 49.45
Iteration: 4262 | Episodes: 259950 | Median Reward: -18.28 | Max Reward: 49.45
Iteration: 4263 | Episodes: 260000 | Median Reward: -18.89 | Max Reward: 49.45
Iteration: 4263 | Episodes: 260050 | Median Reward: -1.16 | Max Reward: 49.45
Iteration: 4264 | Episodes: 260100 | Median Reward: -12.64 | Max Reward: 49.45
Iteration: 4265 | Episodes: 260150 | Median Reward: -17.49 | Max Reward: 49.45
Iteration: 4266 | Episodes: 260200 | Median Reward: -18.53 | Max Reward: 49.45
Iteration: 4267 | Episodes: 260250 | Median Reward: -15.98 | Max Reward: 49.45
Iteration: 4268 | Episodes: 260300 | Median Reward: -5.77 | Max Reward: 49.45
Iteration: 4268 | Episodes: 260350 | Median Reward: -8.78 | Max Reward: 49.45
Iteration: 4269 | Episodes: 260400 | Median Reward: -16.87 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -111       |
| time/                   |            |
|    fps                  | 367        |
|    iterations           | 4270       |
|    time_elapsed         | 71303      |
|    total_timesteps      | 26234880   |
| train/                  |            |
|    approx_kl            | 0.47838032 |
|    clip_fraction        | 0.0452     |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.0001     |
|    loss                 | 10.1       |
|    n_updates            | 42690      |
|    policy_gradient_loss | 0.0114     |
|    std                  | 6.41       |
|    value_loss           | 24.1       |
----------------------------------------
Iteration: 4270 | Episodes: 260450 | Median Reward: -15.73 | Max Reward: 49.45
Iteration: 4271 | Episodes: 260500 | Median Reward: -11.95 | Max Reward: 49.45
Iteration: 4272 | Episodes: 260550 | Median Reward: -4.12 | Max Reward: 49.45
Iteration: 4272 | Episodes: 260600 | Median Reward: 6.56 | Max Reward: 49.45
Iteration: 4273 | Episodes: 260650 | Median Reward: -10.66 | Max Reward: 49.45
Iteration: 4274 | Episodes: 260700 | Median Reward: -10.90 | Max Reward: 49.45
Iteration: 4275 | Episodes: 260750 | Median Reward: -7.74 | Max Reward: 49.45
Iteration: 4276 | Episodes: 260800 | Median Reward: -9.67 | Max Reward: 49.45
Iteration: 4277 | Episodes: 260850 | Median Reward: -8.33 | Max Reward: 49.45
Iteration: 4277 | Episodes: 260900 | Median Reward: -3.17 | Max Reward: 49.45
Iteration: 4278 | Episodes: 260950 | Median Reward: -12.28 | Max Reward: 49.45
Iteration: 4279 | Episodes: 261000 | Median Reward: -17.76 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -117       |
| time/                   |            |
|    fps                  | 367        |
|    iterations           | 4280       |
|    time_elapsed         | 71482      |
|    total_timesteps      | 26296320   |
| train/                  |            |
|    approx_kl            | 0.05776096 |
|    clip_fraction        | 0.0211     |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.843      |
|    n_updates            | 42790      |
|    policy_gradient_loss | 0.00301    |
|    std                  | 6.42       |
|    value_loss           | 20.9       |
----------------------------------------
Iteration: 4280 | Episodes: 261050 | Median Reward: -21.05 | Max Reward: 49.45
Iteration: 4281 | Episodes: 261100 | Median Reward: -16.62 | Max Reward: 49.45
Iteration: 4282 | Episodes: 261150 | Median Reward: -12.64 | Max Reward: 49.45
Iteration: 4282 | Episodes: 261200 | Median Reward: -2.93 | Max Reward: 49.45
Iteration: 4283 | Episodes: 261250 | Median Reward: -13.49 | Max Reward: 49.45
Iteration: 4284 | Episodes: 261300 | Median Reward: -16.48 | Max Reward: 49.45
Iteration: 4285 | Episodes: 261350 | Median Reward: -13.82 | Max Reward: 49.45
Iteration: 4286 | Episodes: 261400 | Median Reward: -0.14 | Max Reward: 49.45
Iteration: 4286 | Episodes: 261450 | Median Reward: 1.71 | Max Reward: 49.45
Iteration: 4287 | Episodes: 261500 | Median Reward: 1.12 | Max Reward: 49.45
Iteration: 4288 | Episodes: 261550 | Median Reward: -14.27 | Max Reward: 49.45
Iteration: 4289 | Episodes: 261600 | Median Reward: -10.98 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -112      |
| time/                   |           |
|    fps                  | 367       |
|    iterations           | 4290      |
|    time_elapsed         | 71673     |
|    total_timesteps      | 26357760  |
| train/                  |           |
|    approx_kl            | 1.4442729 |
|    clip_fraction        | 0.074     |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.923     |
|    learning_rate        | 0.0001    |
|    loss                 | 9.86      |
|    n_updates            | 42890     |
|    policy_gradient_loss | 0.00232   |
|    std                  | 6.44      |
|    value_loss           | 23.9      |
---------------------------------------
Iteration: 4290 | Episodes: 261650 | Median Reward: -7.89 | Max Reward: 49.45
Iteration: 4291 | Episodes: 261700 | Median Reward: -18.80 | Max Reward: 49.45
Iteration: 4291 | Episodes: 261750 | Median Reward: -18.62 | Max Reward: 49.45
Iteration: 4292 | Episodes: 261800 | Median Reward: -10.99 | Max Reward: 49.45
Iteration: 4293 | Episodes: 261850 | Median Reward: -11.34 | Max Reward: 49.45
Iteration: 4294 | Episodes: 261900 | Median Reward: -13.26 | Max Reward: 49.45
Iteration: 4295 | Episodes: 261950 | Median Reward: -16.67 | Max Reward: 49.45
Iteration: 4295 | Episodes: 262000 | Median Reward: -13.83 | Max Reward: 49.45
Iteration: 4296 | Episodes: 262050 | Median Reward: -7.36 | Max Reward: 49.45
Iteration: 4297 | Episodes: 262100 | Median Reward: -23.22 | Max Reward: 49.45
Iteration: 4298 | Episodes: 262150 | Median Reward: -16.39 | Max Reward: 49.45
Iteration: 4299 | Episodes: 262200 | Median Reward: -13.85 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -98.1     |
| time/                   |           |
|    fps                  | 367       |
|    iterations           | 4300      |
|    time_elapsed         | 71852     |
|    total_timesteps      | 26419200  |
| train/                  |           |
|    approx_kl            | 2.6524816 |
|    clip_fraction        | 0.0768    |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0001    |
|    loss                 | 12.9      |
|    n_updates            | 42990     |
|    policy_gradient_loss | -0.00271  |
|    std                  | 6.45      |
|    value_loss           | 34        |
---------------------------------------
Iteration: 4300 | Episodes: 262250 | Median Reward: 5.19 | Max Reward: 49.45
Iteration: 4300 | Episodes: 262300 | Median Reward: -15.93 | Max Reward: 49.45
Iteration: 4301 | Episodes: 262350 | Median Reward: -13.23 | Max Reward: 49.45
Iteration: 4302 | Episodes: 262400 | Median Reward: -15.74 | Max Reward: 49.45
Iteration: 4303 | Episodes: 262450 | Median Reward: 9.07 | Max Reward: 49.45
Iteration: 4304 | Episodes: 262500 | Median Reward: 3.81 | Max Reward: 49.45
Iteration: 4305 | Episodes: 262550 | Median Reward: -10.72 | Max Reward: 49.45
Iteration: 4305 | Episodes: 262600 | Median Reward: -14.79 | Max Reward: 49.45
Iteration: 4306 | Episodes: 262650 | Median Reward: 13.26 | Max Reward: 49.45
Iteration: 4307 | Episodes: 262700 | Median Reward: 8.56 | Max Reward: 49.45
Iteration: 4308 | Episodes: 262750 | Median Reward: 8.56 | Max Reward: 49.45
Iteration: 4309 | Episodes: 262800 | Median Reward: 16.43 | Max Reward: 49.45
Iteration: 4309 | Episodes: 262850 | Median Reward: 23.86 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -86.2     |
| time/                   |           |
|    fps                  | 367       |
|    iterations           | 4310      |
|    time_elapsed         | 72031     |
|    total_timesteps      | 26480640  |
| train/                  |           |
|    approx_kl            | 19.863659 |
|    clip_fraction        | 0.215     |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.908     |
|    learning_rate        | 0.0001    |
|    loss                 | 4.43      |
|    n_updates            | 43090     |
|    policy_gradient_loss | 0.00373   |
|    std                  | 6.47      |
|    value_loss           | 45        |
---------------------------------------
Iteration: 4310 | Episodes: 262900 | Median Reward: 6.11 | Max Reward: 49.45
Iteration: 4311 | Episodes: 262950 | Median Reward: -13.06 | Max Reward: 49.45
Iteration: 4312 | Episodes: 263000 | Median Reward: -13.06 | Max Reward: 49.45
Iteration: 4313 | Episodes: 263050 | Median Reward: -15.77 | Max Reward: 49.45
Iteration: 4314 | Episodes: 263100 | Median Reward: -19.05 | Max Reward: 49.45
Iteration: 4314 | Episodes: 263150 | Median Reward: 15.07 | Max Reward: 49.45
Iteration: 4315 | Episodes: 263200 | Median Reward: 21.83 | Max Reward: 49.45
Iteration: 4316 | Episodes: 263250 | Median Reward: 21.02 | Max Reward: 49.45
Iteration: 4317 | Episodes: 263300 | Median Reward: 2.41 | Max Reward: 49.45
Iteration: 4318 | Episodes: 263350 | Median Reward: -9.81 | Max Reward: 49.45
Iteration: 4319 | Episodes: 263400 | Median Reward: -4.72 | Max Reward: 49.45
Iteration: 4319 | Episodes: 263450 | Median Reward: -1.94 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -106      |
| time/                   |           |
|    fps                  | 367       |
|    iterations           | 4320      |
|    time_elapsed         | 72224     |
|    total_timesteps      | 26542080  |
| train/                  |           |
|    approx_kl            | 1.0107819 |
|    clip_fraction        | 0.0569    |
|    clip_range           | 0.4       |
|    entropy_loss         | -126      |
|    explained_variance   | 0.955     |
|    learning_rate        | 0.0001    |
|    loss                 | -1.99     |
|    n_updates            | 43190     |
|    policy_gradient_loss | 0.0131    |
|    std                  | 6.48      |
|    value_loss           | 12.3      |
---------------------------------------
Iteration: 4320 | Episodes: 263500 | Median Reward: -1.85 | Max Reward: 49.45
Iteration: 4321 | Episodes: 263550 | Median Reward: -16.21 | Max Reward: 49.45
Iteration: 4322 | Episodes: 263600 | Median Reward: -7.56 | Max Reward: 49.45
Iteration: 4323 | Episodes: 263650 | Median Reward: -13.45 | Max Reward: 49.45
Iteration: 4323 | Episodes: 263700 | Median Reward: -11.94 | Max Reward: 49.45
Iteration: 4324 | Episodes: 263750 | Median Reward: 18.65 | Max Reward: 49.45
Iteration: 4325 | Episodes: 263800 | Median Reward: 36.06 | Max Reward: 49.45
Iteration: 4326 | Episodes: 263850 | Median Reward: 28.02 | Max Reward: 49.45
Iteration: 4327 | Episodes: 263900 | Median Reward: 28.02 | Max Reward: 49.45
Iteration: 4328 | Episodes: 263950 | Median Reward: 39.16 | Max Reward: 49.45
Iteration: 4328 | Episodes: 264000 | Median Reward: 14.93 | Max Reward: 49.45
Iteration: 4329 | Episodes: 264050 | Median Reward: -7.41 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -98.5    |
| time/                   |          |
|    fps                  | 367      |
|    iterations           | 4330     |
|    time_elapsed         | 72404    |
|    total_timesteps      | 26603520 |
| train/                  |          |
|    approx_kl            | 7.885057 |
|    clip_fraction        | 0.0436   |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | 0.87     |
|    learning_rate        | 0.0001   |
|    loss                 | 2.01     |
|    n_updates            | 43290    |
|    policy_gradient_loss | 0.0138   |
|    std                  | 6.49     |
|    value_loss           | 57.4     |
--------------------------------------
Iteration: 4330 | Episodes: 264100 | Median Reward: 24.55 | Max Reward: 49.45
Iteration: 4331 | Episodes: 264150 | Median Reward: 25.46 | Max Reward: 49.45
Iteration: 4332 | Episodes: 264200 | Median Reward: 30.87 | Max Reward: 49.45
Iteration: 4332 | Episodes: 264250 | Median Reward: 39.11 | Max Reward: 49.45
Iteration: 4333 | Episodes: 264300 | Median Reward: 32.41 | Max Reward: 49.45
Iteration: 4334 | Episodes: 264350 | Median Reward: 35.94 | Max Reward: 49.45
Iteration: 4335 | Episodes: 264400 | Median Reward: 39.65 | Max Reward: 49.45
Iteration: 4336 | Episodes: 264450 | Median Reward: 42.00 | Max Reward: 49.45
Iteration: 4337 | Episodes: 264500 | Median Reward: 34.30 | Max Reward: 49.45
Iteration: 4337 | Episodes: 264550 | Median Reward: 40.61 | Max Reward: 49.45
Iteration: 4338 | Episodes: 264600 | Median Reward: 38.41 | Max Reward: 49.45
Iteration: 4339 | Episodes: 264650 | Median Reward: 38.36 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -63.5      |
| time/                   |            |
|    fps                  | 367        |
|    iterations           | 4340       |
|    time_elapsed         | 72585      |
|    total_timesteps      | 26664960   |
| train/                  |            |
|    approx_kl            | 13.3695135 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.0001     |
|    loss                 | 42.9       |
|    n_updates            | 43390      |
|    policy_gradient_loss | 0.0441     |
|    std                  | 6.52       |
|    value_loss           | 85.6       |
----------------------------------------
Iteration: 4340 | Episodes: 264700 | Median Reward: 38.31 | Max Reward: 49.45
Iteration: 4341 | Episodes: 264750 | Median Reward: 35.76 | Max Reward: 49.45
Iteration: 4342 | Episodes: 264800 | Median Reward: 35.44 | Max Reward: 49.45
Iteration: 4342 | Episodes: 264850 | Median Reward: 37.53 | Max Reward: 49.45
Iteration: 4343 | Episodes: 264900 | Median Reward: 38.53 | Max Reward: 49.45
Iteration: 4344 | Episodes: 264950 | Median Reward: 38.48 | Max Reward: 49.45
Iteration: 4345 | Episodes: 265000 | Median Reward: 39.83 | Max Reward: 49.45
Iteration: 4346 | Episodes: 265050 | Median Reward: 44.13 | Max Reward: 49.45
Iteration: 4346 | Episodes: 265100 | Median Reward: 43.81 | Max Reward: 49.45
Iteration: 4347 | Episodes: 265150 | Median Reward: 44.74 | Max Reward: 49.45
Iteration: 4348 | Episodes: 265200 | Median Reward: 43.95 | Max Reward: 49.45
Iteration: 4349 | Episodes: 265250 | Median Reward: 44.79 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -65      |
| time/                   |          |
|    fps                  | 367      |
|    iterations           | 4350     |
|    time_elapsed         | 72772    |
|    total_timesteps      | 26726400 |
| train/                  |          |
|    approx_kl            | 8.690464 |
|    clip_fraction        | 0.0749   |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | 0.849    |
|    learning_rate        | 0.0001   |
|    loss                 | 27       |
|    n_updates            | 43490    |
|    policy_gradient_loss | 0.000729 |
|    std                  | 6.53     |
|    value_loss           | 101      |
--------------------------------------
Iteration: 4350 | Episodes: 265300 | Median Reward: 43.53 | Max Reward: 49.45
Iteration: 4351 | Episodes: 265350 | Median Reward: 15.39 | Max Reward: 49.45
Iteration: 4351 | Episodes: 265400 | Median Reward: 34.56 | Max Reward: 49.45
Iteration: 4352 | Episodes: 265450 | Median Reward: -12.24 | Max Reward: 49.45
Iteration: 4353 | Episodes: 265500 | Median Reward: 35.14 | Max Reward: 49.45
Iteration: 4354 | Episodes: 265550 | Median Reward: 37.16 | Max Reward: 49.45
Iteration: 4355 | Episodes: 265600 | Median Reward: 31.27 | Max Reward: 49.45
Iteration: 4355 | Episodes: 265650 | Median Reward: 36.40 | Max Reward: 49.45
Iteration: 4356 | Episodes: 265700 | Median Reward: 35.93 | Max Reward: 49.45
Iteration: 4357 | Episodes: 265750 | Median Reward: 39.71 | Max Reward: 49.45
Iteration: 4358 | Episodes: 265800 | Median Reward: 37.46 | Max Reward: 49.45
Iteration: 4359 | Episodes: 265850 | Median Reward: 36.46 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -73        |
| time/                   |            |
|    fps                  | 367        |
|    iterations           | 4360       |
|    time_elapsed         | 72954      |
|    total_timesteps      | 26787840   |
| train/                  |            |
|    approx_kl            | 0.42154282 |
|    clip_fraction        | 0.0551     |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.893      |
|    learning_rate        | 0.0001     |
|    loss                 | 18.6       |
|    n_updates            | 43590      |
|    policy_gradient_loss | 0.00061    |
|    std                  | 6.54       |
|    value_loss           | 54.2       |
----------------------------------------
Iteration: 4360 | Episodes: 265900 | Median Reward: 40.18 | Max Reward: 49.45
Iteration: 4360 | Episodes: 265950 | Median Reward: 38.53 | Max Reward: 49.45
Iteration: 4361 | Episodes: 266000 | Median Reward: 37.30 | Max Reward: 49.45
Iteration: 4362 | Episodes: 266050 | Median Reward: 38.50 | Max Reward: 49.45
Iteration: 4363 | Episodes: 266100 | Median Reward: 38.15 | Max Reward: 49.45
Iteration: 4364 | Episodes: 266150 | Median Reward: 37.77 | Max Reward: 49.45
Iteration: 4365 | Episodes: 266200 | Median Reward: 36.29 | Max Reward: 49.45
Iteration: 4365 | Episodes: 266250 | Median Reward: 36.54 | Max Reward: 49.45
Iteration: 4366 | Episodes: 266300 | Median Reward: 34.17 | Max Reward: 49.45
Iteration: 4367 | Episodes: 266350 | Median Reward: 38.39 | Max Reward: 49.45
Iteration: 4368 | Episodes: 266400 | Median Reward: 39.08 | Max Reward: 49.45
Iteration: 4369 | Episodes: 266450 | Median Reward: 39.08 | Max Reward: 49.45
Iteration: 4369 | Episodes: 266500 | Median Reward: 39.08 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -61.1     |
| time/                   |           |
|    fps                  | 367       |
|    iterations           | 4370      |
|    time_elapsed         | 73136     |
|    total_timesteps      | 26849280  |
| train/                  |           |
|    approx_kl            | 5.8084164 |
|    clip_fraction        | 0.0764    |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.892     |
|    learning_rate        | 0.0001    |
|    loss                 | 22.1      |
|    n_updates            | 43690     |
|    policy_gradient_loss | 0.00452   |
|    std                  | 6.56      |
|    value_loss           | 67.9      |
---------------------------------------
Iteration: 4370 | Episodes: 266550 | Median Reward: 7.66 | Max Reward: 49.45
Iteration: 4371 | Episodes: 266600 | Median Reward: 19.82 | Max Reward: 49.45
Iteration: 4372 | Episodes: 266650 | Median Reward: 14.08 | Max Reward: 49.45
Iteration: 4373 | Episodes: 266700 | Median Reward: -0.95 | Max Reward: 49.45
Iteration: 4374 | Episodes: 266750 | Median Reward: -2.17 | Max Reward: 49.45
Iteration: 4374 | Episodes: 266800 | Median Reward: 3.92 | Max Reward: 49.45
Iteration: 4375 | Episodes: 266850 | Median Reward: -17.27 | Max Reward: 49.45
Iteration: 4376 | Episodes: 266900 | Median Reward: -7.96 | Max Reward: 49.45
Iteration: 4377 | Episodes: 266950 | Median Reward: -2.46 | Max Reward: 49.45
Iteration: 4378 | Episodes: 267000 | Median Reward: 7.19 | Max Reward: 49.45
Iteration: 4379 | Episodes: 267050 | Median Reward: 5.49 | Max Reward: 49.45
Iteration: 4379 | Episodes: 267100 | Median Reward: 9.37 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -97        |
| time/                   |            |
|    fps                  | 367        |
|    iterations           | 4380       |
|    time_elapsed         | 73317      |
|    total_timesteps      | 26910720   |
| train/                  |            |
|    approx_kl            | 0.40267992 |
|    clip_fraction        | 0.0605     |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.409      |
|    n_updates            | 43790      |
|    policy_gradient_loss | 0.00849    |
|    std                  | 6.57       |
|    value_loss           | 22.6       |
----------------------------------------
Iteration: 4380 | Episodes: 267150 | Median Reward: 15.65 | Max Reward: 49.45
Iteration: 4381 | Episodes: 267200 | Median Reward: 23.34 | Max Reward: 49.45
Iteration: 4382 | Episodes: 267250 | Median Reward: 31.08 | Max Reward: 49.45
Iteration: 4383 | Episodes: 267300 | Median Reward: 33.75 | Max Reward: 49.45
Iteration: 4383 | Episodes: 267350 | Median Reward: 34.11 | Max Reward: 49.45
Iteration: 4384 | Episodes: 267400 | Median Reward: 36.22 | Max Reward: 49.45
Iteration: 4385 | Episodes: 267450 | Median Reward: 29.57 | Max Reward: 49.45
Iteration: 4386 | Episodes: 267500 | Median Reward: 31.16 | Max Reward: 49.45
Iteration: 4387 | Episodes: 267550 | Median Reward: 35.13 | Max Reward: 49.45
Iteration: 4388 | Episodes: 267600 | Median Reward: 13.87 | Max Reward: 49.45
Iteration: 4388 | Episodes: 267650 | Median Reward: 30.78 | Max Reward: 49.45
Iteration: 4389 | Episodes: 267700 | Median Reward: 34.67 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -69.7     |
| time/                   |           |
|    fps                  | 366       |
|    iterations           | 4390      |
|    time_elapsed         | 73504     |
|    total_timesteps      | 26972160  |
| train/                  |           |
|    approx_kl            | 1.0258738 |
|    clip_fraction        | 0.0213    |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0001    |
|    loss                 | 8.45      |
|    n_updates            | 43890     |
|    policy_gradient_loss | 0.00554   |
|    std                  | 6.59      |
|    value_loss           | 36.9      |
---------------------------------------
Iteration: 4390 | Episodes: 267750 | Median Reward: 27.15 | Max Reward: 49.45
Iteration: 4391 | Episodes: 267800 | Median Reward: 5.44 | Max Reward: 49.45
Iteration: 4392 | Episodes: 267850 | Median Reward: 0.56 | Max Reward: 49.45
Iteration: 4392 | Episodes: 267900 | Median Reward: 2.70 | Max Reward: 49.45
Iteration: 4393 | Episodes: 267950 | Median Reward: 8.36 | Max Reward: 49.45
Iteration: 4394 | Episodes: 268000 | Median Reward: 6.36 | Max Reward: 49.45
Iteration: 4395 | Episodes: 268050 | Median Reward: 2.57 | Max Reward: 49.45
Iteration: 4396 | Episodes: 268100 | Median Reward: 2.57 | Max Reward: 49.45
Iteration: 4397 | Episodes: 268150 | Median Reward: 21.13 | Max Reward: 49.45
Iteration: 4397 | Episodes: 268200 | Median Reward: 9.59 | Max Reward: 49.45
Iteration: 4398 | Episodes: 268250 | Median Reward: 1.69 | Max Reward: 49.45
Iteration: 4399 | Episodes: 268300 | Median Reward: 0.16 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -98.5      |
| time/                   |            |
|    fps                  | 366        |
|    iterations           | 4400       |
|    time_elapsed         | 73688      |
|    total_timesteps      | 27033600   |
| train/                  |            |
|    approx_kl            | 0.27467433 |
|    clip_fraction        | 0.0756     |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.953      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.86       |
|    n_updates            | 43990      |
|    policy_gradient_loss | -0.00309   |
|    std                  | 6.6        |
|    value_loss           | 17.9       |
----------------------------------------
Iteration: 4400 | Episodes: 268350 | Median Reward: -7.19 | Max Reward: 49.45
Iteration: 4401 | Episodes: 268400 | Median Reward: -10.47 | Max Reward: 49.45
Iteration: 4402 | Episodes: 268450 | Median Reward: -4.36 | Max Reward: 49.45
Iteration: 4402 | Episodes: 268500 | Median Reward: -0.94 | Max Reward: 49.45
Iteration: 4403 | Episodes: 268550 | Median Reward: 13.19 | Max Reward: 49.45
Iteration: 4404 | Episodes: 268600 | Median Reward: 19.68 | Max Reward: 49.45
Iteration: 4405 | Episodes: 268650 | Median Reward: -11.12 | Max Reward: 49.45
Iteration: 4406 | Episodes: 268700 | Median Reward: -17.40 | Max Reward: 49.45
Iteration: 4406 | Episodes: 268750 | Median Reward: 3.99 | Max Reward: 49.45
Iteration: 4407 | Episodes: 268800 | Median Reward: 7.90 | Max Reward: 49.45
Iteration: 4408 | Episodes: 268850 | Median Reward: -18.15 | Max Reward: 49.45
Iteration: 4409 | Episodes: 268900 | Median Reward: 0.12 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -94.1     |
| time/                   |           |
|    fps                  | 366       |
|    iterations           | 4410      |
|    time_elapsed         | 73865     |
|    total_timesteps      | 27095040  |
| train/                  |           |
|    approx_kl            | 1.2039058 |
|    clip_fraction        | 0.105     |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.914     |
|    learning_rate        | 0.0001    |
|    loss                 | 10.5      |
|    n_updates            | 44090     |
|    policy_gradient_loss | 0.0447    |
|    std                  | 6.61      |
|    value_loss           | 27.4      |
---------------------------------------
Iteration: 4410 | Episodes: 268950 | Median Reward: 20.38 | Max Reward: 49.45
Iteration: 4411 | Episodes: 269000 | Median Reward: 5.30 | Max Reward: 49.45
Iteration: 4411 | Episodes: 269050 | Median Reward: 2.86 | Max Reward: 49.45
Iteration: 4412 | Episodes: 269100 | Median Reward: -13.33 | Max Reward: 49.45
Iteration: 4413 | Episodes: 269150 | Median Reward: 4.40 | Max Reward: 49.45
Iteration: 4414 | Episodes: 269200 | Median Reward: 5.32 | Max Reward: 49.45
Iteration: 4415 | Episodes: 269250 | Median Reward: 7.12 | Max Reward: 49.45
Iteration: 4415 | Episodes: 269300 | Median Reward: 19.23 | Max Reward: 49.45
Iteration: 4416 | Episodes: 269350 | Median Reward: -1.32 | Max Reward: 49.45
Iteration: 4417 | Episodes: 269400 | Median Reward: -3.98 | Max Reward: 49.45
Iteration: 4418 | Episodes: 269450 | Median Reward: -8.50 | Max Reward: 49.45
Iteration: 4419 | Episodes: 269500 | Median Reward: -11.07 | Max Reward: 49.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -113        |
| time/                   |             |
|    fps                  | 366         |
|    iterations           | 4420        |
|    time_elapsed         | 74055       |
|    total_timesteps      | 27156480    |
| train/                  |             |
|    approx_kl            | 0.047545835 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.4         |
|    entropy_loss         | -126        |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0001      |
|    loss                 | 6.26        |
|    n_updates            | 44190       |
|    policy_gradient_loss | 0.00186     |
|    std                  | 6.61        |
|    value_loss           | 21.7        |
-----------------------------------------
Iteration: 4420 | Episodes: 269550 | Median Reward: -10.16 | Max Reward: 49.45
Iteration: 4420 | Episodes: 269600 | Median Reward: 5.16 | Max Reward: 49.45
Iteration: 4421 | Episodes: 269650 | Median Reward: 5.21 | Max Reward: 49.45
Iteration: 4422 | Episodes: 269700 | Median Reward: -3.52 | Max Reward: 49.45
Iteration: 4423 | Episodes: 269750 | Median Reward: -14.69 | Max Reward: 49.45
Iteration: 4424 | Episodes: 269800 | Median Reward: -18.46 | Max Reward: 49.45
Iteration: 4425 | Episodes: 269850 | Median Reward: -5.14 | Max Reward: 49.45
Iteration: 4425 | Episodes: 269900 | Median Reward: 10.65 | Max Reward: 49.45
Iteration: 4426 | Episodes: 269950 | Median Reward: -8.54 | Max Reward: 49.45
Iteration: 4427 | Episodes: 270000 | Median Reward: -5.75 | Max Reward: 49.45
Iteration: 4428 | Episodes: 270050 | Median Reward: 0.12 | Max Reward: 49.45
Iteration: 4429 | Episodes: 270100 | Median Reward: 2.67 | Max Reward: 49.45
Iteration: 4429 | Episodes: 270150 | Median Reward: -13.41 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -95.2     |
| time/                   |           |
|    fps                  | 366       |
|    iterations           | 4430      |
|    time_elapsed         | 74235     |
|    total_timesteps      | 27217920  |
| train/                  |           |
|    approx_kl            | 1.4102213 |
|    clip_fraction        | 0.0533    |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.955     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.71      |
|    n_updates            | 44290     |
|    policy_gradient_loss | 0.00473   |
|    std                  | 6.63      |
|    value_loss           | 16.4      |
---------------------------------------
Iteration: 4430 | Episodes: 270200 | Median Reward: -7.05 | Max Reward: 49.45
Iteration: 4431 | Episodes: 270250 | Median Reward: 2.80 | Max Reward: 49.45
Iteration: 4432 | Episodes: 270300 | Median Reward: 1.51 | Max Reward: 49.45
Iteration: 4433 | Episodes: 270350 | Median Reward: 0.69 | Max Reward: 49.45
Iteration: 4434 | Episodes: 270400 | Median Reward: -1.30 | Max Reward: 49.45
Iteration: 4434 | Episodes: 270450 | Median Reward: -13.08 | Max Reward: 49.45
Iteration: 4435 | Episodes: 270500 | Median Reward: -2.90 | Max Reward: 49.45
Iteration: 4436 | Episodes: 270550 | Median Reward: -4.65 | Max Reward: 49.45
Iteration: 4437 | Episodes: 270600 | Median Reward: -10.33 | Max Reward: 49.45
Iteration: 4438 | Episodes: 270650 | Median Reward: -9.69 | Max Reward: 49.45
Iteration: 4438 | Episodes: 270700 | Median Reward: -2.39 | Max Reward: 49.45
Iteration: 4439 | Episodes: 270750 | Median Reward: -19.79 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -110      |
| time/                   |           |
|    fps                  | 366       |
|    iterations           | 4440      |
|    time_elapsed         | 74415     |
|    total_timesteps      | 27279360  |
| train/                  |           |
|    approx_kl            | 1.3312185 |
|    clip_fraction        | 0.0737    |
|    clip_range           | 0.4       |
|    entropy_loss         | -125      |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0001    |
|    loss                 | 7.66      |
|    n_updates            | 44390     |
|    policy_gradient_loss | -0.00949  |
|    std                  | 6.64      |
|    value_loss           | 24.5      |
---------------------------------------
Iteration: 4440 | Episodes: 270800 | Median Reward: -26.99 | Max Reward: 49.45
Iteration: 4441 | Episodes: 270850 | Median Reward: -18.43 | Max Reward: 49.45
Iteration: 4442 | Episodes: 270900 | Median Reward: 18.44 | Max Reward: 49.45
Iteration: 4443 | Episodes: 270950 | Median Reward: -2.18 | Max Reward: 49.45
Iteration: 4443 | Episodes: 271000 | Median Reward: 8.56 | Max Reward: 49.45
Iteration: 4444 | Episodes: 271050 | Median Reward: 0.64 | Max Reward: 49.45
Iteration: 4445 | Episodes: 271100 | Median Reward: -7.38 | Max Reward: 49.45
Iteration: 4446 | Episodes: 271150 | Median Reward: -8.16 | Max Reward: 49.45
Iteration: 4447 | Episodes: 271200 | Median Reward: -12.07 | Max Reward: 49.45
Iteration: 4448 | Episodes: 271250 | Median Reward: 11.59 | Max Reward: 49.45
Iteration: 4448 | Episodes: 271300 | Median Reward: -2.33 | Max Reward: 49.45
Iteration: 4449 | Episodes: 271350 | Median Reward: -5.28 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 366        |
|    iterations           | 4450       |
|    time_elapsed         | 74594      |
|    total_timesteps      | 27340800   |
| train/                  |            |
|    approx_kl            | 0.81340826 |
|    clip_fraction        | 0.0641     |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.941      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.59       |
|    n_updates            | 44490      |
|    policy_gradient_loss | -0.0088    |
|    std                  | 6.65       |
|    value_loss           | 25.5       |
----------------------------------------
Iteration: 4450 | Episodes: 271400 | Median Reward: -10.80 | Max Reward: 49.45
Iteration: 4451 | Episodes: 271450 | Median Reward: -9.60 | Max Reward: 49.45
Iteration: 4452 | Episodes: 271500 | Median Reward: 1.57 | Max Reward: 49.45
Iteration: 4452 | Episodes: 271550 | Median Reward: 9.32 | Max Reward: 49.45
Iteration: 4453 | Episodes: 271600 | Median Reward: 4.94 | Max Reward: 49.45
Iteration: 4454 | Episodes: 271650 | Median Reward: -0.25 | Max Reward: 49.45
Iteration: 4455 | Episodes: 271700 | Median Reward: 0.48 | Max Reward: 49.45
Iteration: 4456 | Episodes: 271750 | Median Reward: 2.52 | Max Reward: 49.45
Iteration: 4457 | Episodes: 271800 | Median Reward: -1.20 | Max Reward: 49.45
Iteration: 4457 | Episodes: 271850 | Median Reward: 10.89 | Max Reward: 49.45
Iteration: 4458 | Episodes: 271900 | Median Reward: 18.68 | Max Reward: 49.45
Iteration: 4459 | Episodes: 271950 | Median Reward: 18.68 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -89.1      |
| time/                   |            |
|    fps                  | 366        |
|    iterations           | 4460       |
|    time_elapsed         | 74787      |
|    total_timesteps      | 27402240   |
| train/                  |            |
|    approx_kl            | 0.87665045 |
|    clip_fraction        | 0.0469     |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0.943      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.32       |
|    n_updates            | 44590      |
|    policy_gradient_loss | 0.00729    |
|    std                  | 6.66       |
|    value_loss           | 23.4       |
----------------------------------------
Iteration: 4460 | Episodes: 272000 | Median Reward: 1.68 | Max Reward: 49.45
Iteration: 4461 | Episodes: 272050 | Median Reward: -10.98 | Max Reward: 49.45
Iteration: 4462 | Episodes: 272100 | Median Reward: -7.15 | Max Reward: 49.45
Iteration: 4462 | Episodes: 272150 | Median Reward: 4.29 | Max Reward: 49.45
Iteration: 4463 | Episodes: 272200 | Median Reward: -9.37 | Max Reward: 49.45
Iteration: 4464 | Episodes: 272250 | Median Reward: -14.84 | Max Reward: 49.45
Iteration: 4465 | Episodes: 272300 | Median Reward: -7.50 | Max Reward: 49.45
Iteration: 4466 | Episodes: 272350 | Median Reward: -7.06 | Max Reward: 49.45
Iteration: 4466 | Episodes: 272400 | Median Reward: -3.32 | Max Reward: 49.45
Iteration: 4467 | Episodes: 272450 | Median Reward: -0.87 | Max Reward: 49.45
Iteration: 4468 | Episodes: 272500 | Median Reward: 10.24 | Max Reward: 49.45
Iteration: 4469 | Episodes: 272550 | Median Reward: 15.11 | Max Reward: 49.45
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -89.6       |
| time/                   |             |
|    fps                  | 366         |
|    iterations           | 4470        |
|    time_elapsed         | 74967       |
|    total_timesteps      | 27463680    |
| train/                  |             |
|    approx_kl            | 0.030935496 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.4         |
|    entropy_loss         | -128        |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.0001      |
|    loss                 | 6.22        |
|    n_updates            | 44690       |
|    policy_gradient_loss | -0.00484    |
|    std                  | 6.67        |
|    value_loss           | 27.1        |
-----------------------------------------
Iteration: 4470 | Episodes: 272600 | Median Reward: 3.04 | Max Reward: 49.45
Iteration: 4471 | Episodes: 272650 | Median Reward: -6.03 | Max Reward: 49.45
Iteration: 4471 | Episodes: 272700 | Median Reward: -11.97 | Max Reward: 49.45
Iteration: 4472 | Episodes: 272750 | Median Reward: -7.69 | Max Reward: 49.45
Iteration: 4473 | Episodes: 272800 | Median Reward: -10.62 | Max Reward: 49.45
Iteration: 4474 | Episodes: 272850 | Median Reward: -11.76 | Max Reward: 49.45
Iteration: 4475 | Episodes: 272900 | Median Reward: -14.19 | Max Reward: 49.45
Iteration: 4475 | Episodes: 272950 | Median Reward: -10.95 | Max Reward: 49.45
Iteration: 4476 | Episodes: 273000 | Median Reward: -2.84 | Max Reward: 49.45
Iteration: 4477 | Episodes: 273050 | Median Reward: -4.27 | Max Reward: 49.45
Iteration: 4478 | Episodes: 273100 | Median Reward: 17.52 | Max Reward: 49.45
Iteration: 4479 | Episodes: 273150 | Median Reward: 26.41 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -94.1      |
| time/                   |            |
|    fps                  | 366        |
|    iterations           | 4480       |
|    time_elapsed         | 75148      |
|    total_timesteps      | 27525120   |
| train/                  |            |
|    approx_kl            | 0.33900014 |
|    clip_fraction        | 0.0211     |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.916      |
|    learning_rate        | 0.0001     |
|    loss                 | 9.64       |
|    n_updates            | 44790      |
|    policy_gradient_loss | -0.000221  |
|    std                  | 6.68       |
|    value_loss           | 43         |
----------------------------------------
Iteration: 4480 | Episodes: 273200 | Median Reward: -6.49 | Max Reward: 49.45
Iteration: 4480 | Episodes: 273250 | Median Reward: 13.20 | Max Reward: 49.45
Iteration: 4481 | Episodes: 273300 | Median Reward: -12.64 | Max Reward: 49.45
Iteration: 4482 | Episodes: 273350 | Median Reward: -0.61 | Max Reward: 49.45
Iteration: 4483 | Episodes: 273400 | Median Reward: 2.45 | Max Reward: 49.45
Iteration: 4484 | Episodes: 273450 | Median Reward: -12.31 | Max Reward: 49.45
Iteration: 4485 | Episodes: 273500 | Median Reward: -17.39 | Max Reward: 49.45
Iteration: 4485 | Episodes: 273550 | Median Reward: 7.00 | Max Reward: 49.45
Iteration: 4486 | Episodes: 273600 | Median Reward: 24.46 | Max Reward: 49.45
Iteration: 4487 | Episodes: 273650 | Median Reward: -10.31 | Max Reward: 49.45
Iteration: 4488 | Episodes: 273700 | Median Reward: -13.40 | Max Reward: 49.45
Iteration: 4489 | Episodes: 273750 | Median Reward: -2.68 | Max Reward: 49.45
Iteration: 4489 | Episodes: 273800 | Median Reward: -5.14 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -102      |
| time/                   |           |
|    fps                  | 366       |
|    iterations           | 4490      |
|    time_elapsed         | 75340     |
|    total_timesteps      | 27586560  |
| train/                  |           |
|    approx_kl            | 0.5685165 |
|    clip_fraction        | 0.0373    |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.94      |
|    learning_rate        | 0.0001    |
|    loss                 | -0.417    |
|    n_updates            | 44890     |
|    policy_gradient_loss | -0.0135   |
|    std                  | 6.69      |
|    value_loss           | 16        |
---------------------------------------
Iteration: 4490 | Episodes: 273850 | Median Reward: -20.54 | Max Reward: 49.45
Iteration: 4491 | Episodes: 273900 | Median Reward: -13.02 | Max Reward: 49.45
Iteration: 4492 | Episodes: 273950 | Median Reward: -14.03 | Max Reward: 49.45
Iteration: 4493 | Episodes: 274000 | Median Reward: -19.69 | Max Reward: 49.45
Iteration: 4494 | Episodes: 274050 | Median Reward: -21.22 | Max Reward: 49.45
Iteration: 4494 | Episodes: 274100 | Median Reward: -1.28 | Max Reward: 49.45
Iteration: 4495 | Episodes: 274150 | Median Reward: 6.51 | Max Reward: 49.45
Iteration: 4496 | Episodes: 274200 | Median Reward: 26.22 | Max Reward: 49.45
Iteration: 4497 | Episodes: 274250 | Median Reward: 25.11 | Max Reward: 49.45
Iteration: 4498 | Episodes: 274300 | Median Reward: 1.07 | Max Reward: 49.45
Iteration: 4499 | Episodes: 274350 | Median Reward: -7.75 | Max Reward: 49.45
Iteration: 4499 | Episodes: 274400 | Median Reward: 1.72 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -104      |
| time/                   |           |
|    fps                  | 366       |
|    iterations           | 4500      |
|    time_elapsed         | 75519     |
|    total_timesteps      | 27648000  |
| train/                  |           |
|    approx_kl            | 0.5355332 |
|    clip_fraction        | 0.0555    |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.953     |
|    learning_rate        | 0.0001    |
|    loss                 | -1.88     |
|    n_updates            | 44990     |
|    policy_gradient_loss | 0.0143    |
|    std                  | 6.71      |
|    value_loss           | 14.5      |
---------------------------------------
Iteration: 4500 | Episodes: 274450 | Median Reward: -2.25 | Max Reward: 49.45
Iteration: 4501 | Episodes: 274500 | Median Reward: -12.48 | Max Reward: 49.45
Iteration: 4502 | Episodes: 274550 | Median Reward: -9.87 | Max Reward: 49.45
Iteration: 4503 | Episodes: 274600 | Median Reward: -7.01 | Max Reward: 49.45
Iteration: 4503 | Episodes: 274650 | Median Reward: -12.75 | Max Reward: 49.45
Iteration: 4504 | Episodes: 274700 | Median Reward: -2.33 | Max Reward: 49.45
Iteration: 4505 | Episodes: 274750 | Median Reward: 9.74 | Max Reward: 49.45
Iteration: 4506 | Episodes: 274800 | Median Reward: -7.04 | Max Reward: 49.45
Iteration: 4507 | Episodes: 274850 | Median Reward: -10.69 | Max Reward: 49.45
Iteration: 4508 | Episodes: 274900 | Median Reward: -17.79 | Max Reward: 49.45
Iteration: 4508 | Episodes: 274950 | Median Reward: 6.90 | Max Reward: 49.45
Iteration: 4509 | Episodes: 275000 | Median Reward: 1.95 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 366        |
|    iterations           | 4510       |
|    time_elapsed         | 75698      |
|    total_timesteps      | 27709440   |
| train/                  |            |
|    approx_kl            | 0.14311759 |
|    clip_fraction        | 0.0352     |
|    clip_range           | 0.4        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.37       |
|    n_updates            | 45090      |
|    policy_gradient_loss | -0.00212   |
|    std                  | 6.72       |
|    value_loss           | 25.8       |
----------------------------------------
Iteration: 4510 | Episodes: 275050 | Median Reward: -15.92 | Max Reward: 49.45
Iteration: 4511 | Episodes: 275100 | Median Reward: -13.26 | Max Reward: 49.45
Iteration: 4512 | Episodes: 275150 | Median Reward: -10.29 | Max Reward: 49.45
Iteration: 4512 | Episodes: 275200 | Median Reward: -5.85 | Max Reward: 49.45
Iteration: 4513 | Episodes: 275250 | Median Reward: -1.26 | Max Reward: 49.45
Iteration: 4514 | Episodes: 275300 | Median Reward: -5.37 | Max Reward: 49.45
Iteration: 4515 | Episodes: 275350 | Median Reward: 3.31 | Max Reward: 49.45
Iteration: 4516 | Episodes: 275400 | Median Reward: -1.32 | Max Reward: 49.45
Iteration: 4517 | Episodes: 275450 | Median Reward: -8.74 | Max Reward: 49.45
Iteration: 4517 | Episodes: 275500 | Median Reward: 10.75 | Max Reward: 49.45
Iteration: 4518 | Episodes: 275550 | Median Reward: 26.00 | Max Reward: 49.45
Iteration: 4519 | Episodes: 275600 | Median Reward: 3.94 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -93.5    |
| time/                   |          |
|    fps                  | 365      |
|    iterations           | 4520     |
|    time_elapsed         | 75882    |
|    total_timesteps      | 27770880 |
| train/                  |          |
|    approx_kl            | 9.694859 |
|    clip_fraction        | 0.0875   |
|    clip_range           | 0.4      |
|    entropy_loss         | -127     |
|    explained_variance   | 0.924    |
|    learning_rate        | 0.0001   |
|    loss                 | 12.9     |
|    n_updates            | 45190    |
|    policy_gradient_loss | -0.00286 |
|    std                  | 6.73     |
|    value_loss           | 37.6     |
--------------------------------------
Iteration: 4520 | Episodes: 275650 | Median Reward: 3.48 | Max Reward: 49.45
Iteration: 4521 | Episodes: 275700 | Median Reward: 15.67 | Max Reward: 49.45
Iteration: 4522 | Episodes: 275750 | Median Reward: -0.20 | Max Reward: 49.45
Iteration: 4522 | Episodes: 275800 | Median Reward: -7.25 | Max Reward: 49.45
Iteration: 4523 | Episodes: 275850 | Median Reward: -7.41 | Max Reward: 49.45
Iteration: 4524 | Episodes: 275900 | Median Reward: -4.63 | Max Reward: 49.45
Iteration: 4525 | Episodes: 275950 | Median Reward: -6.51 | Max Reward: 49.45
Iteration: 4526 | Episodes: 276000 | Median Reward: -7.13 | Max Reward: 49.45
Iteration: 4526 | Episodes: 276050 | Median Reward: 3.03 | Max Reward: 49.45
Iteration: 4527 | Episodes: 276100 | Median Reward: -9.01 | Max Reward: 49.45
Iteration: 4528 | Episodes: 276150 | Median Reward: -7.36 | Max Reward: 49.45
Iteration: 4529 | Episodes: 276200 | Median Reward: -3.49 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -97.8      |
| time/                   |            |
|    fps                  | 365        |
|    iterations           | 4530       |
|    time_elapsed         | 76065      |
|    total_timesteps      | 27832320   |
| train/                  |            |
|    approx_kl            | 0.12515433 |
|    clip_fraction        | 0.0663     |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.938      |
|    learning_rate        | 0.0001     |
|    loss                 | 13         |
|    n_updates            | 45290      |
|    policy_gradient_loss | -0.00805   |
|    std                  | 6.75       |
|    value_loss           | 20.7       |
----------------------------------------
Iteration: 4530 | Episodes: 276250 | Median Reward: 9.13 | Max Reward: 49.45
Iteration: 4531 | Episodes: 276300 | Median Reward: -9.97 | Max Reward: 49.45
Iteration: 4531 | Episodes: 276350 | Median Reward: -2.57 | Max Reward: 49.45
Iteration: 4532 | Episodes: 276400 | Median Reward: 3.36 | Max Reward: 49.45
Iteration: 4533 | Episodes: 276450 | Median Reward: 2.35 | Max Reward: 49.45
Iteration: 4534 | Episodes: 276500 | Median Reward: -4.72 | Max Reward: 49.45
Iteration: 4535 | Episodes: 276550 | Median Reward: -14.03 | Max Reward: 49.45
Iteration: 4536 | Episodes: 276600 | Median Reward: -13.67 | Max Reward: 49.45
Iteration: 4536 | Episodes: 276650 | Median Reward: -7.61 | Max Reward: 49.45
Iteration: 4537 | Episodes: 276700 | Median Reward: 6.70 | Max Reward: 49.45
Iteration: 4538 | Episodes: 276750 | Median Reward: 5.23 | Max Reward: 49.45
Iteration: 4539 | Episodes: 276800 | Median Reward: 1.93 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -93.5     |
| time/                   |           |
|    fps                  | 365       |
|    iterations           | 4540      |
|    time_elapsed         | 76247     |
|    total_timesteps      | 27893760  |
| train/                  |           |
|    approx_kl            | 3.7246506 |
|    clip_fraction        | 0.0525    |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.934     |
|    learning_rate        | 0.0001    |
|    loss                 | -2.38     |
|    n_updates            | 45390     |
|    policy_gradient_loss | 0.0134    |
|    std                  | 6.76      |
|    value_loss           | 25.1      |
---------------------------------------
Iteration: 4540 | Episodes: 276850 | Median Reward: 7.01 | Max Reward: 49.45
Iteration: 4540 | Episodes: 276900 | Median Reward: 3.92 | Max Reward: 49.45
Iteration: 4541 | Episodes: 276950 | Median Reward: 0.59 | Max Reward: 49.45
Iteration: 4542 | Episodes: 277000 | Median Reward: 3.91 | Max Reward: 49.45
Iteration: 4543 | Episodes: 277050 | Median Reward: 7.11 | Max Reward: 49.45
Iteration: 4544 | Episodes: 277100 | Median Reward: 9.86 | Max Reward: 49.45
Iteration: 4545 | Episodes: 277150 | Median Reward: 10.01 | Max Reward: 49.45
Iteration: 4545 | Episodes: 277200 | Median Reward: -14.10 | Max Reward: 49.45
Iteration: 4546 | Episodes: 277250 | Median Reward: -14.62 | Max Reward: 49.45
Iteration: 4547 | Episodes: 277300 | Median Reward: -15.23 | Max Reward: 49.45
Iteration: 4548 | Episodes: 277350 | Median Reward: -12.89 | Max Reward: 49.45
Iteration: 4549 | Episodes: 277400 | Median Reward: -12.89 | Max Reward: 49.45
Iteration: 4549 | Episodes: 277450 | Median Reward: -9.39 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -97.8     |
| time/                   |           |
|    fps                  | 365       |
|    iterations           | 4550      |
|    time_elapsed         | 76430     |
|    total_timesteps      | 27955200  |
| train/                  |           |
|    approx_kl            | 0.8252855 |
|    clip_fraction        | 0.022     |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.913     |
|    learning_rate        | 0.0001    |
|    loss                 | 6.08      |
|    n_updates            | 45490     |
|    policy_gradient_loss | 0.00895   |
|    std                  | 6.77      |
|    value_loss           | 28        |
---------------------------------------
Iteration: 4550 | Episodes: 277500 | Median Reward: -12.43 | Max Reward: 49.45
Iteration: 4551 | Episodes: 277550 | Median Reward: 1.40 | Max Reward: 49.45
Iteration: 4552 | Episodes: 277600 | Median Reward: 15.37 | Max Reward: 49.45
Iteration: 4553 | Episodes: 277650 | Median Reward: 13.32 | Max Reward: 49.45
Iteration: 4554 | Episodes: 277700 | Median Reward: -15.75 | Max Reward: 49.45
Iteration: 4554 | Episodes: 277750 | Median Reward: -16.35 | Max Reward: 49.45
Iteration: 4555 | Episodes: 277800 | Median Reward: -13.50 | Max Reward: 49.45
Iteration: 4556 | Episodes: 277850 | Median Reward: -6.03 | Max Reward: 49.45
Iteration: 4557 | Episodes: 277900 | Median Reward: 8.13 | Max Reward: 49.45
Iteration: 4558 | Episodes: 277950 | Median Reward: -13.76 | Max Reward: 49.45
Iteration: 4559 | Episodes: 278000 | Median Reward: -16.62 | Max Reward: 49.45
Iteration: 4559 | Episodes: 278050 | Median Reward: -10.21 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -111      |
| time/                   |           |
|    fps                  | 365       |
|    iterations           | 4560      |
|    time_elapsed         | 76622     |
|    total_timesteps      | 28016640  |
| train/                  |           |
|    approx_kl            | 0.8851654 |
|    clip_fraction        | 0.0425    |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.942     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.678    |
|    n_updates            | 45590     |
|    policy_gradient_loss | -0.00284  |
|    std                  | 6.78      |
|    value_loss           | 14.8      |
---------------------------------------
Iteration: 4560 | Episodes: 278100 | Median Reward: 0.30 | Max Reward: 49.45
Iteration: 4561 | Episodes: 278150 | Median Reward: 0.07 | Max Reward: 49.45
Iteration: 4562 | Episodes: 278200 | Median Reward: -9.42 | Max Reward: 49.45
Iteration: 4563 | Episodes: 278250 | Median Reward: -17.53 | Max Reward: 49.45
Iteration: 4563 | Episodes: 278300 | Median Reward: -16.46 | Max Reward: 49.45
Iteration: 4564 | Episodes: 278350 | Median Reward: -8.73 | Max Reward: 49.45
Iteration: 4565 | Episodes: 278400 | Median Reward: -2.41 | Max Reward: 49.45
Iteration: 4566 | Episodes: 278450 | Median Reward: -8.02 | Max Reward: 49.45
Iteration: 4567 | Episodes: 278500 | Median Reward: -4.16 | Max Reward: 49.45
Iteration: 4568 | Episodes: 278550 | Median Reward: -6.43 | Max Reward: 49.45
Iteration: 4568 | Episodes: 278600 | Median Reward: -12.16 | Max Reward: 49.45
Iteration: 4569 | Episodes: 278650 | Median Reward: 14.85 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -92.6     |
| time/                   |           |
|    fps                  | 365       |
|    iterations           | 4570      |
|    time_elapsed         | 76799     |
|    total_timesteps      | 28078080  |
| train/                  |           |
|    approx_kl            | 22.488815 |
|    clip_fraction        | 0.215     |
|    clip_range           | 0.4       |
|    entropy_loss         | -123      |
|    explained_variance   | 0.924     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.79      |
|    n_updates            | 45690     |
|    policy_gradient_loss | -0.012    |
|    std                  | 6.79      |
|    value_loss           | 22.2      |
---------------------------------------
Iteration: 4570 | Episodes: 278700 | Median Reward: 8.86 | Max Reward: 49.45
Iteration: 4571 | Episodes: 278750 | Median Reward: -6.25 | Max Reward: 49.45
Iteration: 4572 | Episodes: 278800 | Median Reward: 4.11 | Max Reward: 49.45
Iteration: 4572 | Episodes: 278850 | Median Reward: -4.13 | Max Reward: 49.45
Iteration: 4573 | Episodes: 278900 | Median Reward: 4.94 | Max Reward: 49.45
Iteration: 4574 | Episodes: 278950 | Median Reward: 5.07 | Max Reward: 49.45
Iteration: 4575 | Episodes: 279000 | Median Reward: 2.60 | Max Reward: 49.45
Iteration: 4576 | Episodes: 279050 | Median Reward: -1.22 | Max Reward: 49.45
Iteration: 4577 | Episodes: 279100 | Median Reward: -9.10 | Max Reward: 49.45
Iteration: 4577 | Episodes: 279150 | Median Reward: -12.19 | Max Reward: 49.45
Iteration: 4578 | Episodes: 279200 | Median Reward: 0.76 | Max Reward: 49.45
Iteration: 4579 | Episodes: 279250 | Median Reward: -12.12 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -107      |
| time/                   |           |
|    fps                  | 365       |
|    iterations           | 4580      |
|    time_elapsed         | 76978     |
|    total_timesteps      | 28139520  |
| train/                  |           |
|    approx_kl            | 2.8290095 |
|    clip_fraction        | 0.0517    |
|    clip_range           | 0.4       |
|    entropy_loss         | -127      |
|    explained_variance   | 0.949     |
|    learning_rate        | 0.0001    |
|    loss                 | -2.41     |
|    n_updates            | 45790     |
|    policy_gradient_loss | 0.00756   |
|    std                  | 6.79      |
|    value_loss           | 18.6      |
---------------------------------------
Iteration: 4580 | Episodes: 279300 | Median Reward: -12.12 | Max Reward: 49.45
Iteration: 4581 | Episodes: 279350 | Median Reward: 0.10 | Max Reward: 49.45
Iteration: 4582 | Episodes: 279400 | Median Reward: 0.30 | Max Reward: 49.45
Iteration: 4582 | Episodes: 279450 | Median Reward: 7.86 | Max Reward: 49.45
Iteration: 4583 | Episodes: 279500 | Median Reward: 17.52 | Max Reward: 49.45
Iteration: 4584 | Episodes: 279550 | Median Reward: 8.82 | Max Reward: 49.45
Iteration: 4585 | Episodes: 279600 | Median Reward: 3.17 | Max Reward: 49.45
Iteration: 4586 | Episodes: 279650 | Median Reward: 23.33 | Max Reward: 49.45
Iteration: 4586 | Episodes: 279700 | Median Reward: -7.54 | Max Reward: 49.45
Iteration: 4587 | Episodes: 279750 | Median Reward: 30.24 | Max Reward: 49.45
Iteration: 4588 | Episodes: 279800 | Median Reward: 36.78 | Max Reward: 49.45
Iteration: 4589 | Episodes: 279850 | Median Reward: 36.56 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -80       |
| time/                   |           |
|    fps                  | 365       |
|    iterations           | 4590      |
|    time_elapsed         | 77173     |
|    total_timesteps      | 28200960  |
| train/                  |           |
|    approx_kl            | 44.646553 |
|    clip_fraction        | 0.0865    |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.876     |
|    learning_rate        | 0.0001    |
|    loss                 | 40.3      |
|    n_updates            | 45890     |
|    policy_gradient_loss | 0.022     |
|    std                  | 6.81      |
|    value_loss           | 90.2      |
---------------------------------------
Iteration: 4590 | Episodes: 279900 | Median Reward: -10.33 | Max Reward: 49.45
Iteration: 4591 | Episodes: 279950 | Median Reward: -10.53 | Max Reward: 49.45
Iteration: 4591 | Episodes: 280000 | Median Reward: -8.31 | Max Reward: 49.45
Iteration: 4592 | Episodes: 280050 | Median Reward: 15.21 | Max Reward: 49.45
Iteration: 4593 | Episodes: 280100 | Median Reward: 15.21 | Max Reward: 49.45
Iteration: 4594 | Episodes: 280150 | Median Reward: -0.65 | Max Reward: 49.45
Iteration: 4595 | Episodes: 280200 | Median Reward: -8.32 | Max Reward: 49.45
Iteration: 4595 | Episodes: 280250 | Median Reward: -6.88 | Max Reward: 49.45
Iteration: 4596 | Episodes: 280300 | Median Reward: 20.67 | Max Reward: 49.45
Iteration: 4597 | Episodes: 280350 | Median Reward: 10.31 | Max Reward: 49.45
Iteration: 4598 | Episodes: 280400 | Median Reward: 2.08 | Max Reward: 49.45
Iteration: 4599 | Episodes: 280450 | Median Reward: 2.08 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -91.1     |
| time/                   |           |
|    fps                  | 365       |
|    iterations           | 4600      |
|    time_elapsed         | 77354     |
|    total_timesteps      | 28262400  |
| train/                  |           |
|    approx_kl            | 1.1267445 |
|    clip_fraction        | 0.0364    |
|    clip_range           | 0.4       |
|    entropy_loss         | -128      |
|    explained_variance   | 0.949     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0461    |
|    n_updates            | 45990     |
|    policy_gradient_loss | 0.00364   |
|    std                  | 6.81      |
|    value_loss           | 17.1      |
---------------------------------------
Iteration: 4600 | Episodes: 280500 | Median Reward: 25.01 | Max Reward: 49.45
Iteration: 4600 | Episodes: 280550 | Median Reward: 24.49 | Max Reward: 49.45
Iteration: 4601 | Episodes: 280600 | Median Reward: 36.08 | Max Reward: 49.45
Iteration: 4602 | Episodes: 280650 | Median Reward: 41.07 | Max Reward: 49.45
Iteration: 4603 | Episodes: 280700 | Median Reward: 38.51 | Max Reward: 49.45
Iteration: 4604 | Episodes: 280750 | Median Reward: 35.12 | Max Reward: 49.45
Iteration: 4605 | Episodes: 280800 | Median Reward: 35.92 | Max Reward: 49.45
Iteration: 4605 | Episodes: 280850 | Median Reward: 37.97 | Max Reward: 49.45
Iteration: 4606 | Episodes: 280900 | Median Reward: 41.71 | Max Reward: 49.45
Iteration: 4607 | Episodes: 280950 | Median Reward: 40.57 | Max Reward: 49.45
Iteration: 4608 | Episodes: 281000 | Median Reward: 39.94 | Max Reward: 49.45
Iteration: 4609 | Episodes: 281050 | Median Reward: 39.87 | Max Reward: 49.45
Iteration: 4609 | Episodes: 281100 | Median Reward: 40.26 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -59.4    |
| time/                   |          |
|    fps                  | 365      |
|    iterations           | 4610     |
|    time_elapsed         | 77535    |
|    total_timesteps      | 28323840 |
| train/                  |          |
|    approx_kl            | 5.992488 |
|    clip_fraction        | 0.0635   |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.873    |
|    learning_rate        | 0.0001   |
|    loss                 | 35       |
|    n_updates            | 46090    |
|    policy_gradient_loss | 0.0107   |
|    std                  | 6.84     |
|    value_loss           | 91.8     |
--------------------------------------
Iteration: 4610 | Episodes: 281150 | Median Reward: 40.75 | Max Reward: 49.45
Iteration: 4611 | Episodes: 281200 | Median Reward: 40.60 | Max Reward: 49.45
Iteration: 4612 | Episodes: 281250 | Median Reward: 41.27 | Max Reward: 49.45
Iteration: 4613 | Episodes: 281300 | Median Reward: 41.09 | Max Reward: 49.45
Iteration: 4614 | Episodes: 281350 | Median Reward: 39.19 | Max Reward: 49.45
Iteration: 4614 | Episodes: 281400 | Median Reward: 44.38 | Max Reward: 49.45
Iteration: 4615 | Episodes: 281450 | Median Reward: 44.50 | Max Reward: 49.45
Iteration: 4616 | Episodes: 281500 | Median Reward: 44.92 | Max Reward: 49.45
Iteration: 4617 | Episodes: 281550 | Median Reward: 44.80 | Max Reward: 49.45
Iteration: 4618 | Episodes: 281600 | Median Reward: 44.66 | Max Reward: 49.45
Iteration: 4619 | Episodes: 281650 | Median Reward: 44.65 | Max Reward: 49.45
Iteration: 4619 | Episodes: 281700 | Median Reward: 45.07 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -56.2     |
| time/                   |           |
|    fps                  | 365       |
|    iterations           | 4620      |
|    time_elapsed         | 77718     |
|    total_timesteps      | 28385280  |
| train/                  |           |
|    approx_kl            | 12.386737 |
|    clip_fraction        | 0.0718    |
|    clip_range           | 0.4       |
|    entropy_loss         | -129      |
|    explained_variance   | 0.877     |
|    learning_rate        | 0.0001    |
|    loss                 | 38.2      |
|    n_updates            | 46190     |
|    policy_gradient_loss | 0.02      |
|    std                  | 6.88      |
|    value_loss           | 91.5      |
---------------------------------------
Iteration: 4620 | Episodes: 281750 | Median Reward: 45.76 | Max Reward: 49.45
Iteration: 4621 | Episodes: 281800 | Median Reward: 46.23 | Max Reward: 49.45
Iteration: 4622 | Episodes: 281850 | Median Reward: 46.32 | Max Reward: 49.45
Iteration: 4623 | Episodes: 281900 | Median Reward: 46.27 | Max Reward: 49.45
Iteration: 4623 | Episodes: 281950 | Median Reward: 45.98 | Max Reward: 49.45
Iteration: 4624 | Episodes: 282000 | Median Reward: 45.57 | Max Reward: 49.45
Iteration: 4625 | Episodes: 282050 | Median Reward: 44.21 | Max Reward: 49.45
Iteration: 4626 | Episodes: 282100 | Median Reward: 43.68 | Max Reward: 49.45
Iteration: 4627 | Episodes: 282150 | Median Reward: 44.64 | Max Reward: 49.45
Iteration: 4628 | Episodes: 282200 | Median Reward: 39.48 | Max Reward: 49.45
Iteration: 4628 | Episodes: 282250 | Median Reward: 42.14 | Max Reward: 49.45
Iteration: 4629 | Episodes: 282300 | Median Reward: 43.90 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -57.1    |
| time/                   |          |
|    fps                  | 365      |
|    iterations           | 4630     |
|    time_elapsed         | 77907    |
|    total_timesteps      | 28446720 |
| train/                  |          |
|    approx_kl            | 21.91138 |
|    clip_fraction        | 0.353    |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.899    |
|    learning_rate        | 0.0001   |
|    loss                 | 21.4     |
|    n_updates            | 46290    |
|    policy_gradient_loss | 0.133    |
|    std                  | 6.93     |
|    value_loss           | 68.5     |
--------------------------------------
Iteration: 4630 | Episodes: 282350 | Median Reward: 44.58 | Max Reward: 49.45
Iteration: 4631 | Episodes: 282400 | Median Reward: 45.97 | Max Reward: 49.45
Iteration: 4632 | Episodes: 282450 | Median Reward: 46.19 | Max Reward: 49.45
Iteration: 4632 | Episodes: 282500 | Median Reward: 46.41 | Max Reward: 49.45
Iteration: 4633 | Episodes: 282550 | Median Reward: 42.48 | Max Reward: 49.45
Iteration: 4634 | Episodes: 282600 | Median Reward: 42.49 | Max Reward: 49.45
Iteration: 4635 | Episodes: 282650 | Median Reward: 42.40 | Max Reward: 49.45
Iteration: 4636 | Episodes: 282700 | Median Reward: 42.83 | Max Reward: 49.45
Iteration: 4637 | Episodes: 282750 | Median Reward: 45.46 | Max Reward: 49.45
Iteration: 4637 | Episodes: 282800 | Median Reward: 44.93 | Max Reward: 49.45
Iteration: 4638 | Episodes: 282850 | Median Reward: 46.56 | Max Reward: 49.45
Iteration: 4639 | Episodes: 282900 | Median Reward: 38.80 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -59.2    |
| time/                   |          |
|    fps                  | 365      |
|    iterations           | 4640     |
|    time_elapsed         | 78088    |
|    total_timesteps      | 28508160 |
| train/                  |          |
|    approx_kl            | 4.677141 |
|    clip_fraction        | 0.0727   |
|    clip_range           | 0.4      |
|    entropy_loss         | -129     |
|    explained_variance   | 0.898    |
|    learning_rate        | 0.0001   |
|    loss                 | 34.9     |
|    n_updates            | 46390    |
|    policy_gradient_loss | 0.0243   |
|    std                  | 6.98     |
|    value_loss           | 74.3     |
--------------------------------------
Iteration: 4640 | Episodes: 282950 | Median Reward: 38.53 | Max Reward: 49.45
Iteration: 4641 | Episodes: 283000 | Median Reward: 42.29 | Max Reward: 49.45
Iteration: 4642 | Episodes: 283050 | Median Reward: 45.13 | Max Reward: 49.45
Iteration: 4642 | Episodes: 283100 | Median Reward: 46.96 | Max Reward: 49.45
Iteration: 4643 | Episodes: 283150 | Median Reward: 47.04 | Max Reward: 49.45
Iteration: 4644 | Episodes: 283200 | Median Reward: 46.20 | Max Reward: 49.45
Iteration: 4645 | Episodes: 283250 | Median Reward: 43.02 | Max Reward: 49.45
Iteration: 4646 | Episodes: 283300 | Median Reward: 40.04 | Max Reward: 49.45
Iteration: 4646 | Episodes: 283350 | Median Reward: 46.79 | Max Reward: 49.45
Iteration: 4647 | Episodes: 283400 | Median Reward: 47.15 | Max Reward: 49.45
Iteration: 4648 | Episodes: 283450 | Median Reward: 45.43 | Max Reward: 49.45
Iteration: 4649 | Episodes: 283500 | Median Reward: 44.73 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -55.3     |
| time/                   |           |
|    fps                  | 365       |
|    iterations           | 4650      |
|    time_elapsed         | 78270     |
|    total_timesteps      | 28569600  |
| train/                  |           |
|    approx_kl            | 0.6160052 |
|    clip_fraction        | 0.0877    |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.907     |
|    learning_rate        | 0.0001    |
|    loss                 | 24.7      |
|    n_updates            | 46490     |
|    policy_gradient_loss | 0.0312    |
|    std                  | 7.05      |
|    value_loss           | 65.4      |
---------------------------------------
Iteration: 4650 | Episodes: 283550 | Median Reward: 44.86 | Max Reward: 49.45
Iteration: 4651 | Episodes: 283600 | Median Reward: 45.80 | Max Reward: 49.45
Iteration: 4651 | Episodes: 283650 | Median Reward: 42.81 | Max Reward: 49.45
Iteration: 4652 | Episodes: 283700 | Median Reward: 44.89 | Max Reward: 49.45
Iteration: 4653 | Episodes: 283750 | Median Reward: 44.30 | Max Reward: 49.45
Iteration: 4654 | Episodes: 283800 | Median Reward: 45.00 | Max Reward: 49.45
Iteration: 4655 | Episodes: 283850 | Median Reward: 46.05 | Max Reward: 49.45
Iteration: 4655 | Episodes: 283900 | Median Reward: 46.05 | Max Reward: 49.45
Iteration: 4656 | Episodes: 283950 | Median Reward: 46.63 | Max Reward: 49.45
Iteration: 4657 | Episodes: 284000 | Median Reward: 46.17 | Max Reward: 49.45
Iteration: 4658 | Episodes: 284050 | Median Reward: 46.17 | Max Reward: 49.45
Iteration: 4659 | Episodes: 284100 | Median Reward: 46.27 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -53.4      |
| time/                   |            |
|    fps                  | 364        |
|    iterations           | 4660       |
|    time_elapsed         | 78459      |
|    total_timesteps      | 28631040   |
| train/                  |            |
|    approx_kl            | 0.13767359 |
|    clip_fraction        | 0.0162     |
|    clip_range           | 0.4        |
|    entropy_loss         | -130       |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0001     |
|    loss                 | 26.8       |
|    n_updates            | 46590      |
|    policy_gradient_loss | 0.0128     |
|    std                  | 7.11       |
|    value_loss           | 67.2       |
----------------------------------------
Iteration: 4660 | Episodes: 284150 | Median Reward: 46.50 | Max Reward: 49.45
Iteration: 4660 | Episodes: 284200 | Median Reward: 46.42 | Max Reward: 49.45
Iteration: 4661 | Episodes: 284250 | Median Reward: 46.10 | Max Reward: 49.45
Iteration: 4662 | Episodes: 284300 | Median Reward: 46.47 | Max Reward: 49.45
Iteration: 4663 | Episodes: 284350 | Median Reward: 46.48 | Max Reward: 49.45
Iteration: 4664 | Episodes: 284400 | Median Reward: 46.48 | Max Reward: 49.45
Iteration: 4665 | Episodes: 284450 | Median Reward: 46.53 | Max Reward: 49.45
Iteration: 4665 | Episodes: 284500 | Median Reward: 44.06 | Max Reward: 49.45
Iteration: 4666 | Episodes: 284550 | Median Reward: 45.16 | Max Reward: 49.45
Iteration: 4667 | Episodes: 284600 | Median Reward: 44.55 | Max Reward: 49.45
Iteration: 4668 | Episodes: 284650 | Median Reward: 43.80 | Max Reward: 49.45
Iteration: 4669 | Episodes: 284700 | Median Reward: 43.21 | Max Reward: 49.45
Iteration: 4669 | Episodes: 284750 | Median Reward: 45.57 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -56.1     |
| time/                   |           |
|    fps                  | 364       |
|    iterations           | 4670      |
|    time_elapsed         | 78640     |
|    total_timesteps      | 28692480  |
| train/                  |           |
|    approx_kl            | 4.4293566 |
|    clip_fraction        | 0.0367    |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 0.927     |
|    learning_rate        | 0.0001    |
|    loss                 | 17.6      |
|    n_updates            | 46690     |
|    policy_gradient_loss | 0.00162   |
|    std                  | 7.19      |
|    value_loss           | 48        |
---------------------------------------
Iteration: 4670 | Episodes: 284800 | Median Reward: 44.35 | Max Reward: 49.45
Iteration: 4671 | Episodes: 284850 | Median Reward: 46.00 | Max Reward: 49.45
Iteration: 4672 | Episodes: 284900 | Median Reward: 45.82 | Max Reward: 49.45
Iteration: 4673 | Episodes: 284950 | Median Reward: 44.79 | Max Reward: 49.45
Iteration: 4674 | Episodes: 285000 | Median Reward: 45.49 | Max Reward: 49.45
Iteration: 4674 | Episodes: 285050 | Median Reward: 45.85 | Max Reward: 49.45
Iteration: 4675 | Episodes: 285100 | Median Reward: 46.04 | Max Reward: 49.45
Iteration: 4676 | Episodes: 285150 | Median Reward: 44.27 | Max Reward: 49.45
Iteration: 4677 | Episodes: 285200 | Median Reward: 44.02 | Max Reward: 49.45
Iteration: 4678 | Episodes: 285250 | Median Reward: 43.18 | Max Reward: 49.45
Iteration: 4679 | Episodes: 285300 | Median Reward: 44.07 | Max Reward: 49.45
Iteration: 4679 | Episodes: 285350 | Median Reward: 43.49 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -56.2     |
| time/                   |           |
|    fps                  | 364       |
|    iterations           | 4680      |
|    time_elapsed         | 78819     |
|    total_timesteps      | 28753920  |
| train/                  |           |
|    approx_kl            | 3.9768467 |
|    clip_fraction        | 0.0647    |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.93      |
|    learning_rate        | 0.0001    |
|    loss                 | 15.4      |
|    n_updates            | 46790     |
|    policy_gradient_loss | 0.0115    |
|    std                  | 7.28      |
|    value_loss           | 46        |
---------------------------------------
Iteration: 4680 | Episodes: 285400 | Median Reward: 44.45 | Max Reward: 49.45
Iteration: 4681 | Episodes: 285450 | Median Reward: 45.67 | Max Reward: 49.45
Iteration: 4682 | Episodes: 285500 | Median Reward: 46.05 | Max Reward: 49.45
Iteration: 4683 | Episodes: 285550 | Median Reward: 46.05 | Max Reward: 49.45
Iteration: 4683 | Episodes: 285600 | Median Reward: 46.27 | Max Reward: 49.45
Iteration: 4684 | Episodes: 285650 | Median Reward: 44.28 | Max Reward: 49.45
Iteration: 4685 | Episodes: 285700 | Median Reward: 46.44 | Max Reward: 49.45
Iteration: 4686 | Episodes: 285750 | Median Reward: 46.55 | Max Reward: 49.45
Iteration: 4687 | Episodes: 285800 | Median Reward: 46.54 | Max Reward: 49.45
Iteration: 4688 | Episodes: 285850 | Median Reward: 46.03 | Max Reward: 49.45
Iteration: 4688 | Episodes: 285900 | Median Reward: 46.15 | Max Reward: 49.45
Iteration: 4689 | Episodes: 285950 | Median Reward: 45.98 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -54.1     |
| time/                   |           |
|    fps                  | 364       |
|    iterations           | 4690      |
|    time_elapsed         | 79011     |
|    total_timesteps      | 28815360  |
| train/                  |           |
|    approx_kl            | 1.0819125 |
|    clip_fraction        | 0.0124    |
|    clip_range           | 0.4       |
|    entropy_loss         | -131      |
|    explained_variance   | 0.926     |
|    learning_rate        | 0.0001    |
|    loss                 | 18.5      |
|    n_updates            | 46890     |
|    policy_gradient_loss | 0.00119   |
|    std                  | 7.36      |
|    value_loss           | 50        |
---------------------------------------
Iteration: 4690 | Episodes: 286000 | Median Reward: 46.06 | Max Reward: 49.45
Iteration: 4691 | Episodes: 286050 | Median Reward: 46.06 | Max Reward: 49.45
Iteration: 4692 | Episodes: 286100 | Median Reward: 46.05 | Max Reward: 49.45
Iteration: 4692 | Episodes: 286150 | Median Reward: 46.67 | Max Reward: 49.45
Iteration: 4693 | Episodes: 286200 | Median Reward: 46.12 | Max Reward: 49.45
Iteration: 4694 | Episodes: 286250 | Median Reward: 46.14 | Max Reward: 49.45
Iteration: 4695 | Episodes: 286300 | Median Reward: 46.15 | Max Reward: 49.45
Iteration: 4696 | Episodes: 286350 | Median Reward: 46.09 | Max Reward: 49.45
Iteration: 4697 | Episodes: 286400 | Median Reward: 46.24 | Max Reward: 49.45
Iteration: 4697 | Episodes: 286450 | Median Reward: 46.44 | Max Reward: 49.45
Iteration: 4698 | Episodes: 286500 | Median Reward: 46.26 | Max Reward: 49.45
Iteration: 4699 | Episodes: 286550 | Median Reward: 46.23 | Max Reward: 49.45
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.7        |
| time/                   |              |
|    fps                  | 364          |
|    iterations           | 4700         |
|    time_elapsed         | 79192        |
|    total_timesteps      | 28876800     |
| train/                  |              |
|    approx_kl            | 0.0038134397 |
|    clip_fraction        | 0.00106      |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 0.93         |
|    learning_rate        | 0.0001       |
|    loss                 | 17.9         |
|    n_updates            | 46990        |
|    policy_gradient_loss | -0.000351    |
|    std                  | 7.47         |
|    value_loss           | 47.2         |
------------------------------------------
Iteration: 4700 | Episodes: 286600 | Median Reward: 46.26 | Max Reward: 49.45
Iteration: 4701 | Episodes: 286650 | Median Reward: 46.68 | Max Reward: 49.45
Iteration: 4702 | Episodes: 286700 | Median Reward: 46.25 | Max Reward: 49.45
Iteration: 4702 | Episodes: 286750 | Median Reward: 46.16 | Max Reward: 49.45
Iteration: 4703 | Episodes: 286800 | Median Reward: 46.28 | Max Reward: 49.45
Iteration: 4704 | Episodes: 286850 | Median Reward: 46.32 | Max Reward: 49.45
Iteration: 4705 | Episodes: 286900 | Median Reward: 46.16 | Max Reward: 49.45
Iteration: 4706 | Episodes: 286950 | Median Reward: 46.16 | Max Reward: 49.45
Iteration: 4706 | Episodes: 287000 | Median Reward: 46.46 | Max Reward: 49.45
Iteration: 4707 | Episodes: 287050 | Median Reward: 46.59 | Max Reward: 49.45
Iteration: 4708 | Episodes: 287100 | Median Reward: 44.52 | Max Reward: 49.45
Iteration: 4709 | Episodes: 287150 | Median Reward: 45.20 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -55.1    |
| time/                   |          |
|    fps                  | 364      |
|    iterations           | 4710     |
|    time_elapsed         | 79369    |
|    total_timesteps      | 28938240 |
| train/                  |          |
|    approx_kl            | 8.769449 |
|    clip_fraction        | 0.0635   |
|    clip_range           | 0.4      |
|    entropy_loss         | -131     |
|    explained_variance   | 0.943    |
|    learning_rate        | 0.0001   |
|    loss                 | 14.2     |
|    n_updates            | 47090    |
|    policy_gradient_loss | 0.035    |
|    std                  | 7.57     |
|    value_loss           | 35.1     |
--------------------------------------
Iteration: 4710 | Episodes: 287200 | Median Reward: 46.24 | Max Reward: 49.45
Iteration: 4711 | Episodes: 287250 | Median Reward: 46.28 | Max Reward: 49.45
Iteration: 4711 | Episodes: 287300 | Median Reward: 46.17 | Max Reward: 49.45
Iteration: 4712 | Episodes: 287350 | Median Reward: 45.05 | Max Reward: 49.45
Iteration: 4713 | Episodes: 287400 | Median Reward: 45.02 | Max Reward: 49.45
Iteration: 4714 | Episodes: 287450 | Median Reward: 44.56 | Max Reward: 49.45
Iteration: 4715 | Episodes: 287500 | Median Reward: 42.55 | Max Reward: 49.45
Iteration: 4716 | Episodes: 287550 | Median Reward: 42.73 | Max Reward: 49.45
Iteration: 4716 | Episodes: 287600 | Median Reward: 44.42 | Max Reward: 49.45
Iteration: 4717 | Episodes: 287650 | Median Reward: 46.12 | Max Reward: 49.45
Iteration: 4718 | Episodes: 287700 | Median Reward: 46.21 | Max Reward: 49.45
Iteration: 4719 | Episodes: 287750 | Median Reward: 46.33 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -55.8     |
| time/                   |           |
|    fps                  | 364       |
|    iterations           | 4720      |
|    time_elapsed         | 79547     |
|    total_timesteps      | 28999680  |
| train/                  |           |
|    approx_kl            | 10.020491 |
|    clip_fraction        | 0.0417    |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.937     |
|    learning_rate        | 0.0001    |
|    loss                 | 14.5      |
|    n_updates            | 47190     |
|    policy_gradient_loss | 0.00848   |
|    std                  | 7.67      |
|    value_loss           | 42.5      |
---------------------------------------
Iteration: 4720 | Episodes: 287800 | Median Reward: 46.28 | Max Reward: 49.45
Iteration: 4720 | Episodes: 287850 | Median Reward: 44.66 | Max Reward: 49.45
Iteration: 4721 | Episodes: 287900 | Median Reward: 46.13 | Max Reward: 49.45
Iteration: 4722 | Episodes: 287950 | Median Reward: 46.44 | Max Reward: 49.45
Iteration: 4723 | Episodes: 288000 | Median Reward: 46.46 | Max Reward: 49.45
Iteration: 4724 | Episodes: 288050 | Median Reward: 46.15 | Max Reward: 49.45
Iteration: 4725 | Episodes: 288100 | Median Reward: 45.04 | Max Reward: 49.45
Iteration: 4725 | Episodes: 288150 | Median Reward: 45.16 | Max Reward: 49.45
Iteration: 4726 | Episodes: 288200 | Median Reward: 45.78 | Max Reward: 49.45
Iteration: 4727 | Episodes: 288250 | Median Reward: 45.32 | Max Reward: 49.45
Iteration: 4728 | Episodes: 288300 | Median Reward: 45.32 | Max Reward: 49.45
Iteration: 4729 | Episodes: 288350 | Median Reward: 45.45 | Max Reward: 49.45
Iteration: 4729 | Episodes: 288400 | Median Reward: 43.57 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -56.3    |
| time/                   |          |
|    fps                  | 364      |
|    iterations           | 4730     |
|    time_elapsed         | 79738    |
|    total_timesteps      | 29061120 |
| train/                  |          |
|    approx_kl            | 8.949883 |
|    clip_fraction        | 0.0597   |
|    clip_range           | 0.4      |
|    entropy_loss         | -132     |
|    explained_variance   | 0.943    |
|    learning_rate        | 0.0001   |
|    loss                 | 11.9     |
|    n_updates            | 47290    |
|    policy_gradient_loss | 0.00399  |
|    std                  | 7.76     |
|    value_loss           | 36.1     |
--------------------------------------
Iteration: 4730 | Episodes: 288450 | Median Reward: 44.70 | Max Reward: 49.45
Iteration: 4731 | Episodes: 288500 | Median Reward: 46.41 | Max Reward: 49.45
Iteration: 4732 | Episodes: 288550 | Median Reward: 46.51 | Max Reward: 49.45
Iteration: 4733 | Episodes: 288600 | Median Reward: 46.53 | Max Reward: 49.45
Iteration: 4734 | Episodes: 288650 | Median Reward: 45.92 | Max Reward: 49.45
Iteration: 4734 | Episodes: 288700 | Median Reward: 45.51 | Max Reward: 49.45
Iteration: 4735 | Episodes: 288750 | Median Reward: 46.58 | Max Reward: 49.45
Iteration: 4736 | Episodes: 288800 | Median Reward: 46.41 | Max Reward: 49.45
Iteration: 4737 | Episodes: 288850 | Median Reward: 46.38 | Max Reward: 49.45
Iteration: 4738 | Episodes: 288900 | Median Reward: 46.28 | Max Reward: 49.45
Iteration: 4739 | Episodes: 288950 | Median Reward: 45.91 | Max Reward: 49.45
Iteration: 4739 | Episodes: 289000 | Median Reward: 46.38 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -54.2     |
| time/                   |           |
|    fps                  | 364       |
|    iterations           | 4740      |
|    time_elapsed         | 79918     |
|    total_timesteps      | 29122560  |
| train/                  |           |
|    approx_kl            | 6.9888954 |
|    clip_fraction        | 0.139     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.942     |
|    learning_rate        | 0.0001    |
|    loss                 | 12.9      |
|    n_updates            | 47390     |
|    policy_gradient_loss | 0.0328    |
|    std                  | 7.86      |
|    value_loss           | 38.7      |
---------------------------------------
Iteration: 4740 | Episodes: 289050 | Median Reward: 46.33 | Max Reward: 49.45
Iteration: 4741 | Episodes: 289100 | Median Reward: 45.29 | Max Reward: 49.45
Iteration: 4742 | Episodes: 289150 | Median Reward: 45.09 | Max Reward: 49.45
Iteration: 4743 | Episodes: 289200 | Median Reward: 45.51 | Max Reward: 49.45
Iteration: 4743 | Episodes: 289250 | Median Reward: 45.63 | Max Reward: 49.45
Iteration: 4744 | Episodes: 289300 | Median Reward: 46.77 | Max Reward: 49.45
Iteration: 4745 | Episodes: 289350 | Median Reward: 46.56 | Max Reward: 49.45
Iteration: 4746 | Episodes: 289400 | Median Reward: 46.43 | Max Reward: 49.45
Iteration: 4747 | Episodes: 289450 | Median Reward: 44.27 | Max Reward: 49.45
Iteration: 4748 | Episodes: 289500 | Median Reward: 46.38 | Max Reward: 49.45
Iteration: 4748 | Episodes: 289550 | Median Reward: 45.29 | Max Reward: 49.45
Iteration: 4749 | Episodes: 289600 | Median Reward: 44.87 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -55.8     |
| time/                   |           |
|    fps                  | 364       |
|    iterations           | 4750      |
|    time_elapsed         | 80097     |
|    total_timesteps      | 29184000  |
| train/                  |           |
|    approx_kl            | 5.9691896 |
|    clip_fraction        | 0.129     |
|    clip_range           | 0.4       |
|    entropy_loss         | -132      |
|    explained_variance   | 0.948     |
|    learning_rate        | 0.0001    |
|    loss                 | 11        |
|    n_updates            | 47490     |
|    policy_gradient_loss | 0.0382    |
|    std                  | 7.96      |
|    value_loss           | 32.9      |
---------------------------------------
Iteration: 4750 | Episodes: 289650 | Median Reward: 43.72 | Max Reward: 49.45
Iteration: 4751 | Episodes: 289700 | Median Reward: 43.49 | Max Reward: 49.45
Iteration: 4752 | Episodes: 289750 | Median Reward: 43.60 | Max Reward: 49.45
Iteration: 4752 | Episodes: 289800 | Median Reward: 45.57 | Max Reward: 49.45
Iteration: 4753 | Episodes: 289850 | Median Reward: 46.00 | Max Reward: 49.45
Iteration: 4754 | Episodes: 289900 | Median Reward: 47.18 | Max Reward: 49.45
Iteration: 4755 | Episodes: 289950 | Median Reward: 47.18 | Max Reward: 49.45
Iteration: 4756 | Episodes: 290000 | Median Reward: 47.24 | Max Reward: 49.45
Iteration: 4757 | Episodes: 290050 | Median Reward: 47.22 | Max Reward: 49.45
Iteration: 4757 | Episodes: 290100 | Median Reward: 47.13 | Max Reward: 49.45
Iteration: 4758 | Episodes: 290150 | Median Reward: 47.19 | Max Reward: 49.45
Iteration: 4759 | Episodes: 290200 | Median Reward: 44.40 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -55.1    |
| time/                   |          |
|    fps                  | 364      |
|    iterations           | 4760     |
|    time_elapsed         | 80289    |
|    total_timesteps      | 29245440 |
| train/                  |          |
|    approx_kl            | 7.85854  |
|    clip_fraction        | 0.138    |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0.943    |
|    learning_rate        | 0.0001   |
|    loss                 | 12       |
|    n_updates            | 47590    |
|    policy_gradient_loss | 0.045    |
|    std                  | 8.05     |
|    value_loss           | 37.5     |
--------------------------------------
Iteration: 4760 | Episodes: 290250 | Median Reward: 43.65 | Max Reward: 49.45
Iteration: 4761 | Episodes: 290300 | Median Reward: 43.26 | Max Reward: 49.45
Iteration: 4762 | Episodes: 290350 | Median Reward: 41.92 | Max Reward: 49.45
Iteration: 4762 | Episodes: 290400 | Median Reward: 45.04 | Max Reward: 49.45
Iteration: 4763 | Episodes: 290450 | Median Reward: 44.86 | Max Reward: 49.45
Iteration: 4764 | Episodes: 290500 | Median Reward: 45.47 | Max Reward: 49.45
Iteration: 4765 | Episodes: 290550 | Median Reward: 45.59 | Max Reward: 49.45
Iteration: 4766 | Episodes: 290600 | Median Reward: 45.15 | Max Reward: 49.45
Iteration: 4766 | Episodes: 290650 | Median Reward: 46.56 | Max Reward: 49.45
Iteration: 4767 | Episodes: 290700 | Median Reward: 45.88 | Max Reward: 49.45
Iteration: 4768 | Episodes: 290750 | Median Reward: 46.91 | Max Reward: 49.45
Iteration: 4769 | Episodes: 290800 | Median Reward: 46.39 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -55.6      |
| time/                   |            |
|    fps                  | 364        |
|    iterations           | 4770       |
|    time_elapsed         | 80473      |
|    total_timesteps      | 29306880   |
| train/                  |            |
|    approx_kl            | 0.80210423 |
|    clip_fraction        | 0.0375     |
|    clip_range           | 0.4        |
|    entropy_loss         | -133       |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.0001     |
|    loss                 | 11.3       |
|    n_updates            | 47690      |
|    policy_gradient_loss | 0.00319    |
|    std                  | 8.15       |
|    value_loss           | 35.6       |
----------------------------------------
Iteration: 4770 | Episodes: 290850 | Median Reward: 41.39 | Max Reward: 49.45
Iteration: 4771 | Episodes: 290900 | Median Reward: 40.08 | Max Reward: 49.45
Iteration: 4771 | Episodes: 290950 | Median Reward: 43.33 | Max Reward: 49.45
Iteration: 4772 | Episodes: 291000 | Median Reward: 43.72 | Max Reward: 49.45
Iteration: 4773 | Episodes: 291050 | Median Reward: 44.11 | Max Reward: 49.45
Iteration: 4774 | Episodes: 291100 | Median Reward: 44.24 | Max Reward: 49.45
Iteration: 4775 | Episodes: 291150 | Median Reward: 44.68 | Max Reward: 49.45
Iteration: 4775 | Episodes: 291200 | Median Reward: 42.61 | Max Reward: 49.45
Iteration: 4776 | Episodes: 291250 | Median Reward: 47.14 | Max Reward: 49.45
Iteration: 4777 | Episodes: 291300 | Median Reward: 47.18 | Max Reward: 49.45
Iteration: 4778 | Episodes: 291350 | Median Reward: 47.16 | Max Reward: 49.45
Iteration: 4779 | Episodes: 291400 | Median Reward: 47.14 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -52.9    |
| time/                   |          |
|    fps                  | 364      |
|    iterations           | 4780     |
|    time_elapsed         | 80655    |
|    total_timesteps      | 29368320 |
| train/                  |          |
|    approx_kl            | 4.390461 |
|    clip_fraction        | 0.041    |
|    clip_range           | 0.4      |
|    entropy_loss         | -133     |
|    explained_variance   | 0.945    |
|    learning_rate        | 0.0001   |
|    loss                 | 11.5     |
|    n_updates            | 47790    |
|    policy_gradient_loss | 0.00517  |
|    std                  | 8.26     |
|    value_loss           | 36.1     |
--------------------------------------
Iteration: 4780 | Episodes: 291450 | Median Reward: 46.92 | Max Reward: 49.45
Iteration: 4780 | Episodes: 291500 | Median Reward: 46.67 | Max Reward: 49.45
Iteration: 4781 | Episodes: 291550 | Median Reward: 46.77 | Max Reward: 49.45
Iteration: 4782 | Episodes: 291600 | Median Reward: 43.79 | Max Reward: 49.45
Iteration: 4783 | Episodes: 291650 | Median Reward: 46.02 | Max Reward: 49.45
Iteration: 4784 | Episodes: 291700 | Median Reward: 46.63 | Max Reward: 49.45
Iteration: 4785 | Episodes: 291750 | Median Reward: 46.77 | Max Reward: 49.45
Iteration: 4785 | Episodes: 291800 | Median Reward: 46.01 | Max Reward: 49.45
Iteration: 4786 | Episodes: 291850 | Median Reward: 46.31 | Max Reward: 49.45
Iteration: 4787 | Episodes: 291900 | Median Reward: 46.33 | Max Reward: 49.45
Iteration: 4788 | Episodes: 291950 | Median Reward: 45.85 | Max Reward: 49.45
Iteration: 4789 | Episodes: 292000 | Median Reward: 44.89 | Max Reward: 49.45
Iteration: 4789 | Episodes: 292050 | Median Reward: 44.12 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -56.3     |
| time/                   |           |
|    fps                  | 364       |
|    iterations           | 4790      |
|    time_elapsed         | 80838     |
|    total_timesteps      | 29429760  |
| train/                  |           |
|    approx_kl            | 3.9498873 |
|    clip_fraction        | 0.0851    |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0001    |
|    loss                 | 9.21      |
|    n_updates            | 47890     |
|    policy_gradient_loss | 0.0206    |
|    std                  | 8.36      |
|    value_loss           | 31.4      |
---------------------------------------
Iteration: 4790 | Episodes: 292100 | Median Reward: 43.66 | Max Reward: 49.45
Iteration: 4791 | Episodes: 292150 | Median Reward: 46.99 | Max Reward: 49.45
Iteration: 4792 | Episodes: 292200 | Median Reward: 46.15 | Max Reward: 49.45
Iteration: 4793 | Episodes: 292250 | Median Reward: 43.80 | Max Reward: 49.45
Iteration: 4794 | Episodes: 292300 | Median Reward: 43.93 | Max Reward: 49.45
Iteration: 4794 | Episodes: 292350 | Median Reward: 42.27 | Max Reward: 49.45
Iteration: 4795 | Episodes: 292400 | Median Reward: 44.56 | Max Reward: 49.45
Iteration: 4796 | Episodes: 292450 | Median Reward: 43.75 | Max Reward: 49.45
Iteration: 4797 | Episodes: 292500 | Median Reward: 42.80 | Max Reward: 49.45
Iteration: 4798 | Episodes: 292550 | Median Reward: 44.71 | Max Reward: 49.45
Iteration: 4799 | Episodes: 292600 | Median Reward: 45.36 | Max Reward: 49.45
Iteration: 4799 | Episodes: 292650 | Median Reward: 42.93 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -56.3     |
| time/                   |           |
|    fps                  | 364       |
|    iterations           | 4800      |
|    time_elapsed         | 81017     |
|    total_timesteps      | 29491200  |
| train/                  |           |
|    approx_kl            | 2.0878687 |
|    clip_fraction        | 0.0383    |
|    clip_range           | 0.4       |
|    entropy_loss         | -134      |
|    explained_variance   | 0.953     |
|    learning_rate        | 0.0001    |
|    loss                 | 7.29      |
|    n_updates            | 47990     |
|    policy_gradient_loss | 0.00242   |
|    std                  | 8.43      |
|    value_loss           | 30.1      |
---------------------------------------
Iteration: 4800 | Episodes: 292700 | Median Reward: 44.72 | Max Reward: 49.45
Iteration: 4801 | Episodes: 292750 | Median Reward: 45.78 | Max Reward: 49.45
Iteration: 4802 | Episodes: 292800 | Median Reward: 46.10 | Max Reward: 49.45
Iteration: 4803 | Episodes: 292850 | Median Reward: 45.44 | Max Reward: 49.45
Iteration: 4803 | Episodes: 292900 | Median Reward: 42.68 | Max Reward: 49.45
Iteration: 4804 | Episodes: 292950 | Median Reward: 43.87 | Max Reward: 49.45
Iteration: 4805 | Episodes: 293000 | Median Reward: 43.78 | Max Reward: 49.45
Iteration: 4806 | Episodes: 293050 | Median Reward: 43.54 | Max Reward: 49.45
Iteration: 4807 | Episodes: 293100 | Median Reward: 44.23 | Max Reward: 49.45
Iteration: 4808 | Episodes: 293150 | Median Reward: 44.22 | Max Reward: 49.45
Iteration: 4808 | Episodes: 293200 | Median Reward: 41.75 | Max Reward: 49.45
Iteration: 4809 | Episodes: 293250 | Median Reward: 46.45 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -55.4    |
| time/                   |          |
|    fps                  | 363      |
|    iterations           | 4810     |
|    time_elapsed         | 81199    |
|    total_timesteps      | 29552640 |
| train/                  |          |
|    approx_kl            | 9.020962 |
|    clip_fraction        | 0.188    |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0.96     |
|    learning_rate        | 0.0001   |
|    loss                 | 4.51     |
|    n_updates            | 48090    |
|    policy_gradient_loss | 0.0715   |
|    std                  | 8.51     |
|    value_loss           | 24.3     |
--------------------------------------
Iteration: 4810 | Episodes: 293300 | Median Reward: 44.49 | Max Reward: 49.45
Iteration: 4811 | Episodes: 293350 | Median Reward: 44.27 | Max Reward: 49.45
Iteration: 4812 | Episodes: 293400 | Median Reward: 42.56 | Max Reward: 49.45
Iteration: 4812 | Episodes: 293450 | Median Reward: 40.14 | Max Reward: 49.45
Iteration: 4813 | Episodes: 293500 | Median Reward: 44.92 | Max Reward: 49.45
Iteration: 4814 | Episodes: 293550 | Median Reward: 44.32 | Max Reward: 49.45
Iteration: 4815 | Episodes: 293600 | Median Reward: 46.05 | Max Reward: 49.45
Iteration: 4816 | Episodes: 293650 | Median Reward: 47.20 | Max Reward: 49.45
Iteration: 4817 | Episodes: 293700 | Median Reward: 46.49 | Max Reward: 49.45
Iteration: 4817 | Episodes: 293750 | Median Reward: 42.89 | Max Reward: 49.45
Iteration: 4818 | Episodes: 293800 | Median Reward: 43.54 | Max Reward: 49.45
Iteration: 4819 | Episodes: 293850 | Median Reward: 43.35 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -56.8    |
| time/                   |          |
|    fps                  | 363      |
|    iterations           | 4820     |
|    time_elapsed         | 81382    |
|    total_timesteps      | 29614080 |
| train/                  |          |
|    approx_kl            | 5.438601 |
|    clip_fraction        | 0.0441   |
|    clip_range           | 0.4      |
|    entropy_loss         | -134     |
|    explained_variance   | 0.955    |
|    learning_rate        | 0.0001   |
|    loss                 | 8.41     |
|    n_updates            | 48190    |
|    policy_gradient_loss | -0.00324 |
|    std                  | 8.6      |
|    value_loss           | 28       |
--------------------------------------
Iteration: 4820 | Episodes: 293900 | Median Reward: 43.66 | Max Reward: 49.45
Iteration: 4821 | Episodes: 293950 | Median Reward: 45.63 | Max Reward: 49.45
Iteration: 4822 | Episodes: 294000 | Median Reward: 46.88 | Max Reward: 49.45
Iteration: 4822 | Episodes: 294050 | Median Reward: 45.01 | Max Reward: 49.45
Iteration: 4823 | Episodes: 294100 | Median Reward: 40.93 | Max Reward: 49.45
Iteration: 4824 | Episodes: 294150 | Median Reward: 41.65 | Max Reward: 49.45
Iteration: 4825 | Episodes: 294200 | Median Reward: 44.13 | Max Reward: 49.45
Iteration: 4826 | Episodes: 294250 | Median Reward: 43.40 | Max Reward: 49.45
Iteration: 4826 | Episodes: 294300 | Median Reward: 42.60 | Max Reward: 49.45
Iteration: 4827 | Episodes: 294350 | Median Reward: 42.89 | Max Reward: 49.45
Iteration: 4828 | Episodes: 294400 | Median Reward: 46.16 | Max Reward: 49.45
Iteration: 4829 | Episodes: 294450 | Median Reward: 46.16 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -54.8     |
| time/                   |           |
|    fps                  | 363       |
|    iterations           | 4830      |
|    time_elapsed         | 81570     |
|    total_timesteps      | 29675520  |
| train/                  |           |
|    approx_kl            | 1.8025491 |
|    clip_fraction        | 0.0374    |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | 0.953     |
|    learning_rate        | 0.0001    |
|    loss                 | 7.65      |
|    n_updates            | 48290     |
|    policy_gradient_loss | 0.0124    |
|    std                  | 8.68      |
|    value_loss           | 30.2      |
---------------------------------------
Iteration: 4830 | Episodes: 294500 | Median Reward: 45.47 | Max Reward: 49.45
Iteration: 4831 | Episodes: 294550 | Median Reward: 41.74 | Max Reward: 49.45
Iteration: 4831 | Episodes: 294600 | Median Reward: 40.69 | Max Reward: 49.45
Iteration: 4832 | Episodes: 294650 | Median Reward: 41.95 | Max Reward: 49.45
Iteration: 4833 | Episodes: 294700 | Median Reward: 39.27 | Max Reward: 49.45
Iteration: 4834 | Episodes: 294750 | Median Reward: 40.22 | Max Reward: 49.45
Iteration: 4835 | Episodes: 294800 | Median Reward: 41.46 | Max Reward: 49.45
Iteration: 4835 | Episodes: 294850 | Median Reward: 47.33 | Max Reward: 49.45
Iteration: 4836 | Episodes: 294900 | Median Reward: 46.68 | Max Reward: 49.45
Iteration: 4837 | Episodes: 294950 | Median Reward: 46.91 | Max Reward: 49.45
Iteration: 4838 | Episodes: 295000 | Median Reward: 46.91 | Max Reward: 49.45
Iteration: 4839 | Episodes: 295050 | Median Reward: 46.78 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -53.2     |
| time/                   |           |
|    fps                  | 363       |
|    iterations           | 4840      |
|    time_elapsed         | 81751     |
|    total_timesteps      | 29736960  |
| train/                  |           |
|    approx_kl            | 2.0505104 |
|    clip_fraction        | 0.0156    |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | 0.949     |
|    learning_rate        | 0.0001    |
|    loss                 | 11        |
|    n_updates            | 48390     |
|    policy_gradient_loss | 0.000955  |
|    std                  | 8.76      |
|    value_loss           | 33.6      |
---------------------------------------
Iteration: 4840 | Episodes: 295100 | Median Reward: 47.10 | Max Reward: 49.45
Iteration: 4840 | Episodes: 295150 | Median Reward: 46.59 | Max Reward: 49.45
Iteration: 4841 | Episodes: 295200 | Median Reward: 46.87 | Max Reward: 49.45
Iteration: 4842 | Episodes: 295250 | Median Reward: 46.32 | Max Reward: 49.45
Iteration: 4843 | Episodes: 295300 | Median Reward: 46.15 | Max Reward: 49.45
Iteration: 4844 | Episodes: 295350 | Median Reward: 44.34 | Max Reward: 49.45
Iteration: 4845 | Episodes: 295400 | Median Reward: 41.65 | Max Reward: 49.45
Iteration: 4845 | Episodes: 295450 | Median Reward: 43.28 | Max Reward: 49.45
Iteration: 4846 | Episodes: 295500 | Median Reward: 44.90 | Max Reward: 49.45
Iteration: 4847 | Episodes: 295550 | Median Reward: 43.06 | Max Reward: 49.45
Iteration: 4848 | Episodes: 295600 | Median Reward: 40.49 | Max Reward: 49.45
Iteration: 4849 | Episodes: 295650 | Median Reward: 44.29 | Max Reward: 49.45
Iteration: 4849 | Episodes: 295700 | Median Reward: 46.41 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -55.2     |
| time/                   |           |
|    fps                  | 363       |
|    iterations           | 4850      |
|    time_elapsed         | 81931     |
|    total_timesteps      | 29798400  |
| train/                  |           |
|    approx_kl            | 1.2866474 |
|    clip_fraction        | 0.0256    |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | 0.958     |
|    learning_rate        | 0.0001    |
|    loss                 | 6.21      |
|    n_updates            | 48490     |
|    policy_gradient_loss | 0.00392   |
|    std                  | 8.87      |
|    value_loss           | 26.3      |
---------------------------------------
Iteration: 4850 | Episodes: 295750 | Median Reward: 44.51 | Max Reward: 49.45
Iteration: 4851 | Episodes: 295800 | Median Reward: 44.51 | Max Reward: 49.45
Iteration: 4852 | Episodes: 295850 | Median Reward: 43.61 | Max Reward: 49.45
Iteration: 4853 | Episodes: 295900 | Median Reward: 43.27 | Max Reward: 49.45
Iteration: 4854 | Episodes: 295950 | Median Reward: 44.65 | Max Reward: 49.45
Iteration: 4854 | Episodes: 296000 | Median Reward: 41.66 | Max Reward: 49.45
Iteration: 4855 | Episodes: 296050 | Median Reward: 40.30 | Max Reward: 49.45
Iteration: 4856 | Episodes: 296100 | Median Reward: 46.62 | Max Reward: 49.45
Iteration: 4857 | Episodes: 296150 | Median Reward: 47.01 | Max Reward: 49.45
Iteration: 4858 | Episodes: 296200 | Median Reward: 46.91 | Max Reward: 49.45
Iteration: 4859 | Episodes: 296250 | Median Reward: 46.99 | Max Reward: 49.45
Iteration: 4859 | Episodes: 296300 | Median Reward: 44.74 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -55.7     |
| time/                   |           |
|    fps                  | 363       |
|    iterations           | 4860      |
|    time_elapsed         | 82123     |
|    total_timesteps      | 29859840  |
| train/                  |           |
|    approx_kl            | 5.9621296 |
|    clip_fraction        | 0.066     |
|    clip_range           | 0.4       |
|    entropy_loss         | -135      |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0001    |
|    loss                 | 10.3      |
|    n_updates            | 48590     |
|    policy_gradient_loss | 0.0172    |
|    std                  | 8.98      |
|    value_loss           | 31.9      |
---------------------------------------
Iteration: 4860 | Episodes: 296350 | Median Reward: 45.19 | Max Reward: 49.45
Iteration: 4861 | Episodes: 296400 | Median Reward: 45.16 | Max Reward: 49.45
Iteration: 4862 | Episodes: 296450 | Median Reward: 41.38 | Max Reward: 49.45
Iteration: 4863 | Episodes: 296500 | Median Reward: 39.88 | Max Reward: 49.45
Iteration: 4863 | Episodes: 296550 | Median Reward: 40.88 | Max Reward: 49.45
Iteration: 4864 | Episodes: 296600 | Median Reward: 46.71 | Max Reward: 49.45
Iteration: 4865 | Episodes: 296650 | Median Reward: 45.24 | Max Reward: 49.45
Iteration: 4866 | Episodes: 296700 | Median Reward: 42.76 | Max Reward: 49.45
Iteration: 4867 | Episodes: 296750 | Median Reward: 40.94 | Max Reward: 49.45
Iteration: 4868 | Episodes: 296800 | Median Reward: 42.86 | Max Reward: 49.45
Iteration: 4868 | Episodes: 296850 | Median Reward: 41.74 | Max Reward: 49.45
Iteration: 4869 | Episodes: 296900 | Median Reward: 41.56 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -57.9     |
| time/                   |           |
|    fps                  | 363       |
|    iterations           | 4870      |
|    time_elapsed         | 82302     |
|    total_timesteps      | 29921280  |
| train/                  |           |
|    approx_kl            | 4.1885653 |
|    clip_fraction        | 0.0828    |
|    clip_range           | 0.4       |
|    entropy_loss         | -136      |
|    explained_variance   | 0.96      |
|    learning_rate        | 0.0001    |
|    loss                 | 5.78      |
|    n_updates            | 48690     |
|    policy_gradient_loss | 0.0186    |
|    std                  | 9.07      |
|    value_loss           | 24.7      |
---------------------------------------
Iteration: 4870 | Episodes: 296950 | Median Reward: 42.11 | Max Reward: 49.45
Iteration: 4871 | Episodes: 297000 | Median Reward: 42.95 | Max Reward: 49.45
Iteration: 4872 | Episodes: 297050 | Median Reward: 44.50 | Max Reward: 49.45
Iteration: 4872 | Episodes: 297100 | Median Reward: 44.85 | Max Reward: 49.45
Iteration: 4873 | Episodes: 297150 | Median Reward: 45.30 | Max Reward: 49.45
Iteration: 4874 | Episodes: 297200 | Median Reward: 45.51 | Max Reward: 49.45
Iteration: 4875 | Episodes: 297250 | Median Reward: 46.23 | Max Reward: 49.45
Iteration: 4876 | Episodes: 297300 | Median Reward: 46.23 | Max Reward: 49.45
Iteration: 4877 | Episodes: 297350 | Median Reward: 47.28 | Max Reward: 49.45
Iteration: 4877 | Episodes: 297400 | Median Reward: 46.60 | Max Reward: 49.45
Iteration: 4878 | Episodes: 297450 | Median Reward: 47.09 | Max Reward: 49.45
Iteration: 4879 | Episodes: 297500 | Median Reward: 47.09 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -52.9    |
| time/                   |          |
|    fps                  | 363      |
|    iterations           | 4880     |
|    time_elapsed         | 82481    |
|    total_timesteps      | 29982720 |
| train/                  |          |
|    approx_kl            | 3.059504 |
|    clip_fraction        | 0.0544   |
|    clip_range           | 0.4      |
|    entropy_loss         | -136     |
|    explained_variance   | 0.952    |
|    learning_rate        | 0.0001   |
|    loss                 | 9.4      |
|    n_updates            | 48790    |
|    policy_gradient_loss | 0.0132   |
|    std                  | 9.16     |
|    value_loss           | 32.1     |
--------------------------------------
Iteration: 4880 | Episodes: 297550 | Median Reward: 47.29 | Max Reward: 49.45
Iteration: 4881 | Episodes: 297600 | Median Reward: 46.71 | Max Reward: 49.45
Iteration: 4882 | Episodes: 297650 | Median Reward: 37.11 | Max Reward: 49.45
Iteration: 4882 | Episodes: 297700 | Median Reward: 39.96 | Max Reward: 49.45
Iteration: 4883 | Episodes: 297750 | Median Reward: 39.54 | Max Reward: 49.45
Iteration: 4884 | Episodes: 297800 | Median Reward: 38.15 | Max Reward: 49.45
Iteration: 4885 | Episodes: 297850 | Median Reward: 40.28 | Max Reward: 49.45
Iteration: 4886 | Episodes: 297900 | Median Reward: 42.15 | Max Reward: 49.45
Iteration: 4886 | Episodes: 297950 | Median Reward: 45.66 | Max Reward: 49.45
Iteration: 4887 | Episodes: 298000 | Median Reward: 42.19 | Max Reward: 49.45
Iteration: 4888 | Episodes: 298050 | Median Reward: 39.79 | Max Reward: 49.45
Iteration: 4889 | Episodes: 298100 | Median Reward: 38.53 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -61.2    |
| time/                   |          |
|    fps                  | 363      |
|    iterations           | 4890     |
|    time_elapsed         | 82659    |
|    total_timesteps      | 30044160 |
| train/                  |          |
|    approx_kl            | 1.665401 |
|    clip_fraction        | 0.0414   |
|    clip_range           | 0.4      |
|    entropy_loss         | -136     |
|    explained_variance   | 0.971    |
|    learning_rate        | 0.0001   |
|    loss                 | 2.39     |
|    n_updates            | 48890    |
|    policy_gradient_loss | 0.00837  |
|    std                  | 9.26     |
|    value_loss           | 16.9     |
--------------------------------------
Iteration: 4890 | Episodes: 298150 | Median Reward: 41.59 | Max Reward: 49.45
Iteration: 4891 | Episodes: 298200 | Median Reward: 43.93 | Max Reward: 49.45
Iteration: 4891 | Episodes: 298250 | Median Reward: 46.75 | Max Reward: 49.45
Iteration: 4892 | Episodes: 298300 | Median Reward: 43.16 | Max Reward: 49.45
Iteration: 4893 | Episodes: 298350 | Median Reward: 45.16 | Max Reward: 49.45
Iteration: 4894 | Episodes: 298400 | Median Reward: 45.18 | Max Reward: 49.45
Iteration: 4895 | Episodes: 298450 | Median Reward: 41.78 | Max Reward: 49.45
Iteration: 4896 | Episodes: 298500 | Median Reward: 40.85 | Max Reward: 49.45
Iteration: 4896 | Episodes: 298550 | Median Reward: 39.37 | Max Reward: 49.45
Iteration: 4897 | Episodes: 298600 | Median Reward: 43.27 | Max Reward: 49.45
Iteration: 4898 | Episodes: 298650 | Median Reward: 44.27 | Max Reward: 49.45
Iteration: 4899 | Episodes: 298700 | Median Reward: 45.73 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -56       |
| time/                   |           |
|    fps                  | 363       |
|    iterations           | 4900      |
|    time_elapsed         | 82852     |
|    total_timesteps      | 30105600  |
| train/                  |           |
|    approx_kl            | 2.6887078 |
|    clip_fraction        | 0.0251    |
|    clip_range           | 0.4       |
|    entropy_loss         | -137      |
|    explained_variance   | 0.953     |
|    learning_rate        | 0.0001    |
|    loss                 | 8.13      |
|    n_updates            | 48990     |
|    policy_gradient_loss | 0.00682   |
|    std                  | 9.35      |
|    value_loss           | 31.3      |
---------------------------------------
Iteration: 4900 | Episodes: 298750 | Median Reward: 41.68 | Max Reward: 49.45
Iteration: 4900 | Episodes: 298800 | Median Reward: 41.87 | Max Reward: 49.45
Iteration: 4901 | Episodes: 298850 | Median Reward: 43.76 | Max Reward: 49.45
Iteration: 4902 | Episodes: 298900 | Median Reward: 46.05 | Max Reward: 49.45
Iteration: 4903 | Episodes: 298950 | Median Reward: 46.91 | Max Reward: 49.45
Iteration: 4904 | Episodes: 299000 | Median Reward: 47.27 | Max Reward: 49.45
Iteration: 4905 | Episodes: 299050 | Median Reward: 47.06 | Max Reward: 49.45
Iteration: 4905 | Episodes: 299100 | Median Reward: 45.88 | Max Reward: 49.45
Iteration: 4906 | Episodes: 299150 | Median Reward: 46.43 | Max Reward: 49.45
Iteration: 4907 | Episodes: 299200 | Median Reward: 47.20 | Max Reward: 49.45
Iteration: 4908 | Episodes: 299250 | Median Reward: 47.21 | Max Reward: 49.45
Iteration: 4909 | Episodes: 299300 | Median Reward: 46.95 | Max Reward: 49.45
Iteration: 4909 | Episodes: 299350 | Median Reward: 45.62 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -54      |
| time/                   |          |
|    fps                  | 363      |
|    iterations           | 4910     |
|    time_elapsed         | 83031    |
|    total_timesteps      | 30167040 |
| train/                  |          |
|    approx_kl            | 2.409862 |
|    clip_fraction        | 0.0615   |
|    clip_range           | 0.4      |
|    entropy_loss         | -137     |
|    explained_variance   | 0.953    |
|    learning_rate        | 0.0001   |
|    loss                 | 8.5      |
|    n_updates            | 49090    |
|    policy_gradient_loss | 0.0205   |
|    std                  | 9.44     |
|    value_loss           | 31.1     |
--------------------------------------
Iteration: 4910 | Episodes: 299400 | Median Reward: 45.42 | Max Reward: 49.45
Iteration: 4911 | Episodes: 299450 | Median Reward: 46.96 | Max Reward: 49.45
Iteration: 4912 | Episodes: 299500 | Median Reward: 46.38 | Max Reward: 49.45
Iteration: 4913 | Episodes: 299550 | Median Reward: 46.38 | Max Reward: 49.45
Iteration: 4914 | Episodes: 299600 | Median Reward: 47.44 | Max Reward: 49.45
Iteration: 4914 | Episodes: 299650 | Median Reward: 45.12 | Max Reward: 49.45
Iteration: 4915 | Episodes: 299700 | Median Reward: 41.74 | Max Reward: 49.45
Iteration: 4916 | Episodes: 299750 | Median Reward: 41.71 | Max Reward: 49.45
Iteration: 4917 | Episodes: 299800 | Median Reward: 40.50 | Max Reward: 49.45
Iteration: 4918 | Episodes: 299850 | Median Reward: 40.38 | Max Reward: 49.45
Iteration: 4918 | Episodes: 299900 | Median Reward: 39.63 | Max Reward: 49.45
Iteration: 4919 | Episodes: 299950 | Median Reward: 40.09 | Max Reward: 49.45
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -59.3      |
| time/                   |            |
|    fps                  | 363        |
|    iterations           | 4920       |
|    time_elapsed         | 83210      |
|    total_timesteps      | 30228480   |
| train/                  |            |
|    approx_kl            | 0.10530021 |
|    clip_fraction        | 0.00759    |
|    clip_range           | 0.4        |
|    entropy_loss         | -137       |
|    explained_variance   | 0.97       |
|    learning_rate        | 0.0001     |
|    loss                 | 1.86       |
|    n_updates            | 49190      |
|    policy_gradient_loss | -0.00353   |
|    std                  | 9.54       |
|    value_loss           | 18.3       |
----------------------------------------
Iteration: 4920 | Episodes: 300000 | Median Reward: 39.89 | Max Reward: 49.45
Iteration: 4921 | Episodes: 300050 | Median Reward: 44.95 | Max Reward: 49.45
Iteration: 4922 | Episodes: 300100 | Median Reward: 46.24 | Max Reward: 49.45
Iteration: 4923 | Episodes: 300150 | Median Reward: 46.14 | Max Reward: 49.45
Iteration: 4923 | Episodes: 300200 | Median Reward: 46.57 | Max Reward: 49.45
Iteration: 4924 | Episodes: 300250 | Median Reward: 46.26 | Max Reward: 49.45
Iteration: 4925 | Episodes: 300300 | Median Reward: 46.82 | Max Reward: 49.45
Iteration: 4926 | Episodes: 300350 | Median Reward: 46.19 | Max Reward: 49.45
Iteration: 4927 | Episodes: 300400 | Median Reward: 45.43 | Max Reward: 49.45
Iteration: 4928 | Episodes: 300450 | Median Reward: 46.66 | Max Reward: 49.45
Iteration: 4928 | Episodes: 300500 | Median Reward: 46.48 | Max Reward: 49.45
Iteration: 4929 | Episodes: 300550 | Median Reward: 46.26 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -53.9     |
| time/                   |           |
|    fps                  | 363       |
|    iterations           | 4930      |
|    time_elapsed         | 83402     |
|    total_timesteps      | 30289920  |
| train/                  |           |
|    approx_kl            | 2.2660556 |
|    clip_fraction        | 0.0187    |
|    clip_range           | 0.4       |
|    entropy_loss         | -137      |
|    explained_variance   | 0.956     |
|    learning_rate        | 0.0001    |
|    loss                 | 8.34      |
|    n_updates            | 49290     |
|    policy_gradient_loss | -0.000798 |
|    std                  | 9.64      |
|    value_loss           | 29        |
---------------------------------------
Iteration: 4930 | Episodes: 300600 | Median Reward: 46.09 | Max Reward: 49.45
Iteration: 4931 | Episodes: 300650 | Median Reward: 45.89 | Max Reward: 49.45
Iteration: 4932 | Episodes: 300700 | Median Reward: 46.13 | Max Reward: 49.45
Iteration: 4932 | Episodes: 300750 | Median Reward: 46.72 | Max Reward: 49.45
Iteration: 4933 | Episodes: 300800 | Median Reward: 39.40 | Max Reward: 49.45
Iteration: 4934 | Episodes: 300850 | Median Reward: 37.85 | Max Reward: 49.45
Iteration: 4935 | Episodes: 300900 | Median Reward: 38.89 | Max Reward: 49.45
Iteration: 4936 | Episodes: 300950 | Median Reward: 44.17 | Max Reward: 49.45
Iteration: 4937 | Episodes: 301000 | Median Reward: 45.21 | Max Reward: 49.45
Iteration: 4937 | Episodes: 301050 | Median Reward: 44.55 | Max Reward: 49.45
Iteration: 4938 | Episodes: 301100 | Median Reward: 44.14 | Max Reward: 49.45
Iteration: 4939 | Episodes: 301150 | Median Reward: 40.59 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -58.9    |
| time/                   |          |
|    fps                  | 363      |
|    iterations           | 4940     |
|    time_elapsed         | 83584    |
|    total_timesteps      | 30351360 |
| train/                  |          |
|    approx_kl            | 4.063359 |
|    clip_fraction        | 0.113    |
|    clip_range           | 0.4      |
|    entropy_loss         | -138     |
|    explained_variance   | 0.962    |
|    learning_rate        | 0.0001   |
|    loss                 | 5.49     |
|    n_updates            | 49390    |
|    policy_gradient_loss | 0.0321   |
|    std                  | 9.74     |
|    value_loss           | 23.9     |
--------------------------------------
Iteration: 4940 | Episodes: 301200 | Median Reward: 40.58 | Max Reward: 49.45
Iteration: 4941 | Episodes: 301250 | Median Reward: 40.58 | Max Reward: 49.45
Iteration: 4942 | Episodes: 301300 | Median Reward: 43.06 | Max Reward: 49.45
Iteration: 4942 | Episodes: 301350 | Median Reward: 46.97 | Max Reward: 49.45
Iteration: 4943 | Episodes: 301400 | Median Reward: 46.27 | Max Reward: 49.45
Iteration: 4944 | Episodes: 301450 | Median Reward: 44.57 | Max Reward: 49.45
Iteration: 4945 | Episodes: 301500 | Median Reward: 44.57 | Max Reward: 49.45
Iteration: 4946 | Episodes: 301550 | Median Reward: 44.67 | Max Reward: 49.45
Iteration: 4946 | Episodes: 301600 | Median Reward: 41.68 | Max Reward: 49.45
Iteration: 4947 | Episodes: 301650 | Median Reward: 37.41 | Max Reward: 49.45
Iteration: 4948 | Episodes: 301700 | Median Reward: 40.11 | Max Reward: 49.45
Iteration: 4949 | Episodes: 301750 | Median Reward: 41.34 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -58.8     |
| time/                   |           |
|    fps                  | 363       |
|    iterations           | 4950      |
|    time_elapsed         | 83760     |
|    total_timesteps      | 30412800  |
| train/                  |           |
|    approx_kl            | 2.7889035 |
|    clip_fraction        | 0.0349    |
|    clip_range           | 0.4       |
|    entropy_loss         | -138      |
|    explained_variance   | 0.966     |
|    learning_rate        | 0.0001    |
|    loss                 | 4.83      |
|    n_updates            | 49490     |
|    policy_gradient_loss | 0.000512  |
|    std                  | 9.82      |
|    value_loss           | 20.7      |
---------------------------------------
Iteration: 4950 | Episodes: 301800 | Median Reward: 41.06 | Max Reward: 49.45
Iteration: 4951 | Episodes: 301850 | Median Reward: 43.07 | Max Reward: 49.45
Iteration: 4951 | Episodes: 301900 | Median Reward: 41.46 | Max Reward: 49.45
Iteration: 4952 | Episodes: 301950 | Median Reward: 39.42 | Max Reward: 49.45
Iteration: 4953 | Episodes: 302000 | Median Reward: 40.55 | Max Reward: 49.45
Iteration: 4954 | Episodes: 302050 | Median Reward: 44.52 | Max Reward: 49.45
Iteration: 4955 | Episodes: 302100 | Median Reward: 47.60 | Max Reward: 49.45
Iteration: 4955 | Episodes: 302150 | Median Reward: 46.97 | Max Reward: 49.45
Iteration: 4956 | Episodes: 302200 | Median Reward: 47.25 | Max Reward: 49.45
Iteration: 4957 | Episodes: 302250 | Median Reward: 47.04 | Max Reward: 49.45
Iteration: 4958 | Episodes: 302300 | Median Reward: 46.72 | Max Reward: 49.45
Iteration: 4959 | Episodes: 302350 | Median Reward: 45.92 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -57.9    |
| time/                   |          |
|    fps                  | 363      |
|    iterations           | 4960     |
|    time_elapsed         | 83947    |
|    total_timesteps      | 30474240 |
| train/                  |          |
|    approx_kl            | 7.498436 |
|    clip_fraction        | 0.219    |
|    clip_range           | 0.4      |
|    entropy_loss         | -138     |
|    explained_variance   | 0.956    |
|    learning_rate        | 0.0001   |
|    loss                 | 6.96     |
|    n_updates            | 49590    |
|    policy_gradient_loss | 0.0859   |
|    std                  | 9.92     |
|    value_loss           | 29       |
--------------------------------------
Iteration: 4960 | Episodes: 302400 | Median Reward: 38.86 | Max Reward: 49.45
Iteration: 4960 | Episodes: 302450 | Median Reward: 46.13 | Max Reward: 49.45
Iteration: 4961 | Episodes: 302500 | Median Reward: 47.40 | Max Reward: 49.45
Iteration: 4962 | Episodes: 302550 | Median Reward: 43.70 | Max Reward: 49.45
Iteration: 4963 | Episodes: 302600 | Median Reward: 46.75 | Max Reward: 49.45
Iteration: 4964 | Episodes: 302650 | Median Reward: 47.04 | Max Reward: 49.45
Iteration: 4965 | Episodes: 302700 | Median Reward: 47.57 | Max Reward: 49.45
Iteration: 4965 | Episodes: 302750 | Median Reward: 47.50 | Max Reward: 49.45
Iteration: 4966 | Episodes: 302800 | Median Reward: 47.47 | Max Reward: 49.45
Iteration: 4967 | Episodes: 302850 | Median Reward: 47.25 | Max Reward: 49.45
Iteration: 4968 | Episodes: 302900 | Median Reward: 46.82 | Max Reward: 49.45
Iteration: 4969 | Episodes: 302950 | Median Reward: 41.85 | Max Reward: 49.45
Iteration: 4969 | Episodes: 303000 | Median Reward: 45.66 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -56.6     |
| time/                   |           |
|    fps                  | 362       |
|    iterations           | 4970      |
|    time_elapsed         | 84130     |
|    total_timesteps      | 30535680  |
| train/                  |           |
|    approx_kl            | 1.8750947 |
|    clip_fraction        | 0.0487    |
|    clip_range           | 0.4       |
|    entropy_loss         | -138      |
|    explained_variance   | 0.97      |
|    learning_rate        | 0.0001    |
|    loss                 | 1.85      |
|    n_updates            | 49690     |
|    policy_gradient_loss | 0.0151    |
|    std                  | 10        |
|    value_loss           | 18.1      |
---------------------------------------
Iteration: 4970 | Episodes: 303050 | Median Reward: 45.14 | Max Reward: 49.45
Iteration: 4971 | Episodes: 303100 | Median Reward: 45.76 | Max Reward: 49.45
Iteration: 4972 | Episodes: 303150 | Median Reward: 46.57 | Max Reward: 49.45
Iteration: 4973 | Episodes: 303200 | Median Reward: 45.22 | Max Reward: 49.45
Iteration: 4974 | Episodes: 303250 | Median Reward: 45.08 | Max Reward: 49.45
Iteration: 4974 | Episodes: 303300 | Median Reward: 45.61 | Max Reward: 49.45
Iteration: 4975 | Episodes: 303350 | Median Reward: 47.45 | Max Reward: 49.45
Iteration: 4976 | Episodes: 303400 | Median Reward: 45.43 | Max Reward: 49.45
Iteration: 4977 | Episodes: 303450 | Median Reward: 38.11 | Max Reward: 49.45
Iteration: 4978 | Episodes: 303500 | Median Reward: 37.43 | Max Reward: 49.45
Iteration: 4978 | Episodes: 303550 | Median Reward: 35.67 | Max Reward: 49.45
Iteration: 4979 | Episodes: 303600 | Median Reward: 39.19 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -63.4     |
| time/                   |           |
|    fps                  | 362       |
|    iterations           | 4980      |
|    time_elapsed         | 84308     |
|    total_timesteps      | 30597120  |
| train/                  |           |
|    approx_kl            | 1.8247087 |
|    clip_fraction        | 0.0362    |
|    clip_range           | 0.4       |
|    entropy_loss         | -138      |
|    explained_variance   | 0.98      |
|    learning_rate        | 0.0001    |
|    loss                 | -1.42     |
|    n_updates            | 49790     |
|    policy_gradient_loss | 0.0164    |
|    std                  | 10.1      |
|    value_loss           | 12.3      |
---------------------------------------
Iteration: 4980 | Episodes: 303650 | Median Reward: 38.13 | Max Reward: 49.45
Iteration: 4981 | Episodes: 303700 | Median Reward: 44.15 | Max Reward: 49.45
Iteration: 4982 | Episodes: 303750 | Median Reward: 45.69 | Max Reward: 49.45
Iteration: 4983 | Episodes: 303800 | Median Reward: 44.54 | Max Reward: 49.45
Iteration: 4983 | Episodes: 303850 | Median Reward: 46.71 | Max Reward: 49.45
Iteration: 4984 | Episodes: 303900 | Median Reward: 46.89 | Max Reward: 49.45
Iteration: 4985 | Episodes: 303950 | Median Reward: 46.50 | Max Reward: 49.45
Iteration: 4986 | Episodes: 304000 | Median Reward: 46.15 | Max Reward: 49.45
Iteration: 4987 | Episodes: 304050 | Median Reward: 45.81 | Max Reward: 49.45
Iteration: 4988 | Episodes: 304100 | Median Reward: 47.15 | Max Reward: 49.45
Iteration: 4988 | Episodes: 304150 | Median Reward: 46.62 | Max Reward: 49.45
Iteration: 4989 | Episodes: 304200 | Median Reward: 45.06 | Max Reward: 49.45
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 97.2      |
|    ep_rew_mean          | -50.6     |
| time/                   |           |
|    fps                  | 362       |
|    iterations           | 4990      |
|    time_elapsed         | 84492     |
|    total_timesteps      | 30658560  |
| train/                  |           |
|    approx_kl            | 0.9827876 |
|    clip_fraction        | 0.0216    |
|    clip_range           | 0.4       |
|    entropy_loss         | -139      |
|    explained_variance   | 0.88      |
|    learning_rate        | 0.0001    |
|    loss                 | 32.8      |
|    n_updates            | 49890     |
|    policy_gradient_loss | -2.03e-05 |
|    std                  | 10.2      |
|    value_loss           | 71.4      |
---------------------------------------
Iteration: 4990 | Episodes: 304250 | Median Reward: 45.72 | Max Reward: 49.45
Iteration: 4991 | Episodes: 304300 | Median Reward: 46.51 | Max Reward: 49.45
Iteration: 4992 | Episodes: 304350 | Median Reward: 46.40 | Max Reward: 49.45
Iteration: 4992 | Episodes: 304400 | Median Reward: 45.82 | Max Reward: 49.45
Iteration: 4993 | Episodes: 304450 | Median Reward: 46.09 | Max Reward: 49.45
Iteration: 4994 | Episodes: 304500 | Median Reward: 38.35 | Max Reward: 49.45
Iteration: 4995 | Episodes: 304550 | Median Reward: 31.33 | Max Reward: 49.45
Iteration: 4996 | Episodes: 304600 | Median Reward: 30.54 | Max Reward: 49.45
Iteration: 4997 | Episodes: 304650 | Median Reward: 35.32 | Max Reward: 49.45
Iteration: 4997 | Episodes: 304700 | Median Reward: 36.20 | Max Reward: 49.45
Iteration: 4998 | Episodes: 304750 | Median Reward: 44.78 | Max Reward: 49.45
Iteration: 4999 | Episodes: 304800 | Median Reward: 40.08 | Max Reward: 49.45
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -61.8    |
| time/                   |          |
|    fps                  | 362      |
|    iterations           | 5000     |
|    time_elapsed         | 84684    |
|    total_timesteps      | 30720000 |
| train/                  |          |
|    approx_kl            | 1.431665 |
|    clip_fraction        | 0.0297   |
|    clip_range           | 0.4      |
|    entropy_loss         | -139     |
|    explained_variance   | 0.959    |
|    learning_rate        | 0.0001   |
|    loss                 | 7.11     |
|    n_updates            | 49990    |
|    policy_gradient_loss | 0.00626  |
|    std                  | 10.3     |
|    value_loss           | 25.8     |
--------------------------------------
Training End | Episodes: 304833 | Median Reward: 33.25 | Max Reward: 49.45
Plot saved as fig_g3d_50int_weighted.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> daaaaaexit()
Badly placed ()'s.
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> e[K(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> exit()
exit
Script done, file is d_50int_logs_5k.txt
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> exit()
exit

Script done on 2024-10-17 15:04:21-04:00 [COMMAND_EXIT_CODE="0"]
