Script started on 2024-10-16 03:01:09-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/25" COLUMNS="120" LINES="30"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python s[Kp[Kenv_setup_g3_d_2.py
Traceback (most recent call last):
  File "/home/easgrad/dgusain/Bot_hand/codes/env_setup_g3_d_2.py", line 2, in <module>
    import gymnasium as gym
ModuleNotFoundError: No module named 'gymnasium'
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> conda activate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> conda activate mujoco_test[43Gpython env_setup_g3_d_2.py
GPU 4: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
^[[B^[[B^[[BIteration: 2 | Episodes: 100 | Median Reward: 21.33 | Max Reward: 42.61
Iteration: 4 | Episodes: 200 | Median Reward: 18.85 | Max Reward: 42.61
Iteration: 7 | Episodes: 300 | Median Reward: 21.75 | Max Reward: 42.61
Iteration: 9 | Episodes: 400 | Median Reward: 14.70 | Max Reward: 42.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -85.2       |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 10          |
|    time_elapsed         | 142         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.032811187 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.4         |
|    entropy_loss         | -53.9       |
|    explained_variance   | -0.0227     |
|    learning_rate        | 0.0005      |
|    loss                 | 78.6        |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.00686     |
|    std                  | 1.01        |
|    value_loss           | 166         |
-----------------------------------------
aIteration: 12 | Episodes: 500 | Median Reward: 22.22 | Max Reward: 42.61
Iteration: 14 | Episodes: 600 | Median Reward: 14.99 | Max Reward: 42.61
Iteration: 17 | Episodes: 700 | Median Reward: 21.00 | Max Reward: 42.61
Iteration: 19 | Episodes: 800 | Median Reward: 21.25 | Max Reward: 42.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.6       |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 20          |
|    time_elapsed         | 286         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.014462075 |
|    clip_fraction        | 0.0662      |
|    clip_range           | 0.4         |
|    entropy_loss         | -63.9       |
|    explained_variance   | -0.00783    |
|    learning_rate        | 0.0005      |
|    loss                 | 83.4        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0116     |
|    std                  | 1.01        |
|    value_loss           | 175         |
-----------------------------------------
^CTraceback (most recent call last):
  File "env_setup_g3_d_2.py", line 339, in <module>
    main()
  File "env_setup_g3_d_2.py", line 318, in main
    callback = train_on_gpu(gpu_id, num_envs, total_timesteps, n_steps, save_interval)
  File "env_setup_g3_d_2.py", line 299, in train_on_gpu
    model.learn(total_timesteps=total_timesteps, callback=callback, log_interval=10)
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py", line 454, in learn
    return super().learn(
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 300, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py", line 241, in collect_rollouts
    episode_starts = th.tensor(self._last_episode_starts, dtype=th.float32, device=self.device)
KeyboardInterrupt
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> na[K[Kpython env_setup_g3_d_2.py[43Gconda activate mujoco_test[43Gpython env_setup_g3_d_2.py[43Gconda activate mujoco_test[43Gpython env_setup_g3_d_2.py[43G[Knano env_setup_g3_d_2.y[Kpy
[?2004h[?1049h[22;0;0t[1;30r(B[m[4l[?7h[39;49m[?1h=[?1h=[?25l[39;49m(B[m[H[2J[28;54H(B[0;7m[ Reading... ](B[m[28;52H(B[0;7m[ Read 340 lines ](B[m[H(B[0;7m  GNU nano 4.8                                      env_setup_g3_d_2.py                                                 [1;119H(B[m[29d(B[0;7m^G(B[m Get Help    (B[0;7m^O(B[m Write Out   (B[0;7m^W(B[m Where Is    (B[0;7m^K(B[m Cut Text    (B[0;7m^J(B[m Justify     (B[0;7m^C(B[m Cur Pos     (B[0;7mM-U(B[m Undo[106G(B[0;7mM-A(B[m Mark Text[30d(B[0;7m^X(B[m Exit[30;16H(B[0;7m^R(B[m Read File   (B[0;7m^\(B[m Replace     (B[0;7m^U(B[m Paste Text  (B[0;7m^T(B[m To Spell    (B[0;7m^_(B[m Go To Line  (B[0;7mM-E(B[m Redo[106G(B[0;7mM-6(B[m Copy Text[28d[2d(B[0;1m[31m# Reset the env state back to initial, after each episode.[3d[36mimport[39m(B[m gymnasium (B[0;1m[36mas[39m(B[m gym[4d(B[0;1m[36mfrom[39m(B[m gymnasium (B[0;1m[36mimport[39m(B[m spaces[5d(B[0;1m[36mfrom[39m(B[m statistics (B[0;1m[36mimport[39m(B[m median[6d(B[0;1m[36mimport[39m(B[m numpy (B[0;1m[36mas[39m(B[m np[7d(B[0;1m[36mimport[39m(B[m mujoco_py[8d(B[0;1m[36mimport[39m(B[m os[9d(B[0;1m[36mimport[39m(B[m torch[10d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.vec_env (B[0;1m[36mimport[39m(B[m DummyVecEnv, VecMonitor[11d(B[0;1m[36mfrom[39m(B[m sb3_contrib (B[0;1m[36mimport[39m(B[m RecurrentPPO[12d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.callbacks (B[0;1m[36mimport[39m(B[m BaseCallback[13d(B[0;1m[36mimport[39m(B[m logging[14d(B[0;1m[36mimport[39m(B[m matplotlib.pyplot (B[0;1m[36mas[39m(B[m plt (B[0;1m[31m # Import matplotlib for plotting[16d# Configure logging[17d[39m(B[mlogging.basicConfig(level=logging.INFO)[19d(B[0;1m[36mclass[39m(B[m HandEnv(gym.Env):[20;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self):[21;9Hsuper(HandEnv, self).__init__()[22;9Hxml_path = os.path.expanduser((B[0;1m[32m'/home/easgrad/dgusain/Bot_hand/bot_hand.xml'[39m(B[m)[23;9Hlogging.info(f(B[0;1m[32m"Attempting to load Mujoco model from: {xml_path}"[39m(B[m)[25;9H(B[0;1m[36mif[39m(B[m (B[0;1m[36mnot[39m(B[m os.path.isfile(xml_path):[26;13Hlogging.error(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[27;13H(B[0;1m[36mraise[39m(B[m FileNotFoundError(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[2d[?12l[?25h[?25l[4d[?12l[?25h[?25l[5d[?12l[?25h[?25l[7d[?12l[?25h[?25l[9d[?12l[?25h[?25l[11d[?12l[?25h[?25l[13d[?12l[?25h[?25l[14d[?12l[?25h[?25l[16d[?12l[?25h[?25l[17d[?12l[?25h[?25l[18d[?12l[?25h[?25l[19d[?12l[?25h[?25l[20d[?12l[?25h[?25l[22d[?12l[?25h[?25l[25d[?12l[?25h[?25l[26d[?12l[?25h[?25l[27d[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;1H[K[27;9H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hself.model = mujoco_py.load_model_from_path(xml_path)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hself.sim = mujoco_py.MjSim(self.model)[27;13Hlogging.info((B[0;1m[32m"Mujoco model loaded successfully."[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hlogging.error(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[27;13H(B[0;1m[36mraise[39m(B[m RuntimeError(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hjoint_weights = [[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H0.5, (B[0;1m[31m # ID 0: WristJoint1[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H0.5, (B[0;1m[31m # ID 1: WristJoint0[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H3.0, (B[0;1m[31m # ID 2: ForeFingerJoint3 (MCP)[27;13H[39m(B[m2.5, (B[0;1m[31m # ID 3: ForeFingerJoint2 (MCP proximal)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H2.0, (B[0;1m[31m # ID 4: ForeFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[3S[1;30r[25;13H1.0, (B[0;1m[31m # ID 5: ForeFingerJoint0 (DIP)[26;13H[39m(B[m3.0, (B[0;1m[31m # ID 6: MiddleFingerJoint3 (MCP)[27;13H[39m(B[m2.5, (B[0;1m[31m # ID 7: MiddleFingerJoint2 (MCP proximal)[A[39m(B[m[?12l[?25h[?25l[27d[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H2.0, (B[0;1m[31m # ID 8: MiddleFingerJoint1 (PIP)[27;13H[39m(B[m1.0, (B[0;1m[31m # ID 9: MiddleFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 10: RingFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H2.5, (B[0;1m[31m # ID 11: RingFingerJoint2 (MCP proximal)[27;13H[39m(B[m2.0, (B[0;1m[31m # ID 12: RingFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H1.0, (B[0;1m[31m # ID 13: RingFingerJoint0 (DIP)[27;13H[39m(B[m1.5, (B[0;1m[31m # ID 14: LittleFingerJoint4 (CMC Joint rotation)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H3.0, (B[0;1m[31m # ID 15: LittleFingerJoint3 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H2.5, (B[0;1m[31m # ID 16: LittleFingerJoint2 (MCP proximal)[27;13H[39m(B[m2.0, (B[0;1m[31m # ID 17: LittleFingerJoint1 (PIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H1.0, (B[0;1m[31m # ID 18: LittleFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H1.5, (B[0;1m[31m # ID 19: ThumbJoint4 (CMC Joint abduction/adduction)[27;13H[39m(B[m1.5, (B[0;1m[31m # ID 20: ThumbJoint3 (CMC Joint flexion/extension)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H1.5, (B[0;1m[31m # ID 21: ThumbJoint2 (CMC Joint rotation)[27;13H[39m(B[m3.0, (B[0;1m[31m # ID 22: ThumbJoint1 (MCP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H0.5, (B[0;1m[31m # ID 23: ThumbJoint0 (IP)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H0.0  (B[0;1m[31m # ID 24: None[27;9H[39m(B[m][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Convertiing joint weights to a tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.joint_weights = torch.tensor(joint_weights, dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Normalize the weights so that their sum equals 1[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.joint_weights /= self.joint_weights.sum()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.initial_state = self.sim.get_state()[27;8H(B[0;1m[31m # Define observation and action spaces[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(24,), dtype=np.float32)[27;9Hself.target_threshold = 99[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.max_steps = 100[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.steps_taken = 0[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Initialize actuator ranges[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hactuator_ranges = self.sim.model.actuator_ctrlrange[27;9Hself.actuator_min = torch.tensor(actuator_ranges[:, 0], dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.actuator_max = torch.tensor(actuator_ranges[:, 1], dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;8H(B[0;1m[31m # Create normalized action space between [-1, 1][39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.action_space = spaces.Box(low=-1, high=1, shape=(self.actuator_min.shape[0],), dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Precompute ground truth quaternions as a tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.ground_truth_quats = torch.tensor(self.get_ground_truth_quaternion(), dtype=torch.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m reset[39m(B[m(self, seed=(B[0;1m[35mNone[39m(B[m, options=(B[0;1m[35mNone[39m(B[m):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hsuper().reset(seed=seed)[?12l[?25h[?25l7[2;28r8[28d[3S[1;30r[25;8H(B[0;1m[31m #self.sim.reset()[26;9H[39m(B[mself.sim.set_state(self.initial_state)[27;9Hself.sim.forward()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.steps_taken = 0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hobs = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mreturn[39m(B[m obs, {}[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m step[39m(B[m(self, action: np.ndarray):[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m # Rescale action using tensor operations for efficiency[27;9H[39m(B[maction_tensor = torch.from_numpy(action).float()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # adding some Gaussian noise to the action to avoid getting stuck in local optima[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m #noise = torch.normal(mean=0, std=0.05, size=action_tensor.size())[27;8H #action_tensor += noise[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hrescaled_action = self.actuator_min + (action_tensor + 1) * (self.actuator_max - self.actuator_min) / 2[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m # rescaled_action[0:2] = 0[27;9H[39m(B[mself.sim.data.ctrl[:] = rescaled_action.numpy()[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.sim.step()[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Fetch state as a tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hstate = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[27d[42m        [49m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Calculate done condition[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hdone = self.calculate_done(state, flag=(B[0;1m[35mFalse[39m(B[m) (B[0;1m[36mor[39m(B[m self.steps_taken >= self.max_steps[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m done:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hreward = self.calculate_reward(state, flag=(B[0;1m[35mTrue[39m(B[m).item()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36melse[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hreward = -1.0 (B[0;1m[31m # Penalize extra steps[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;9Hself.steps_taken += 1[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Htruncated = (B[0;1m[35mFalse[27;9H[39m(B[minfo = {}[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m state.numpy(), reward, done, truncated, info[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5H(B[0;1m[36mdef[34m calculate_done[39m(B[m(self, state: torch.Tensor, flag: bool) -> bool:[27;9H(B[0;1m[36mif[39m(B[m flag:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mFalse[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hconfidence = self.calculate_confidence(state)[27;9H(B[0;1m[36mreturn[39m(B[m confidence >= self.target_threshold[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;5H(B[0;1m[36mdef[34m calculate_reward[39m(B[m(self, state: torch.Tensor, flag: bool) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m flag:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13Hconfidence = self.calculate_confidence(state)[27;13H(B[0;1m[36mreturn[39m(B[m confidence - 50 (B[0;1m[31m # Reward calculation[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36melse[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mif[39m(B[m confidence > 80:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17H(B[0;1m[36mreturn[39m(B[m (confidence-50)/2.0(B[0;1m[31m # encouraging the model in the right direction[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36melse[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17H(B[0;1m[36mreturn[39m(B[m (confidence-50)/4.0(B[0;1m[31m # lesser reward[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;5H(B[0;1m[36mdef[34m calculate_confidence[39m(B[m(self, state: torch.Tensor) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hrendered_quat = self.get_rendered_pose_quaternion()[27;9Hrendered_quat = self.normalize_quaternion(rendered_quat)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hgt_quat = self.normalize_quaternion(self.ground_truth_quats)[27;9Hsimilarity = torch.abs(torch.sum(rendered_quat * gt_quat, dim=1)) * 100[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hweighted_similarity = similarity * self.joint_weights[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Havg_confidence = weighted_similarity.sum()[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mreturn[39m(B[m avg_confidence[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m get_rendered_pose_quaternion[39m(B[m(self) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hquaternions = [][27;9H(B[0;1m[36mfor[39m(B[m joint (B[0;1m[36min[39m(B[m range(self.model.njnt):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hq = self.sim.data.qpos[self.model.jnt_qposadr[joint]:self.model.jnt_qposadr[joint] + 4][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hquaternions.append(q)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Convert list of arrays to a single NumPy array[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hquaternions_np = np.array(quaternions, dtype=np.float32)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Convert NumPy array to PyTorch tensor[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m torch.from_numpy(quaternions_np)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m normalize_quaternion[39m(B[m(self, q: torch.Tensor) -> torch.Tensor:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hnorm = torch.norm(q, dim=1, keepdim=(B[0;1m[35mTrue[39m(B[m)[27;9H(B[0;1m[36mreturn[39m(B[m q / norm.clamp(min=1e-8) (B[0;1m[31m # Prevent division by zero[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;5H(B[0;1m[36mdef[34m get_ground_truth_quaternion[39m(B[m(self) -> np.ndarray:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hground_truth_quats = [[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[6.32658406e-04,  1.25473697e-03, -9.99718591e-03,  1.56702220e+00],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.25473697e-03, -9.99718591e-03,  1.56702220e+00,  1.56730258e+00],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[-0.00999719,  1.5670222,   1.56730258,  1.5674143],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.5670222,   1.56730258,  1.5674143,  -0.00999801],[?12l[?25h[?25l7[2;28r8[28d[3S[1;30r[25;13H[1.56730258,  1.5674143,  -0.00999801,  1.56701926],[26;13H[1.5674143,  -0.00999801,  1.56701926,  1.56730194],[27;13H[-0.00999801,  1.56701926,  1.56730194,  1.5674143],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[1.56701926,  1.56730194,  1.5674143,  -0.00999756],[27;13H[1.56730194,  1.5674143,  -0.00999756,  1.56701717],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[1.5674143,  -0.00999756,  1.56701717,  1.56730177],[27;13H[-0.00999756,  1.56701717,  1.56730177,  1.56741429],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[1.56701717, 1.56730177, 1.56741429, 0.00868252],[27;13H[1.56730177,  1.56741429,  0.00868252, -0.01000805],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[1.56741429,  0.00868252, -0.01000805,  1.56708349],[27;13H[0.00868252, -0.01000805,  1.56708349,  1.56730911],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[-0.01000805,  1.56708349,  1.56730911,  1.56741508],[27;13H[1.56708349, 1.56730911, 1.56741508, 0.49916711],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56730911, 1.56741508, 0.49916711, 0.50022545],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[1.56741508, 0.49916711, 0.50022545, 0.25330455],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[4.99167109e-01, 5.00225452e-01, 2.53304550e-01, 4.08440608e-04],[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H[5.00225452e-01,  2.53304550e-01,  4.08440608e-04, -9.00000689e-01],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[2.53304550e-01,  4.08440608e-04, -9.00000689e-01,  1.00000000e-01],[27;13H[4.08440608e-04, -9.00000689e-01,  1.00000000e-01, -1.00000000e-01],[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;13H[-0.90000069,  0.1,[26;40H-0.1,[26;54H0.01463168],[27;13H[0.1,[27;26H-0.1,[27;40H0.01463168,  1.0][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H][?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mreturn[39m(B[m np.array(ground_truth_quats, dtype=np.float32)[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mclass[39m(B[m RewardCallback(BaseCallback):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;1H(B[0;1m[32m    Custom callback for logging average rewards over every 100 episodes, episode numbers, and iteration numbers.[27d    Also stores average rewards for plotting.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m    """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self, avg_interval=100):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hsuper(RewardCallback, self).__init__()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.episode_num = 0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.max_reward = -np.inf[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.avg_interval = avg_interval (B[0;1m[31m # Number of episodes per average[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.sum_rewards = 0.0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.count_rewards = 0[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.avg_rewards = {} (B[0;1m[31m # Dictionary to store average rewards, key: block_num[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.iteration_num = 0    (B[0;1m[31m # Counter for iterations[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hself.median_rewards = {}[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _on_rollout_end[39m(B[m(self) -> bool:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Called at the end of a rollout.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;1H(B[0;1m[32m        Used to track the number of iterations.[27d        """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hself.iteration_num += 1[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[27d[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _on_step[39m(B[m(self) -> bool:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Called at every step. Checks if any episode has finished and logs the reward.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Retrieve 'dones' and 'rewards' from the current step[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[3S[1;30r[25;9Hdones = self.locals.get((B[0;1m[32m'dones'[39m(B[m, [])[26;9Hrewards = self.locals.get((B[0;1m[32m'rewards'[39m(B[m, [])[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;8H(B[0;1m[31m # Check if any of the environments are done[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9H(B[0;1m[36mif[39m(B[m np.any(dones):[27;13H(B[0;1m[36mfor[39m(B[m done, reward (B[0;1m[36min[39m(B[m zip(dones, rewards):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17H(B[0;1m[36mif[39m(B[m done:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;21Hself.episode_num += 1[27;21H(B[0;1m[36mif[39m(B[m reward > self.max_reward:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hself.max_reward = reward[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;21Hself.rewards_list.append(reward)[27;21H(B[0;1m[36mif[39m(B[m len(self.rewards_list) == self.avg_interval:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hmedian_reward = median(self.rewards_list)[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;25Hblock_num = self.episode_num // self.avg_interval[27;25Hself.median_rewards[block_num] = median_reward[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hprint(f(B[0;1m[32m"Iteration: {self.iteration_num} | Episodes: {block_num * self.avg_interval} | Median Re[39m(B[0;7m>[27;1H(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;24H(B[0;1m[31m # Reset sum and count[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;25Hself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _on_training_end[39m(B[m(self) -> (B[0;1m[35mNone[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        Called at the end of training. If there are remaining episodes, compute and store the average.[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[32m        """[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mif[39m(B[m len(self.rewards_list) > 0:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hmedian_reward = median(self.rewards_list)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hblock_num = self.episode_num // self.avg_interval + 1[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hself.median_rewards[block_num] = median_reward[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hcurrent_max = max(self.rewards_list)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mif[39m(B[m current_max > self.max_reward:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;17Hself.max_reward = current_max[42m           [49m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hprint(f(B[0;1m[32m"Training End | Episodes: {self.episode_num} | Median Reward: {median_reward:.2f} | Max Reward: {sel[39m(B[0;7m>[27;1H(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Hself.rewards_list = [][?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mdef[34m make_env[39m(B[m():[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[32m"""Utility function to create a HandEnv without Monitor."""[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mdef[34m _init[39m(B[m():[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l[A[?12l[?25h[?25l[27d[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13Henv = HandEnv()[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;13H(B[0;1m[36mreturn[39m(B[m env[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[?12l[?25h[?25l7[2;28r8[28d[3S[1;30r[25;13Hlogging.error(f(B[0;1m[32m"Failed to initialize HandEnv: {e}"[39m(B[m)[26;13H(B[0;1m[36mraise[39m(B[m e[27;5H(B[0;1m[36mreturn[39m(B[m _init[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mdef[34m train_on_gpu[39m(B[m(gpu_id: int, num_envs: int, total_timesteps: int, num_steps: int, save_interval: int):[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hos.environ[(B[0;1m[32m'CUDA_VISIBLE_DEVICES'[39m(B[m] = str(gpu_id)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hdevice = (B[0;1m[32m'cuda'[39m(B[m (B[0;1m[36mif[39m(B[m torch.cuda.is_available() (B[0;1m[36melse[39m(B[m (B[0;1m[32m'cpu'[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hprint(f(B[0;1m[32m"GPU {gpu_id}: Using {device} device"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Choose between SubprocVecEnv and DummyVecEnv based on your needs[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # env = SubprocVecEnv([make_env() for _ in range(num_envs)])[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[3S[1;30r[25;5Henv = DummyVecEnv([make_env() (B[0;1m[36mfor[39m(B[m _ (B[0;1m[36min[39m(B[m range(num_envs)]) (B[0;1m[31m # Use DummyVecEnv for testing[26;5H[39m(B[menv = VecMonitor(env) (B[0;1m[31m # Use VecMonitor instead of individual Monitor wrappers[27d[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Initialize the RecurrentPPO model with optimized parameters[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hmodel = RecurrentPPO([?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[32m'MlpLstmPolicy'[39m(B[m,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Henv,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hverbose=1,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hdevice=device,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hent_coef=0.05,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hlearning_rate=0.0005,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hclip_range=0.4,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hn_steps=num_steps,[33G(B[0;1m[31m # Steps per environment per update[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hbatch_size=4096,[27;34H(B[0;1m[31m # Increased batch size[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;9Hgamma=0.99,[27;9Hgae_lambda=0.95,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hmax_grad_norm=0.5,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hvf_coef=0.5,[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Huse_sde=(B[0;1m[35mTrue[39m(B[m,[27;33H(B[0;1m[31m # Use State Dependent Exploration for better exploration[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hcallback = RewardCallback(avg_interval=100)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;8H(B[0;1m[31m # Train the model with the specified total timesteps[27;9H[39m(B[mmodel.learn(total_timesteps=total_timesteps, callback=callback, log_interval=10)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9Hlogging.error(f(B[0;1m[32m"Error during training: {e}"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;9H(B[0;1m[36mraise[39m(B[m e[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Save the final model[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hmodel.save(f(B[0;1m[32m"/home/easgrad/dgusain/Bot_hand/agents/agent_g3d_{gpu_id}_2_weighted"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5H(B[0;1m[36mreturn[39m(B[m callback (B[0;1m[31m # Return the callback to access episode_rewards[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H(B[0;1m[36mdef[34m main[39m(B[m():[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5Hgpu_id = 4[26;24H(B[0;1m[31m # Set your GPU ID here[27;5H[39m(B[mnum_envs = 4[24G(B[0;1m[31m # Number of parallel environments (adjust based on your GPU memory)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hn_iter = 5000[27;26H(B[0;1m[31m # Number of iterations you want to run[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5Hn_steps = 1024     (B[0;1m[31m # Steps per environment per update[27;5H[39m(B[mtotal_timesteps = n_iter * n_steps * num_envs (B[0;1m[31m # Total training timesteps[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hsave_interval = 25 (B[0;1m[31m # Not used in this optimized version[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Train the model and get the callback with rewards data[39m(B[m[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hcallback = train_on_gpu(gpu_id, num_envs, total_timesteps, n_steps, save_interval)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Extract block numbers and average rewards from the callback[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5Hblock_numbers = list(callback.median_rewards.keys())[27;5Hblock_rewards = list(callback.median_rewards.values())[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;1H[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;4H(B[0;1m[31m # Calculate the corresponding episode numbers (100, 200, 300, ...)[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5Hepisode_numbers = [block_num * 100 (B[0;1m[36mfor[39m(B[m block_num (B[0;1m[36min[39m(B[m block_numbers][27d[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;4H(B[0;1m[31m # Plotting the average rewards vs episodes[27;5H[39m(B[mplt.figure(figsize=(10, 6))[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hplt.plot(episode_numbers, block_rewards, marker=(B[0;1m[32m'o'[39m(B[m, linestyle=(B[0;1m[32m'-'[39m(B[m, color=(B[0;1m[32m'b'[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hplt.xlabel((B[0;1m[32m'Episode Number'[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hplt.ylabel((B[0;1m[32m'Median Reward (per 100 episodes)'[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hplt.title((B[0;1m[32m'Median Reward vs Episode Number'[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hplt.grid((B[0;1m[35mTrue[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d
[1;30r[27;5Hplt.savefig((B[0;1m[32m'/home/easgrad/dgusain/Bot_hand/figs/fig_g3d_weighted.png'[39m(B[m) (B[0;1m[31m # Save the figure[39m(B[m[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[26;5Hplt.close() (B[0;1m[31m # Close the figure to free memory[27;5H[39m(B[mprint((B[0;1m[32m"Plot saved as fig_g3d_2_weighted.png"[39m(B[m)[?12l[?25h[?25l7[2;28r8[28d[3S[1;30r[26;1H(B[0;1m[36mif[39m(B[m __name__ == (B[0;1m[32m"__main__"[39m(B[m:[27;5Hmain()[?12l[?25h[?25l7[2;28r8[28d[2S[1;30r[27;1H[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l[A[?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25lp[?12l[?25h[?25ll[?12l[?25h[?25lt.[?12l[?25h[?25ls[?12l[?25h[?25la[?12l[?25h[?25lv[?12l[?25h[?25le[?12l[?25h[?25lf[?12l[?25h[?25li[?12l[?25h[?25lg[?12l[?25h[?25l([?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[38G[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[C[?12l[?25h[?25l[1;111H(B[0;7mModified(B[m[20;62H(B[0;1m[32m2weighted.png'[39m(B[m) (B[0;1m[31m # Save the figure[20;63H[39m(B[m[?12l[?25h[?25l(B[0;1m[32m_weighted.png'[39m(B[m) (B[0;1m[31m # Save the figure[20;64H[39m(B[m[?12l[?25h[?25l[28d(B[0;7mSave modified buffer?                                                                                                   [29;1H Y(B[m Yes[K[30d(B[0;7m N(B[m No  [30;16H (B[0;7m^C(B[m Cancel[K[28;23H[?12l[?25h[?25l[29d(B[0;7m^G(B[m Get Help[29;31H(B[0;7mM-D(B[m DOS Format[29;61H(B[0;7mM-A(B[m Append[29;91H(B[0;7mM-B(B[m Backup File[30d(B[0;7m^C(B[m Cancel[17G[14X[30;31H(B[0;7mM-M(B[m Mac Format[30;61H(B[0;7mM-P(B[m Prepend[30;91H(B[0;7m^T(B[m To Files[28d(B[0;7mFile Name to Write: env_setup_g3_d_2.py(B[m[28;40H[?12l[?25h[?25l[28;53H[1K (B[0;7m[ Writing... ](B[m[K[1;111H(B[0;7m        (B[m[28;51H(B[0;7m[ Wrote 340 lines ](B[m[J[30d[?12l[?25h[30;1H[?1049l[23;0;0t[?1l>[?2004l(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python env_setup_g3_d_2.py
GPU 4: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
Iteration: 2 | Episodes: 100 | Median Reward: 24.91 | Max Reward: 31.75
Iteration: 4 | Episodes: 200 | Median Reward: 23.21 | Max Reward: 35.09
Iteration: 7 | Episodes: 300 | Median Reward: 19.09 | Max Reward: 35.09
Iteration: 9 | Episodes: 400 | Median Reward: 22.68 | Max Reward: 35.09
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -79.3     |
| time/                   |           |
|    fps                  | 276       |
|    iterations           | 10        |
|    time_elapsed         | 148       |
|    total_timesteps      | 40960     |
| train/                  |           |
|    approx_kl            | 0.2696638 |
|    clip_fraction        | 0.36      |
|    clip_range           | 0.4       |
|    entropy_loss         | -40.1     |
|    explained_variance   | -0.0184   |
|    learning_rate        | 0.0005    |
|    loss                 | 88.3      |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.0227   |
|    std                  | 1.01      |
|    value_loss           | 184       |
---------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 16.62 | Max Reward: 35.09
Iteration: 14 | Episodes: 600 | Median Reward: 16.40 | Max Reward: 35.09
Iteration: 17 | Episodes: 700 | Median Reward: 15.39 | Max Reward: 35.09
Iteration: 19 | Episodes: 800 | Median Reward: 16.66 | Max Reward: 35.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -82.8       |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 20          |
|    time_elapsed         | 289         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.028832464 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.4         |
|    entropy_loss         | -53.6       |
|    explained_variance   | -0.00842    |
|    learning_rate        | 0.0005      |
|    loss                 | 79.3        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0214     |
|    std                  | 1.02        |
|    value_loss           | 166         |
-----------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 14.54 | Max Reward: 35.09
Iteration: 24 | Episodes: 1000 | Median Reward: 16.25 | Max Reward: 35.09
Iteration: 27 | Episodes: 1100 | Median Reward: 10.36 | Max Reward: 35.09
Iteration: 29 | Episodes: 1200 | Median Reward: 16.34 | Max Reward: 35.09
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -82.2       |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 30          |
|    time_elapsed         | 430         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.063767076 |
|    clip_fraction        | 0.0425      |
|    clip_range           | 0.4         |
|    entropy_loss         | -73         |
|    explained_variance   | -0.00262    |
|    learning_rate        | 0.0005      |
|    loss                 | 93.2        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00863    |
|    std                  | 1.02        |
|    value_loss           | 195         |
-----------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 17.36 | Max Reward: 35.09
Iteration: 34 | Episodes: 1400 | Median Reward: 13.93 | Max Reward: 35.09
Iteration: 36 | Episodes: 1500 | Median Reward: 20.51 | Max Reward: 43.73
Iteration: 39 | Episodes: 1600 | Median Reward: 22.99 | Max Reward: 43.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -84.6       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 40          |
|    time_elapsed         | 570         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.028710593 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -76.3       |
|    explained_variance   | -0.0014     |
|    learning_rate        | 0.0005      |
|    loss                 | 68.1        |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00503    |
|    std                  | 1.03        |
|    value_loss           | 146         |
-----------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 25.36 | Max Reward: 43.73
Iteration: 44 | Episodes: 1800 | Median Reward: 23.35 | Max Reward: 43.73
Iteration: 46 | Episodes: 1900 | Median Reward: 20.12 | Max Reward: 43.73
Iteration: 49 | Episodes: 2000 | Median Reward: 22.42 | Max Reward: 43.73
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -77        |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 50         |
|    time_elapsed         | 713        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.01500395 |
|    clip_fraction        | 0.0083     |
|    clip_range           | 0.4        |
|    entropy_loss         | -82.3      |
|    explained_variance   | -0.000418  |
|    learning_rate        | 0.0005     |
|    loss                 | 116        |
|    n_updates            | 490        |
|    policy_gradient_loss | 0.00349    |
|    std                  | 1.05       |
|    value_loss           | 241        |
----------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 28.42 | Max Reward: 43.73
Iteration: 54 | Episodes: 2200 | Median Reward: 27.17 | Max Reward: 43.73
Iteration: 56 | Episodes: 2300 | Median Reward: 26.38 | Max Reward: 43.73
Iteration: 59 | Episodes: 2400 | Median Reward: 25.27 | Max Reward: 43.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -72.4       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 60          |
|    time_elapsed         | 854         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.040669683 |
|    clip_fraction        | 0.0169      |
|    clip_range           | 0.4         |
|    entropy_loss         | -83.1       |
|    explained_variance   | -0.000176   |
|    learning_rate        | 0.0005      |
|    loss                 | 123         |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00746    |
|    std                  | 1.08        |
|    value_loss           | 254         |
-----------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 29.12 | Max Reward: 44.91
Iteration: 64 | Episodes: 2600 | Median Reward: 25.82 | Max Reward: 46.40
Iteration: 66 | Episodes: 2700 | Median Reward: 26.61 | Max Reward: 46.40
Iteration: 69 | Episodes: 2800 | Median Reward: 28.80 | Max Reward: 46.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74.8       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 70          |
|    time_elapsed         | 997         |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.045203805 |
|    clip_fraction        | 0.0112      |
|    clip_range           | 0.4         |
|    entropy_loss         | -86.1       |
|    explained_variance   | -6e-05      |
|    learning_rate        | 0.0005      |
|    loss                 | 128         |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.00308    |
|    std                  | 1.13        |
|    value_loss           | 264         |
-----------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 26.01 | Max Reward: 46.40
Iteration: 73 | Episodes: 3000 | Median Reward: 25.44 | Max Reward: 46.40
Iteration: 76 | Episodes: 3100 | Median Reward: 22.70 | Max Reward: 46.40
Iteration: 78 | Episodes: 3200 | Median Reward: 24.15 | Max Reward: 46.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -72.5       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 80          |
|    time_elapsed         | 1142        |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.007527562 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.4         |
|    entropy_loss         | -87.3       |
|    explained_variance   | -2.46e-05   |
|    learning_rate        | 0.0005      |
|    loss                 | 123         |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00106    |
|    std                  | 1.16        |
|    value_loss           | 254         |
-----------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 29.96 | Max Reward: 46.40
Iteration: 83 | Episodes: 3400 | Median Reward: 25.58 | Max Reward: 46.40
Iteration: 86 | Episodes: 3500 | Median Reward: 29.57 | Max Reward: 46.40
Iteration: 88 | Episodes: 3600 | Median Reward: 35.75 | Max Reward: 46.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.9       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 90          |
|    time_elapsed         | 1285        |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.001124275 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -88.4       |
|    explained_variance   | -1.05e-05   |
|    learning_rate        | 0.0005      |
|    loss                 | 166         |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00132    |
|    std                  | 1.21        |
|    value_loss           | 341         |
-----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 28.76 | Max Reward: 46.40
Iteration: 93 | Episodes: 3800 | Median Reward: 23.63 | Max Reward: 46.40
Iteration: 96 | Episodes: 3900 | Median Reward: 32.40 | Max Reward: 46.40
Iteration: 98 | Episodes: 4000 | Median Reward: 28.95 | Max Reward: 46.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74.2       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 100         |
|    time_elapsed         | 1427        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.001796321 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -89.1       |
|    explained_variance   | -7.63e-06   |
|    learning_rate        | 0.0005      |
|    loss                 | 125         |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00114    |
|    std                  | 1.24        |
|    value_loss           | 258         |
-----------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 28.90 | Max Reward: 46.40
Iteration: 103 | Episodes: 4200 | Median Reward: 27.58 | Max Reward: 46.40
Iteration: 106 | Episodes: 4300 | Median Reward: 31.41 | Max Reward: 46.40
Iteration: 108 | Episodes: 4400 | Median Reward: 29.32 | Max Reward: 46.40
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -72.7       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 110         |
|    time_elapsed         | 1573        |
|    total_timesteps      | 450560      |
| train/                  |             |
|    approx_kl            | 0.001867001 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -89.5       |
|    explained_variance   | -5.13e-06   |
|    learning_rate        | 0.0005      |
|    loss                 | 120         |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.00256    |
|    std                  | 1.28        |
|    value_loss           | 249         |
-----------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 29.42 | Max Reward: 47.71
Iteration: 113 | Episodes: 4600 | Median Reward: 28.73 | Max Reward: 47.71
Iteration: 115 | Episodes: 4700 | Median Reward: 31.82 | Max Reward: 47.71
Iteration: 118 | Episodes: 4800 | Median Reward: 30.56 | Max Reward: 47.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.6         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 120           |
|    time_elapsed         | 1717          |
|    total_timesteps      | 491520        |
| train/                  |               |
|    approx_kl            | 0.00010153337 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -89.9         |
|    explained_variance   | -3.81e-06     |
|    learning_rate        | 0.0005        |
|    loss                 | 172           |
|    n_updates            | 1190          |
|    policy_gradient_loss | -4.3e-05      |
|    std                  | 1.29          |
|    value_loss           | 353           |
-------------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 27.99 | Max Reward: 47.71
Iteration: 123 | Episodes: 5000 | Median Reward: 30.04 | Max Reward: 47.71
Iteration: 125 | Episodes: 5100 | Median Reward: 30.03 | Max Reward: 47.71
Iteration: 128 | Episodes: 5200 | Median Reward: 27.61 | Max Reward: 47.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.9       |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 130         |
|    time_elapsed         | 1863        |
|    total_timesteps      | 532480      |
| train/                  |             |
|    approx_kl            | 0.007931646 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -90.6       |
|    explained_variance   | -3.22e-06   |
|    learning_rate        | 0.0005      |
|    loss                 | 141         |
|    n_updates            | 1290        |
|    policy_gradient_loss | 0.000974    |
|    std                  | 1.33        |
|    value_loss           | 290         |
-----------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 34.10 | Max Reward: 47.71
Iteration: 133 | Episodes: 5400 | Median Reward: 27.39 | Max Reward: 47.71
Iteration: 135 | Episodes: 5500 | Median Reward: 29.72 | Max Reward: 47.71
Iteration: 138 | Episodes: 5600 | Median Reward: 29.48 | Max Reward: 47.71
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74.4       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 140         |
|    time_elapsed         | 2004        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.022401802 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.4         |
|    entropy_loss         | -91.4       |
|    explained_variance   | -2.15e-06   |
|    learning_rate        | 0.0005      |
|    loss                 | 138         |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0027     |
|    std                  | 1.37        |
|    value_loss           | 285         |
-----------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 26.28 | Max Reward: 47.71
Iteration: 143 | Episodes: 5800 | Median Reward: 30.90 | Max Reward: 47.71
Iteration: 145 | Episodes: 5900 | Median Reward: 32.84 | Max Reward: 47.71
Iteration: 147 | Episodes: 6000 | Median Reward: 32.51 | Max Reward: 47.71
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.5        |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 150          |
|    time_elapsed         | 2151         |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 4.726053e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -92.2        |
|    explained_variance   | -9.54e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 155          |
|    n_updates            | 1490         |
|    policy_gradient_loss | 1.53e-05     |
|    std                  | 1.42         |
|    value_loss           | 319          |
------------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 30.52 | Max Reward: 47.71
Iteration: 152 | Episodes: 6200 | Median Reward: 27.84 | Max Reward: 47.71
Iteration: 155 | Episodes: 6300 | Median Reward: 29.22 | Max Reward: 47.71
Iteration: 157 | Episodes: 6400 | Median Reward: 33.01 | Max Reward: 47.71
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.2        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 160          |
|    time_elapsed         | 2303         |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 6.733695e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93          |
|    explained_variance   | 0.0181       |
|    learning_rate        | 0.0005       |
|    loss                 | 175          |
|    n_updates            | 1590         |
|    policy_gradient_loss | -8.1e-05     |
|    std                  | 1.46         |
|    value_loss           | 361          |
------------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 36.37 | Max Reward: 47.71
Iteration: 162 | Episodes: 6600 | Median Reward: 36.53 | Max Reward: 47.71
Iteration: 165 | Episodes: 6700 | Median Reward: 35.98 | Max Reward: 47.71
Iteration: 167 | Episodes: 6800 | Median Reward: 32.04 | Max Reward: 47.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.5         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 170           |
|    time_elapsed         | 2448          |
|    total_timesteps      | 696320        |
| train/                  |               |
|    approx_kl            | 0.00010175214 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93           |
|    explained_variance   | 0.13          |
|    learning_rate        | 0.0005        |
|    loss                 | 116           |
|    n_updates            | 1690          |
|    policy_gradient_loss | -0.00046      |
|    std                  | 1.46          |
|    value_loss           | 242           |
-------------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 30.41 | Max Reward: 47.71
Iteration: 172 | Episodes: 7000 | Median Reward: 28.93 | Max Reward: 47.71
Iteration: 175 | Episodes: 7100 | Median Reward: 37.45 | Max Reward: 47.71
Iteration: 177 | Episodes: 7200 | Median Reward: 31.80 | Max Reward: 47.71
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -72.2          |
| time/                   |                |
|    fps                  | 284            |
|    iterations           | 180            |
|    time_elapsed         | 2592           |
|    total_timesteps      | 737280         |
| train/                  |                |
|    approx_kl            | 0.000114505456 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -93            |
|    explained_variance   | 0.207          |
|    learning_rate        | 0.0005         |
|    loss                 | 121            |
|    n_updates            | 1790           |
|    policy_gradient_loss | -0.000497      |
|    std                  | 1.46           |
|    value_loss           | 251            |
--------------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 29.82 | Max Reward: 47.71
Iteration: 182 | Episodes: 7400 | Median Reward: 33.50 | Max Reward: 47.71
Iteration: 184 | Episodes: 7500 | Median Reward: 29.49 | Max Reward: 47.71
Iteration: 187 | Episodes: 7600 | Median Reward: 32.56 | Max Reward: 47.71
Iteration: 189 | Episodes: 7700 | Median Reward: 32.07 | Max Reward: 47.71
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.4        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 190          |
|    time_elapsed         | 2737         |
|    total_timesteps      | 778240       |
| train/                  |              |
|    approx_kl            | 6.124115e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.1        |
|    explained_variance   | 0.287        |
|    learning_rate        | 0.0005       |
|    loss                 | 96.8         |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.000384    |
|    std                  | 1.46         |
|    value_loss           | 204          |
------------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 32.84 | Max Reward: 47.71
Iteration: 194 | Episodes: 7900 | Median Reward: 27.47 | Max Reward: 47.71
Iteration: 197 | Episodes: 8000 | Median Reward: 30.81 | Max Reward: 47.71
Iteration: 199 | Episodes: 8100 | Median Reward: 34.36 | Max Reward: 47.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66           |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 200           |
|    time_elapsed         | 2880          |
|    total_timesteps      | 819200        |
| train/                  |               |
|    approx_kl            | 2.2345484e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.1         |
|    explained_variance   | 0.373         |
|    learning_rate        | 0.0005        |
|    loss                 | 123           |
|    n_updates            | 1990          |
|    policy_gradient_loss | -2.67e-05     |
|    std                  | 1.47          |
|    value_loss           | 260           |
-------------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 34.02 | Max Reward: 47.71
Iteration: 204 | Episodes: 8300 | Median Reward: 29.19 | Max Reward: 47.71
Iteration: 207 | Episodes: 8400 | Median Reward: 32.56 | Max Reward: 47.71
Iteration: 209 | Episodes: 8500 | Median Reward: 34.79 | Max Reward: 47.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.9         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 210           |
|    time_elapsed         | 3025          |
|    total_timesteps      | 860160        |
| train/                  |               |
|    approx_kl            | 3.2132957e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.2         |
|    explained_variance   | 0.476         |
|    learning_rate        | 0.0005        |
|    loss                 | 99.6          |
|    n_updates            | 2090          |
|    policy_gradient_loss | -0.000187     |
|    std                  | 1.47          |
|    value_loss           | 210           |
-------------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 31.81 | Max Reward: 47.71
Iteration: 214 | Episodes: 8700 | Median Reward: 35.01 | Max Reward: 47.71
Iteration: 216 | Episodes: 8800 | Median Reward: 35.06 | Max Reward: 47.71
Iteration: 219 | Episodes: 8900 | Median Reward: 33.19 | Max Reward: 47.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.8         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 220           |
|    time_elapsed         | 3169          |
|    total_timesteps      | 901120        |
| train/                  |               |
|    approx_kl            | 3.5791687e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.2         |
|    explained_variance   | 0.572         |
|    learning_rate        | 0.0005        |
|    loss                 | 79.6          |
|    n_updates            | 2190          |
|    policy_gradient_loss | -0.000258     |
|    std                  | 1.47          |
|    value_loss           | 170           |
-------------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 33.23 | Max Reward: 47.71
Iteration: 224 | Episodes: 9100 | Median Reward: 34.53 | Max Reward: 47.71
Iteration: 226 | Episodes: 9200 | Median Reward: 31.47 | Max Reward: 47.71
Iteration: 229 | Episodes: 9300 | Median Reward: 34.40 | Max Reward: 47.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64           |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 230           |
|    time_elapsed         | 3313          |
|    total_timesteps      | 942080        |
| train/                  |               |
|    approx_kl            | 1.3265977e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.2         |
|    explained_variance   | 0.641         |
|    learning_rate        | 0.0005        |
|    loss                 | 71.4          |
|    n_updates            | 2290          |
|    policy_gradient_loss | -8.95e-05     |
|    std                  | 1.47          |
|    value_loss           | 153           |
-------------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 35.35 | Max Reward: 47.71
Iteration: 234 | Episodes: 9500 | Median Reward: 35.05 | Max Reward: 47.71
Iteration: 236 | Episodes: 9600 | Median Reward: 31.55 | Max Reward: 47.71
Iteration: 239 | Episodes: 9700 | Median Reward: 33.87 | Max Reward: 47.71
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.4        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 240          |
|    time_elapsed         | 3456         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 1.653102e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.3        |
|    explained_variance   | 0.708        |
|    learning_rate        | 0.0005       |
|    loss                 | 59.8         |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.000139    |
|    std                  | 1.48         |
|    value_loss           | 130          |
------------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 33.58 | Max Reward: 47.71
Iteration: 244 | Episodes: 9900 | Median Reward: 33.47 | Max Reward: 47.71
Iteration: 246 | Episodes: 10000 | Median Reward: 29.31 | Max Reward: 47.71
Iteration: 249 | Episodes: 10100 | Median Reward: 32.01 | Max Reward: 47.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.8         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 250           |
|    time_elapsed         | 3599          |
|    total_timesteps      | 1024000       |
| train/                  |               |
|    approx_kl            | 4.6599453e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.3         |
|    explained_variance   | 0.76          |
|    learning_rate        | 0.0005        |
|    loss                 | 44.9          |
|    n_updates            | 2490          |
|    policy_gradient_loss | -9.64e-06     |
|    std                  | 1.48          |
|    value_loss           | 99.9          |
-------------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 33.60 | Max Reward: 47.71
Iteration: 253 | Episodes: 10300 | Median Reward: 33.65 | Max Reward: 47.71
Iteration: 256 | Episodes: 10400 | Median Reward: 34.14 | Max Reward: 47.71
Iteration: 258 | Episodes: 10500 | Median Reward: 26.90 | Max Reward: 47.71
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -68.8          |
| time/                   |                |
|    fps                  | 284            |
|    iterations           | 260            |
|    time_elapsed         | 3742           |
|    total_timesteps      | 1064960        |
| train/                  |                |
|    approx_kl            | 0.000113248054 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -93.4          |
|    explained_variance   | 0.773          |
|    learning_rate        | 0.0005         |
|    loss                 | 44.8           |
|    n_updates            | 2590           |
|    policy_gradient_loss | -0.000363      |
|    std                  | 1.48           |
|    value_loss           | 99             |
--------------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 36.53 | Max Reward: 47.71
Iteration: 263 | Episodes: 10700 | Median Reward: 35.46 | Max Reward: 47.71
Iteration: 266 | Episodes: 10800 | Median Reward: 34.32 | Max Reward: 47.71
Iteration: 268 | Episodes: 10900 | Median Reward: 30.68 | Max Reward: 47.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.3         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 270           |
|    time_elapsed         | 3886          |
|    total_timesteps      | 1105920       |
| train/                  |               |
|    approx_kl            | 0.00012899653 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.4         |
|    explained_variance   | 0.819         |
|    learning_rate        | 0.0005        |
|    loss                 | 34.4          |
|    n_updates            | 2690          |
|    policy_gradient_loss | -0.000653     |
|    std                  | 1.49          |
|    value_loss           | 78.5          |
-------------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 34.69 | Max Reward: 47.71
Iteration: 273 | Episodes: 11100 | Median Reward: 34.00 | Max Reward: 47.71
Iteration: 276 | Episodes: 11200 | Median Reward: 35.23 | Max Reward: 47.71
Iteration: 278 | Episodes: 11300 | Median Reward: 35.66 | Max Reward: 47.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.1         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 280           |
|    time_elapsed         | 4031          |
|    total_timesteps      | 1146880       |
| train/                  |               |
|    approx_kl            | 3.6388054e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.5         |
|    explained_variance   | 0.836         |
|    learning_rate        | 0.0005        |
|    loss                 | 32.2          |
|    n_updates            | 2790          |
|    policy_gradient_loss | -0.000218     |
|    std                  | 1.49          |
|    value_loss           | 74.2          |
-------------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 30.07 | Max Reward: 47.71
Iteration: 283 | Episodes: 11500 | Median Reward: 34.20 | Max Reward: 47.71
Iteration: 286 | Episodes: 11600 | Median Reward: 33.80 | Max Reward: 47.71
Iteration: 288 | Episodes: 11700 | Median Reward: 30.62 | Max Reward: 47.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.4         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 290           |
|    time_elapsed         | 4184          |
|    total_timesteps      | 1187840       |
| train/                  |               |
|    approx_kl            | 1.8127874e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.5         |
|    explained_variance   | 0.861         |
|    learning_rate        | 0.0005        |
|    loss                 | 27.4          |
|    n_updates            | 2890          |
|    policy_gradient_loss | 5.51e-06      |
|    std                  | 1.49          |
|    value_loss           | 64.3          |
-------------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 30.12 | Max Reward: 47.71
Iteration: 293 | Episodes: 11900 | Median Reward: 37.40 | Max Reward: 47.71
Iteration: 295 | Episodes: 12000 | Median Reward: 30.99 | Max Reward: 47.71
Iteration: 298 | Episodes: 12100 | Median Reward: 34.90 | Max Reward: 47.71
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.4         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 300           |
|    time_elapsed         | 4328          |
|    total_timesteps      | 1228800       |
| train/                  |               |
|    approx_kl            | 0.00042597775 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.6         |
|    explained_variance   | 0.853         |
|    learning_rate        | 0.0005        |
|    loss                 | 31.3          |
|    n_updates            | 2990          |
|    policy_gradient_loss | -0.00171      |
|    std                  | 1.5           |
|    value_loss           | 72.5          |
-------------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 35.82 | Max Reward: 47.71
Iteration: 303 | Episodes: 12300 | Median Reward: 29.10 | Max Reward: 47.71
Iteration: 305 | Episodes: 12400 | Median Reward: 33.03 | Max Reward: 47.71
Iteration: 308 | Episodes: 12500 | Median Reward: 31.68 | Max Reward: 49.04
Iteration: 308 | Episodes: 12600 | Median Reward: 49.04 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 55.4          |
|    ep_rew_mean          | -13.8         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 310           |
|    time_elapsed         | 4472          |
|    total_timesteps      | 1269760       |
| train/                  |               |
|    approx_kl            | 0.00042456534 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.6         |
|    explained_variance   | -0.0479       |
|    learning_rate        | 0.0005        |
|    loss                 | 617           |
|    n_updates            | 3090          |
|    policy_gradient_loss | -0.0104       |
|    std                  | 1.5           |
|    value_loss           | 1.25e+03      |
-------------------------------------------
Iteration: 310 | Episodes: 12700 | Median Reward: 41.14 | Max Reward: 49.04
Iteration: 312 | Episodes: 12800 | Median Reward: 31.42 | Max Reward: 49.04
Iteration: 315 | Episodes: 12900 | Median Reward: 36.42 | Max Reward: 49.04
Iteration: 317 | Episodes: 13000 | Median Reward: 36.44 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.9         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 320           |
|    time_elapsed         | 4617          |
|    total_timesteps      | 1310720       |
| train/                  |               |
|    approx_kl            | 0.00031827495 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.7         |
|    explained_variance   | 0.868         |
|    learning_rate        | 0.0005        |
|    loss                 | 31.6          |
|    n_updates            | 3190          |
|    policy_gradient_loss | -0.000948     |
|    std                  | 1.5           |
|    value_loss           | 73            |
-------------------------------------------
Iteration: 320 | Episodes: 13100 | Median Reward: 39.64 | Max Reward: 49.04
Iteration: 322 | Episodes: 13200 | Median Reward: 34.97 | Max Reward: 49.04
Iteration: 325 | Episodes: 13300 | Median Reward: 31.31 | Max Reward: 49.04
Iteration: 327 | Episodes: 13400 | Median Reward: 31.37 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.1         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 330           |
|    time_elapsed         | 4762          |
|    total_timesteps      | 1351680       |
| train/                  |               |
|    approx_kl            | 0.00015997127 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.8         |
|    explained_variance   | 0.889         |
|    learning_rate        | 0.0005        |
|    loss                 | 24.8          |
|    n_updates            | 3290          |
|    policy_gradient_loss | -0.00063      |
|    std                  | 1.51          |
|    value_loss           | 59.1          |
-------------------------------------------
Iteration: 330 | Episodes: 13500 | Median Reward: 32.98 | Max Reward: 49.04
Iteration: 332 | Episodes: 13600 | Median Reward: 34.20 | Max Reward: 49.04
Iteration: 335 | Episodes: 13700 | Median Reward: 34.87 | Max Reward: 49.04
Iteration: 337 | Episodes: 13800 | Median Reward: 31.92 | Max Reward: 49.04
Iteration: 339 | Episodes: 13900 | Median Reward: 34.05 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68           |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 340           |
|    time_elapsed         | 4905          |
|    total_timesteps      | 1392640       |
| train/                  |               |
|    approx_kl            | 0.00093971705 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.8         |
|    explained_variance   | 0.926         |
|    learning_rate        | 0.0005        |
|    loss                 | 12.5          |
|    n_updates            | 3390          |
|    policy_gradient_loss | -0.00233      |
|    std                  | 1.51          |
|    value_loss           | 34.6          |
-------------------------------------------
Iteration: 342 | Episodes: 14000 | Median Reward: 33.90 | Max Reward: 49.04
Iteration: 344 | Episodes: 14100 | Median Reward: 33.01 | Max Reward: 49.04
Iteration: 347 | Episodes: 14200 | Median Reward: 37.20 | Max Reward: 49.04
Iteration: 349 | Episodes: 14300 | Median Reward: 36.42 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.6         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 350           |
|    time_elapsed         | 5048          |
|    total_timesteps      | 1433600       |
| train/                  |               |
|    approx_kl            | 0.00014150288 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -93.9         |
|    explained_variance   | 0.925         |
|    learning_rate        | 0.0005        |
|    loss                 | 14.4          |
|    n_updates            | 3490          |
|    policy_gradient_loss | -0.000662     |
|    std                  | 1.52          |
|    value_loss           | 38.5          |
-------------------------------------------
Iteration: 352 | Episodes: 14400 | Median Reward: 35.36 | Max Reward: 49.04
Iteration: 354 | Episodes: 14500 | Median Reward: 32.98 | Max Reward: 49.04
Iteration: 357 | Episodes: 14600 | Median Reward: 39.20 | Max Reward: 49.04
Iteration: 359 | Episodes: 14700 | Median Reward: 39.62 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62           |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 360           |
|    time_elapsed         | 5191          |
|    total_timesteps      | 1474560       |
| train/                  |               |
|    approx_kl            | 0.00065716374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94           |
|    explained_variance   | 0.911         |
|    learning_rate        | 0.0005        |
|    loss                 | 20            |
|    n_updates            | 3590          |
|    policy_gradient_loss | -0.000947     |
|    std                  | 1.52          |
|    value_loss           | 49.7          |
-------------------------------------------
Iteration: 362 | Episodes: 14800 | Median Reward: 33.77 | Max Reward: 49.04
Iteration: 364 | Episodes: 14900 | Median Reward: 36.88 | Max Reward: 49.04
Iteration: 367 | Episodes: 15000 | Median Reward: 36.41 | Max Reward: 49.04
Iteration: 369 | Episodes: 15100 | Median Reward: 36.19 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.6         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 370           |
|    time_elapsed         | 5335          |
|    total_timesteps      | 1515520       |
| train/                  |               |
|    approx_kl            | 7.6330616e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94.1         |
|    explained_variance   | 0.931         |
|    learning_rate        | 0.0005        |
|    loss                 | 13.4          |
|    n_updates            | 3690          |
|    policy_gradient_loss | -0.000688     |
|    std                  | 1.53          |
|    value_loss           | 36.3          |
-------------------------------------------
Iteration: 372 | Episodes: 15200 | Median Reward: 34.75 | Max Reward: 49.04
Iteration: 374 | Episodes: 15300 | Median Reward: 33.94 | Max Reward: 49.04
Iteration: 376 | Episodes: 15400 | Median Reward: 34.80 | Max Reward: 49.04
Iteration: 379 | Episodes: 15500 | Median Reward: 36.64 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.9         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 380           |
|    time_elapsed         | 5475          |
|    total_timesteps      | 1556480       |
| train/                  |               |
|    approx_kl            | 3.9865292e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94.2         |
|    explained_variance   | 0.931         |
|    learning_rate        | 0.0005        |
|    loss                 | 14.5          |
|    n_updates            | 3790          |
|    policy_gradient_loss | 5e-05         |
|    std                  | 1.53          |
|    value_loss           | 38.7          |
-------------------------------------------
Iteration: 381 | Episodes: 15600 | Median Reward: 35.26 | Max Reward: 49.04
Iteration: 384 | Episodes: 15700 | Median Reward: 36.38 | Max Reward: 49.04
Iteration: 386 | Episodes: 15800 | Median Reward: 35.02 | Max Reward: 49.04
Iteration: 389 | Episodes: 15900 | Median Reward: 38.32 | Max Reward: 49.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.1        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 390          |
|    time_elapsed         | 5620         |
|    total_timesteps      | 1597440      |
| train/                  |              |
|    approx_kl            | 0.0011044365 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -94.3        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.0005       |
|    loss                 | 16.7         |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.000472    |
|    std                  | 1.54         |
|    value_loss           | 43.3         |
------------------------------------------
Iteration: 391 | Episodes: 16000 | Median Reward: 38.61 | Max Reward: 49.04
Iteration: 394 | Episodes: 16100 | Median Reward: 34.71 | Max Reward: 49.04
Iteration: 396 | Episodes: 16200 | Median Reward: 31.13 | Max Reward: 49.04
Iteration: 399 | Episodes: 16300 | Median Reward: 36.10 | Max Reward: 49.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 400          |
|    time_elapsed         | 5769         |
|    total_timesteps      | 1638400      |
| train/                  |              |
|    approx_kl            | 0.0027849702 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -94.3        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.0005       |
|    loss                 | 8.91         |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.00646     |
|    std                  | 1.54         |
|    value_loss           | 27.7         |
------------------------------------------
Iteration: 401 | Episodes: 16400 | Median Reward: 38.17 | Max Reward: 49.04
Iteration: 404 | Episodes: 16500 | Median Reward: 34.53 | Max Reward: 49.04
Iteration: 406 | Episodes: 16600 | Median Reward: 37.28 | Max Reward: 49.04
Iteration: 409 | Episodes: 16700 | Median Reward: 35.38 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.8         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 410           |
|    time_elapsed         | 5916          |
|    total_timesteps      | 1679360       |
| train/                  |               |
|    approx_kl            | 0.00012793347 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94.4         |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.0005        |
|    loss                 | 6.87          |
|    n_updates            | 4090          |
|    policy_gradient_loss | -0.000404     |
|    std                  | 1.55          |
|    value_loss           | 23.2          |
-------------------------------------------
Iteration: 411 | Episodes: 16800 | Median Reward: 35.30 | Max Reward: 49.04
Iteration: 413 | Episodes: 16900 | Median Reward: 31.78 | Max Reward: 49.04
Iteration: 416 | Episodes: 17000 | Median Reward: 36.36 | Max Reward: 49.04
Iteration: 418 | Episodes: 17100 | Median Reward: 35.82 | Max Reward: 49.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67         |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 420         |
|    time_elapsed         | 6057        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.003196174 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -94.5       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0005      |
|    loss                 | 7.75        |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.0063     |
|    std                  | 1.55        |
|    value_loss           | 24.1        |
-----------------------------------------
Iteration: 421 | Episodes: 17200 | Median Reward: 31.71 | Max Reward: 49.04
Iteration: 423 | Episodes: 17300 | Median Reward: 29.80 | Max Reward: 49.04
Iteration: 426 | Episodes: 17400 | Median Reward: 34.46 | Max Reward: 49.04
Iteration: 428 | Episodes: 17500 | Median Reward: 39.71 | Max Reward: 49.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62          |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 430          |
|    time_elapsed         | 6200         |
|    total_timesteps      | 1761280      |
| train/                  |              |
|    approx_kl            | 4.956461e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -94.6        |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.0005       |
|    loss                 | 7.85         |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.000296    |
|    std                  | 1.56         |
|    value_loss           | 25.1         |
------------------------------------------
Iteration: 431 | Episodes: 17600 | Median Reward: 35.41 | Max Reward: 49.04
Iteration: 433 | Episodes: 17700 | Median Reward: 36.28 | Max Reward: 49.04
Iteration: 436 | Episodes: 17800 | Median Reward: 36.27 | Max Reward: 49.04
Iteration: 438 | Episodes: 17900 | Median Reward: 38.16 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.3         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 440           |
|    time_elapsed         | 6344          |
|    total_timesteps      | 1802240       |
| train/                  |               |
|    approx_kl            | 0.00030857167 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94.8         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.0005        |
|    loss                 | 5.57          |
|    n_updates            | 4390          |
|    policy_gradient_loss | -0.000511     |
|    std                  | 1.57          |
|    value_loss           | 20.8          |
-------------------------------------------
Iteration: 441 | Episodes: 18000 | Median Reward: 35.18 | Max Reward: 49.04
Iteration: 443 | Episodes: 18100 | Median Reward: 40.31 | Max Reward: 49.04
Iteration: 446 | Episodes: 18200 | Median Reward: 39.40 | Max Reward: 49.04
Iteration: 448 | Episodes: 18300 | Median Reward: 39.53 | Max Reward: 49.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.6       |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 450         |
|    time_elapsed         | 6489        |
|    total_timesteps      | 1843200     |
| train/                  |             |
|    approx_kl            | 0.004109266 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -94.9       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.72        |
|    n_updates            | 4490        |
|    policy_gradient_loss | -0.00807    |
|    std                  | 1.58        |
|    value_loss           | 13          |
-----------------------------------------
Iteration: 450 | Episodes: 18400 | Median Reward: 37.90 | Max Reward: 49.04
Iteration: 453 | Episodes: 18500 | Median Reward: 38.57 | Max Reward: 49.04
Iteration: 455 | Episodes: 18600 | Median Reward: 37.20 | Max Reward: 49.04
Iteration: 458 | Episodes: 18700 | Median Reward: 36.48 | Max Reward: 49.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.6       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 460         |
|    time_elapsed         | 6635        |
|    total_timesteps      | 1884160     |
| train/                  |             |
|    approx_kl            | 0.006173801 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -95         |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.36        |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0138     |
|    std                  | 1.59        |
|    value_loss           | 14.4        |
-----------------------------------------
Iteration: 460 | Episodes: 18800 | Median Reward: 37.22 | Max Reward: 49.04
Iteration: 463 | Episodes: 18900 | Median Reward: 38.58 | Max Reward: 49.04
Iteration: 465 | Episodes: 19000 | Median Reward: 40.85 | Max Reward: 49.04
Iteration: 468 | Episodes: 19100 | Median Reward: 35.54 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.3         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 470           |
|    time_elapsed         | 6780          |
|    total_timesteps      | 1925120       |
| train/                  |               |
|    approx_kl            | 0.00022763757 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95.1         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.0005        |
|    loss                 | 3.44          |
|    n_updates            | 4690          |
|    policy_gradient_loss | -0.000803     |
|    std                  | 1.59          |
|    value_loss           | 17.5          |
-------------------------------------------
Iteration: 470 | Episodes: 19200 | Median Reward: 39.11 | Max Reward: 49.04
Iteration: 473 | Episodes: 19300 | Median Reward: 38.48 | Max Reward: 49.04
Iteration: 475 | Episodes: 19400 | Median Reward: 39.18 | Max Reward: 49.04
Iteration: 478 | Episodes: 19500 | Median Reward: 42.25 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.9         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 480           |
|    time_elapsed         | 6924          |
|    total_timesteps      | 1966080       |
| train/                  |               |
|    approx_kl            | 0.00087107345 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95.2         |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.0005        |
|    loss                 | 3.54          |
|    n_updates            | 4790          |
|    policy_gradient_loss | -0.00202      |
|    std                  | 1.6           |
|    value_loss           | 17.3          |
-------------------------------------------
Iteration: 480 | Episodes: 19600 | Median Reward: 34.61 | Max Reward: 49.04
Iteration: 483 | Episodes: 19700 | Median Reward: 38.30 | Max Reward: 49.04
Iteration: 485 | Episodes: 19800 | Median Reward: 38.97 | Max Reward: 49.04
Iteration: 487 | Episodes: 19900 | Median Reward: 38.64 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.8         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 490           |
|    time_elapsed         | 7067          |
|    total_timesteps      | 2007040       |
| train/                  |               |
|    approx_kl            | 0.00034320858 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95.4         |
|    explained_variance   | 0.972         |
|    learning_rate        | 0.0005        |
|    loss                 | 3.65          |
|    n_updates            | 4890          |
|    policy_gradient_loss | -0.0013       |
|    std                  | 1.61          |
|    value_loss           | 16.9          |
-------------------------------------------
Iteration: 490 | Episodes: 20000 | Median Reward: 39.48 | Max Reward: 49.04
Iteration: 492 | Episodes: 20100 | Median Reward: 37.55 | Max Reward: 49.04
Iteration: 495 | Episodes: 20200 | Median Reward: 35.39 | Max Reward: 49.04
Iteration: 497 | Episodes: 20300 | Median Reward: 41.22 | Max Reward: 49.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.1       |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 500         |
|    time_elapsed         | 7210        |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.008638836 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -95.5       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0005      |
|    loss                 | 1.94        |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.0151     |
|    std                  | 1.62        |
|    value_loss           | 13.8        |
-----------------------------------------
Iteration: 500 | Episodes: 20400 | Median Reward: 37.30 | Max Reward: 49.04
Iteration: 502 | Episodes: 20500 | Median Reward: 39.37 | Max Reward: 49.04
Iteration: 505 | Episodes: 20600 | Median Reward: 39.10 | Max Reward: 49.04
Iteration: 507 | Episodes: 20700 | Median Reward: 36.60 | Max Reward: 49.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62          |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 510          |
|    time_elapsed         | 7355         |
|    total_timesteps      | 2088960      |
| train/                  |              |
|    approx_kl            | 0.0051323967 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -95.6        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0005       |
|    loss                 | 2.2          |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.00859     |
|    std                  | 1.63         |
|    value_loss           | 14.2         |
------------------------------------------
Iteration: 510 | Episodes: 20800 | Median Reward: 41.61 | Max Reward: 49.04
Iteration: 512 | Episodes: 20900 | Median Reward: 35.77 | Max Reward: 49.04
Iteration: 515 | Episodes: 21000 | Median Reward: 39.08 | Max Reward: 49.04
Iteration: 517 | Episodes: 21100 | Median Reward: 42.52 | Max Reward: 49.04
Iteration: 519 | Episodes: 21200 | Median Reward: 40.13 | Max Reward: 49.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 520          |
|    time_elapsed         | 7507         |
|    total_timesteps      | 2129920      |
| train/                  |              |
|    approx_kl            | 0.0014004186 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -95.7        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0005       |
|    loss                 | 2.42         |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00238     |
|    std                  | 1.64         |
|    value_loss           | 14.6         |
------------------------------------------
Iteration: 522 | Episodes: 21300 | Median Reward: 41.49 | Max Reward: 49.04
Iteration: 524 | Episodes: 21400 | Median Reward: 38.11 | Max Reward: 49.04
Iteration: 527 | Episodes: 21500 | Median Reward: 40.21 | Max Reward: 49.04
Iteration: 529 | Episodes: 21600 | Median Reward: 35.08 | Max Reward: 49.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.7        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 530          |
|    time_elapsed         | 7652         |
|    total_timesteps      | 2170880      |
| train/                  |              |
|    approx_kl            | 0.0033081444 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -95.8        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0005       |
|    loss                 | -1.69        |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.00538     |
|    std                  | 1.65         |
|    value_loss           | 6.4          |
------------------------------------------
Iteration: 532 | Episodes: 21700 | Median Reward: 38.79 | Max Reward: 49.04
Iteration: 534 | Episodes: 21800 | Median Reward: 38.76 | Max Reward: 49.04
Iteration: 537 | Episodes: 21900 | Median Reward: 40.10 | Max Reward: 49.04
Iteration: 539 | Episodes: 22000 | Median Reward: 38.09 | Max Reward: 49.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.9        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 540          |
|    time_elapsed         | 7795         |
|    total_timesteps      | 2211840      |
| train/                  |              |
|    approx_kl            | 0.0015045966 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -96          |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0005       |
|    loss                 | -0.657       |
|    n_updates            | 5390         |
|    policy_gradient_loss | -0.00286     |
|    std                  | 1.66         |
|    value_loss           | 8.25         |
------------------------------------------
Iteration: 542 | Episodes: 22100 | Median Reward: 37.77 | Max Reward: 49.04
Iteration: 544 | Episodes: 22200 | Median Reward: 38.54 | Max Reward: 49.04
Iteration: 547 | Episodes: 22300 | Median Reward: 40.23 | Max Reward: 49.04
Iteration: 549 | Episodes: 22400 | Median Reward: 31.79 | Max Reward: 49.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.2       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 550         |
|    time_elapsed         | 7939        |
|    total_timesteps      | 2252800     |
| train/                  |             |
|    approx_kl            | 0.019861756 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -96.1       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0005      |
|    loss                 | 0.965       |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.0211     |
|    std                  | 1.66        |
|    value_loss           | 11.3        |
-----------------------------------------
Iteration: 552 | Episodes: 22500 | Median Reward: 39.94 | Max Reward: 49.04
Iteration: 554 | Episodes: 22600 | Median Reward: 42.59 | Max Reward: 49.04
Iteration: 556 | Episodes: 22700 | Median Reward: 39.15 | Max Reward: 49.04
Iteration: 559 | Episodes: 22800 | Median Reward: 40.24 | Max Reward: 49.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.8         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 560           |
|    time_elapsed         | 8080          |
|    total_timesteps      | 2293760       |
| train/                  |               |
|    approx_kl            | 0.00072612555 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -96.2         |
|    explained_variance   | 0.983         |
|    learning_rate        | 0.0005        |
|    loss                 | 0.746         |
|    n_updates            | 5590          |
|    policy_gradient_loss | -0.00138      |
|    std                  | 1.67          |
|    value_loss           | 11.1          |
-------------------------------------------
Iteration: 561 | Episodes: 22900 | Median Reward: 39.18 | Max Reward: 49.06
Iteration: 564 | Episodes: 23000 | Median Reward: 40.13 | Max Reward: 49.06
Iteration: 566 | Episodes: 23100 | Median Reward: 40.14 | Max Reward: 49.06
Iteration: 569 | Episodes: 23200 | Median Reward: 40.30 | Max Reward: 49.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.4        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 570          |
|    time_elapsed         | 8220         |
|    total_timesteps      | 2334720      |
| train/                  |              |
|    approx_kl            | 0.0009254261 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -96.3        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.68         |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.68         |
|    value_loss           | 13.2         |
------------------------------------------
Iteration: 571 | Episodes: 23300 | Median Reward: 40.05 | Max Reward: 49.06
Iteration: 574 | Episodes: 23400 | Median Reward: 40.18 | Max Reward: 49.06
Iteration: 576 | Episodes: 23500 | Median Reward: 39.71 | Max Reward: 49.06
Iteration: 579 | Episodes: 23600 | Median Reward: 42.28 | Max Reward: 49.06
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.5         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 580           |
|    time_elapsed         | 8361          |
|    total_timesteps      | 2375680       |
| train/                  |               |
|    approx_kl            | 0.00049189664 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -96.5         |
|    explained_variance   | 0.989         |
|    learning_rate        | 0.0005        |
|    loss                 | -1.65         |
|    n_updates            | 5790          |
|    policy_gradient_loss | -0.00115      |
|    std                  | 1.69          |
|    value_loss           | 6.48          |
-------------------------------------------
Iteration: 581 | Episodes: 23700 | Median Reward: 36.35 | Max Reward: 49.06
Iteration: 584 | Episodes: 23800 | Median Reward: 42.83 | Max Reward: 49.06
Iteration: 586 | Episodes: 23900 | Median Reward: 43.27 | Max Reward: 49.06
Iteration: 589 | Episodes: 24000 | Median Reward: 42.05 | Max Reward: 49.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.6        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 590          |
|    time_elapsed         | 8506         |
|    total_timesteps      | 2416640      |
| train/                  |              |
|    approx_kl            | 0.0044898447 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -96.6        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0005       |
|    loss                 | -1.87        |
|    n_updates            | 5890         |
|    policy_gradient_loss | -0.00998     |
|    std                  | 1.7          |
|    value_loss           | 6.08         |
------------------------------------------
Iteration: 591 | Episodes: 24100 | Median Reward: 43.15 | Max Reward: 49.06
Iteration: 593 | Episodes: 24200 | Median Reward: 42.61 | Max Reward: 49.06
Iteration: 596 | Episodes: 24300 | Median Reward: 39.96 | Max Reward: 49.06
Iteration: 598 | Episodes: 24400 | Median Reward: 39.08 | Max Reward: 49.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.4        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 600          |
|    time_elapsed         | 8651         |
|    total_timesteps      | 2457600      |
| train/                  |              |
|    approx_kl            | 0.0030096052 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -96.8        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0005       |
|    loss                 | -1.25        |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.00394     |
|    std                  | 1.71         |
|    value_loss           | 7.22         |
------------------------------------------
Iteration: 601 | Episodes: 24500 | Median Reward: 43.51 | Max Reward: 49.06
Iteration: 603 | Episodes: 24600 | Median Reward: 44.31 | Max Reward: 49.06
Iteration: 606 | Episodes: 24700 | Median Reward: 42.83 | Max Reward: 49.06
Iteration: 608 | Episodes: 24800 | Median Reward: 45.52 | Max Reward: 49.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 610          |
|    time_elapsed         | 8795         |
|    total_timesteps      | 2498560      |
| train/                  |              |
|    approx_kl            | 0.0008732201 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.1        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0005       |
|    loss                 | -0.477       |
|    n_updates            | 6090         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 1.73         |
|    value_loss           | 8.81         |
------------------------------------------
Iteration: 611 | Episodes: 24900 | Median Reward: 41.61 | Max Reward: 49.06
Iteration: 613 | Episodes: 25000 | Median Reward: 43.55 | Max Reward: 49.06
Iteration: 616 | Episodes: 25100 | Median Reward: 43.12 | Max Reward: 49.06
Iteration: 618 | Episodes: 25200 | Median Reward: 39.51 | Max Reward: 49.06
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.9         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 620           |
|    time_elapsed         | 8939          |
|    total_timesteps      | 2539520       |
| train/                  |               |
|    approx_kl            | 0.00038237555 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -97.2         |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -2.99         |
|    n_updates            | 6190          |
|    policy_gradient_loss | -0.000628     |
|    std                  | 1.74          |
|    value_loss           | 4.05          |
-------------------------------------------
Iteration: 621 | Episodes: 25300 | Median Reward: 46.11 | Max Reward: 49.06
Iteration: 623 | Episodes: 25400 | Median Reward: 41.92 | Max Reward: 49.06
Iteration: 625 | Episodes: 25500 | Median Reward: 38.33 | Max Reward: 49.06
Iteration: 628 | Episodes: 25600 | Median Reward: 40.28 | Max Reward: 49.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.7        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 630          |
|    time_elapsed         | 9082         |
|    total_timesteps      | 2580480      |
| train/                  |              |
|    approx_kl            | 0.0035126503 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.3        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0005       |
|    loss                 | -2.07        |
|    n_updates            | 6290         |
|    policy_gradient_loss | -0.00606     |
|    std                  | 1.75         |
|    value_loss           | 5.69         |
------------------------------------------
Iteration: 630 | Episodes: 25700 | Median Reward: 41.25 | Max Reward: 49.06
Iteration: 633 | Episodes: 25800 | Median Reward: 42.21 | Max Reward: 49.06
Iteration: 635 | Episodes: 25900 | Median Reward: 42.29 | Max Reward: 49.06
Iteration: 638 | Episodes: 26000 | Median Reward: 43.66 | Max Reward: 49.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.6       |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 640         |
|    time_elapsed         | 9226        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.004553348 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -97.4       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.23       |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.00484    |
|    std                  | 1.76        |
|    value_loss           | 5.18        |
-----------------------------------------
Iteration: 640 | Episodes: 26100 | Median Reward: 39.31 | Max Reward: 49.06
Iteration: 643 | Episodes: 26200 | Median Reward: 43.11 | Max Reward: 49.06
Iteration: 645 | Episodes: 26300 | Median Reward: 43.16 | Max Reward: 49.06
Iteration: 648 | Episodes: 26400 | Median Reward: 43.32 | Max Reward: 49.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 650          |
|    time_elapsed         | 9382         |
|    total_timesteps      | 2662400      |
| train/                  |              |
|    approx_kl            | 0.0022531003 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.6        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -2.59        |
|    n_updates            | 6490         |
|    policy_gradient_loss | -0.00495     |
|    std                  | 1.77         |
|    value_loss           | 4.34         |
------------------------------------------
Iteration: 650 | Episodes: 26500 | Median Reward: 42.80 | Max Reward: 49.06
Iteration: 653 | Episodes: 26600 | Median Reward: 42.52 | Max Reward: 49.06
Iteration: 655 | Episodes: 26700 | Median Reward: 41.71 | Max Reward: 49.06
Iteration: 658 | Episodes: 26800 | Median Reward: 42.11 | Max Reward: 49.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.6       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 660         |
|    time_elapsed         | 9525        |
|    total_timesteps      | 2703360     |
| train/                  |             |
|    approx_kl            | 0.006853181 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -97.7       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.59       |
|    n_updates            | 6590        |
|    policy_gradient_loss | -0.00836    |
|    std                  | 1.78        |
|    value_loss           | 4.16        |
-----------------------------------------
Iteration: 660 | Episodes: 26900 | Median Reward: 40.28 | Max Reward: 49.06
Iteration: 662 | Episodes: 27000 | Median Reward: 43.84 | Max Reward: 49.06
Iteration: 665 | Episodes: 27100 | Median Reward: 43.01 | Max Reward: 49.06
Iteration: 667 | Episodes: 27200 | Median Reward: 42.07 | Max Reward: 49.06
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -59.8      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 670        |
|    time_elapsed         | 9670       |
|    total_timesteps      | 2744320    |
| train/                  |            |
|    approx_kl            | 0.00590514 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -97.9      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0005     |
|    loss                 | -3.55      |
|    n_updates            | 6690       |
|    policy_gradient_loss | -0.00809   |
|    std                  | 1.8        |
|    value_loss           | 2.5        |
----------------------------------------
Iteration: 670 | Episodes: 27300 | Median Reward: 42.49 | Max Reward: 49.06
Iteration: 672 | Episodes: 27400 | Median Reward: 43.84 | Max Reward: 49.06
Iteration: 675 | Episodes: 27500 | Median Reward: 44.46 | Max Reward: 49.06
Iteration: 677 | Episodes: 27600 | Median Reward: 42.90 | Max Reward: 49.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57         |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 680         |
|    time_elapsed         | 9814        |
|    total_timesteps      | 2785280     |
| train/                  |             |
|    approx_kl            | 0.003851836 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -98         |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.85       |
|    n_updates            | 6790        |
|    policy_gradient_loss | -0.00361    |
|    std                  | 1.81        |
|    value_loss           | 4.46        |
-----------------------------------------
Iteration: 680 | Episodes: 27700 | Median Reward: 43.05 | Max Reward: 49.06
Iteration: 682 | Episodes: 27800 | Median Reward: 42.65 | Max Reward: 49.06
Iteration: 685 | Episodes: 27900 | Median Reward: 43.57 | Max Reward: 49.06
Iteration: 687 | Episodes: 28000 | Median Reward: 43.62 | Max Reward: 49.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.9       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 690         |
|    time_elapsed         | 9959        |
|    total_timesteps      | 2826240     |
| train/                  |             |
|    approx_kl            | 0.002784369 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -98.1       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -3.56       |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.00908    |
|    std                  | 1.81        |
|    value_loss           | 3.25        |
-----------------------------------------
Iteration: 690 | Episodes: 28100 | Median Reward: 38.79 | Max Reward: 49.06
Iteration: 692 | Episodes: 28200 | Median Reward: 42.55 | Max Reward: 49.06
Iteration: 695 | Episodes: 28300 | Median Reward: 42.48 | Max Reward: 49.06
Iteration: 697 | Episodes: 28400 | Median Reward: 42.91 | Max Reward: 49.06
Iteration: 699 | Episodes: 28500 | Median Reward: 43.20 | Max Reward: 49.06
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 700          |
|    time_elapsed         | 10101        |
|    total_timesteps      | 2867200      |
| train/                  |              |
|    approx_kl            | 0.0007256694 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -98.3        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -3.15        |
|    n_updates            | 6990         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.83         |
|    value_loss           | 3.38         |
------------------------------------------
Iteration: 702 | Episodes: 28600 | Median Reward: 42.22 | Max Reward: 49.06
Iteration: 704 | Episodes: 28700 | Median Reward: 43.13 | Max Reward: 49.06
Iteration: 707 | Episodes: 28800 | Median Reward: 46.14 | Max Reward: 49.06
Iteration: 709 | Episodes: 28900 | Median Reward: 44.54 | Max Reward: 49.06
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.5         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 710           |
|    time_elapsed         | 10245         |
|    total_timesteps      | 2908160       |
| train/                  |               |
|    approx_kl            | 4.3592285e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -98.5         |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -3.08         |
|    n_updates            | 7090          |
|    policy_gradient_loss | -0.000226     |
|    std                  | 1.84          |
|    value_loss           | 4.83          |
-------------------------------------------
Iteration: 712 | Episodes: 29000 | Median Reward: 44.17 | Max Reward: 49.06
Iteration: 714 | Episodes: 29100 | Median Reward: 44.31 | Max Reward: 49.06
Iteration: 717 | Episodes: 29200 | Median Reward: 43.88 | Max Reward: 49.06
Iteration: 719 | Episodes: 29300 | Median Reward: 43.94 | Max Reward: 49.06
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.3       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 720         |
|    time_elapsed         | 10390       |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.004773261 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -98.7       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -3.79       |
|    n_updates            | 7190        |
|    policy_gradient_loss | -0.00555    |
|    std                  | 1.85        |
|    value_loss           | 2.34        |
-----------------------------------------
Iteration: 722 | Episodes: 29400 | Median Reward: 44.32 | Max Reward: 49.09
Iteration: 724 | Episodes: 29500 | Median Reward: 44.59 | Max Reward: 49.09
Iteration: 727 | Episodes: 29600 | Median Reward: 43.17 | Max Reward: 49.09
Iteration: 729 | Episodes: 29700 | Median Reward: 43.17 | Max Reward: 49.09
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 730          |
|    time_elapsed         | 10534        |
|    total_timesteps      | 2990080      |
| train/                  |              |
|    approx_kl            | 0.0021002248 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -98.8        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.15        |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.00242     |
|    std                  | 1.87         |
|    value_loss           | 2.15         |
------------------------------------------
Iteration: 731 | Episodes: 29800 | Median Reward: 43.68 | Max Reward: 49.09
Iteration: 734 | Episodes: 29900 | Median Reward: 45.37 | Max Reward: 49.09
Iteration: 736 | Episodes: 30000 | Median Reward: 43.47 | Max Reward: 49.10
Iteration: 739 | Episodes: 30100 | Median Reward: 47.26 | Max Reward: 49.10
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.6         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 740           |
|    time_elapsed         | 10677         |
|    total_timesteps      | 3031040       |
| train/                  |               |
|    approx_kl            | 0.00024816376 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -99           |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -3.05         |
|    n_updates            | 7390          |
|    policy_gradient_loss | -0.000158     |
|    std                  | 1.88          |
|    value_loss           | 4.21          |
-------------------------------------------
Iteration: 741 | Episodes: 30200 | Median Reward: 42.22 | Max Reward: 49.10
Iteration: 744 | Episodes: 30300 | Median Reward: 43.22 | Max Reward: 49.10
Iteration: 746 | Episodes: 30400 | Median Reward: 44.34 | Max Reward: 49.10
Iteration: 749 | Episodes: 30500 | Median Reward: 41.48 | Max Reward: 49.10
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 750          |
|    time_elapsed         | 10821        |
|    total_timesteps      | 3072000      |
| train/                  |              |
|    approx_kl            | 0.0051417113 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -99.2        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.41        |
|    n_updates            | 7490         |
|    policy_gradient_loss | -0.00516     |
|    std                  | 1.9          |
|    value_loss           | 1.32         |
------------------------------------------
Iteration: 751 | Episodes: 30600 | Median Reward: 46.30 | Max Reward: 49.10
Iteration: 754 | Episodes: 30700 | Median Reward: 43.07 | Max Reward: 49.10
Iteration: 756 | Episodes: 30800 | Median Reward: 45.80 | Max Reward: 49.10
Iteration: 759 | Episodes: 30900 | Median Reward: 44.27 | Max Reward: 49.10
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 760          |
|    time_elapsed         | 10966        |
|    total_timesteps      | 3112960      |
| train/                  |              |
|    approx_kl            | 0.0004949423 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -99.4        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.09        |
|    n_updates            | 7590         |
|    policy_gradient_loss | -0.000276    |
|    std                  | 1.92         |
|    value_loss           | 1.5          |
------------------------------------------
Iteration: 761 | Episodes: 31000 | Median Reward: 45.84 | Max Reward: 49.10
Iteration: 764 | Episodes: 31100 | Median Reward: 45.35 | Max Reward: 49.10
Iteration: 766 | Episodes: 31200 | Median Reward: 44.34 | Max Reward: 49.12
Iteration: 768 | Episodes: 31300 | Median Reward: 43.61 | Max Reward: 49.12
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.9         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 770           |
|    time_elapsed         | 11112         |
|    total_timesteps      | 3153920       |
| train/                  |               |
|    approx_kl            | 0.00067640824 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -99.6         |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.03         |
|    n_updates            | 7690          |
|    policy_gradient_loss | -0.00206      |
|    std                  | 1.93          |
|    value_loss           | 1.82          |
-------------------------------------------
Iteration: 771 | Episodes: 31400 | Median Reward: 45.30 | Max Reward: 49.12
Iteration: 773 | Episodes: 31500 | Median Reward: 43.38 | Max Reward: 49.12
Iteration: 776 | Episodes: 31600 | Median Reward: 44.03 | Max Reward: 49.12
Iteration: 778 | Episodes: 31700 | Median Reward: 44.55 | Max Reward: 49.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.4        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 780          |
|    time_elapsed         | 11264        |
|    total_timesteps      | 3194880      |
| train/                  |              |
|    approx_kl            | 0.0009325033 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -99.8        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -3.84        |
|    n_updates            | 7790         |
|    policy_gradient_loss | -0.00246     |
|    std                  | 1.95         |
|    value_loss           | 2.52         |
------------------------------------------
Iteration: 781 | Episodes: 31800 | Median Reward: 45.48 | Max Reward: 49.12
Iteration: 783 | Episodes: 31900 | Median Reward: 43.45 | Max Reward: 49.12
Iteration: 786 | Episodes: 32000 | Median Reward: 44.38 | Max Reward: 49.12
Iteration: 788 | Episodes: 32100 | Median Reward: 44.39 | Max Reward: 49.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 790          |
|    time_elapsed         | 11410        |
|    total_timesteps      | 3235840      |
| train/                  |              |
|    approx_kl            | 0.0010609737 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -100         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.27        |
|    n_updates            | 7890         |
|    policy_gradient_loss | -0.00199     |
|    std                  | 1.96         |
|    value_loss           | 1.28         |
------------------------------------------
Iteration: 791 | Episodes: 32200 | Median Reward: 43.10 | Max Reward: 49.12
Iteration: 793 | Episodes: 32300 | Median Reward: 44.71 | Max Reward: 49.12
Iteration: 795 | Episodes: 32400 | Median Reward: 47.37 | Max Reward: 49.12
Iteration: 795 | Episodes: 32500 | Median Reward: 49.02 | Max Reward: 49.12
Iteration: 798 | Episodes: 32600 | Median Reward: 45.96 | Max Reward: 49.12
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.6        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 800          |
|    time_elapsed         | 11556        |
|    total_timesteps      | 3276800      |
| train/                  |              |
|    approx_kl            | 0.0027039242 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -100         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -3.83        |
|    n_updates            | 7990         |
|    policy_gradient_loss | -0.00245     |
|    std                  | 1.98         |
|    value_loss           | 2.29         |
------------------------------------------
Iteration: 800 | Episodes: 32700 | Median Reward: 46.44 | Max Reward: 49.13
Iteration: 803 | Episodes: 32800 | Median Reward: 45.52 | Max Reward: 49.13
Iteration: 805 | Episodes: 32900 | Median Reward: 45.49 | Max Reward: 49.13
Iteration: 808 | Episodes: 33000 | Median Reward: 42.38 | Max Reward: 49.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 810          |
|    time_elapsed         | 11699        |
|    total_timesteps      | 3317760      |
| train/                  |              |
|    approx_kl            | 0.0112988185 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -100         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.6         |
|    n_updates            | 8090         |
|    policy_gradient_loss | -0.0128      |
|    std                  | 2            |
|    value_loss           | 0.954        |
------------------------------------------
Iteration: 810 | Episodes: 33100 | Median Reward: 43.86 | Max Reward: 49.13
Iteration: 813 | Episodes: 33200 | Median Reward: 43.89 | Max Reward: 49.13
Iteration: 815 | Episodes: 33300 | Median Reward: 43.86 | Max Reward: 49.13
Iteration: 817 | Episodes: 33400 | Median Reward: 46.67 | Max Reward: 49.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.3       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 820         |
|    time_elapsed         | 11842       |
|    total_timesteps      | 3358720     |
| train/                  |             |
|    approx_kl            | 0.009130301 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -101        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -4.44       |
|    n_updates            | 8190        |
|    policy_gradient_loss | -0.0164     |
|    std                  | 2.01        |
|    value_loss           | 1.61        |
-----------------------------------------
Iteration: 820 | Episodes: 33500 | Median Reward: 44.96 | Max Reward: 49.13
Iteration: 822 | Episodes: 33600 | Median Reward: 44.80 | Max Reward: 49.13
Iteration: 825 | Episodes: 33700 | Median Reward: 44.14 | Max Reward: 49.13
Iteration: 827 | Episodes: 33800 | Median Reward: 46.01 | Max Reward: 49.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.5        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 830          |
|    time_elapsed         | 11986        |
|    total_timesteps      | 3399680      |
| train/                  |              |
|    approx_kl            | 0.0010056484 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -101         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.57        |
|    n_updates            | 8290         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 2.03         |
|    value_loss           | 1.16         |
------------------------------------------
Iteration: 830 | Episodes: 33900 | Median Reward: 43.68 | Max Reward: 49.13
Iteration: 832 | Episodes: 34000 | Median Reward: 46.22 | Max Reward: 49.13
Iteration: 835 | Episodes: 34100 | Median Reward: 46.30 | Max Reward: 49.15
Iteration: 837 | Episodes: 34200 | Median Reward: 43.99 | Max Reward: 49.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.2       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 840         |
|    time_elapsed         | 12130       |
|    total_timesteps      | 3440640     |
| train/                  |             |
|    approx_kl            | 0.001078391 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -101        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -4.48       |
|    n_updates            | 8390        |
|    policy_gradient_loss | 0.000255    |
|    std                  | 2.04        |
|    value_loss           | 1.21        |
-----------------------------------------
Iteration: 840 | Episodes: 34300 | Median Reward: 46.38 | Max Reward: 49.15
Iteration: 842 | Episodes: 34400 | Median Reward: 44.61 | Max Reward: 49.15
Iteration: 845 | Episodes: 34500 | Median Reward: 47.56 | Max Reward: 49.15
Iteration: 847 | Episodes: 34600 | Median Reward: 46.09 | Max Reward: 49.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.1        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 850          |
|    time_elapsed         | 12277        |
|    total_timesteps      | 3481600      |
| train/                  |              |
|    approx_kl            | 0.0025291452 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -101         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.19        |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.00497     |
|    std                  | 2.05         |
|    value_loss           | 1.11         |
------------------------------------------
Iteration: 850 | Episodes: 34700 | Median Reward: 46.97 | Max Reward: 49.15
Iteration: 852 | Episodes: 34800 | Median Reward: 43.85 | Max Reward: 49.15
Iteration: 854 | Episodes: 34900 | Median Reward: 46.03 | Max Reward: 49.15
Iteration: 857 | Episodes: 35000 | Median Reward: 45.74 | Max Reward: 49.15
Iteration: 859 | Episodes: 35100 | Median Reward: 46.97 | Max Reward: 49.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.9       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 860         |
|    time_elapsed         | 12421       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.005882987 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -101        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -4.24       |
|    n_updates            | 8590        |
|    policy_gradient_loss | -0.00733    |
|    std                  | 2.07        |
|    value_loss           | 0.97        |
-----------------------------------------
Iteration: 862 | Episodes: 35200 | Median Reward: 47.27 | Max Reward: 49.15
Iteration: 864 | Episodes: 35300 | Median Reward: 46.26 | Max Reward: 49.15
Iteration: 867 | Episodes: 35400 | Median Reward: 45.22 | Max Reward: 49.15
Iteration: 869 | Episodes: 35500 | Median Reward: 46.82 | Max Reward: 49.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.7        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 870          |
|    time_elapsed         | 12564        |
|    total_timesteps      | 3563520      |
| train/                  |              |
|    approx_kl            | 0.0014789945 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -101         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.34        |
|    n_updates            | 8690         |
|    policy_gradient_loss | -0.0028      |
|    std                  | 2.08         |
|    value_loss           | 1.47         |
------------------------------------------
Iteration: 872 | Episodes: 35600 | Median Reward: 43.64 | Max Reward: 49.15
Iteration: 874 | Episodes: 35700 | Median Reward: 46.83 | Max Reward: 49.15
Iteration: 877 | Episodes: 35800 | Median Reward: 46.33 | Max Reward: 49.15
Iteration: 879 | Episodes: 35900 | Median Reward: 45.94 | Max Reward: 49.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | -55.3       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 880         |
|    time_elapsed         | 12708       |
|    total_timesteps      | 3604480     |
| train/                  |             |
|    approx_kl            | 0.009408897 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -101        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -4.63       |
|    n_updates            | 8790        |
|    policy_gradient_loss | -0.0294     |
|    std                  | 2.09        |
|    value_loss           | 1.31        |
-----------------------------------------
Iteration: 882 | Episodes: 36000 | Median Reward: 47.24 | Max Reward: 49.15
Iteration: 884 | Episodes: 36100 | Median Reward: 46.05 | Max Reward: 49.15
Iteration: 886 | Episodes: 36200 | Median Reward: 46.95 | Max Reward: 49.15
Iteration: 889 | Episodes: 36300 | Median Reward: 46.99 | Max Reward: 49.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54          |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 890          |
|    time_elapsed         | 12865        |
|    total_timesteps      | 3645440      |
| train/                  |              |
|    approx_kl            | 0.0010853574 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.93        |
|    n_updates            | 8890         |
|    policy_gradient_loss | -0.00398     |
|    std                  | 2.11         |
|    value_loss           | 0.578        |
------------------------------------------
Iteration: 891 | Episodes: 36400 | Median Reward: 46.69 | Max Reward: 49.15
Iteration: 894 | Episodes: 36500 | Median Reward: 44.00 | Max Reward: 49.15
Iteration: 896 | Episodes: 36600 | Median Reward: 45.76 | Max Reward: 49.15
Iteration: 899 | Episodes: 36700 | Median Reward: 45.64 | Max Reward: 49.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 900          |
|    time_elapsed         | 13007        |
|    total_timesteps      | 3686400      |
| train/                  |              |
|    approx_kl            | 0.0006694186 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.66        |
|    n_updates            | 8990         |
|    policy_gradient_loss | -0.00222     |
|    std                  | 2.12         |
|    value_loss           | 0.642        |
------------------------------------------
Iteration: 901 | Episodes: 36800 | Median Reward: 44.36 | Max Reward: 49.15
Iteration: 904 | Episodes: 36900 | Median Reward: 46.26 | Max Reward: 49.15
Iteration: 906 | Episodes: 37000 | Median Reward: 46.97 | Max Reward: 49.15
Iteration: 909 | Episodes: 37100 | Median Reward: 44.67 | Max Reward: 49.15
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.9         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 910           |
|    time_elapsed         | 13150         |
|    total_timesteps      | 3727360       |
| train/                  |               |
|    approx_kl            | 0.00045922617 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.89         |
|    n_updates            | 9090          |
|    policy_gradient_loss | 0.000372      |
|    std                  | 2.13          |
|    value_loss           | 0.554         |
-------------------------------------------
Iteration: 911 | Episodes: 37200 | Median Reward: 46.30 | Max Reward: 49.15
Iteration: 914 | Episodes: 37300 | Median Reward: 44.26 | Max Reward: 49.15
Iteration: 916 | Episodes: 37400 | Median Reward: 45.76 | Max Reward: 49.15
Iteration: 918 | Episodes: 37500 | Median Reward: 44.10 | Max Reward: 49.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.8        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 920          |
|    time_elapsed         | 13295        |
|    total_timesteps      | 3768320      |
| train/                  |              |
|    approx_kl            | 5.358555e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -2.95        |
|    n_updates            | 9190         |
|    policy_gradient_loss | -0.000495    |
|    std                  | 2.15         |
|    value_loss           | 4.09         |
------------------------------------------
Iteration: 921 | Episodes: 37600 | Median Reward: 46.19 | Max Reward: 49.15
Iteration: 923 | Episodes: 37700 | Median Reward: 46.08 | Max Reward: 49.15
Iteration: 926 | Episodes: 37800 | Median Reward: 46.97 | Max Reward: 49.15
Iteration: 928 | Episodes: 37900 | Median Reward: 46.33 | Max Reward: 49.15
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -54.5         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 930           |
|    time_elapsed         | 13437         |
|    total_timesteps      | 3809280       |
| train/                  |               |
|    approx_kl            | 0.00045397482 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0005        |
|    loss                 | -0.875        |
|    n_updates            | 9290          |
|    policy_gradient_loss | -0.00171      |
|    std                  | 2.15          |
|    value_loss           | 7.59          |
-------------------------------------------
Iteration: 931 | Episodes: 38000 | Median Reward: 46.17 | Max Reward: 49.20
Iteration: 933 | Episodes: 38100 | Median Reward: 46.65 | Max Reward: 49.20
Iteration: 936 | Episodes: 38200 | Median Reward: 46.30 | Max Reward: 49.20
Iteration: 938 | Episodes: 38300 | Median Reward: 44.42 | Max Reward: 49.20
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 940          |
|    time_elapsed         | 13579        |
|    total_timesteps      | 3850240      |
| train/                  |              |
|    approx_kl            | 0.0010358959 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -3.31        |
|    n_updates            | 9390         |
|    policy_gradient_loss | -0.00892     |
|    std                  | 2.15         |
|    value_loss           | 3.62         |
------------------------------------------
Iteration: 941 | Episodes: 38400 | Median Reward: 42.75 | Max Reward: 49.20
Iteration: 943 | Episodes: 38500 | Median Reward: 46.31 | Max Reward: 49.20
Iteration: 946 | Episodes: 38600 | Median Reward: 46.31 | Max Reward: 49.23
Iteration: 948 | Episodes: 38700 | Median Reward: 46.82 | Max Reward: 49.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.8        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 950          |
|    time_elapsed         | 13721        |
|    total_timesteps      | 3891200      |
| train/                  |              |
|    approx_kl            | 0.0013427045 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -3.1         |
|    n_updates            | 9490         |
|    policy_gradient_loss | -0.00816     |
|    std                  | 2.16         |
|    value_loss           | 3.99         |
------------------------------------------
Iteration: 951 | Episodes: 38800 | Median Reward: 47.08 | Max Reward: 49.23
Iteration: 953 | Episodes: 38900 | Median Reward: 45.27 | Max Reward: 49.23
Iteration: 955 | Episodes: 39000 | Median Reward: 44.15 | Max Reward: 49.23
Iteration: 958 | Episodes: 39100 | Median Reward: 46.45 | Max Reward: 49.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55          |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 960          |
|    time_elapsed         | 13866        |
|    total_timesteps      | 3932160      |
| train/                  |              |
|    approx_kl            | 0.0005039909 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.33        |
|    n_updates            | 9590         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 2.16         |
|    value_loss           | 2.25         |
------------------------------------------
Iteration: 960 | Episodes: 39200 | Median Reward: 46.89 | Max Reward: 49.23
Iteration: 963 | Episodes: 39300 | Median Reward: 45.24 | Max Reward: 49.23
Iteration: 965 | Episodes: 39400 | Median Reward: 45.01 | Max Reward: 49.23
Iteration: 968 | Episodes: 39500 | Median Reward: 46.02 | Max Reward: 49.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54          |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 970          |
|    time_elapsed         | 14010        |
|    total_timesteps      | 3973120      |
| train/                  |              |
|    approx_kl            | 0.0075422013 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.88        |
|    n_updates            | 9690         |
|    policy_gradient_loss | -0.00705     |
|    std                  | 2.18         |
|    value_loss           | 0.658        |
------------------------------------------
Iteration: 970 | Episodes: 39600 | Median Reward: 47.12 | Max Reward: 49.23
Iteration: 973 | Episodes: 39700 | Median Reward: 46.67 | Max Reward: 49.23
Iteration: 975 | Episodes: 39800 | Median Reward: 46.52 | Max Reward: 49.23
Iteration: 978 | Episodes: 39900 | Median Reward: 45.71 | Max Reward: 49.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -54.2        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 980          |
|    time_elapsed         | 14151        |
|    total_timesteps      | 4014080      |
| train/                  |              |
|    approx_kl            | 0.0024819993 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -103         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0005       |
|    loss                 | -2.25        |
|    n_updates            | 9790         |
|    policy_gradient_loss | -0.00253     |
|    std                  | 2.21         |
|    value_loss           | 5.89         |
------------------------------------------
Iteration: 980 | Episodes: 40000 | Median Reward: 46.43 | Max Reward: 49.23
Iteration: 983 | Episodes: 40100 | Median Reward: 47.57 | Max Reward: 49.23
Iteration: 985 | Episodes: 40200 | Median Reward: 47.02 | Max Reward: 49.23
Iteration: 987 | Episodes: 40300 | Median Reward: 47.51 | Max Reward: 49.23
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 100        |
|    ep_rew_mean          | -53.9      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 990        |
|    time_elapsed         | 14297      |
|    total_timesteps      | 4055040    |
| train/                  |            |
|    approx_kl            | 0.00925709 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -103       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -4.09      |
|    n_updates            | 9890       |
|    policy_gradient_loss | -0.019     |
|    std                  | 2.22       |
|    value_loss           | 1.1        |
----------------------------------------
Iteration: 990 | Episodes: 40400 | Median Reward: 47.19 | Max Reward: 49.23
Iteration: 992 | Episodes: 40500 | Median Reward: 47.21 | Max Reward: 49.23
Iteration: 995 | Episodes: 40600 | Median Reward: 45.46 | Max Reward: 49.23
Iteration: 997 | Episodes: 40700 | Median Reward: 45.64 | Max Reward: 49.23
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -54.2          |
| time/                   |                |
|    fps                  | 283            |
|    iterations           | 1000           |
|    time_elapsed         | 14443          |
|    total_timesteps      | 4096000        |
| train/                  |                |
|    approx_kl            | 0.000101146186 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -103           |
|    explained_variance   | 0.999          |
|    learning_rate        | 0.0005         |
|    loss                 | -4.93          |
|    n_updates            | 9990           |
|    policy_gradient_loss | -8.52e-05      |
|    std                  | 2.23           |
|    value_loss           | 0.958          |
--------------------------------------------
Iteration: 1000 | Episodes: 40800 | Median Reward: 47.17 | Max Reward: 49.23
Iteration: 1002 | Episodes: 40900 | Median Reward: 47.13 | Max Reward: 49.23
Iteration: 1005 | Episodes: 41000 | Median Reward: 47.24 | Max Reward: 49.23
Iteration: 1007 | Episodes: 41100 | Median Reward: 47.02 | Max Reward: 49.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.9       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1010        |
|    time_elapsed         | 14592       |
|    total_timesteps      | 4136960     |
| train/                  |             |
|    approx_kl            | 0.008587511 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -103        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -4.89       |
|    n_updates            | 10090       |
|    policy_gradient_loss | -0.0102     |
|    std                  | 2.25        |
|    value_loss           | 0.655       |
-----------------------------------------
Iteration: 1010 | Episodes: 41200 | Median Reward: 46.94 | Max Reward: 49.23
Iteration: 1012 | Episodes: 41300 | Median Reward: 46.30 | Max Reward: 49.23
Iteration: 1015 | Episodes: 41400 | Median Reward: 47.16 | Max Reward: 49.23
Iteration: 1017 | Episodes: 41500 | Median Reward: 47.17 | Max Reward: 49.23
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.1         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1020          |
|    time_elapsed         | 14742         |
|    total_timesteps      | 4177920       |
| train/                  |               |
|    approx_kl            | 0.00021995344 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.74         |
|    n_updates            | 10190         |
|    policy_gradient_loss | -0.000611     |
|    std                  | 2.26          |
|    value_loss           | 0.85          |
-------------------------------------------
Iteration: 1020 | Episodes: 41600 | Median Reward: 47.40 | Max Reward: 49.23
Iteration: 1022 | Episodes: 41700 | Median Reward: 46.08 | Max Reward: 49.23
Iteration: 1024 | Episodes: 41800 | Median Reward: 47.04 | Max Reward: 49.23
Iteration: 1027 | Episodes: 41900 | Median Reward: 46.13 | Max Reward: 49.23
Iteration: 1029 | Episodes: 42000 | Median Reward: 45.92 | Max Reward: 49.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.1        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1030         |
|    time_elapsed         | 14884        |
|    total_timesteps      | 4218880      |
| train/                  |              |
|    approx_kl            | 0.0013580613 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -103         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.99        |
|    n_updates            | 10290        |
|    policy_gradient_loss | -0.00299     |
|    std                  | 2.28         |
|    value_loss           | 0.676        |
------------------------------------------
Iteration: 1032 | Episodes: 42100 | Median Reward: 46.67 | Max Reward: 49.23
Iteration: 1034 | Episodes: 42200 | Median Reward: 46.13 | Max Reward: 49.23
Iteration: 1037 | Episodes: 42300 | Median Reward: 47.22 | Max Reward: 49.23
Iteration: 1039 | Episodes: 42400 | Median Reward: 46.44 | Max Reward: 49.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.5       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1040        |
|    time_elapsed         | 15028       |
|    total_timesteps      | 4259840     |
| train/                  |             |
|    approx_kl            | 0.008002568 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -104        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.09       |
|    n_updates            | 10390       |
|    policy_gradient_loss | -0.00862    |
|    std                  | 2.3         |
|    value_loss           | 0.368       |
-----------------------------------------
Iteration: 1042 | Episodes: 42500 | Median Reward: 47.29 | Max Reward: 49.23
Iteration: 1044 | Episodes: 42600 | Median Reward: 45.50 | Max Reward: 49.23
Iteration: 1047 | Episodes: 42700 | Median Reward: 46.94 | Max Reward: 49.23
Iteration: 1049 | Episodes: 42800 | Median Reward: 46.60 | Max Reward: 49.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.9        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1050         |
|    time_elapsed         | 15172        |
|    total_timesteps      | 4300800      |
| train/                  |              |
|    approx_kl            | 0.0032416005 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -104         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.06        |
|    n_updates            | 10490        |
|    policy_gradient_loss | -0.00422     |
|    std                  | 2.31         |
|    value_loss           | 0.414        |
------------------------------------------
Iteration: 1051 | Episodes: 42900 | Median Reward: 44.25 | Max Reward: 49.23
Iteration: 1054 | Episodes: 43000 | Median Reward: 46.48 | Max Reward: 49.23
Iteration: 1056 | Episodes: 43100 | Median Reward: 47.31 | Max Reward: 49.23
Iteration: 1059 | Episodes: 43200 | Median Reward: 45.72 | Max Reward: 49.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.3       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1060        |
|    time_elapsed         | 15314       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.011166068 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -104        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.1        |
|    n_updates            | 10590       |
|    policy_gradient_loss | -0.0165     |
|    std                  | 2.34        |
|    value_loss           | 0.273       |
-----------------------------------------
Iteration: 1061 | Episodes: 43300 | Median Reward: 46.82 | Max Reward: 49.23
Iteration: 1064 | Episodes: 43400 | Median Reward: 46.73 | Max Reward: 49.23
Iteration: 1066 | Episodes: 43500 | Median Reward: 44.05 | Max Reward: 49.23
Iteration: 1069 | Episodes: 43600 | Median Reward: 45.07 | Max Reward: 49.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55         |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1070        |
|    time_elapsed         | 15459       |
|    total_timesteps      | 4382720     |
| train/                  |             |
|    approx_kl            | 0.001595469 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -104        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -4.96       |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.00286    |
|    std                  | 2.36        |
|    value_loss           | 0.508       |
-----------------------------------------
Iteration: 1071 | Episodes: 43700 | Median Reward: 45.23 | Max Reward: 49.23
Iteration: 1074 | Episodes: 43800 | Median Reward: 47.04 | Max Reward: 49.23
Iteration: 1076 | Episodes: 43900 | Median Reward: 47.08 | Max Reward: 49.23
Iteration: 1079 | Episodes: 44000 | Median Reward: 47.01 | Max Reward: 49.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.2        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1080         |
|    time_elapsed         | 15603        |
|    total_timesteps      | 4423680      |
| train/                  |              |
|    approx_kl            | 0.0016906583 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -104         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.39        |
|    n_updates            | 10790        |
|    policy_gradient_loss | -0.00399     |
|    std                  | 2.38         |
|    value_loss           | 1.06         |
------------------------------------------
Iteration: 1081 | Episodes: 44100 | Median Reward: 47.11 | Max Reward: 49.23
Iteration: 1084 | Episodes: 44200 | Median Reward: 46.69 | Max Reward: 49.23
Iteration: 1086 | Episodes: 44300 | Median Reward: 46.30 | Max Reward: 49.23
Iteration: 1088 | Episodes: 44400 | Median Reward: 46.18 | Max Reward: 49.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.6       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1090        |
|    time_elapsed         | 15747       |
|    total_timesteps      | 4464640     |
| train/                  |             |
|    approx_kl            | 0.002105166 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -105        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -4.9        |
|    n_updates            | 10890       |
|    policy_gradient_loss | -0.00884    |
|    std                  | 2.4         |
|    value_loss           | 1.92        |
-----------------------------------------
Iteration: 1091 | Episodes: 44500 | Median Reward: 46.28 | Max Reward: 49.23
Iteration: 1093 | Episodes: 44600 | Median Reward: 46.98 | Max Reward: 49.23
Iteration: 1096 | Episodes: 44700 | Median Reward: 46.92 | Max Reward: 49.23
Iteration: 1098 | Episodes: 44800 | Median Reward: 46.97 | Max Reward: 49.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.7        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1100         |
|    time_elapsed         | 15893        |
|    total_timesteps      | 4505600      |
| train/                  |              |
|    approx_kl            | 0.0036864104 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -105         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.07        |
|    n_updates            | 10990        |
|    policy_gradient_loss | -0.00474     |
|    std                  | 2.42         |
|    value_loss           | 0.4          |
------------------------------------------
Iteration: 1101 | Episodes: 44900 | Median Reward: 45.71 | Max Reward: 49.23
Iteration: 1103 | Episodes: 45000 | Median Reward: 46.93 | Max Reward: 49.23
Iteration: 1106 | Episodes: 45100 | Median Reward: 46.30 | Max Reward: 49.23
Iteration: 1108 | Episodes: 45200 | Median Reward: 45.74 | Max Reward: 49.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -53.7        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1110         |
|    time_elapsed         | 16038        |
|    total_timesteps      | 4546560      |
| train/                  |              |
|    approx_kl            | 0.0011458376 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -105         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5           |
|    n_updates            | 11090        |
|    policy_gradient_loss | -0.00369     |
|    std                  | 2.44         |
|    value_loss           | 0.397        |
------------------------------------------
Iteration: 1111 | Episodes: 45300 | Median Reward: 46.92 | Max Reward: 49.28
Iteration: 1113 | Episodes: 45400 | Median Reward: 46.89 | Max Reward: 49.28
Iteration: 1115 | Episodes: 45500 | Median Reward: 46.48 | Max Reward: 49.28
Iteration: 1118 | Episodes: 45600 | Median Reward: 47.09 | Max Reward: 49.28
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54           |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1120          |
|    time_elapsed         | 16182         |
|    total_timesteps      | 4587520       |
| train/                  |               |
|    approx_kl            | 5.9641563e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -105          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -4.6          |
|    n_updates            | 11190         |
|    policy_gradient_loss | -6.14e-05     |
|    std                  | 2.46          |
|    value_loss           | 1.08          |
-------------------------------------------
Iteration: 1120 | Episodes: 45700 | Median Reward: 46.95 | Max Reward: 49.28
Iteration: 1123 | Episodes: 45800 | Median Reward: 47.19 | Max Reward: 49.28
Iteration: 1125 | Episodes: 45900 | Median Reward: 43.96 | Max Reward: 49.28
Iteration: 1128 | Episodes: 46000 | Median Reward: 47.18 | Max Reward: 49.28
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.2         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1130          |
|    time_elapsed         | 16325         |
|    total_timesteps      | 4628480       |
| train/                  |               |
|    approx_kl            | 0.00047487923 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -105          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.15         |
|    n_updates            | 11290         |
|    policy_gradient_loss | -0.00229      |
|    std                  | 2.46          |
|    value_loss           | 0.647         |
-------------------------------------------
Iteration: 1130 | Episodes: 46100 | Median Reward: 47.41 | Max Reward: 49.28
Iteration: 1133 | Episodes: 46200 | Median Reward: 46.97 | Max Reward: 49.28
Iteration: 1135 | Episodes: 46300 | Median Reward: 46.97 | Max Reward: 49.28
Iteration: 1138 | Episodes: 46400 | Median Reward: 47.25 | Max Reward: 49.28
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | -53.9       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1140        |
|    time_elapsed         | 16482       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.016622962 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -105        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -4.81       |
|    n_updates            | 11390       |
|    policy_gradient_loss | -0.0323     |
|    std                  | 2.47        |
|    value_loss           | 0.675       |
-----------------------------------------
Iteration: 1140 | Episodes: 46500 | Median Reward: 47.13 | Max Reward: 49.34
Iteration: 1143 | Episodes: 46600 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 1145 | Episodes: 46700 | Median Reward: 47.02 | Max Reward: 49.34
Iteration: 1147 | Episodes: 46800 | Median Reward: 46.31 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.8       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1150        |
|    time_elapsed         | 16627       |
|    total_timesteps      | 4710400     |
| train/                  |             |
|    approx_kl            | 0.006109087 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -105        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -4.82       |
|    n_updates            | 11490       |
|    policy_gradient_loss | -0.0135     |
|    std                  | 2.49        |
|    value_loss           | 0.71        |
-----------------------------------------
Iteration: 1150 | Episodes: 46900 | Median Reward: 46.31 | Max Reward: 49.34
Iteration: 1152 | Episodes: 47000 | Median Reward: 47.15 | Max Reward: 49.34
Iteration: 1155 | Episodes: 47100 | Median Reward: 47.19 | Max Reward: 49.34
Iteration: 1157 | Episodes: 47200 | Median Reward: 47.12 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.4        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1160         |
|    time_elapsed         | 16770        |
|    total_timesteps      | 4751360      |
| train/                  |              |
|    approx_kl            | 0.0024393878 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -106         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.2         |
|    n_updates            | 11590        |
|    policy_gradient_loss | -0.00523     |
|    std                  | 2.51         |
|    value_loss           | 0.231        |
------------------------------------------
Iteration: 1160 | Episodes: 47300 | Median Reward: 45.73 | Max Reward: 49.34
Iteration: 1162 | Episodes: 47400 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 1165 | Episodes: 47500 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 1167 | Episodes: 47600 | Median Reward: 47.47 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.4         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1170          |
|    time_elapsed         | 16914         |
|    total_timesteps      | 4792320       |
| train/                  |               |
|    approx_kl            | 0.00014474119 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -106          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.74         |
|    n_updates            | 11690         |
|    policy_gradient_loss | -0.000827     |
|    std                  | 2.53          |
|    value_loss           | 0.981         |
-------------------------------------------
Iteration: 1170 | Episodes: 47700 | Median Reward: 45.67 | Max Reward: 49.34
Iteration: 1172 | Episodes: 47800 | Median Reward: 46.71 | Max Reward: 49.34
Iteration: 1175 | Episodes: 47900 | Median Reward: 46.43 | Max Reward: 49.34
Iteration: 1177 | Episodes: 48000 | Median Reward: 46.42 | Max Reward: 49.34
Iteration: 1179 | Episodes: 48100 | Median Reward: 47.05 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.3       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1180        |
|    time_elapsed         | 17058       |
|    total_timesteps      | 4833280     |
| train/                  |             |
|    approx_kl            | 0.005943655 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -106        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -4.86       |
|    n_updates            | 11790       |
|    policy_gradient_loss | -0.00943    |
|    std                  | 2.53        |
|    value_loss           | 0.278       |
-----------------------------------------
Iteration: 1182 | Episodes: 48200 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 1184 | Episodes: 48300 | Median Reward: 46.27 | Max Reward: 49.34
Iteration: 1187 | Episodes: 48400 | Median Reward: 47.48 | Max Reward: 49.34
Iteration: 1189 | Episodes: 48500 | Median Reward: 47.01 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.2         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1190          |
|    time_elapsed         | 17204         |
|    total_timesteps      | 4874240       |
| train/                  |               |
|    approx_kl            | 0.00012518757 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -106          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -4.54         |
|    n_updates            | 11890         |
|    policy_gradient_loss | -0.000468     |
|    std                  | 2.55          |
|    value_loss           | 1.02          |
-------------------------------------------
Iteration: 1192 | Episodes: 48600 | Median Reward: 46.61 | Max Reward: 49.34
Iteration: 1194 | Episodes: 48700 | Median Reward: 47.17 | Max Reward: 49.34
Iteration: 1197 | Episodes: 48800 | Median Reward: 47.53 | Max Reward: 49.34
Iteration: 1199 | Episodes: 48900 | Median Reward: 47.42 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.6         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1200          |
|    time_elapsed         | 17348         |
|    total_timesteps      | 4915200       |
| train/                  |               |
|    approx_kl            | 3.9843406e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -106          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.01         |
|    n_updates            | 11990         |
|    policy_gradient_loss | -0.000122     |
|    std                  | 2.58          |
|    value_loss           | 0.359         |
-------------------------------------------
Iteration: 1202 | Episodes: 49000 | Median Reward: 47.51 | Max Reward: 49.34
Iteration: 1204 | Episodes: 49100 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 1207 | Episodes: 49200 | Median Reward: 47.54 | Max Reward: 49.34
Iteration: 1209 | Episodes: 49300 | Median Reward: 47.01 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.2        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1210         |
|    time_elapsed         | 17491        |
|    total_timesteps      | 4956160      |
| train/                  |              |
|    approx_kl            | 0.0003238776 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -106         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -4.9         |
|    n_updates            | 12090        |
|    policy_gradient_loss | -0.000365    |
|    std                  | 2.59         |
|    value_loss           | 0.835        |
------------------------------------------
Iteration: 1212 | Episodes: 49400 | Median Reward: 47.14 | Max Reward: 49.34
Iteration: 1214 | Episodes: 49500 | Median Reward: 47.46 | Max Reward: 49.34
Iteration: 1216 | Episodes: 49600 | Median Reward: 47.31 | Max Reward: 49.34
Iteration: 1219 | Episodes: 49700 | Median Reward: 46.95 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.9         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1220          |
|    time_elapsed         | 17636         |
|    total_timesteps      | 4997120       |
| train/                  |               |
|    approx_kl            | 0.00035193382 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -106          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -4.97         |
|    n_updates            | 12190         |
|    policy_gradient_loss | -0.00101      |
|    std                  | 2.6           |
|    value_loss           | 0.603         |
-------------------------------------------
Iteration: 1221 | Episodes: 49800 | Median Reward: 46.40 | Max Reward: 49.34
Iteration: 1224 | Episodes: 49900 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 1226 | Episodes: 50000 | Median Reward: 47.10 | Max Reward: 49.34
Iteration: 1229 | Episodes: 50100 | Median Reward: 43.77 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.1        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1230         |
|    time_elapsed         | 17780        |
|    total_timesteps      | 5038080      |
| train/                  |              |
|    approx_kl            | 0.0010822555 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -106         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.07        |
|    n_updates            | 12290        |
|    policy_gradient_loss | -0.00269     |
|    std                  | 2.62         |
|    value_loss           | 0.778        |
------------------------------------------
Iteration: 1231 | Episodes: 50200 | Median Reward: 47.04 | Max Reward: 49.34
Iteration: 1234 | Episodes: 50300 | Median Reward: 46.77 | Max Reward: 49.34
Iteration: 1236 | Episodes: 50400 | Median Reward: 46.32 | Max Reward: 49.34
Iteration: 1239 | Episodes: 50500 | Median Reward: 47.25 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.4        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1240         |
|    time_elapsed         | 17925        |
|    total_timesteps      | 5079040      |
| train/                  |              |
|    approx_kl            | 0.0031379377 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -107         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.13        |
|    n_updates            | 12390        |
|    policy_gradient_loss | -0.00729     |
|    std                  | 2.64         |
|    value_loss           | 0.424        |
------------------------------------------
Iteration: 1241 | Episodes: 50600 | Median Reward: 47.36 | Max Reward: 49.34
Iteration: 1244 | Episodes: 50700 | Median Reward: 46.84 | Max Reward: 49.34
Iteration: 1246 | Episodes: 50800 | Median Reward: 47.08 | Max Reward: 49.34
Iteration: 1248 | Episodes: 50900 | Median Reward: 46.93 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54           |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1250          |
|    time_elapsed         | 18070         |
|    total_timesteps      | 5120000       |
| train/                  |               |
|    approx_kl            | 0.00043684265 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -107          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -4.19         |
|    n_updates            | 12490         |
|    policy_gradient_loss | 5.9e-05       |
|    std                  | 2.65          |
|    value_loss           | 1.07          |
-------------------------------------------
Iteration: 1251 | Episodes: 51000 | Median Reward: 46.40 | Max Reward: 49.34
Iteration: 1253 | Episodes: 51100 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 1256 | Episodes: 51200 | Median Reward: 47.12 | Max Reward: 49.34
Iteration: 1258 | Episodes: 51300 | Median Reward: 47.29 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.6       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1260        |
|    time_elapsed         | 18218       |
|    total_timesteps      | 5160960     |
| train/                  |             |
|    approx_kl            | 0.014039366 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -107        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.14       |
|    n_updates            | 12590       |
|    policy_gradient_loss | -0.0144     |
|    std                  | 2.67        |
|    value_loss           | 0.304       |
-----------------------------------------
Iteration: 1261 | Episodes: 51400 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 1263 | Episodes: 51500 | Median Reward: 47.00 | Max Reward: 49.34
Iteration: 1266 | Episodes: 51600 | Median Reward: 47.36 | Max Reward: 49.34
Iteration: 1268 | Episodes: 51700 | Median Reward: 46.17 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.5        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1270         |
|    time_elapsed         | 18371        |
|    total_timesteps      | 5201920      |
| train/                  |              |
|    approx_kl            | 0.0010778068 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -107         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.89        |
|    n_updates            | 12690        |
|    policy_gradient_loss | -0.00279     |
|    std                  | 2.69         |
|    value_loss           | 0.509        |
------------------------------------------
Iteration: 1271 | Episodes: 51800 | Median Reward: 46.77 | Max Reward: 49.34
Iteration: 1273 | Episodes: 51900 | Median Reward: 47.10 | Max Reward: 49.34
Iteration: 1276 | Episodes: 52000 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 1278 | Episodes: 52100 | Median Reward: 47.46 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.5         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1280          |
|    time_elapsed         | 18516         |
|    total_timesteps      | 5242880       |
| train/                  |               |
|    approx_kl            | 0.00035489388 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -107          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.52         |
|    n_updates            | 12790         |
|    policy_gradient_loss | -0.00221      |
|    std                  | 2.7           |
|    value_loss           | 2.72          |
-------------------------------------------
Iteration: 1280 | Episodes: 52200 | Median Reward: 47.24 | Max Reward: 49.34
Iteration: 1283 | Episodes: 52300 | Median Reward: 47.37 | Max Reward: 49.34
Iteration: 1285 | Episodes: 52400 | Median Reward: 47.36 | Max Reward: 49.34
Iteration: 1288 | Episodes: 52500 | Median Reward: 47.29 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54          |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1290         |
|    time_elapsed         | 18659        |
|    total_timesteps      | 5283840      |
| train/                  |              |
|    approx_kl            | 0.0022780101 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -107         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.31        |
|    n_updates            | 12890        |
|    policy_gradient_loss | -0.00236     |
|    std                  | 2.72         |
|    value_loss           | 0.149        |
------------------------------------------
Iteration: 1290 | Episodes: 52600 | Median Reward: 47.33 | Max Reward: 49.34
Iteration: 1293 | Episodes: 52700 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 1295 | Episodes: 52800 | Median Reward: 47.59 | Max Reward: 49.34
Iteration: 1298 | Episodes: 52900 | Median Reward: 47.22 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.5       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1300        |
|    time_elapsed         | 18805       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.003712599 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -107        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.31       |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.00818    |
|    std                  | 2.75        |
|    value_loss           | 0.359       |
-----------------------------------------
Iteration: 1300 | Episodes: 53000 | Median Reward: 47.54 | Max Reward: 49.34
Iteration: 1303 | Episodes: 53100 | Median Reward: 47.02 | Max Reward: 49.34
Iteration: 1305 | Episodes: 53200 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 1308 | Episodes: 53300 | Median Reward: 47.55 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99.8          |
|    ep_rew_mean          | -52           |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1310          |
|    time_elapsed         | 18948         |
|    total_timesteps      | 5365760       |
| train/                  |               |
|    approx_kl            | 0.00012752665 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -108          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.35         |
|    n_updates            | 13090         |
|    policy_gradient_loss | -0.000444     |
|    std                  | 2.78          |
|    value_loss           | 2.03          |
-------------------------------------------
Iteration: 1310 | Episodes: 53400 | Median Reward: 47.51 | Max Reward: 49.34
Iteration: 1312 | Episodes: 53500 | Median Reward: 47.54 | Max Reward: 49.34
Iteration: 1315 | Episodes: 53600 | Median Reward: 46.82 | Max Reward: 49.34
Iteration: 1317 | Episodes: 53700 | Median Reward: 47.46 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.1         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1320          |
|    time_elapsed         | 19089         |
|    total_timesteps      | 5406720       |
| train/                  |               |
|    approx_kl            | 0.00053068623 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -108          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.08         |
|    n_updates            | 13190         |
|    policy_gradient_loss | -0.00122      |
|    std                  | 2.8           |
|    value_loss           | 2.71          |
-------------------------------------------
Iteration: 1320 | Episodes: 53800 | Median Reward: 47.18 | Max Reward: 49.34
Iteration: 1322 | Episodes: 53900 | Median Reward: 47.70 | Max Reward: 49.34
Iteration: 1325 | Episodes: 54000 | Median Reward: 47.53 | Max Reward: 49.34
Iteration: 1327 | Episodes: 54100 | Median Reward: 47.59 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.3         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1330          |
|    time_elapsed         | 19234         |
|    total_timesteps      | 5447680       |
| train/                  |               |
|    approx_kl            | 6.5636996e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -108          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.23         |
|    n_updates            | 13290         |
|    policy_gradient_loss | 0.000364      |
|    std                  | 2.81          |
|    value_loss           | 0.554         |
-------------------------------------------
Iteration: 1330 | Episodes: 54200 | Median Reward: 47.07 | Max Reward: 49.34
Iteration: 1332 | Episodes: 54300 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 1335 | Episodes: 54400 | Median Reward: 47.00 | Max Reward: 49.34
Iteration: 1337 | Episodes: 54500 | Median Reward: 46.76 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.3       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1340        |
|    time_elapsed         | 19377       |
|    total_timesteps      | 5488640     |
| train/                  |             |
|    approx_kl            | 0.008098319 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -108        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.35       |
|    n_updates            | 13390       |
|    policy_gradient_loss | -0.014      |
|    std                  | 2.85        |
|    value_loss           | 0.599       |
-----------------------------------------
Iteration: 1340 | Episodes: 54600 | Median Reward: 46.70 | Max Reward: 49.34
Iteration: 1342 | Episodes: 54700 | Median Reward: 47.38 | Max Reward: 49.34
Iteration: 1344 | Episodes: 54800 | Median Reward: 47.53 | Max Reward: 49.34
Iteration: 1347 | Episodes: 54900 | Median Reward: 47.05 | Max Reward: 49.34
Iteration: 1349 | Episodes: 55000 | Median Reward: 46.90 | Max Reward: 49.34
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -54.9      |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 1350       |
|    time_elapsed         | 19523      |
|    total_timesteps      | 5529600    |
| train/                  |            |
|    approx_kl            | 0.06608581 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -5.18      |
|    n_updates            | 13490      |
|    policy_gradient_loss | -0.0549    |
|    std                  | 2.86       |
|    value_loss           | 0.807      |
----------------------------------------
Iteration: 1352 | Episodes: 55100 | Median Reward: 46.61 | Max Reward: 49.34
Iteration: 1354 | Episodes: 55200 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 1357 | Episodes: 55300 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 1359 | Episodes: 55400 | Median Reward: 46.72 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1360         |
|    time_elapsed         | 19667        |
|    total_timesteps      | 5570560      |
| train/                  |              |
|    approx_kl            | 0.0011138401 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -109         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.36        |
|    n_updates            | 13590        |
|    policy_gradient_loss | -0.000352    |
|    std                  | 2.89         |
|    value_loss           | 0.378        |
------------------------------------------
Iteration: 1362 | Episodes: 55500 | Median Reward: 46.86 | Max Reward: 49.34
Iteration: 1364 | Episodes: 55600 | Median Reward: 47.18 | Max Reward: 49.34
Iteration: 1367 | Episodes: 55700 | Median Reward: 47.13 | Max Reward: 49.34
Iteration: 1369 | Episodes: 55800 | Median Reward: 47.25 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.9         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1370          |
|    time_elapsed         | 19812         |
|    total_timesteps      | 5611520       |
| train/                  |               |
|    approx_kl            | 0.00039547012 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -109          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -4.67         |
|    n_updates            | 13690         |
|    policy_gradient_loss | -0.000388     |
|    std                  | 2.89          |
|    value_loss           | 0.698         |
-------------------------------------------
Iteration: 1372 | Episodes: 55900 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 1374 | Episodes: 56000 | Median Reward: 46.29 | Max Reward: 49.34
Iteration: 1376 | Episodes: 56100 | Median Reward: 46.31 | Max Reward: 49.34
Iteration: 1379 | Episodes: 56200 | Median Reward: 46.97 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54         |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1380        |
|    time_elapsed         | 19957       |
|    total_timesteps      | 5652480     |
| train/                  |             |
|    approx_kl            | 0.010204802 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -109        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -4.95       |
|    n_updates            | 13790       |
|    policy_gradient_loss | -0.0212     |
|    std                  | 2.92        |
|    value_loss           | 0.533       |
-----------------------------------------
Iteration: 1381 | Episodes: 56300 | Median Reward: 47.11 | Max Reward: 49.34
Iteration: 1384 | Episodes: 56400 | Median Reward: 47.64 | Max Reward: 49.34
Iteration: 1386 | Episodes: 56500 | Median Reward: 47.82 | Max Reward: 49.34
Iteration: 1389 | Episodes: 56600 | Median Reward: 47.24 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.6         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1390          |
|    time_elapsed         | 20107         |
|    total_timesteps      | 5693440       |
| train/                  |               |
|    approx_kl            | 0.00016352275 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -109          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.34         |
|    n_updates            | 13890         |
|    policy_gradient_loss | -0.000722     |
|    std                  | 2.94          |
|    value_loss           | 0.242         |
-------------------------------------------
Iteration: 1391 | Episodes: 56700 | Median Reward: 47.32 | Max Reward: 49.34
Iteration: 1394 | Episodes: 56800 | Median Reward: 47.46 | Max Reward: 49.34
Iteration: 1396 | Episodes: 56900 | Median Reward: 47.14 | Max Reward: 49.34
Iteration: 1399 | Episodes: 57000 | Median Reward: 47.28 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.3       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1400        |
|    time_elapsed         | 20262       |
|    total_timesteps      | 5734400     |
| train/                  |             |
|    approx_kl            | 0.004715382 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -109        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.34       |
|    n_updates            | 13990       |
|    policy_gradient_loss | -0.0097     |
|    std                  | 2.96        |
|    value_loss           | 0.278       |
-----------------------------------------
Iteration: 1401 | Episodes: 57100 | Median Reward: 47.13 | Max Reward: 49.34
Iteration: 1404 | Episodes: 57200 | Median Reward: 47.54 | Max Reward: 49.34
Iteration: 1406 | Episodes: 57300 | Median Reward: 46.65 | Max Reward: 49.34
Iteration: 1408 | Episodes: 57400 | Median Reward: 46.91 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.9         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1410          |
|    time_elapsed         | 20405         |
|    total_timesteps      | 5775360       |
| train/                  |               |
|    approx_kl            | 5.7784535e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -109          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.38         |
|    n_updates            | 14090         |
|    policy_gradient_loss | -1.09e-05     |
|    std                  | 2.98          |
|    value_loss           | 0.658         |
-------------------------------------------
Iteration: 1411 | Episodes: 57500 | Median Reward: 47.14 | Max Reward: 49.34
Iteration: 1413 | Episodes: 57600 | Median Reward: 46.59 | Max Reward: 49.34
Iteration: 1416 | Episodes: 57700 | Median Reward: 47.48 | Max Reward: 49.34
Iteration: 1418 | Episodes: 57800 | Median Reward: 46.30 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.6       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1420        |
|    time_elapsed         | 20550       |
|    total_timesteps      | 5816320     |
| train/                  |             |
|    approx_kl            | 0.003427669 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -109        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.29       |
|    n_updates            | 14190       |
|    policy_gradient_loss | -0.00847    |
|    std                  | 3.01        |
|    value_loss           | 0.654       |
-----------------------------------------
Iteration: 1421 | Episodes: 57900 | Median Reward: 47.31 | Max Reward: 49.34
Iteration: 1423 | Episodes: 58000 | Median Reward: 47.56 | Max Reward: 49.34
Iteration: 1426 | Episodes: 58100 | Median Reward: 47.56 | Max Reward: 49.34
Iteration: 1428 | Episodes: 58200 | Median Reward: 47.53 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.8        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1430         |
|    time_elapsed         | 20695        |
|    total_timesteps      | 5857280      |
| train/                  |              |
|    approx_kl            | 0.0009825799 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -110         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.13        |
|    n_updates            | 14290        |
|    policy_gradient_loss | -0.0031      |
|    std                  | 3.04         |
|    value_loss           | 0.532        |
------------------------------------------
Iteration: 1431 | Episodes: 58300 | Median Reward: 47.48 | Max Reward: 49.34
Iteration: 1433 | Episodes: 58400 | Median Reward: 47.32 | Max Reward: 49.34
Iteration: 1436 | Episodes: 58500 | Median Reward: 47.40 | Max Reward: 49.34
Iteration: 1438 | Episodes: 58600 | Median Reward: 45.22 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.4        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1440         |
|    time_elapsed         | 20838        |
|    total_timesteps      | 5898240      |
| train/                  |              |
|    approx_kl            | 0.0029524807 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -110         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.34        |
|    n_updates            | 14390        |
|    policy_gradient_loss | -0.00518     |
|    std                  | 3.05         |
|    value_loss           | 0.346        |
------------------------------------------
Iteration: 1440 | Episodes: 58700 | Median Reward: 47.36 | Max Reward: 49.34
Iteration: 1443 | Episodes: 58800 | Median Reward: 47.06 | Max Reward: 49.34
Iteration: 1445 | Episodes: 58900 | Median Reward: 47.39 | Max Reward: 49.34
Iteration: 1448 | Episodes: 59000 | Median Reward: 47.29 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.9       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1450        |
|    time_elapsed         | 20982       |
|    total_timesteps      | 5939200     |
| train/                  |             |
|    approx_kl            | 0.028973684 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -110        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.35       |
|    n_updates            | 14490       |
|    policy_gradient_loss | -0.0503     |
|    std                  | 3.07        |
|    value_loss           | 0.576       |
-----------------------------------------
Iteration: 1450 | Episodes: 59100 | Median Reward: 47.00 | Max Reward: 49.34
Iteration: 1453 | Episodes: 59200 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 1455 | Episodes: 59300 | Median Reward: 47.38 | Max Reward: 49.34
Iteration: 1458 | Episodes: 59400 | Median Reward: 46.05 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.2        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1460         |
|    time_elapsed         | 21128        |
|    total_timesteps      | 5980160      |
| train/                  |              |
|    approx_kl            | 0.0025736985 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -110         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.33        |
|    n_updates            | 14590        |
|    policy_gradient_loss | -0.00677     |
|    std                  | 3.09         |
|    value_loss           | 0.422        |
------------------------------------------
Iteration: 1460 | Episodes: 59500 | Median Reward: 46.30 | Max Reward: 49.34
Iteration: 1463 | Episodes: 59600 | Median Reward: 47.57 | Max Reward: 49.34
Iteration: 1465 | Episodes: 59700 | Median Reward: 47.64 | Max Reward: 49.34
Iteration: 1468 | Episodes: 59800 | Median Reward: 47.29 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.9        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1470         |
|    time_elapsed         | 21272        |
|    total_timesteps      | 6021120      |
| train/                  |              |
|    approx_kl            | 0.0025920202 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -110         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.98        |
|    n_updates            | 14690        |
|    policy_gradient_loss | -0.00431     |
|    std                  | 3.1          |
|    value_loss           | 0.658        |
------------------------------------------
Iteration: 1470 | Episodes: 59900 | Median Reward: 46.67 | Max Reward: 49.34
Iteration: 1472 | Episodes: 60000 | Median Reward: 46.76 | Max Reward: 49.34
Iteration: 1475 | Episodes: 60100 | Median Reward: 46.97 | Max Reward: 49.34
Iteration: 1477 | Episodes: 60200 | Median Reward: 47.11 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.8         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1480          |
|    time_elapsed         | 21416         |
|    total_timesteps      | 6062080       |
| train/                  |               |
|    approx_kl            | 1.3761892e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -110          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.4          |
|    n_updates            | 14790         |
|    policy_gradient_loss | 3.09e-05      |
|    std                  | 3.13          |
|    value_loss           | 0.279         |
-------------------------------------------
Iteration: 1480 | Episodes: 60300 | Median Reward: 47.52 | Max Reward: 49.34
Iteration: 1482 | Episodes: 60400 | Median Reward: 45.67 | Max Reward: 49.34
Iteration: 1485 | Episodes: 60500 | Median Reward: 47.17 | Max Reward: 49.34
Iteration: 1487 | Episodes: 60600 | Median Reward: 47.22 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.6        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1490         |
|    time_elapsed         | 21561        |
|    total_timesteps      | 6103040      |
| train/                  |              |
|    approx_kl            | 0.0015749073 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -110         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -4.47        |
|    n_updates            | 14890        |
|    policy_gradient_loss | -0.00397     |
|    std                  | 3.14         |
|    value_loss           | 0.92         |
------------------------------------------
Iteration: 1490 | Episodes: 60700 | Median Reward: 46.09 | Max Reward: 49.34
Iteration: 1492 | Episodes: 60800 | Median Reward: 47.11 | Max Reward: 49.34
Iteration: 1495 | Episodes: 60900 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 1497 | Episodes: 61000 | Median Reward: 47.13 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1500         |
|    time_elapsed         | 21706        |
|    total_timesteps      | 6144000      |
| train/                  |              |
|    approx_kl            | 0.0018215302 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.34        |
|    n_updates            | 14990        |
|    policy_gradient_loss | -0.00203     |
|    std                  | 3.17         |
|    value_loss           | 0.365        |
------------------------------------------
Iteration: 1500 | Episodes: 61100 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 1502 | Episodes: 61200 | Median Reward: 47.07 | Max Reward: 49.34
Iteration: 1504 | Episodes: 61300 | Median Reward: 47.37 | Max Reward: 49.34
Iteration: 1507 | Episodes: 61400 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 1509 | Episodes: 61500 | Median Reward: 46.44 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1510         |
|    time_elapsed         | 21848        |
|    total_timesteps      | 6184960      |
| train/                  |              |
|    approx_kl            | 0.0029744143 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.47        |
|    n_updates            | 15090        |
|    policy_gradient_loss | -0.0073      |
|    std                  | 3.2          |
|    value_loss           | 0.222        |
------------------------------------------
Iteration: 1512 | Episodes: 61600 | Median Reward: 47.33 | Max Reward: 49.34
Iteration: 1514 | Episodes: 61700 | Median Reward: 47.26 | Max Reward: 49.34
Iteration: 1517 | Episodes: 61800 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 1519 | Episodes: 61900 | Median Reward: 46.33 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54         |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1520        |
|    time_elapsed         | 21993       |
|    total_timesteps      | 6225920     |
| train/                  |             |
|    approx_kl            | 0.002104558 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -111        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.24       |
|    n_updates            | 15190       |
|    policy_gradient_loss | -0.00568    |
|    std                  | 3.23        |
|    value_loss           | 0.813       |
-----------------------------------------
Iteration: 1522 | Episodes: 62000 | Median Reward: 46.07 | Max Reward: 49.34
Iteration: 1524 | Episodes: 62100 | Median Reward: 47.11 | Max Reward: 49.34
Iteration: 1527 | Episodes: 62200 | Median Reward: 46.75 | Max Reward: 49.34
Iteration: 1529 | Episodes: 62300 | Median Reward: 46.91 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.1       |
| time/                   |             |
|    fps                  | 282         |
|    iterations           | 1530        |
|    time_elapsed         | 22149       |
|    total_timesteps      | 6266880     |
| train/                  |             |
|    approx_kl            | 0.026345536 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -111        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.51       |
|    n_updates            | 15290       |
|    policy_gradient_loss | -0.0379     |
|    std                  | 3.28        |
|    value_loss           | 0.321       |
-----------------------------------------
Iteration: 1532 | Episodes: 62400 | Median Reward: 46.59 | Max Reward: 49.34
Iteration: 1534 | Episodes: 62500 | Median Reward: 46.39 | Max Reward: 49.34
Iteration: 1537 | Episodes: 62600 | Median Reward: 46.98 | Max Reward: 49.34
Iteration: 1539 | Episodes: 62700 | Median Reward: 47.17 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.9         |
| time/                   |               |
|    fps                  | 282           |
|    iterations           | 1540          |
|    time_elapsed         | 22293         |
|    total_timesteps      | 6307840       |
| train/                  |               |
|    approx_kl            | 3.8466183e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.43         |
|    n_updates            | 15390         |
|    policy_gradient_loss | -0.000176     |
|    std                  | 3.3           |
|    value_loss           | 0.59          |
-------------------------------------------
Iteration: 1541 | Episodes: 62800 | Median Reward: 46.49 | Max Reward: 49.34
Iteration: 1544 | Episodes: 62900 | Median Reward: 46.27 | Max Reward: 49.34
Iteration: 1546 | Episodes: 63000 | Median Reward: 47.64 | Max Reward: 49.34
Iteration: 1549 | Episodes: 63100 | Median Reward: 46.86 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54           |
| time/                   |               |
|    fps                  | 282           |
|    iterations           | 1550          |
|    time_elapsed         | 22436         |
|    total_timesteps      | 6348800       |
| train/                  |               |
|    approx_kl            | 0.00064540014 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -4.47         |
|    n_updates            | 15490         |
|    policy_gradient_loss | -0.00215      |
|    std                  | 3.31          |
|    value_loss           | 1.3           |
-------------------------------------------
Iteration: 1551 | Episodes: 63200 | Median Reward: 47.36 | Max Reward: 49.34
Iteration: 1554 | Episodes: 63300 | Median Reward: 46.34 | Max Reward: 49.34
Iteration: 1556 | Episodes: 63400 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 1559 | Episodes: 63500 | Median Reward: 46.95 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.6        |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 1560         |
|    time_elapsed         | 22579        |
|    total_timesteps      | 6389760      |
| train/                  |              |
|    approx_kl            | 0.0088896565 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.55        |
|    n_updates            | 15590        |
|    policy_gradient_loss | -0.0185      |
|    std                  | 3.34         |
|    value_loss           | 0.185        |
------------------------------------------
Iteration: 1561 | Episodes: 63600 | Median Reward: 46.70 | Max Reward: 49.34
Iteration: 1564 | Episodes: 63700 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 1566 | Episodes: 63800 | Median Reward: 47.08 | Max Reward: 49.34
Iteration: 1569 | Episodes: 63900 | Median Reward: 46.99 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54          |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1570         |
|    time_elapsed         | 22721        |
|    total_timesteps      | 6430720      |
| train/                  |              |
|    approx_kl            | 0.0042197895 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.15        |
|    n_updates            | 15690        |
|    policy_gradient_loss | -0.0111      |
|    std                  | 3.36         |
|    value_loss           | 0.406        |
------------------------------------------
Iteration: 1571 | Episodes: 64000 | Median Reward: 46.36 | Max Reward: 49.34
Iteration: 1573 | Episodes: 64100 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 1576 | Episodes: 64200 | Median Reward: 46.14 | Max Reward: 49.34
Iteration: 1578 | Episodes: 64300 | Median Reward: 47.42 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.5        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1580         |
|    time_elapsed         | 22863        |
|    total_timesteps      | 6471680      |
| train/                  |              |
|    approx_kl            | 0.0052088536 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.58        |
|    n_updates            | 15790        |
|    policy_gradient_loss | -0.0107      |
|    std                  | 3.39         |
|    value_loss           | 0.333        |
------------------------------------------
Iteration: 1581 | Episodes: 64400 | Median Reward: 46.19 | Max Reward: 49.34
Iteration: 1583 | Episodes: 64500 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 1586 | Episodes: 64600 | Median Reward: 46.50 | Max Reward: 49.34
Iteration: 1588 | Episodes: 64700 | Median Reward: 47.29 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1590         |
|    time_elapsed         | 23008        |
|    total_timesteps      | 6512640      |
| train/                  |              |
|    approx_kl            | 0.0031239395 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.47        |
|    n_updates            | 15890        |
|    policy_gradient_loss | -0.00566     |
|    std                  | 3.42         |
|    value_loss           | 0.361        |
------------------------------------------
Iteration: 1591 | Episodes: 64800 | Median Reward: 46.82 | Max Reward: 49.34
Iteration: 1593 | Episodes: 64900 | Median Reward: 47.10 | Max Reward: 49.34
Iteration: 1596 | Episodes: 65000 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 1598 | Episodes: 65100 | Median Reward: 47.47 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.2        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1600         |
|    time_elapsed         | 23152        |
|    total_timesteps      | 6553600      |
| train/                  |              |
|    approx_kl            | 0.0013557556 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.47        |
|    n_updates            | 15990        |
|    policy_gradient_loss | -0.00414     |
|    std                  | 3.45         |
|    value_loss           | 0.413        |
------------------------------------------
Iteration: 1601 | Episodes: 65200 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 1603 | Episodes: 65300 | Median Reward: 46.70 | Max Reward: 49.34
Iteration: 1605 | Episodes: 65400 | Median Reward: 47.38 | Max Reward: 49.34
Iteration: 1608 | Episodes: 65500 | Median Reward: 47.48 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.6        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1610         |
|    time_elapsed         | 23295        |
|    total_timesteps      | 6594560      |
| train/                  |              |
|    approx_kl            | 8.309234e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -113         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.53        |
|    n_updates            | 16090        |
|    policy_gradient_loss | -0.000173    |
|    std                  | 3.48         |
|    value_loss           | 0.476        |
------------------------------------------
Iteration: 1610 | Episodes: 65600 | Median Reward: 47.20 | Max Reward: 49.34
Iteration: 1613 | Episodes: 65700 | Median Reward: 47.13 | Max Reward: 49.34
Iteration: 1615 | Episodes: 65800 | Median Reward: 47.71 | Max Reward: 49.34
Iteration: 1618 | Episodes: 65900 | Median Reward: 46.75 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.9       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1620        |
|    time_elapsed         | 23440       |
|    total_timesteps      | 6635520     |
| train/                  |             |
|    approx_kl            | 0.003929366 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -113        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.52       |
|    n_updates            | 16190       |
|    policy_gradient_loss | -0.00873    |
|    std                  | 3.51        |
|    value_loss           | 0.333       |
-----------------------------------------
Iteration: 1620 | Episodes: 66000 | Median Reward: 46.40 | Max Reward: 49.34
Iteration: 1623 | Episodes: 66100 | Median Reward: 47.76 | Max Reward: 49.34
Iteration: 1625 | Episodes: 66200 | Median Reward: 46.66 | Max Reward: 49.34
Iteration: 1628 | Episodes: 66300 | Median Reward: 46.73 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.1         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1630          |
|    time_elapsed         | 23583         |
|    total_timesteps      | 6676480       |
| train/                  |               |
|    approx_kl            | 0.00011847717 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.54         |
|    n_updates            | 16290         |
|    policy_gradient_loss | -0.000215     |
|    std                  | 3.54          |
|    value_loss           | 0.179         |
-------------------------------------------
Iteration: 1630 | Episodes: 66400 | Median Reward: 47.02 | Max Reward: 49.34
Iteration: 1633 | Episodes: 66500 | Median Reward: 45.68 | Max Reward: 49.34
Iteration: 1635 | Episodes: 66600 | Median Reward: 47.16 | Max Reward: 49.34
Iteration: 1637 | Episodes: 66700 | Median Reward: 46.95 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.2        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1640         |
|    time_elapsed         | 23724        |
|    total_timesteps      | 6717440      |
| train/                  |              |
|    approx_kl            | 0.0014901762 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -113         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.3         |
|    n_updates            | 16390        |
|    policy_gradient_loss | -0.00502     |
|    std                  | 3.56         |
|    value_loss           | 0.609        |
------------------------------------------
Iteration: 1640 | Episodes: 66800 | Median Reward: 44.32 | Max Reward: 49.34
Iteration: 1642 | Episodes: 66900 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 1645 | Episodes: 67000 | Median Reward: 46.96 | Max Reward: 49.34
Iteration: 1647 | Episodes: 67100 | Median Reward: 47.51 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.6         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1650          |
|    time_elapsed         | 23867         |
|    total_timesteps      | 6758400       |
| train/                  |               |
|    approx_kl            | 0.00048921455 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.6          |
|    n_updates            | 16490         |
|    policy_gradient_loss | -0.00081      |
|    std                  | 3.59          |
|    value_loss           | 0.248         |
-------------------------------------------
Iteration: 1650 | Episodes: 67200 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 1652 | Episodes: 67300 | Median Reward: 47.12 | Max Reward: 49.34
Iteration: 1655 | Episodes: 67400 | Median Reward: 46.39 | Max Reward: 49.34
Iteration: 1657 | Episodes: 67500 | Median Reward: 46.31 | Max Reward: 49.34
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -53.7          |
| time/                   |                |
|    fps                  | 283            |
|    iterations           | 1660           |
|    time_elapsed         | 24009          |
|    total_timesteps      | 6799360        |
| train/                  |                |
|    approx_kl            | 0.000102297025 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -114           |
|    explained_variance   | 1              |
|    learning_rate        | 0.0005         |
|    loss                 | -5.6           |
|    n_updates            | 16590          |
|    policy_gradient_loss | 0.000216       |
|    std                  | 3.62           |
|    value_loss           | 0.447          |
--------------------------------------------
Iteration: 1660 | Episodes: 67600 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 1662 | Episodes: 67700 | Median Reward: 47.49 | Max Reward: 49.34
Iteration: 1665 | Episodes: 67800 | Median Reward: 47.33 | Max Reward: 49.34
Iteration: 1667 | Episodes: 67900 | Median Reward: 47.54 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.7        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1670         |
|    time_elapsed         | 24152        |
|    total_timesteps      | 6840320      |
| train/                  |              |
|    approx_kl            | 0.0001603677 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -114         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.65        |
|    n_updates            | 16690        |
|    policy_gradient_loss | -0.000294    |
|    std                  | 3.66         |
|    value_loss           | 0.22         |
------------------------------------------
Iteration: 1670 | Episodes: 68000 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 1672 | Episodes: 68100 | Median Reward: 46.10 | Max Reward: 49.34
Iteration: 1674 | Episodes: 68200 | Median Reward: 47.29 | Max Reward: 49.34
Iteration: 1677 | Episodes: 68300 | Median Reward: 47.36 | Max Reward: 49.34
Iteration: 1679 | Episodes: 68400 | Median Reward: 47.54 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1680         |
|    time_elapsed         | 24301        |
|    total_timesteps      | 6881280      |
| train/                  |              |
|    approx_kl            | 0.0016062664 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -114         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.32        |
|    n_updates            | 16790        |
|    policy_gradient_loss | -0.00268     |
|    std                  | 3.69         |
|    value_loss           | 0.356        |
------------------------------------------
Iteration: 1682 | Episodes: 68500 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 1684 | Episodes: 68600 | Median Reward: 46.82 | Max Reward: 49.34
Iteration: 1687 | Episodes: 68700 | Median Reward: 47.00 | Max Reward: 49.34
Iteration: 1689 | Episodes: 68800 | Median Reward: 47.38 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1690         |
|    time_elapsed         | 24448        |
|    total_timesteps      | 6922240      |
| train/                  |              |
|    approx_kl            | 0.0048487186 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -114         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.64        |
|    n_updates            | 16890        |
|    policy_gradient_loss | -0.00894     |
|    std                  | 3.73         |
|    value_loss           | 0.257        |
------------------------------------------
Iteration: 1692 | Episodes: 68900 | Median Reward: 46.07 | Max Reward: 49.34
Iteration: 1694 | Episodes: 69000 | Median Reward: 46.61 | Max Reward: 49.34
Iteration: 1697 | Episodes: 69100 | Median Reward: 47.41 | Max Reward: 49.34
Iteration: 1699 | Episodes: 69200 | Median Reward: 47.42 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.8         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1700          |
|    time_elapsed         | 24593         |
|    total_timesteps      | 6963200       |
| train/                  |               |
|    approx_kl            | 0.00066331774 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -114          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.62         |
|    n_updates            | 16990         |
|    policy_gradient_loss | -0.0033       |
|    std                  | 3.76          |
|    value_loss           | 0.729         |
-------------------------------------------
Iteration: 1702 | Episodes: 69300 | Median Reward: 46.45 | Max Reward: 49.34
Iteration: 1704 | Episodes: 69400 | Median Reward: 47.25 | Max Reward: 49.34
Iteration: 1706 | Episodes: 69500 | Median Reward: 47.25 | Max Reward: 49.34
Iteration: 1709 | Episodes: 69600 | Median Reward: 47.22 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.4        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1710         |
|    time_elapsed         | 24736        |
|    total_timesteps      | 7004160      |
| train/                  |              |
|    approx_kl            | 0.0058373213 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.65        |
|    n_updates            | 17090        |
|    policy_gradient_loss | -0.00964     |
|    std                  | 3.79         |
|    value_loss           | 0.194        |
------------------------------------------
Iteration: 1711 | Episodes: 69700 | Median Reward: 47.09 | Max Reward: 49.34
Iteration: 1714 | Episodes: 69800 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 1716 | Episodes: 69900 | Median Reward: 47.59 | Max Reward: 49.34
Iteration: 1719 | Episodes: 70000 | Median Reward: 47.51 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1720         |
|    time_elapsed         | 24878        |
|    total_timesteps      | 7045120      |
| train/                  |              |
|    approx_kl            | 0.0015086127 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.45        |
|    n_updates            | 17190        |
|    policy_gradient_loss | -0.00426     |
|    std                  | 3.83         |
|    value_loss           | 0.437        |
------------------------------------------
Iteration: 1721 | Episodes: 70100 | Median Reward: 47.29 | Max Reward: 49.34
Iteration: 1724 | Episodes: 70200 | Median Reward: 44.32 | Max Reward: 49.34
Iteration: 1726 | Episodes: 70300 | Median Reward: 47.31 | Max Reward: 49.34
Iteration: 1729 | Episodes: 70400 | Median Reward: 46.92 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.4         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1730          |
|    time_elapsed         | 25024         |
|    total_timesteps      | 7086080       |
| train/                  |               |
|    approx_kl            | 0.00027289023 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -115          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.66         |
|    n_updates            | 17290         |
|    policy_gradient_loss | -0.000158     |
|    std                  | 3.87          |
|    value_loss           | 0.296         |
-------------------------------------------
Iteration: 1731 | Episodes: 70500 | Median Reward: 44.29 | Max Reward: 49.34
Iteration: 1734 | Episodes: 70600 | Median Reward: 47.42 | Max Reward: 49.34
Iteration: 1736 | Episodes: 70700 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 1739 | Episodes: 70800 | Median Reward: 46.95 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.9        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1740         |
|    time_elapsed         | 25169        |
|    total_timesteps      | 7127040      |
| train/                  |              |
|    approx_kl            | 0.0005249189 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.61        |
|    n_updates            | 17390        |
|    policy_gradient_loss | -0.00118     |
|    std                  | 3.91         |
|    value_loss           | 0.877        |
------------------------------------------
Iteration: 1741 | Episodes: 70900 | Median Reward: 47.51 | Max Reward: 49.34
Iteration: 1743 | Episodes: 71000 | Median Reward: 47.18 | Max Reward: 49.34
Iteration: 1746 | Episodes: 71100 | Median Reward: 46.97 | Max Reward: 49.34
Iteration: 1748 | Episodes: 71200 | Median Reward: 46.93 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.6         |
|    ep_rew_mean          | -52.6        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1750         |
|    time_elapsed         | 25313        |
|    total_timesteps      | 7168000      |
| train/                  |              |
|    approx_kl            | 0.0012580925 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0005       |
|    loss                 | -3.44        |
|    n_updates            | 17490        |
|    policy_gradient_loss | -0.00227     |
|    std                  | 3.95         |
|    value_loss           | 4.96         |
------------------------------------------
Iteration: 1751 | Episodes: 71300 | Median Reward: 47.03 | Max Reward: 49.34
Iteration: 1753 | Episodes: 71400 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 1756 | Episodes: 71500 | Median Reward: 46.59 | Max Reward: 49.34
Iteration: 1758 | Episodes: 71600 | Median Reward: 46.99 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54         |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1760        |
|    time_elapsed         | 25459       |
|    total_timesteps      | 7208960     |
| train/                  |             |
|    approx_kl            | 0.009521033 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -116        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.72       |
|    n_updates            | 17590       |
|    policy_gradient_loss | -0.0142     |
|    std                  | 4.01        |
|    value_loss           | 0.182       |
-----------------------------------------
Iteration: 1761 | Episodes: 71700 | Median Reward: 47.08 | Max Reward: 49.34
Iteration: 1763 | Episodes: 71800 | Median Reward: 47.40 | Max Reward: 49.34
Iteration: 1766 | Episodes: 71900 | Median Reward: 46.48 | Max Reward: 49.34
Iteration: 1768 | Episodes: 72000 | Median Reward: 47.21 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.3       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1770        |
|    time_elapsed         | 25606       |
|    total_timesteps      | 7249920     |
| train/                  |             |
|    approx_kl            | 0.000511917 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -116        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.73       |
|    n_updates            | 17690       |
|    policy_gradient_loss | -0.000182   |
|    std                  | 4.04        |
|    value_loss           | 0.173       |
-----------------------------------------
Iteration: 1771 | Episodes: 72100 | Median Reward: 47.48 | Max Reward: 49.34
Iteration: 1773 | Episodes: 72200 | Median Reward: 47.25 | Max Reward: 49.34
Iteration: 1775 | Episodes: 72300 | Median Reward: 47.23 | Max Reward: 49.34
Iteration: 1778 | Episodes: 72400 | Median Reward: 47.28 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.5        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1780         |
|    time_elapsed         | 25757        |
|    total_timesteps      | 7290880      |
| train/                  |              |
|    approx_kl            | 0.0006680126 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.67        |
|    n_updates            | 17790        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 4.08         |
|    value_loss           | 0.492        |
------------------------------------------
Iteration: 1780 | Episodes: 72500 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 1783 | Episodes: 72600 | Median Reward: 47.70 | Max Reward: 49.34
Iteration: 1785 | Episodes: 72700 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 1788 | Episodes: 72800 | Median Reward: 47.23 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1790         |
|    time_elapsed         | 25900        |
|    total_timesteps      | 7331840      |
| train/                  |              |
|    approx_kl            | 0.0017759687 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.68        |
|    n_updates            | 17890        |
|    policy_gradient_loss | -0.00205     |
|    std                  | 4.13         |
|    value_loss           | 0.202        |
------------------------------------------
Iteration: 1790 | Episodes: 72900 | Median Reward: 47.35 | Max Reward: 49.34
Iteration: 1793 | Episodes: 73000 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 1795 | Episodes: 73100 | Median Reward: 47.59 | Max Reward: 49.34
Iteration: 1798 | Episodes: 73200 | Median Reward: 47.52 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.2       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1800        |
|    time_elapsed         | 26050       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 0.000292723 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -116        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.66       |
|    n_updates            | 17990       |
|    policy_gradient_loss | -0.00164    |
|    std                  | 4.17        |
|    value_loss           | 0.531       |
-----------------------------------------
Iteration: 1800 | Episodes: 73300 | Median Reward: 45.62 | Max Reward: 49.34
Iteration: 1803 | Episodes: 73400 | Median Reward: 47.36 | Max Reward: 49.34
Iteration: 1805 | Episodes: 73500 | Median Reward: 46.79 | Max Reward: 49.34
Iteration: 1808 | Episodes: 73600 | Median Reward: 44.76 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.9        |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 1810         |
|    time_elapsed         | 26197        |
|    total_timesteps      | 7413760      |
| train/                  |              |
|    approx_kl            | 0.0033995986 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -117         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.77        |
|    n_updates            | 18090        |
|    policy_gradient_loss | -0.00558     |
|    std                  | 4.2          |
|    value_loss           | 0.27         |
------------------------------------------
Iteration: 1810 | Episodes: 73700 | Median Reward: 47.24 | Max Reward: 49.34
Iteration: 1812 | Episodes: 73800 | Median Reward: 47.65 | Max Reward: 49.34
Iteration: 1815 | Episodes: 73900 | Median Reward: 47.16 | Max Reward: 49.34
Iteration: 1817 | Episodes: 74000 | Median Reward: 47.71 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.2         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1820          |
|    time_elapsed         | 26338         |
|    total_timesteps      | 7454720       |
| train/                  |               |
|    approx_kl            | 4.3693813e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -117          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.68         |
|    n_updates            | 18190         |
|    policy_gradient_loss | -0.000136     |
|    std                  | 4.21          |
|    value_loss           | 0.313         |
-------------------------------------------
Iteration: 1820 | Episodes: 74100 | Median Reward: 46.96 | Max Reward: 49.34
Iteration: 1822 | Episodes: 74200 | Median Reward: 46.44 | Max Reward: 49.34
Iteration: 1825 | Episodes: 74300 | Median Reward: 46.96 | Max Reward: 49.34
Iteration: 1827 | Episodes: 74400 | Median Reward: 45.66 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.6       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1830        |
|    time_elapsed         | 26478       |
|    total_timesteps      | 7495680     |
| train/                  |             |
|    approx_kl            | 0.013400207 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -117        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.78       |
|    n_updates            | 18290       |
|    policy_gradient_loss | -0.01       |
|    std                  | 4.27        |
|    value_loss           | 0.171       |
-----------------------------------------
Iteration: 1830 | Episodes: 74500 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 1832 | Episodes: 74600 | Median Reward: 47.36 | Max Reward: 49.34
Iteration: 1835 | Episodes: 74700 | Median Reward: 47.19 | Max Reward: 49.34
Iteration: 1837 | Episodes: 74800 | Median Reward: 46.23 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.6        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1840         |
|    time_elapsed         | 26617        |
|    total_timesteps      | 7536640      |
| train/                  |              |
|    approx_kl            | 0.0007866082 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -117         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.37        |
|    n_updates            | 18390        |
|    policy_gradient_loss | -0.000808    |
|    std                  | 4.32         |
|    value_loss           | 0.504        |
------------------------------------------
Iteration: 1840 | Episodes: 74900 | Median Reward: 45.18 | Max Reward: 49.34
Iteration: 1842 | Episodes: 75000 | Median Reward: 47.05 | Max Reward: 49.34
Iteration: 1844 | Episodes: 75100 | Median Reward: 47.02 | Max Reward: 49.34
Iteration: 1847 | Episodes: 75200 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 1849 | Episodes: 75300 | Median Reward: 46.37 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.2         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1850          |
|    time_elapsed         | 26759         |
|    total_timesteps      | 7577600       |
| train/                  |               |
|    approx_kl            | 0.00016703032 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -117          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.71         |
|    n_updates            | 18490         |
|    policy_gradient_loss | -0.000599     |
|    std                  | 4.33          |
|    value_loss           | 0.243         |
-------------------------------------------
Iteration: 1852 | Episodes: 75400 | Median Reward: 46.67 | Max Reward: 49.34
Iteration: 1854 | Episodes: 75500 | Median Reward: 46.33 | Max Reward: 49.34
Iteration: 1857 | Episodes: 75600 | Median Reward: 46.72 | Max Reward: 49.34
Iteration: 1859 | Episodes: 75700 | Median Reward: 47.14 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.6        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1860         |
|    time_elapsed         | 26899        |
|    total_timesteps      | 7618560      |
| train/                  |              |
|    approx_kl            | 2.206712e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -117         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.74        |
|    n_updates            | 18590        |
|    policy_gradient_loss | -5.07e-05    |
|    std                  | 4.36         |
|    value_loss           | 0.431        |
------------------------------------------
Iteration: 1862 | Episodes: 75800 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 1864 | Episodes: 75900 | Median Reward: 47.16 | Max Reward: 49.34
Iteration: 1867 | Episodes: 76000 | Median Reward: 46.98 | Max Reward: 49.34
Iteration: 1869 | Episodes: 76100 | Median Reward: 47.44 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.8        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1870         |
|    time_elapsed         | 27040        |
|    total_timesteps      | 7659520      |
| train/                  |              |
|    approx_kl            | 0.0011694621 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -118         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.71        |
|    n_updates            | 18690        |
|    policy_gradient_loss | -0.00286     |
|    std                  | 4.4          |
|    value_loss           | 0.468        |
------------------------------------------
Iteration: 1872 | Episodes: 76200 | Median Reward: 46.97 | Max Reward: 49.34
Iteration: 1874 | Episodes: 76300 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 1876 | Episodes: 76400 | Median Reward: 46.72 | Max Reward: 49.34
Iteration: 1879 | Episodes: 76500 | Median Reward: 47.67 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.6        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1880         |
|    time_elapsed         | 27179        |
|    total_timesteps      | 7700480      |
| train/                  |              |
|    approx_kl            | 0.0002710675 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -118         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.65        |
|    n_updates            | 18790        |
|    policy_gradient_loss | -0.00133     |
|    std                  | 4.43         |
|    value_loss           | 0.585        |
------------------------------------------
Iteration: 1881 | Episodes: 76600 | Median Reward: 46.76 | Max Reward: 49.34
Iteration: 1884 | Episodes: 76700 | Median Reward: 47.76 | Max Reward: 49.34
Iteration: 1886 | Episodes: 76800 | Median Reward: 47.06 | Max Reward: 49.34
Iteration: 1889 | Episodes: 76900 | Median Reward: 46.89 | Max Reward: 49.34
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -54.5          |
| time/                   |                |
|    fps                  | 283            |
|    iterations           | 1890           |
|    time_elapsed         | 27321          |
|    total_timesteps      | 7741440        |
| train/                  |                |
|    approx_kl            | 0.000104482446 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -118           |
|    explained_variance   | 0.999          |
|    learning_rate        | 0.0005         |
|    loss                 | -5.11          |
|    n_updates            | 18890          |
|    policy_gradient_loss | -0.001         |
|    std                  | 4.47           |
|    value_loss           | 0.883          |
--------------------------------------------
Iteration: 1891 | Episodes: 77000 | Median Reward: 47.10 | Max Reward: 49.34
Iteration: 1894 | Episodes: 77100 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 1896 | Episodes: 77200 | Median Reward: 47.30 | Max Reward: 49.34
Iteration: 1899 | Episodes: 77300 | Median Reward: 47.28 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.6        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1900         |
|    time_elapsed         | 27462        |
|    total_timesteps      | 7782400      |
| train/                  |              |
|    approx_kl            | 0.0013683662 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -118         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.74        |
|    n_updates            | 18990        |
|    policy_gradient_loss | -0.00424     |
|    std                  | 4.49         |
|    value_loss           | 0.478        |
------------------------------------------
Iteration: 1901 | Episodes: 77400 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 1904 | Episodes: 77500 | Median Reward: 46.37 | Max Reward: 49.34
Iteration: 1906 | Episodes: 77600 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 1909 | Episodes: 77700 | Median Reward: 47.05 | Max Reward: 49.34
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -52.8          |
| time/                   |                |
|    fps                  | 283            |
|    iterations           | 1910           |
|    time_elapsed         | 27602          |
|    total_timesteps      | 7823360        |
| train/                  |                |
|    approx_kl            | 0.000116447336 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -118           |
|    explained_variance   | 1              |
|    learning_rate        | 0.0005         |
|    loss                 | -5.7           |
|    n_updates            | 19090          |
|    policy_gradient_loss | -0.000396      |
|    std                  | 4.53           |
|    value_loss           | 0.318          |
--------------------------------------------
Iteration: 1911 | Episodes: 77800 | Median Reward: 47.33 | Max Reward: 49.34
Iteration: 1913 | Episodes: 77900 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 1916 | Episodes: 78000 | Median Reward: 47.71 | Max Reward: 49.34
Iteration: 1918 | Episodes: 78100 | Median Reward: 47.21 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.6       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1920        |
|    time_elapsed         | 27744       |
|    total_timesteps      | 7864320     |
| train/                  |             |
|    approx_kl            | 0.003736216 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -118        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.82       |
|    n_updates            | 19190       |
|    policy_gradient_loss | -0.01       |
|    std                  | 4.54        |
|    value_loss           | 0.356       |
-----------------------------------------
Iteration: 1921 | Episodes: 78200 | Median Reward: 46.59 | Max Reward: 49.34
Iteration: 1923 | Episodes: 78300 | Median Reward: 47.51 | Max Reward: 49.34
Iteration: 1926 | Episodes: 78400 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 1928 | Episodes: 78500 | Median Reward: 46.96 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.5        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 1930         |
|    time_elapsed         | 27885        |
|    total_timesteps      | 7905280      |
| train/                  |              |
|    approx_kl            | 0.0013948414 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -118         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.83        |
|    n_updates            | 19290        |
|    policy_gradient_loss | -0.00118     |
|    std                  | 4.58         |
|    value_loss           | 0.2          |
------------------------------------------
Iteration: 1931 | Episodes: 78600 | Median Reward: 47.05 | Max Reward: 49.34
Iteration: 1933 | Episodes: 78700 | Median Reward: 44.32 | Max Reward: 49.34
Iteration: 1936 | Episodes: 78800 | Median Reward: 46.73 | Max Reward: 49.34
Iteration: 1938 | Episodes: 78900 | Median Reward: 47.42 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53           |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1940          |
|    time_elapsed         | 28026         |
|    total_timesteps      | 7946240       |
| train/                  |               |
|    approx_kl            | 0.00025441824 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -119          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.89         |
|    n_updates            | 19390         |
|    policy_gradient_loss | -0.000204     |
|    std                  | 4.63          |
|    value_loss           | 0.197         |
-------------------------------------------
Iteration: 1941 | Episodes: 79000 | Median Reward: 47.32 | Max Reward: 49.34
Iteration: 1943 | Episodes: 79100 | Median Reward: 47.04 | Max Reward: 49.34
Iteration: 1945 | Episodes: 79200 | Median Reward: 47.57 | Max Reward: 49.34
Iteration: 1948 | Episodes: 79300 | Median Reward: 47.55 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.9         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1950          |
|    time_elapsed         | 28168         |
|    total_timesteps      | 7987200       |
| train/                  |               |
|    approx_kl            | 0.00014692046 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -119          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.9          |
|    n_updates            | 19490         |
|    policy_gradient_loss | -0.000313     |
|    std                  | 4.66          |
|    value_loss           | 0.387         |
-------------------------------------------
Iteration: 1950 | Episodes: 79400 | Median Reward: 47.16 | Max Reward: 49.34
Iteration: 1953 | Episodes: 79500 | Median Reward: 46.77 | Max Reward: 49.34
Iteration: 1955 | Episodes: 79600 | Median Reward: 47.46 | Max Reward: 49.34
Iteration: 1958 | Episodes: 79700 | Median Reward: 47.23 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.8       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 1960        |
|    time_elapsed         | 28309       |
|    total_timesteps      | 8028160     |
| train/                  |             |
|    approx_kl            | 0.034935575 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.4         |
|    entropy_loss         | -119        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.65       |
|    n_updates            | 19590       |
|    policy_gradient_loss | -0.0552     |
|    std                  | 4.71        |
|    value_loss           | 0.46        |
-----------------------------------------
Iteration: 1960 | Episodes: 79800 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 1963 | Episodes: 79900 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 1965 | Episodes: 80000 | Median Reward: 47.57 | Max Reward: 49.34
Iteration: 1968 | Episodes: 80100 | Median Reward: 47.73 | Max Reward: 49.34
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 100            |
|    ep_rew_mean          | -52.5          |
| time/                   |                |
|    fps                  | 283            |
|    iterations           | 1970           |
|    time_elapsed         | 28453          |
|    total_timesteps      | 8069120        |
| train/                  |                |
|    approx_kl            | 0.000103638085 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -119           |
|    explained_variance   | 0.987          |
|    learning_rate        | 0.0005         |
|    loss                 | -1.54          |
|    n_updates            | 19690          |
|    policy_gradient_loss | -0.00034       |
|    std                  | 4.72           |
|    value_loss           | 9.05           |
--------------------------------------------
Iteration: 1970 | Episodes: 80200 | Median Reward: 47.29 | Max Reward: 49.34
Iteration: 1973 | Episodes: 80300 | Median Reward: 47.52 | Max Reward: 49.34
Iteration: 1975 | Episodes: 80400 | Median Reward: 47.11 | Max Reward: 49.34
Iteration: 1977 | Episodes: 80500 | Median Reward: 46.90 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.2         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1980          |
|    time_elapsed         | 28602         |
|    total_timesteps      | 8110080       |
| train/                  |               |
|    approx_kl            | 0.00018364124 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -119          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.62         |
|    n_updates            | 19790         |
|    policy_gradient_loss | -0.000781     |
|    std                  | 4.76          |
|    value_loss           | 0.474         |
-------------------------------------------
Iteration: 1980 | Episodes: 80600 | Median Reward: 47.18 | Max Reward: 49.34
Iteration: 1982 | Episodes: 80700 | Median Reward: 46.69 | Max Reward: 49.34
Iteration: 1985 | Episodes: 80800 | Median Reward: 46.97 | Max Reward: 49.34
Iteration: 1987 | Episodes: 80900 | Median Reward: 47.56 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.3         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 1990          |
|    time_elapsed         | 28742         |
|    total_timesteps      | 8151040       |
| train/                  |               |
|    approx_kl            | 6.0465623e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -119          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.69         |
|    n_updates            | 19890         |
|    policy_gradient_loss | -8.74e-05     |
|    std                  | 4.78          |
|    value_loss           | 0.503         |
-------------------------------------------
Iteration: 1990 | Episodes: 81000 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 1992 | Episodes: 81100 | Median Reward: 46.18 | Max Reward: 49.34
Iteration: 1995 | Episodes: 81200 | Median Reward: 47.51 | Max Reward: 49.34
Iteration: 1997 | Episodes: 81300 | Median Reward: 46.92 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.6        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 2000         |
|    time_elapsed         | 28884        |
|    total_timesteps      | 8192000      |
| train/                  |              |
|    approx_kl            | 0.0005709025 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.89        |
|    n_updates            | 19990        |
|    policy_gradient_loss | -0.00111     |
|    std                  | 4.82         |
|    value_loss           | 0.26         |
------------------------------------------
Iteration: 2000 | Episodes: 81400 | Median Reward: 46.86 | Max Reward: 49.34
Iteration: 2002 | Episodes: 81500 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 2005 | Episodes: 81600 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 2007 | Episodes: 81700 | Median Reward: 47.26 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.5        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 2010         |
|    time_elapsed         | 29026        |
|    total_timesteps      | 8232960      |
| train/                  |              |
|    approx_kl            | 0.0016521409 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.89        |
|    n_updates            | 20090        |
|    policy_gradient_loss | -0.00409     |
|    std                  | 4.85         |
|    value_loss           | 0.161        |
------------------------------------------
Iteration: 2010 | Episodes: 81800 | Median Reward: 47.25 | Max Reward: 49.34
Iteration: 2012 | Episodes: 81900 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 2014 | Episodes: 82000 | Median Reward: 47.32 | Max Reward: 49.34
Iteration: 2017 | Episodes: 82100 | Median Reward: 47.30 | Max Reward: 49.34
Iteration: 2019 | Episodes: 82200 | Median Reward: 47.15 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 2020         |
|    time_elapsed         | 29168        |
|    total_timesteps      | 8273920      |
| train/                  |              |
|    approx_kl            | 0.0045957793 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.96        |
|    n_updates            | 20190        |
|    policy_gradient_loss | -0.0084      |
|    std                  | 4.89         |
|    value_loss           | 0.249        |
------------------------------------------
Iteration: 2022 | Episodes: 82300 | Median Reward: 47.33 | Max Reward: 49.34
Iteration: 2024 | Episodes: 82400 | Median Reward: 47.15 | Max Reward: 49.34
Iteration: 2027 | Episodes: 82500 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 2029 | Episodes: 82600 | Median Reward: 47.50 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.9        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 2030         |
|    time_elapsed         | 29309        |
|    total_timesteps      | 8314880      |
| train/                  |              |
|    approx_kl            | 0.0035466012 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.99        |
|    n_updates            | 20290        |
|    policy_gradient_loss | -0.00918     |
|    std                  | 4.92         |
|    value_loss           | 0.135        |
------------------------------------------
Iteration: 2032 | Episodes: 82700 | Median Reward: 46.66 | Max Reward: 49.34
Iteration: 2034 | Episodes: 82800 | Median Reward: 47.06 | Max Reward: 49.34
Iteration: 2037 | Episodes: 82900 | Median Reward: 46.77 | Max Reward: 49.34
Iteration: 2039 | Episodes: 83000 | Median Reward: 46.41 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.8        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 2040         |
|    time_elapsed         | 29450        |
|    total_timesteps      | 8355840      |
| train/                  |              |
|    approx_kl            | 0.0020883912 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.95        |
|    n_updates            | 20390        |
|    policy_gradient_loss | -0.00444     |
|    std                  | 4.96         |
|    value_loss           | 0.201        |
------------------------------------------
Iteration: 2041 | Episodes: 83100 | Median Reward: 46.33 | Max Reward: 49.34
Iteration: 2044 | Episodes: 83200 | Median Reward: 47.11 | Max Reward: 49.34
Iteration: 2046 | Episodes: 83300 | Median Reward: 47.16 | Max Reward: 49.34
Iteration: 2049 | Episodes: 83400 | Median Reward: 46.59 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54          |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 2050         |
|    time_elapsed         | 29590        |
|    total_timesteps      | 8396800      |
| train/                  |              |
|    approx_kl            | 0.0012722011 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.71        |
|    n_updates            | 20490        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 4.99         |
|    value_loss           | 0.342        |
------------------------------------------
Iteration: 2051 | Episodes: 83500 | Median Reward: 47.01 | Max Reward: 49.34
Iteration: 2054 | Episodes: 83600 | Median Reward: 47.48 | Max Reward: 49.34
Iteration: 2056 | Episodes: 83700 | Median Reward: 47.01 | Max Reward: 49.34
Iteration: 2059 | Episodes: 83800 | Median Reward: 47.74 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.1         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 2060          |
|    time_elapsed         | 29730         |
|    total_timesteps      | 8437760       |
| train/                  |               |
|    approx_kl            | 0.00043488236 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -121          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.91         |
|    n_updates            | 20590         |
|    policy_gradient_loss | -0.0011       |
|    std                  | 5.03          |
|    value_loss           | 0.312         |
-------------------------------------------
Iteration: 2061 | Episodes: 83900 | Median Reward: 47.62 | Max Reward: 49.34
Iteration: 2064 | Episodes: 84000 | Median Reward: 46.98 | Max Reward: 49.34
Iteration: 2066 | Episodes: 84100 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 2069 | Episodes: 84200 | Median Reward: 47.33 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54          |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 2070         |
|    time_elapsed         | 29871        |
|    total_timesteps      | 8478720      |
| train/                  |              |
|    approx_kl            | 8.401682e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -121         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.52        |
|    n_updates            | 20690        |
|    policy_gradient_loss | -0.000155    |
|    std                  | 5.07         |
|    value_loss           | 3.32         |
------------------------------------------
Iteration: 2071 | Episodes: 84300 | Median Reward: 44.33 | Max Reward: 49.34
Iteration: 2074 | Episodes: 84400 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 2076 | Episodes: 84500 | Median Reward: 46.84 | Max Reward: 49.34
Iteration: 2078 | Episodes: 84600 | Median Reward: 47.23 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | -53         |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 2080        |
|    time_elapsed         | 30013       |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.007734464 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -121        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.91       |
|    n_updates            | 20790       |
|    policy_gradient_loss | -0.0195     |
|    std                  | 5.13        |
|    value_loss           | 0.342       |
-----------------------------------------
Iteration: 2081 | Episodes: 84700 | Median Reward: 47.49 | Max Reward: 49.34
Iteration: 2083 | Episodes: 84800 | Median Reward: 47.18 | Max Reward: 49.34
Iteration: 2086 | Episodes: 84900 | Median Reward: 47.23 | Max Reward: 49.34
Iteration: 2088 | Episodes: 85000 | Median Reward: 47.24 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.2         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 2090          |
|    time_elapsed         | 30153         |
|    total_timesteps      | 8560640       |
| train/                  |               |
|    approx_kl            | 2.8191047e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -121          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.96         |
|    n_updates            | 20890         |
|    policy_gradient_loss | -3.9e-06      |
|    std                  | 5.19          |
|    value_loss           | 0.149         |
-------------------------------------------
Iteration: 2091 | Episodes: 85100 | Median Reward: 46.42 | Max Reward: 49.34
Iteration: 2093 | Episodes: 85200 | Median Reward: 46.70 | Max Reward: 49.34
Iteration: 2096 | Episodes: 85300 | Median Reward: 47.46 | Max Reward: 49.34
Iteration: 2098 | Episodes: 85400 | Median Reward: 45.22 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55         |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 2100        |
|    time_elapsed         | 30292       |
|    total_timesteps      | 8601600     |
| train/                  |             |
|    approx_kl            | 0.004017392 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -121        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6          |
|    n_updates            | 20990       |
|    policy_gradient_loss | -0.0155     |
|    std                  | 5.23        |
|    value_loss           | 0.506       |
-----------------------------------------
Iteration: 2101 | Episodes: 85500 | Median Reward: 46.39 | Max Reward: 49.34
Iteration: 2103 | Episodes: 85600 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 2106 | Episodes: 85700 | Median Reward: 47.33 | Max Reward: 49.34
Iteration: 2108 | Episodes: 85800 | Median Reward: 46.87 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.5       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 2110        |
|    time_elapsed         | 30434       |
|    total_timesteps      | 8642560     |
| train/                  |             |
|    approx_kl            | 0.000406681 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -122        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.04       |
|    n_updates            | 21090       |
|    policy_gradient_loss | -0.00178    |
|    std                  | 5.28        |
|    value_loss           | 0.176       |
-----------------------------------------
Iteration: 2110 | Episodes: 85900 | Median Reward: 46.30 | Max Reward: 49.34
Iteration: 2113 | Episodes: 86000 | Median Reward: 47.41 | Max Reward: 49.34
Iteration: 2115 | Episodes: 86100 | Median Reward: 47.50 | Max Reward: 49.34
Iteration: 2118 | Episodes: 86200 | Median Reward: 47.13 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.2         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2120          |
|    time_elapsed         | 30574         |
|    total_timesteps      | 8683520       |
| train/                  |               |
|    approx_kl            | 0.00022745076 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -122          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.99         |
|    n_updates            | 21190         |
|    policy_gradient_loss | -0.00142      |
|    std                  | 5.32          |
|    value_loss           | 0.237         |
-------------------------------------------
Iteration: 2120 | Episodes: 86300 | Median Reward: 46.33 | Max Reward: 49.34
Iteration: 2123 | Episodes: 86400 | Median Reward: 46.55 | Max Reward: 49.34
Iteration: 2125 | Episodes: 86500 | Median Reward: 47.12 | Max Reward: 49.34
Iteration: 2128 | Episodes: 86600 | Median Reward: 46.98 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.7         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2130          |
|    time_elapsed         | 30713         |
|    total_timesteps      | 8724480       |
| train/                  |               |
|    approx_kl            | 0.00019800608 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -122          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.01         |
|    n_updates            | 21290         |
|    policy_gradient_loss | -0.000258     |
|    std                  | 5.37          |
|    value_loss           | 0.118         |
-------------------------------------------
Iteration: 2130 | Episodes: 86700 | Median Reward: 47.16 | Max Reward: 49.34
Iteration: 2133 | Episodes: 86800 | Median Reward: 46.88 | Max Reward: 49.34
Iteration: 2135 | Episodes: 86900 | Median Reward: 46.80 | Max Reward: 49.34
Iteration: 2138 | Episodes: 87000 | Median Reward: 47.02 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.7       |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 2140        |
|    time_elapsed         | 30853       |
|    total_timesteps      | 8765440     |
| train/                  |             |
|    approx_kl            | 0.008725608 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -122        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.98       |
|    n_updates            | 21390       |
|    policy_gradient_loss | -0.0203     |
|    std                  | 5.4         |
|    value_loss           | 0.353       |
-----------------------------------------
Iteration: 2140 | Episodes: 87100 | Median Reward: 47.10 | Max Reward: 49.34
Iteration: 2142 | Episodes: 87200 | Median Reward: 47.53 | Max Reward: 49.34
Iteration: 2145 | Episodes: 87300 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 2147 | Episodes: 87400 | Median Reward: 47.58 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.7        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2150         |
|    time_elapsed         | 31001        |
|    total_timesteps      | 8806400      |
| train/                  |              |
|    approx_kl            | 0.0009926783 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -122         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.97        |
|    n_updates            | 21490        |
|    policy_gradient_loss | -0.00401     |
|    std                  | 5.43         |
|    value_loss           | 0.302        |
------------------------------------------
Iteration: 2150 | Episodes: 87500 | Median Reward: 46.86 | Max Reward: 49.34
Iteration: 2152 | Episodes: 87600 | Median Reward: 47.55 | Max Reward: 49.34
Iteration: 2155 | Episodes: 87700 | Median Reward: 46.37 | Max Reward: 49.34
Iteration: 2157 | Episodes: 87800 | Median Reward: 47.47 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -53.3         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2160          |
|    time_elapsed         | 31146         |
|    total_timesteps      | 8847360       |
| train/                  |               |
|    approx_kl            | 0.00047273462 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -122          |
|    explained_variance   | 0.989         |
|    learning_rate        | 0.0005        |
|    loss                 | -2.3          |
|    n_updates            | 21590         |
|    policy_gradient_loss | -0.000811     |
|    std                  | 5.47          |
|    value_loss           | 7.75          |
-------------------------------------------
Iteration: 2160 | Episodes: 87900 | Median Reward: 46.35 | Max Reward: 49.34
Iteration: 2162 | Episodes: 88000 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 2165 | Episodes: 88100 | Median Reward: 45.60 | Max Reward: 49.34
Iteration: 2167 | Episodes: 88200 | Median Reward: 46.26 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.9         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2170          |
|    time_elapsed         | 31287         |
|    total_timesteps      | 8888320       |
| train/                  |               |
|    approx_kl            | 3.9943494e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -122          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.75         |
|    n_updates            | 21690         |
|    policy_gradient_loss | -0.000258     |
|    std                  | 5.5           |
|    value_loss           | 0.735         |
-------------------------------------------
Iteration: 2170 | Episodes: 88300 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 2172 | Episodes: 88400 | Median Reward: 46.70 | Max Reward: 49.34
Iteration: 2174 | Episodes: 88500 | Median Reward: 47.41 | Max Reward: 49.34
Iteration: 2177 | Episodes: 88600 | Median Reward: 46.97 | Max Reward: 49.34
Iteration: 2179 | Episodes: 88700 | Median Reward: 46.99 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.2        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2180         |
|    time_elapsed         | 31424        |
|    total_timesteps      | 8929280      |
| train/                  |              |
|    approx_kl            | 0.0050212387 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -122         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.05        |
|    n_updates            | 21790        |
|    policy_gradient_loss | -0.0121      |
|    std                  | 5.52         |
|    value_loss           | 0.205        |
------------------------------------------
Iteration: 2182 | Episodes: 88800 | Median Reward: 47.55 | Max Reward: 49.34
Iteration: 2184 | Episodes: 88900 | Median Reward: 47.48 | Max Reward: 49.34
Iteration: 2187 | Episodes: 89000 | Median Reward: 47.02 | Max Reward: 49.34
Iteration: 2189 | Episodes: 89100 | Median Reward: 47.25 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.4         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2190          |
|    time_elapsed         | 31565         |
|    total_timesteps      | 8970240       |
| train/                  |               |
|    approx_kl            | 0.00013404172 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.02         |
|    n_updates            | 21890         |
|    policy_gradient_loss | -0.000544     |
|    std                  | 5.57          |
|    value_loss           | 0.196         |
-------------------------------------------
Iteration: 2192 | Episodes: 89200 | Median Reward: 47.27 | Max Reward: 49.34
Iteration: 2194 | Episodes: 89300 | Median Reward: 46.57 | Max Reward: 49.34
Iteration: 2197 | Episodes: 89400 | Median Reward: 46.35 | Max Reward: 49.34
Iteration: 2199 | Episodes: 89500 | Median Reward: 44.09 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.5         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2200          |
|    time_elapsed         | 31707         |
|    total_timesteps      | 9011200       |
| train/                  |               |
|    approx_kl            | 0.00043363095 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.8          |
|    n_updates            | 21990         |
|    policy_gradient_loss | -0.00192      |
|    std                  | 5.62          |
|    value_loss           | 0.552         |
-------------------------------------------
Iteration: 2201 | Episodes: 89600 | Median Reward: 47.64 | Max Reward: 49.34
Iteration: 2204 | Episodes: 89700 | Median Reward: 47.00 | Max Reward: 49.34
Iteration: 2206 | Episodes: 89800 | Median Reward: 47.08 | Max Reward: 49.34
Iteration: 2209 | Episodes: 89900 | Median Reward: 46.82 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.1         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2210          |
|    time_elapsed         | 31848         |
|    total_timesteps      | 9052160       |
| train/                  |               |
|    approx_kl            | 0.00046734087 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.05         |
|    n_updates            | 22090         |
|    policy_gradient_loss | -0.00122      |
|    std                  | 5.67          |
|    value_loss           | 0.288         |
-------------------------------------------
Iteration: 2211 | Episodes: 90000 | Median Reward: 47.25 | Max Reward: 49.34
Iteration: 2214 | Episodes: 90100 | Median Reward: 47.50 | Max Reward: 49.34
Iteration: 2216 | Episodes: 90200 | Median Reward: 47.05 | Max Reward: 49.34
Iteration: 2219 | Episodes: 90300 | Median Reward: 47.58 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -52.7         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2220          |
|    time_elapsed         | 31988         |
|    total_timesteps      | 9093120       |
| train/                  |               |
|    approx_kl            | 0.00037100737 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 0.99          |
|    learning_rate        | 0.0005        |
|    loss                 | -2.39         |
|    n_updates            | 22190         |
|    policy_gradient_loss | 0.000168      |
|    std                  | 5.74          |
|    value_loss           | 7.56          |
-------------------------------------------
Iteration: 2221 | Episodes: 90400 | Median Reward: 46.08 | Max Reward: 49.34
Iteration: 2224 | Episodes: 90500 | Median Reward: 46.96 | Max Reward: 49.34
Iteration: 2226 | Episodes: 90600 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 2229 | Episodes: 90700 | Median Reward: 47.12 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.9        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2230         |
|    time_elapsed         | 32130        |
|    total_timesteps      | 9134080      |
| train/                  |              |
|    approx_kl            | 0.0006902128 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -123         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.9         |
|    n_updates            | 22290        |
|    policy_gradient_loss | -0.0044      |
|    std                  | 5.76         |
|    value_loss           | 0.457        |
------------------------------------------
Iteration: 2231 | Episodes: 90800 | Median Reward: 47.51 | Max Reward: 49.34
Iteration: 2233 | Episodes: 90900 | Median Reward: 47.32 | Max Reward: 49.34
Iteration: 2236 | Episodes: 91000 | Median Reward: 46.64 | Max Reward: 49.34
Iteration: 2238 | Episodes: 91100 | Median Reward: 46.90 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.5       |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 2240        |
|    time_elapsed         | 32271       |
|    total_timesteps      | 9175040     |
| train/                  |             |
|    approx_kl            | 0.002187021 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -123        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.97       |
|    n_updates            | 22390       |
|    policy_gradient_loss | -0.00744    |
|    std                  | 5.79        |
|    value_loss           | 0.577       |
-----------------------------------------
Iteration: 2241 | Episodes: 91200 | Median Reward: 47.24 | Max Reward: 49.34
Iteration: 2243 | Episodes: 91300 | Median Reward: 47.46 | Max Reward: 49.34
Iteration: 2246 | Episodes: 91400 | Median Reward: 46.30 | Max Reward: 49.34
Iteration: 2248 | Episodes: 91500 | Median Reward: 47.02 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.9        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2250         |
|    time_elapsed         | 32411        |
|    total_timesteps      | 9216000      |
| train/                  |              |
|    approx_kl            | 0.0044977358 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -124         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.16        |
|    n_updates            | 22490        |
|    policy_gradient_loss | -0.0113      |
|    std                  | 5.83         |
|    value_loss           | 0.16         |
------------------------------------------
Iteration: 2251 | Episodes: 91600 | Median Reward: 47.40 | Max Reward: 49.34
Iteration: 2253 | Episodes: 91700 | Median Reward: 47.41 | Max Reward: 49.34
Iteration: 2256 | Episodes: 91800 | Median Reward: 45.64 | Max Reward: 49.34
Iteration: 2258 | Episodes: 91900 | Median Reward: 46.20 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.2        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2260         |
|    time_elapsed         | 32551        |
|    total_timesteps      | 9256960      |
| train/                  |              |
|    approx_kl            | 0.0030799648 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -124         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.11        |
|    n_updates            | 22590        |
|    policy_gradient_loss | -0.00858     |
|    std                  | 5.87         |
|    value_loss           | 0.176        |
------------------------------------------
Iteration: 2261 | Episodes: 92000 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 2263 | Episodes: 92100 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 2265 | Episodes: 92200 | Median Reward: 47.42 | Max Reward: 49.34
Iteration: 2268 | Episodes: 92300 | Median Reward: 47.41 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -53.4         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2270          |
|    time_elapsed         | 32694         |
|    total_timesteps      | 9297920       |
| train/                  |               |
|    approx_kl            | 0.00023746889 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -124          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.15         |
|    n_updates            | 22690         |
|    policy_gradient_loss | -0.00174      |
|    std                  | 5.94          |
|    value_loss           | 0.186         |
-------------------------------------------
Iteration: 2270 | Episodes: 92400 | Median Reward: 46.63 | Max Reward: 49.34
Iteration: 2273 | Episodes: 92500 | Median Reward: 47.17 | Max Reward: 49.34
Iteration: 2275 | Episodes: 92600 | Median Reward: 46.80 | Max Reward: 49.34
Iteration: 2278 | Episodes: 92700 | Median Reward: 46.91 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2280         |
|    time_elapsed         | 32834        |
|    total_timesteps      | 9338880      |
| train/                  |              |
|    approx_kl            | 9.314521e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -124         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.16        |
|    n_updates            | 22790        |
|    policy_gradient_loss | -0.000262    |
|    std                  | 5.99         |
|    value_loss           | 0.253        |
------------------------------------------
Iteration: 2280 | Episodes: 92800 | Median Reward: 47.40 | Max Reward: 49.34
Iteration: 2283 | Episodes: 92900 | Median Reward: 47.42 | Max Reward: 49.34
Iteration: 2285 | Episodes: 93000 | Median Reward: 47.42 | Max Reward: 49.34
Iteration: 2288 | Episodes: 93100 | Median Reward: 47.30 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.3         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2290          |
|    time_elapsed         | 32974         |
|    total_timesteps      | 9379840       |
| train/                  |               |
|    approx_kl            | 0.00026928223 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -124          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.07         |
|    n_updates            | 22890         |
|    policy_gradient_loss | -0.000483     |
|    std                  | 6.05          |
|    value_loss           | 0.422         |
-------------------------------------------
Iteration: 2290 | Episodes: 93200 | Median Reward: 47.42 | Max Reward: 49.34
Iteration: 2293 | Episodes: 93300 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 2295 | Episodes: 93400 | Median Reward: 47.13 | Max Reward: 49.34
Iteration: 2298 | Episodes: 93500 | Median Reward: 47.05 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.5         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2300          |
|    time_elapsed         | 33116         |
|    total_timesteps      | 9420800       |
| train/                  |               |
|    approx_kl            | 0.00010242731 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -125          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.95         |
|    n_updates            | 22990         |
|    policy_gradient_loss | -0.00104      |
|    std                  | 6.1           |
|    value_loss           | 0.623         |
-------------------------------------------
Iteration: 2300 | Episodes: 93600 | Median Reward: 47.29 | Max Reward: 49.34
Iteration: 2302 | Episodes: 93700 | Median Reward: 47.53 | Max Reward: 49.34
Iteration: 2305 | Episodes: 93800 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 2307 | Episodes: 93900 | Median Reward: 46.99 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.8         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2310          |
|    time_elapsed         | 33256         |
|    total_timesteps      | 9461760       |
| train/                  |               |
|    approx_kl            | 0.00020744179 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -125          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.07         |
|    n_updates            | 23090         |
|    policy_gradient_loss | -0.00119      |
|    std                  | 6.17          |
|    value_loss           | 0.298         |
-------------------------------------------
Iteration: 2310 | Episodes: 94000 | Median Reward: 44.21 | Max Reward: 49.34
Iteration: 2312 | Episodes: 94100 | Median Reward: 47.56 | Max Reward: 49.34
Iteration: 2315 | Episodes: 94200 | Median Reward: 46.98 | Max Reward: 49.34
Iteration: 2317 | Episodes: 94300 | Median Reward: 46.76 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.2         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2320          |
|    time_elapsed         | 33396         |
|    total_timesteps      | 9502720       |
| train/                  |               |
|    approx_kl            | 8.0477155e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -125          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.02         |
|    n_updates            | 23190         |
|    policy_gradient_loss | -0.00027      |
|    std                  | 6.2           |
|    value_loss           | 0.377         |
-------------------------------------------
Iteration: 2320 | Episodes: 94400 | Median Reward: 47.32 | Max Reward: 49.34
Iteration: 2322 | Episodes: 94500 | Median Reward: 47.32 | Max Reward: 49.34
Iteration: 2325 | Episodes: 94600 | Median Reward: 46.81 | Max Reward: 49.34
Iteration: 2327 | Episodes: 94700 | Median Reward: 47.00 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.1         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2330          |
|    time_elapsed         | 33534         |
|    total_timesteps      | 9543680       |
| train/                  |               |
|    approx_kl            | 0.00019893692 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -125          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.06         |
|    n_updates            | 23290         |
|    policy_gradient_loss | -0.000378     |
|    std                  | 6.25          |
|    value_loss           | 0.595         |
-------------------------------------------
Iteration: 2330 | Episodes: 94800 | Median Reward: 47.08 | Max Reward: 49.34
Iteration: 2332 | Episodes: 94900 | Median Reward: 46.33 | Max Reward: 49.34
Iteration: 2334 | Episodes: 95000 | Median Reward: 47.31 | Max Reward: 49.34
Iteration: 2337 | Episodes: 95100 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 2339 | Episodes: 95200 | Median Reward: 46.91 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2340         |
|    time_elapsed         | 33683        |
|    total_timesteps      | 9584640      |
| train/                  |              |
|    approx_kl            | 0.0005165016 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -125         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.96        |
|    n_updates            | 23390        |
|    policy_gradient_loss | -0.000993    |
|    std                  | 6.28         |
|    value_loss           | 0.45         |
------------------------------------------
Iteration: 2342 | Episodes: 95300 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 2344 | Episodes: 95400 | Median Reward: 46.96 | Max Reward: 49.34
Iteration: 2347 | Episodes: 95500 | Median Reward: 47.05 | Max Reward: 49.34
Iteration: 2349 | Episodes: 95600 | Median Reward: 47.40 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53          |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2350         |
|    time_elapsed         | 33828        |
|    total_timesteps      | 9625600      |
| train/                  |              |
|    approx_kl            | 0.0038893125 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -125         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.15        |
|    n_updates            | 23490        |
|    policy_gradient_loss | -0.0121      |
|    std                  | 6.33         |
|    value_loss           | 0.255        |
------------------------------------------
Iteration: 2352 | Episodes: 95700 | Median Reward: 47.31 | Max Reward: 49.34
Iteration: 2354 | Episodes: 95800 | Median Reward: 46.37 | Max Reward: 49.34
Iteration: 2357 | Episodes: 95900 | Median Reward: 47.02 | Max Reward: 49.34
Iteration: 2359 | Episodes: 96000 | Median Reward: 47.25 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2360         |
|    time_elapsed         | 33968        |
|    total_timesteps      | 9666560      |
| train/                  |              |
|    approx_kl            | 8.989958e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -125         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.64        |
|    n_updates            | 23590        |
|    policy_gradient_loss | -0.000201    |
|    std                  | 6.39         |
|    value_loss           | 3.48         |
------------------------------------------
Iteration: 2362 | Episodes: 96100 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 2364 | Episodes: 96200 | Median Reward: 47.12 | Max Reward: 49.34
Iteration: 2366 | Episodes: 96300 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 2369 | Episodes: 96400 | Median Reward: 45.81 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.1        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2370         |
|    time_elapsed         | 34110        |
|    total_timesteps      | 9707520      |
| train/                  |              |
|    approx_kl            | 0.0019276624 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -126         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.2         |
|    n_updates            | 23690        |
|    policy_gradient_loss | -0.00439     |
|    std                  | 6.45         |
|    value_loss           | 0.23         |
------------------------------------------
Iteration: 2371 | Episodes: 96500 | Median Reward: 47.13 | Max Reward: 49.34
Iteration: 2374 | Episodes: 96600 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 2376 | Episodes: 96700 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 2379 | Episodes: 96800 | Median Reward: 47.18 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54         |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 2380        |
|    time_elapsed         | 34249       |
|    total_timesteps      | 9748480     |
| train/                  |             |
|    approx_kl            | 0.001232624 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -126        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.23       |
|    n_updates            | 23790       |
|    policy_gradient_loss | -0.00449    |
|    std                  | 6.49        |
|    value_loss           | 0.199       |
-----------------------------------------
Iteration: 2381 | Episodes: 96900 | Median Reward: 47.50 | Max Reward: 49.34
Iteration: 2384 | Episodes: 97000 | Median Reward: 47.19 | Max Reward: 49.34
Iteration: 2386 | Episodes: 97100 | Median Reward: 45.92 | Max Reward: 49.34
Iteration: 2389 | Episodes: 97200 | Median Reward: 45.23 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99.6          |
|    ep_rew_mean          | -52.8         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2390          |
|    time_elapsed         | 34385         |
|    total_timesteps      | 9789440       |
| train/                  |               |
|    approx_kl            | 0.00082979334 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -126          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.02         |
|    n_updates            | 23890         |
|    policy_gradient_loss | -0.00185      |
|    std                  | 6.56          |
|    value_loss           | 4.65          |
-------------------------------------------
Iteration: 2391 | Episodes: 97300 | Median Reward: 47.27 | Max Reward: 49.34
Iteration: 2394 | Episodes: 97400 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 2396 | Episodes: 97500 | Median Reward: 47.27 | Max Reward: 49.34
Iteration: 2399 | Episodes: 97600 | Median Reward: 44.64 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 2400        |
|    time_elapsed         | 34524       |
|    total_timesteps      | 9830400     |
| train/                  |             |
|    approx_kl            | 0.005322881 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -126        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -6.04       |
|    n_updates            | 23990       |
|    policy_gradient_loss | -0.028      |
|    std                  | 6.6         |
|    value_loss           | 1.07        |
-----------------------------------------
Iteration: 2401 | Episodes: 97700 | Median Reward: 45.24 | Max Reward: 49.34
Iteration: 2403 | Episodes: 97800 | Median Reward: 47.35 | Max Reward: 49.34
Iteration: 2406 | Episodes: 97900 | Median Reward: 47.11 | Max Reward: 49.34
Iteration: 2408 | Episodes: 98000 | Median Reward: 47.33 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.9        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2410         |
|    time_elapsed         | 34662        |
|    total_timesteps      | 9871360      |
| train/                  |              |
|    approx_kl            | 0.0005161146 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -126         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.2         |
|    n_updates            | 24090        |
|    policy_gradient_loss | -0.00188     |
|    std                  | 6.66         |
|    value_loss           | 0.312        |
------------------------------------------
Iteration: 2411 | Episodes: 98100 | Median Reward: 47.26 | Max Reward: 49.34
Iteration: 2413 | Episodes: 98200 | Median Reward: 46.88 | Max Reward: 49.34
Iteration: 2416 | Episodes: 98300 | Median Reward: 46.85 | Max Reward: 49.34
Iteration: 2418 | Episodes: 98400 | Median Reward: 47.14 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.7        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2420         |
|    time_elapsed         | 34798        |
|    total_timesteps      | 9912320      |
| train/                  |              |
|    approx_kl            | 0.0043412754 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -127         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.27        |
|    n_updates            | 24190        |
|    policy_gradient_loss | -0.00916     |
|    std                  | 6.73         |
|    value_loss           | 0.198        |
------------------------------------------
Iteration: 2421 | Episodes: 98500 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 2423 | Episodes: 98600 | Median Reward: 47.25 | Max Reward: 49.34
Iteration: 2426 | Episodes: 98700 | Median Reward: 46.06 | Max Reward: 49.34
Iteration: 2428 | Episodes: 98800 | Median Reward: 47.02 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.6        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2430         |
|    time_elapsed         | 34937        |
|    total_timesteps      | 9953280      |
| train/                  |              |
|    approx_kl            | 0.0028800864 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -127         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.13        |
|    n_updates            | 24290        |
|    policy_gradient_loss | -0.0103      |
|    std                  | 6.78         |
|    value_loss           | 0.294        |
------------------------------------------
Iteration: 2431 | Episodes: 98900 | Median Reward: 47.28 | Max Reward: 49.34
Iteration: 2433 | Episodes: 99000 | Median Reward: 44.40 | Max Reward: 49.34
Iteration: 2435 | Episodes: 99100 | Median Reward: 47.36 | Max Reward: 49.34
Iteration: 2438 | Episodes: 99200 | Median Reward: 45.51 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.3        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2440         |
|    time_elapsed         | 35076        |
|    total_timesteps      | 9994240      |
| train/                  |              |
|    approx_kl            | 0.0037031686 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -127         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.06        |
|    n_updates            | 24390        |
|    policy_gradient_loss | -0.0109      |
|    std                  | 6.82         |
|    value_loss           | 0.315        |
------------------------------------------
Iteration: 2440 | Episodes: 99300 | Median Reward: 47.09 | Max Reward: 49.34
Iteration: 2443 | Episodes: 99400 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 2445 | Episodes: 99500 | Median Reward: 47.13 | Max Reward: 49.34
Iteration: 2448 | Episodes: 99600 | Median Reward: 46.91 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.4        |
| time/                   |              |
|    fps                  | 284          |
|    iterations           | 2450         |
|    time_elapsed         | 35215        |
|    total_timesteps      | 10035200     |
| train/                  |              |
|    approx_kl            | 0.0009099216 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -127         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.02        |
|    n_updates            | 24490        |
|    policy_gradient_loss | -0.00327     |
|    std                  | 6.87         |
|    value_loss           | 0.453        |
------------------------------------------
Iteration: 2450 | Episodes: 99700 | Median Reward: 46.42 | Max Reward: 49.34
Iteration: 2453 | Episodes: 99800 | Median Reward: 46.39 | Max Reward: 49.34
Iteration: 2455 | Episodes: 99900 | Median Reward: 44.00 | Max Reward: 49.34
Iteration: 2458 | Episodes: 100000 | Median Reward: 46.68 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.5         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 2460          |
|    time_elapsed         | 35355         |
|    total_timesteps      | 10076160      |
| train/                  |               |
|    approx_kl            | 0.00019042777 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -127          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.27         |
|    n_updates            | 24590         |
|    policy_gradient_loss | -0.000756     |
|    std                  | 6.92          |
|    value_loss           | 0.298         |
-------------------------------------------
Iteration: 2460 | Episodes: 100100 | Median Reward: 47.20 | Max Reward: 49.34
Iteration: 2463 | Episodes: 100200 | Median Reward: 47.10 | Max Reward: 49.34
Iteration: 2465 | Episodes: 100300 | Median Reward: 44.14 | Max Reward: 49.34
Iteration: 2468 | Episodes: 100400 | Median Reward: 46.31 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.6       |
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 2470        |
|    time_elapsed         | 35495       |
|    total_timesteps      | 10117120    |
| train/                  |             |
|    approx_kl            | 0.001003897 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -127        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.29       |
|    n_updates            | 24690       |
|    policy_gradient_loss | -0.00636    |
|    std                  | 6.94        |
|    value_loss           | 0.312       |
-----------------------------------------
Iteration: 2470 | Episodes: 100500 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 2472 | Episodes: 100600 | Median Reward: 47.60 | Max Reward: 49.34
Iteration: 2475 | Episodes: 100700 | Median Reward: 46.69 | Max Reward: 49.34
Iteration: 2477 | Episodes: 100800 | Median Reward: 47.31 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.9        |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 2480         |
|    time_elapsed         | 35631        |
|    total_timesteps      | 10158080     |
| train/                  |              |
|    approx_kl            | 7.824712e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -127         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.18        |
|    n_updates            | 24790        |
|    policy_gradient_loss | -0.000637    |
|    std                  | 6.97         |
|    value_loss           | 0.66         |
------------------------------------------
Iteration: 2480 | Episodes: 100900 | Median Reward: 45.22 | Max Reward: 49.34
Iteration: 2482 | Episodes: 101000 | Median Reward: 46.26 | Max Reward: 49.34
Iteration: 2485 | Episodes: 101100 | Median Reward: 46.38 | Max Reward: 49.34
Iteration: 2487 | Episodes: 101200 | Median Reward: 46.94 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55          |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 2490         |
|    time_elapsed         | 35701        |
|    total_timesteps      | 10199040     |
| train/                  |              |
|    approx_kl            | 0.0002690604 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -127         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0005       |
|    loss                 | 92           |
|    n_updates            | 24890        |
|    policy_gradient_loss | -0.00113     |
|    std                  | 7.03         |
|    value_loss           | 130          |
------------------------------------------
Iteration: 2490 | Episodes: 101300 | Median Reward: 46.66 | Max Reward: 49.34
Iteration: 2492 | Episodes: 101400 | Median Reward: 46.31 | Max Reward: 49.34
Iteration: 2495 | Episodes: 101500 | Median Reward: 47.36 | Max Reward: 49.34
Iteration: 2497 | Episodes: 101600 | Median Reward: 46.95 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.2         |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 2500          |
|    time_elapsed         | 35846         |
|    total_timesteps      | 10240000      |
| train/                  |               |
|    approx_kl            | 0.00063566223 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -128          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.24         |
|    n_updates            | 24990         |
|    policy_gradient_loss | -0.00312      |
|    std                  | 7.06          |
|    value_loss           | 0.287         |
-------------------------------------------
Iteration: 2500 | Episodes: 101700 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 2502 | Episodes: 101800 | Median Reward: 45.29 | Max Reward: 49.34
Iteration: 2504 | Episodes: 101900 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 2507 | Episodes: 102000 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 2509 | Episodes: 102100 | Median Reward: 46.19 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.4         |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 2510          |
|    time_elapsed         | 35989         |
|    total_timesteps      | 10280960      |
| train/                  |               |
|    approx_kl            | 5.1282826e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -128          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.14         |
|    n_updates            | 25090         |
|    policy_gradient_loss | -0.000214     |
|    std                  | 7.11          |
|    value_loss           | 0.622         |
-------------------------------------------
Iteration: 2512 | Episodes: 102200 | Median Reward: 43.32 | Max Reward: 49.34
Iteration: 2514 | Episodes: 102300 | Median Reward: 47.09 | Max Reward: 49.34
Iteration: 2517 | Episodes: 102400 | Median Reward: 46.57 | Max Reward: 49.34
Iteration: 2519 | Episodes: 102500 | Median Reward: 46.32 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.1         |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 2520          |
|    time_elapsed         | 36129         |
|    total_timesteps      | 10321920      |
| train/                  |               |
|    approx_kl            | 0.00036976952 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -128          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.2          |
|    n_updates            | 25190         |
|    policy_gradient_loss | -0.00252      |
|    std                  | 7.17          |
|    value_loss           | 0.913         |
-------------------------------------------
Iteration: 2522 | Episodes: 102600 | Median Reward: 46.96 | Max Reward: 49.34
Iteration: 2524 | Episodes: 102700 | Median Reward: 44.26 | Max Reward: 49.34
Iteration: 2527 | Episodes: 102800 | Median Reward: 45.93 | Max Reward: 49.34
Iteration: 2529 | Episodes: 102900 | Median Reward: 46.94 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -53.1         |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 2530          |
|    time_elapsed         | 36267         |
|    total_timesteps      | 10362880      |
| train/                  |               |
|    approx_kl            | 4.5464636e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -128          |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.0005        |
|    loss                 | -1.06         |
|    n_updates            | 25290         |
|    policy_gradient_loss | -0.000323     |
|    std                  | 7.22          |
|    value_loss           | 10.6          |
-------------------------------------------
Iteration: 2532 | Episodes: 103000 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 2534 | Episodes: 103100 | Median Reward: 44.98 | Max Reward: 49.34
Iteration: 2536 | Episodes: 103200 | Median Reward: 43.91 | Max Reward: 49.34
Iteration: 2539 | Episodes: 103300 | Median Reward: 46.93 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -54.9         |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 2540          |
|    time_elapsed         | 36402         |
|    total_timesteps      | 10403840      |
| train/                  |               |
|    approx_kl            | 5.3219657e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -128          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.32         |
|    n_updates            | 25390         |
|    policy_gradient_loss | -0.000208     |
|    std                  | 7.28          |
|    value_loss           | 8.01          |
-------------------------------------------
Iteration: 2541 | Episodes: 103400 | Median Reward: 47.01 | Max Reward: 49.34
Iteration: 2544 | Episodes: 103500 | Median Reward: 46.33 | Max Reward: 49.34
Iteration: 2546 | Episodes: 103600 | Median Reward: 46.37 | Max Reward: 49.34
Iteration: 2549 | Episodes: 103700 | Median Reward: 47.21 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.9         |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 2550          |
|    time_elapsed         | 36540         |
|    total_timesteps      | 10444800      |
| train/                  |               |
|    approx_kl            | 0.00022432787 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -128          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.31         |
|    n_updates            | 25490         |
|    policy_gradient_loss | -0.00192      |
|    std                  | 7.32          |
|    value_loss           | 0.594         |
-------------------------------------------
Iteration: 2551 | Episodes: 103800 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 2554 | Episodes: 103900 | Median Reward: 47.26 | Max Reward: 49.34
Iteration: 2556 | Episodes: 104000 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 2559 | Episodes: 104100 | Median Reward: 47.24 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.3         |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 2560          |
|    time_elapsed         | 36673         |
|    total_timesteps      | 10485760      |
| train/                  |               |
|    approx_kl            | 0.00018704515 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -129          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.31         |
|    n_updates            | 25590         |
|    policy_gradient_loss | -0.000812     |
|    std                  | 7.37          |
|    value_loss           | 0.243         |
-------------------------------------------
Iteration: 2561 | Episodes: 104200 | Median Reward: 43.96 | Max Reward: 49.34
Iteration: 2564 | Episodes: 104300 | Median Reward: 44.06 | Max Reward: 49.34
Iteration: 2566 | Episodes: 104400 | Median Reward: 43.95 | Max Reward: 49.34
Iteration: 2569 | Episodes: 104500 | Median Reward: 47.49 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 2570         |
|    time_elapsed         | 36809        |
|    total_timesteps      | 10526720     |
| train/                  |              |
|    approx_kl            | 0.0017983711 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -129         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.35        |
|    n_updates            | 25690        |
|    policy_gradient_loss | -0.00983     |
|    std                  | 7.43         |
|    value_loss           | 0.321        |
------------------------------------------
Iteration: 2571 | Episodes: 104600 | Median Reward: 46.88 | Max Reward: 49.34
Iteration: 2573 | Episodes: 104700 | Median Reward: 47.01 | Max Reward: 49.34
Iteration: 2576 | Episodes: 104800 | Median Reward: 46.36 | Max Reward: 49.34
Iteration: 2578 | Episodes: 104900 | Median Reward: 43.83 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.1       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 2580        |
|    time_elapsed         | 36947       |
|    total_timesteps      | 10567680    |
| train/                  |             |
|    approx_kl            | 0.015618354 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -129        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -6.3        |
|    n_updates            | 25790       |
|    policy_gradient_loss | -0.0352     |
|    std                  | 7.49        |
|    value_loss           | 0.463       |
-----------------------------------------
Iteration: 2581 | Episodes: 105000 | Median Reward: 43.07 | Max Reward: 49.34
Iteration: 2583 | Episodes: 105100 | Median Reward: 46.33 | Max Reward: 49.34
Iteration: 2586 | Episodes: 105200 | Median Reward: 47.23 | Max Reward: 49.34
Iteration: 2588 | Episodes: 105300 | Median Reward: 47.42 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.9        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2590         |
|    time_elapsed         | 37083        |
|    total_timesteps      | 10608640     |
| train/                  |              |
|    approx_kl            | 0.0003368472 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -129         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.39        |
|    n_updates            | 25890        |
|    policy_gradient_loss | -0.000404    |
|    std                  | 7.54         |
|    value_loss           | 0.267        |
------------------------------------------
Iteration: 2591 | Episodes: 105400 | Median Reward: 43.74 | Max Reward: 49.34
Iteration: 2593 | Episodes: 105500 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 2596 | Episodes: 105600 | Median Reward: 47.18 | Max Reward: 49.34
Iteration: 2598 | Episodes: 105700 | Median Reward: 46.86 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.6         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2600          |
|    time_elapsed         | 37221         |
|    total_timesteps      | 10649600      |
| train/                  |               |
|    approx_kl            | 0.00034357753 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -129          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.73         |
|    n_updates            | 25990         |
|    policy_gradient_loss | 5.92e-05      |
|    std                  | 7.64          |
|    value_loss           | 1.21          |
-------------------------------------------
Iteration: 2601 | Episodes: 105800 | Median Reward: 43.93 | Max Reward: 49.34
Iteration: 2603 | Episodes: 105900 | Median Reward: 44.46 | Max Reward: 49.34
Iteration: 2605 | Episodes: 106000 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 2608 | Episodes: 106100 | Median Reward: 46.92 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54          |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2610         |
|    time_elapsed         | 37358        |
|    total_timesteps      | 10690560     |
| train/                  |              |
|    approx_kl            | 0.0014917991 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -129         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.44        |
|    n_updates            | 26090        |
|    policy_gradient_loss | -0.00363     |
|    std                  | 7.7          |
|    value_loss           | 0.145        |
------------------------------------------
Iteration: 2610 | Episodes: 106200 | Median Reward: 46.55 | Max Reward: 49.34
Iteration: 2613 | Episodes: 106300 | Median Reward: 43.47 | Max Reward: 49.34
Iteration: 2615 | Episodes: 106400 | Median Reward: 47.46 | Max Reward: 49.34
Iteration: 2618 | Episodes: 106500 | Median Reward: 47.22 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.8       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 2620        |
|    time_elapsed         | 37493       |
|    total_timesteps      | 10731520    |
| train/                  |             |
|    approx_kl            | 0.000860599 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -130        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.21       |
|    n_updates            | 26190       |
|    policy_gradient_loss | -0.00399    |
|    std                  | 7.76        |
|    value_loss           | 0.448       |
-----------------------------------------
Iteration: 2620 | Episodes: 106600 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 2623 | Episodes: 106700 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 2625 | Episodes: 106800 | Median Reward: 47.00 | Max Reward: 49.34
Iteration: 2628 | Episodes: 106900 | Median Reward: 46.69 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.2         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2630          |
|    time_elapsed         | 37630         |
|    total_timesteps      | 10772480      |
| train/                  |               |
|    approx_kl            | 0.00014788029 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -130          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.36         |
|    n_updates            | 26290         |
|    policy_gradient_loss | -0.000719     |
|    std                  | 7.82          |
|    value_loss           | 0.425         |
-------------------------------------------
Iteration: 2630 | Episodes: 107000 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 2633 | Episodes: 107100 | Median Reward: 47.16 | Max Reward: 49.34
Iteration: 2635 | Episodes: 107200 | Median Reward: 46.96 | Max Reward: 49.34
Iteration: 2638 | Episodes: 107300 | Median Reward: 46.61 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.6        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2640         |
|    time_elapsed         | 37767        |
|    total_timesteps      | 10813440     |
| train/                  |              |
|    approx_kl            | 0.0005487504 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -130         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.2         |
|    n_updates            | 26390        |
|    policy_gradient_loss | -0.00202     |
|    std                  | 7.87         |
|    value_loss           | 0.323        |
------------------------------------------
Iteration: 2640 | Episodes: 107400 | Median Reward: 47.11 | Max Reward: 49.34
Iteration: 2642 | Episodes: 107500 | Median Reward: 46.69 | Max Reward: 49.34
Iteration: 2645 | Episodes: 107600 | Median Reward: 47.12 | Max Reward: 49.34
Iteration: 2647 | Episodes: 107700 | Median Reward: 46.46 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.7        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2650         |
|    time_elapsed         | 37902        |
|    total_timesteps      | 10854400     |
| train/                  |              |
|    approx_kl            | 0.0020685247 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -130         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.41        |
|    n_updates            | 26490        |
|    policy_gradient_loss | -0.00523     |
|    std                  | 7.94         |
|    value_loss           | 0.184        |
------------------------------------------
Iteration: 2650 | Episodes: 107800 | Median Reward: 46.66 | Max Reward: 49.34
Iteration: 2652 | Episodes: 107900 | Median Reward: 44.36 | Max Reward: 49.34
Iteration: 2655 | Episodes: 108000 | Median Reward: 47.42 | Max Reward: 49.34
Iteration: 2657 | Episodes: 108100 | Median Reward: 46.28 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55          |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2660         |
|    time_elapsed         | 38038        |
|    total_timesteps      | 10895360     |
| train/                  |              |
|    approx_kl            | 3.873861e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -130         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.45        |
|    n_updates            | 26590        |
|    policy_gradient_loss | 0.000146     |
|    std                  | 8.02         |
|    value_loss           | 0.121        |
------------------------------------------
Iteration: 2660 | Episodes: 108200 | Median Reward: 44.51 | Max Reward: 49.34
Iteration: 2662 | Episodes: 108300 | Median Reward: 47.14 | Max Reward: 49.34
Iteration: 2665 | Episodes: 108400 | Median Reward: 47.11 | Max Reward: 49.34
Iteration: 2667 | Episodes: 108500 | Median Reward: 46.82 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.5        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2670         |
|    time_elapsed         | 38176        |
|    total_timesteps      | 10936320     |
| train/                  |              |
|    approx_kl            | 0.0033751964 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -130         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.46        |
|    n_updates            | 26690        |
|    policy_gradient_loss | -0.00591     |
|    std                  | 8.07         |
|    value_loss           | 0.234        |
------------------------------------------
Iteration: 2670 | Episodes: 108600 | Median Reward: 46.81 | Max Reward: 49.34
Iteration: 2672 | Episodes: 108700 | Median Reward: 46.56 | Max Reward: 49.34
Iteration: 2674 | Episodes: 108800 | Median Reward: 47.42 | Max Reward: 49.34
Iteration: 2677 | Episodes: 108900 | Median Reward: 47.55 | Max Reward: 49.34
Iteration: 2679 | Episodes: 109000 | Median Reward: 45.89 | Max Reward: 49.34
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -55       |
| time/                   |           |
|    fps                  | 286       |
|    iterations           | 2680      |
|    time_elapsed         | 38313     |
|    total_timesteps      | 10977280  |
| train/                  |           |
|    approx_kl            | 0.0053838 |
|    clip_fraction        | 0         |
|    clip_range           | 0.4       |
|    entropy_loss         | -130      |
|    explained_variance   | 1         |
|    learning_rate        | 0.0005    |
|    loss                 | -6.43     |
|    n_updates            | 26790     |
|    policy_gradient_loss | -0.0139   |
|    std                  | 8.13      |
|    value_loss           | 0.352     |
---------------------------------------
Iteration: 2682 | Episodes: 109100 | Median Reward: 45.58 | Max Reward: 49.34
Iteration: 2684 | Episodes: 109200 | Median Reward: 47.40 | Max Reward: 49.34
Iteration: 2687 | Episodes: 109300 | Median Reward: 45.16 | Max Reward: 49.34
Iteration: 2689 | Episodes: 109400 | Median Reward: 44.43 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.3         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2690          |
|    time_elapsed         | 38452         |
|    total_timesteps      | 11018240      |
| train/                  |               |
|    approx_kl            | 0.00038517517 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.45         |
|    n_updates            | 26890         |
|    policy_gradient_loss | -0.00202      |
|    std                  | 8.17          |
|    value_loss           | 0.328         |
-------------------------------------------
Iteration: 2692 | Episodes: 109500 | Median Reward: 46.68 | Max Reward: 49.34
Iteration: 2694 | Episodes: 109600 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 2697 | Episodes: 109700 | Median Reward: 45.14 | Max Reward: 49.34
Iteration: 2699 | Episodes: 109800 | Median Reward: 46.59 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.1        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2700         |
|    time_elapsed         | 38590        |
|    total_timesteps      | 11059200     |
| train/                  |              |
|    approx_kl            | 0.0016222212 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.39        |
|    n_updates            | 26990        |
|    policy_gradient_loss | -0.00467     |
|    std                  | 8.21         |
|    value_loss           | 0.315        |
------------------------------------------
Iteration: 2702 | Episodes: 109900 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 2704 | Episodes: 110000 | Median Reward: 44.02 | Max Reward: 49.34
Iteration: 2706 | Episodes: 110100 | Median Reward: 47.19 | Max Reward: 49.34
Iteration: 2709 | Episodes: 110200 | Median Reward: 47.31 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.6        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2710         |
|    time_elapsed         | 38728        |
|    total_timesteps      | 11100160     |
| train/                  |              |
|    approx_kl            | 0.0008803791 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.43        |
|    n_updates            | 27090        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 8.26         |
|    value_loss           | 0.153        |
------------------------------------------
Iteration: 2711 | Episodes: 110300 | Median Reward: 47.15 | Max Reward: 49.34
aIteration: 2714 | Episodes: 110400 | Median Reward: 44.91 | Max Reward: 49.34
Iteration: 2716 | Episodes: 110500 | Median Reward: 47.11 | Max Reward: 49.34
Iteration: 2719 | Episodes: 110600 | Median Reward: 47.36 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | -53.7       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 2720        |
|    time_elapsed         | 38866       |
|    total_timesteps      | 11141120    |
| train/                  |             |
|    approx_kl            | 0.004542656 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -131        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.51       |
|    n_updates            | 27190       |
|    policy_gradient_loss | -0.00839    |
|    std                  | 8.31        |
|    value_loss           | 0.148       |
-----------------------------------------
Iteration: 2721 | Episodes: 110700 | Median Reward: 46.28 | Max Reward: 49.34
Iteration: 2724 | Episodes: 110800 | Median Reward: 47.20 | Max Reward: 49.34
Iteration: 2726 | Episodes: 110900 | Median Reward: 47.52 | Max Reward: 49.34
Iteration: 2729 | Episodes: 111000 | Median Reward: 47.00 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -53.6         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2730          |
|    time_elapsed         | 39003         |
|    total_timesteps      | 11182080      |
| train/                  |               |
|    approx_kl            | 3.4730765e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.03         |
|    n_updates            | 27290         |
|    policy_gradient_loss | -8.45e-06     |
|    std                  | 8.34          |
|    value_loss           | 5.13          |
-------------------------------------------
Iteration: 2731 | Episodes: 111100 | Median Reward: 46.97 | Max Reward: 49.34
Iteration: 2734 | Episodes: 111200 | Median Reward: 45.92 | Max Reward: 49.34
Iteration: 2736 | Episodes: 111300 | Median Reward: 46.50 | Max Reward: 49.34
Iteration: 2738 | Episodes: 111400 | Median Reward: 46.43 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.2        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2740         |
|    time_elapsed         | 39143        |
|    total_timesteps      | 11223040     |
| train/                  |              |
|    approx_kl            | 0.0006295728 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.5         |
|    n_updates            | 27390        |
|    policy_gradient_loss | -0.00294     |
|    std                  | 8.39         |
|    value_loss           | 0.317        |
------------------------------------------
Iteration: 2741 | Episodes: 111500 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 2743 | Episodes: 111600 | Median Reward: 45.54 | Max Reward: 49.34
Iteration: 2746 | Episodes: 111700 | Median Reward: 47.08 | Max Reward: 49.34
Iteration: 2748 | Episodes: 111800 | Median Reward: 46.99 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.1         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2750          |
|    time_elapsed         | 39283         |
|    total_timesteps      | 11264000      |
| train/                  |               |
|    approx_kl            | 0.00013985197 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.52         |
|    n_updates            | 27490         |
|    policy_gradient_loss | -0.00107      |
|    std                  | 8.43          |
|    value_loss           | 0.322         |
-------------------------------------------
Iteration: 2751 | Episodes: 111900 | Median Reward: 44.14 | Max Reward: 49.34
Iteration: 2753 | Episodes: 112000 | Median Reward: 46.41 | Max Reward: 49.34
Iteration: 2756 | Episodes: 112100 | Median Reward: 47.25 | Max Reward: 49.34
Iteration: 2758 | Episodes: 112200 | Median Reward: 47.27 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.1        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2760         |
|    time_elapsed         | 39424        |
|    total_timesteps      | 11304960     |
| train/                  |              |
|    approx_kl            | 0.0017266632 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.49        |
|    n_updates            | 27590        |
|    policy_gradient_loss | -0.00581     |
|    std                  | 8.45         |
|    value_loss           | 0.189        |
------------------------------------------
Iteration: 2761 | Episodes: 112300 | Median Reward: 47.34 | Max Reward: 49.34
Iteration: 2763 | Episodes: 112400 | Median Reward: 46.75 | Max Reward: 49.34
Iteration: 2766 | Episodes: 112500 | Median Reward: 47.52 | Max Reward: 49.34
Iteration: 2768 | Episodes: 112600 | Median Reward: 47.18 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.6         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2770          |
|    time_elapsed         | 39563         |
|    total_timesteps      | 11345920      |
| train/                  |               |
|    approx_kl            | 0.00025345443 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.49         |
|    n_updates            | 27690         |
|    policy_gradient_loss | -0.00211      |
|    std                  | 8.51          |
|    value_loss           | 0.366         |
-------------------------------------------
Iteration: 2770 | Episodes: 112700 | Median Reward: 45.29 | Max Reward: 49.34
Iteration: 2773 | Episodes: 112800 | Median Reward: 47.38 | Max Reward: 49.34
Iteration: 2775 | Episodes: 112900 | Median Reward: 46.68 | Max Reward: 49.34
Iteration: 2778 | Episodes: 113000 | Median Reward: 47.21 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.2        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2780         |
|    time_elapsed         | 39702        |
|    total_timesteps      | 11386880     |
| train/                  |              |
|    approx_kl            | 0.0010395532 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -132         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.51        |
|    n_updates            | 27790        |
|    policy_gradient_loss | -0.00614     |
|    std                  | 8.57         |
|    value_loss           | 0.214        |
------------------------------------------
Iteration: 2780 | Episodes: 113100 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 2783 | Episodes: 113200 | Median Reward: 46.85 | Max Reward: 49.34
Iteration: 2785 | Episodes: 113300 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 2788 | Episodes: 113400 | Median Reward: 47.21 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.9         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2790          |
|    time_elapsed         | 39840         |
|    total_timesteps      | 11427840      |
| train/                  |               |
|    approx_kl            | 0.00035207713 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -132          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.45         |
|    n_updates            | 27890         |
|    policy_gradient_loss | -0.00148      |
|    std                  | 8.68          |
|    value_loss           | 0.225         |
-------------------------------------------
Iteration: 2790 | Episodes: 113500 | Median Reward: 44.27 | Max Reward: 49.34
Iteration: 2793 | Episodes: 113600 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 2795 | Episodes: 113700 | Median Reward: 44.77 | Max Reward: 49.34
Iteration: 2798 | Episodes: 113800 | Median Reward: 47.04 | Max Reward: 49.34
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 100            |
|    ep_rew_mean          | -52.9          |
| time/                   |                |
|    fps                  | 286            |
|    iterations           | 2800           |
|    time_elapsed         | 39979          |
|    total_timesteps      | 11468800       |
| train/                  |                |
|    approx_kl            | 0.000106134336 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -132           |
|    explained_variance   | 0.994          |
|    learning_rate        | 0.0005         |
|    loss                 | -4.45          |
|    n_updates            | 27990          |
|    policy_gradient_loss | -0.000383      |
|    std                  | 8.75           |
|    value_loss           | 4.25           |
--------------------------------------------
Iteration: 2800 | Episodes: 113900 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 2802 | Episodes: 114000 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 2805 | Episodes: 114100 | Median Reward: 47.10 | Max Reward: 49.34
Iteration: 2807 | Episodes: 114200 | Median Reward: 46.69 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | -54.4       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 2810        |
|    time_elapsed         | 40117       |
|    total_timesteps      | 11509760    |
| train/                  |             |
|    approx_kl            | 3.13888e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -132        |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0005      |
|    loss                 | -2.59       |
|    n_updates            | 28090       |
|    policy_gradient_loss | -0.000237   |
|    std                  | 8.84        |
|    value_loss           | 7.99        |
-----------------------------------------
Iteration: 2810 | Episodes: 114300 | Median Reward: 46.42 | Max Reward: 49.34
Iteration: 2812 | Episodes: 114400 | Median Reward: 47.18 | Max Reward: 49.34
Iteration: 2815 | Episodes: 114500 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 2817 | Episodes: 114600 | Median Reward: 47.17 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.5         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2820          |
|    time_elapsed         | 40257         |
|    total_timesteps      | 11550720      |
| train/                  |               |
|    approx_kl            | 0.00012106681 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -132          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.32         |
|    n_updates            | 28190         |
|    policy_gradient_loss | -0.000329     |
|    std                  | 8.89          |
|    value_loss           | 0.309         |
-------------------------------------------
Iteration: 2820 | Episodes: 114700 | Median Reward: 47.08 | Max Reward: 49.34
Iteration: 2822 | Episodes: 114800 | Median Reward: 45.73 | Max Reward: 49.34
Iteration: 2825 | Episodes: 114900 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 2827 | Episodes: 115000 | Median Reward: 46.96 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.1        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2830         |
|    time_elapsed         | 40394        |
|    total_timesteps      | 11591680     |
| train/                  |              |
|    approx_kl            | 0.0008680533 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -132         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.54        |
|    n_updates            | 28290        |
|    policy_gradient_loss | -0.00397     |
|    std                  | 8.96         |
|    value_loss           | 0.223        |
------------------------------------------
Iteration: 2830 | Episodes: 115100 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 2832 | Episodes: 115200 | Median Reward: 47.30 | Max Reward: 49.34
Iteration: 2834 | Episodes: 115300 | Median Reward: 47.44 | Max Reward: 49.34
Iteration: 2837 | Episodes: 115400 | Median Reward: 47.11 | Max Reward: 49.34
Iteration: 2839 | Episodes: 115500 | Median Reward: 47.32 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.7         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2840          |
|    time_elapsed         | 40537         |
|    total_timesteps      | 11632640      |
| train/                  |               |
|    approx_kl            | 0.00021984067 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -132          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.55         |
|    n_updates            | 28390         |
|    policy_gradient_loss | -0.00138      |
|    std                  | 9.02          |
|    value_loss           | 0.143         |
-------------------------------------------
Iteration: 2842 | Episodes: 115600 | Median Reward: 44.95 | Max Reward: 49.34
Iteration: 2844 | Episodes: 115700 | Median Reward: 44.06 | Max Reward: 49.34
Iteration: 2847 | Episodes: 115800 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 2849 | Episodes: 115900 | Median Reward: 46.24 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.8         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2850          |
|    time_elapsed         | 40681         |
|    total_timesteps      | 11673600      |
| train/                  |               |
|    approx_kl            | 0.00038076582 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.5          |
|    n_updates            | 28490         |
|    policy_gradient_loss | -0.00137      |
|    std                  | 9.09          |
|    value_loss           | 0.194         |
-------------------------------------------
Iteration: 2852 | Episodes: 116000 | Median Reward: 47.07 | Max Reward: 49.34
Iteration: 2854 | Episodes: 116100 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 2857 | Episodes: 116200 | Median Reward: 47.13 | Max Reward: 49.34
Iteration: 2859 | Episodes: 116300 | Median Reward: 46.39 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.4         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2860          |
|    time_elapsed         | 40824         |
|    total_timesteps      | 11714560      |
| train/                  |               |
|    approx_kl            | 0.00045120023 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.6          |
|    n_updates            | 28590         |
|    policy_gradient_loss | -0.00143      |
|    std                  | 9.14          |
|    value_loss           | 0.131         |
-------------------------------------------
Iteration: 2862 | Episodes: 116400 | Median Reward: 44.35 | Max Reward: 49.34
Iteration: 2864 | Episodes: 116500 | Median Reward: 46.55 | Max Reward: 49.34
Iteration: 2867 | Episodes: 116600 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 2869 | Episodes: 116700 | Median Reward: 46.91 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.6        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2870         |
|    time_elapsed         | 40968        |
|    total_timesteps      | 11755520     |
| train/                  |              |
|    approx_kl            | 0.0005679552 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -133         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.29        |
|    n_updates            | 28690        |
|    policy_gradient_loss | -0.00112     |
|    std                  | 9.18         |
|    value_loss           | 0.42         |
------------------------------------------
Iteration: 2871 | Episodes: 116800 | Median Reward: 47.03 | Max Reward: 49.34
Iteration: 2874 | Episodes: 116900 | Median Reward: 47.12 | Max Reward: 49.34
Iteration: 2876 | Episodes: 117000 | Median Reward: 46.30 | Max Reward: 49.34
Iteration: 2879 | Episodes: 117100 | Median Reward: 46.33 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.6        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2880         |
|    time_elapsed         | 41112        |
|    total_timesteps      | 11796480     |
| train/                  |              |
|    approx_kl            | 3.498151e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -133         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.58        |
|    n_updates            | 28790        |
|    policy_gradient_loss | -5.76e-05    |
|    std                  | 9.24         |
|    value_loss           | 0.434        |
------------------------------------------
Iteration: 2881 | Episodes: 117200 | Median Reward: 47.08 | Max Reward: 49.34
Iteration: 2884 | Episodes: 117300 | Median Reward: 46.64 | Max Reward: 49.34
Iteration: 2886 | Episodes: 117400 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 2889 | Episodes: 117500 | Median Reward: 46.54 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.6       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 2890        |
|    time_elapsed         | 41257       |
|    total_timesteps      | 11837440    |
| train/                  |             |
|    approx_kl            | 0.018918686 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -133        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.7        |
|    n_updates            | 28890       |
|    policy_gradient_loss | -0.038      |
|    std                  | 9.26        |
|    value_loss           | 0.12        |
-----------------------------------------
Iteration: 2891 | Episodes: 117600 | Median Reward: 46.54 | Max Reward: 49.34
Iteration: 2894 | Episodes: 117700 | Median Reward: 45.60 | Max Reward: 49.34
Iteration: 2896 | Episodes: 117800 | Median Reward: 46.66 | Max Reward: 49.34
Iteration: 2899 | Episodes: 117900 | Median Reward: 46.89 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.5        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2900         |
|    time_elapsed         | 41401        |
|    total_timesteps      | 11878400     |
| train/                  |              |
|    approx_kl            | 6.645563e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -133         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.53        |
|    n_updates            | 28990        |
|    policy_gradient_loss | -0.000307    |
|    std                  | 9.34         |
|    value_loss           | 0.213        |
------------------------------------------
Iteration: 2901 | Episodes: 118000 | Median Reward: 43.98 | Max Reward: 49.34
Iteration: 2903 | Episodes: 118100 | Median Reward: 43.95 | Max Reward: 49.34
Iteration: 2906 | Episodes: 118200 | Median Reward: 46.87 | Max Reward: 49.34
Iteration: 2908 | Episodes: 118300 | Median Reward: 46.32 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.9        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2910         |
|    time_elapsed         | 41544        |
|    total_timesteps      | 11919360     |
| train/                  |              |
|    approx_kl            | 7.564796e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -133         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.59        |
|    n_updates            | 29090        |
|    policy_gradient_loss | -0.000609    |
|    std                  | 9.39         |
|    value_loss           | 0.556        |
------------------------------------------
Iteration: 2911 | Episodes: 118400 | Median Reward: 46.58 | Max Reward: 49.34
Iteration: 2913 | Episodes: 118500 | Median Reward: 46.41 | Max Reward: 49.34
Iteration: 2916 | Episodes: 118600 | Median Reward: 47.26 | Max Reward: 49.34
Iteration: 2918 | Episodes: 118700 | Median Reward: 45.50 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.9        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2920         |
|    time_elapsed         | 41691        |
|    total_timesteps      | 11960320     |
| train/                  |              |
|    approx_kl            | 0.0055462485 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -133         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.65        |
|    n_updates            | 29190        |
|    policy_gradient_loss | -0.0126      |
|    std                  | 9.42         |
|    value_loss           | 0.0957       |
------------------------------------------
Iteration: 2921 | Episodes: 118800 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 2923 | Episodes: 118900 | Median Reward: 46.56 | Max Reward: 49.34
Iteration: 2926 | Episodes: 119000 | Median Reward: 46.80 | Max Reward: 49.34
Iteration: 2928 | Episodes: 119100 | Median Reward: 46.11 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54          |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2930         |
|    time_elapsed         | 41835        |
|    total_timesteps      | 12001280     |
| train/                  |              |
|    approx_kl            | 0.0005872749 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -133         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.53        |
|    n_updates            | 29290        |
|    policy_gradient_loss | -0.00292     |
|    std                  | 9.49         |
|    value_loss           | 0.241        |
------------------------------------------
Iteration: 2931 | Episodes: 119200 | Median Reward: 46.54 | Max Reward: 49.34
Iteration: 2933 | Episodes: 119300 | Median Reward: 46.66 | Max Reward: 49.34
Iteration: 2935 | Episodes: 119400 | Median Reward: 43.65 | Max Reward: 49.34
Iteration: 2938 | Episodes: 119500 | Median Reward: 46.06 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.6         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2940          |
|    time_elapsed         | 41979         |
|    total_timesteps      | 12042240      |
| train/                  |               |
|    approx_kl            | 0.00088963157 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.64         |
|    n_updates            | 29390         |
|    policy_gradient_loss | -0.0013       |
|    std                  | 9.57          |
|    value_loss           | 0.103         |
-------------------------------------------
Iteration: 2940 | Episodes: 119600 | Median Reward: 45.44 | Max Reward: 49.34
Iteration: 2943 | Episodes: 119700 | Median Reward: 46.43 | Max Reward: 49.34
Iteration: 2945 | Episodes: 119800 | Median Reward: 47.57 | Max Reward: 49.34
Iteration: 2948 | Episodes: 119900 | Median Reward: 46.35 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55         |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 2950        |
|    time_elapsed         | 42123       |
|    total_timesteps      | 12083200    |
| train/                  |             |
|    approx_kl            | 0.007783259 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -134        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.69       |
|    n_updates            | 29490       |
|    policy_gradient_loss | -0.023      |
|    std                  | 9.64        |
|    value_loss           | 0.135       |
-----------------------------------------
Iteration: 2950 | Episodes: 120000 | Median Reward: 44.29 | Max Reward: 49.34
Iteration: 2953 | Episodes: 120100 | Median Reward: 44.05 | Max Reward: 49.34
Iteration: 2955 | Episodes: 120200 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 2958 | Episodes: 120300 | Median Reward: 47.01 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.7         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2960          |
|    time_elapsed         | 42266         |
|    total_timesteps      | 12124160      |
| train/                  |               |
|    approx_kl            | 0.00053285604 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.4          |
|    n_updates            | 29590         |
|    policy_gradient_loss | -0.00468      |
|    std                  | 9.73          |
|    value_loss           | 0.251         |
-------------------------------------------
Iteration: 2960 | Episodes: 120400 | Median Reward: 44.88 | Max Reward: 49.34
Iteration: 2963 | Episodes: 120500 | Median Reward: 46.23 | Max Reward: 49.34
Iteration: 2965 | Episodes: 120600 | Median Reward: 45.92 | Max Reward: 49.34
Iteration: 2968 | Episodes: 120700 | Median Reward: 47.33 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -53.1         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2970          |
|    time_elapsed         | 42404         |
|    total_timesteps      | 12165120      |
| train/                  |               |
|    approx_kl            | 0.00079912547 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.52         |
|    n_updates            | 29690         |
|    policy_gradient_loss | -0.00315      |
|    std                  | 9.76          |
|    value_loss           | 0.328         |
-------------------------------------------
Iteration: 2970 | Episodes: 120800 | Median Reward: 47.46 | Max Reward: 49.34
Iteration: 2972 | Episodes: 120900 | Median Reward: 46.97 | Max Reward: 49.34
Iteration: 2975 | Episodes: 121000 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 2977 | Episodes: 121100 | Median Reward: 46.37 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.1        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 2980         |
|    time_elapsed         | 42537        |
|    total_timesteps      | 12206080     |
| train/                  |              |
|    approx_kl            | 0.0001750958 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -134         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.64        |
|    n_updates            | 29790        |
|    policy_gradient_loss | -0.000662    |
|    std                  | 9.83         |
|    value_loss           | 0.302        |
------------------------------------------
Iteration: 2980 | Episodes: 121200 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 2982 | Episodes: 121300 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 2985 | Episodes: 121400 | Median Reward: 46.68 | Max Reward: 49.34
Iteration: 2987 | Episodes: 121500 | Median Reward: 44.02 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.3         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 2990          |
|    time_elapsed         | 42681         |
|    total_timesteps      | 12247040      |
| train/                  |               |
|    approx_kl            | 0.00073384354 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.69         |
|    n_updates            | 29890         |
|    policy_gradient_loss | -0.00288      |
|    std                  | 9.9           |
|    value_loss           | 0.477         |
-------------------------------------------
Iteration: 2990 | Episodes: 121600 | Median Reward: 47.35 | Max Reward: 49.34
Iteration: 2992 | Episodes: 121700 | Median Reward: 46.55 | Max Reward: 49.34
Iteration: 2995 | Episodes: 121800 | Median Reward: 47.42 | Max Reward: 49.34
Iteration: 2997 | Episodes: 121900 | Median Reward: 43.91 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -53.3         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 3000          |
|    time_elapsed         | 42822         |
|    total_timesteps      | 12288000      |
| train/                  |               |
|    approx_kl            | 0.00020468817 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -5.7          |
|    n_updates            | 29990         |
|    policy_gradient_loss | -9.33e-05     |
|    std                  | 9.96          |
|    value_loss           | 0.709         |
-------------------------------------------
Iteration: 3000 | Episodes: 122000 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 3002 | Episodes: 122100 | Median Reward: 46.84 | Max Reward: 49.34
Iteration: 3004 | Episodes: 122200 | Median Reward: 46.35 | Max Reward: 49.34
Iteration: 3007 | Episodes: 122300 | Median Reward: 45.48 | Max Reward: 49.34
Iteration: 3009 | Episodes: 122400 | Median Reward: 47.22 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.6         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 3010          |
|    time_elapsed         | 42963         |
|    total_timesteps      | 12328960      |
| train/                  |               |
|    approx_kl            | 1.6843143e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -135          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.62         |
|    n_updates            | 30090         |
|    policy_gradient_loss | -0.000346     |
|    std                  | 10            |
|    value_loss           | 0.748         |
-------------------------------------------
Iteration: 3012 | Episodes: 122500 | Median Reward: 46.98 | Max Reward: 49.34
Iteration: 3014 | Episodes: 122600 | Median Reward: 47.35 | Max Reward: 49.34
Iteration: 3017 | Episodes: 122700 | Median Reward: 46.08 | Max Reward: 49.34
Iteration: 3019 | Episodes: 122800 | Median Reward: 45.88 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.8        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 3020         |
|    time_elapsed         | 43105        |
|    total_timesteps      | 12369920     |
| train/                  |              |
|    approx_kl            | 0.0015933653 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -135         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.67        |
|    n_updates            | 30190        |
|    policy_gradient_loss | -0.00364     |
|    std                  | 10           |
|    value_loss           | 0.16         |
------------------------------------------
Iteration: 3022 | Episodes: 122900 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 3024 | Episodes: 123000 | Median Reward: 46.98 | Max Reward: 49.34
Iteration: 3027 | Episodes: 123100 | Median Reward: 47.33 | Max Reward: 49.34
Iteration: 3029 | Episodes: 123200 | Median Reward: 46.38 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.8       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 3030        |
|    time_elapsed         | 43246       |
|    total_timesteps      | 12410880    |
| train/                  |             |
|    approx_kl            | 0.004962994 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -135        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.74       |
|    n_updates            | 30290       |
|    policy_gradient_loss | -0.017      |
|    std                  | 10.1        |
|    value_loss           | 0.114       |
-----------------------------------------
Iteration: 3032 | Episodes: 123300 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 3034 | Episodes: 123400 | Median Reward: 45.16 | Max Reward: 49.34
Iteration: 3036 | Episodes: 123500 | Median Reward: 47.32 | Max Reward: 49.34
Iteration: 3039 | Episodes: 123600 | Median Reward: 45.67 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -54.3        |
| time/                   |              |
|    fps                  | 286          |
|    iterations           | 3040         |
|    time_elapsed         | 43388        |
|    total_timesteps      | 12451840     |
| train/                  |              |
|    approx_kl            | 0.0001409531 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -135         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0005       |
|    loss                 | -3.8         |
|    n_updates            | 30390        |
|    policy_gradient_loss | -0.00077     |
|    std                  | 10.2         |
|    value_loss           | 5.51         |
------------------------------------------
Iteration: 3041 | Episodes: 123700 | Median Reward: 46.04 | Max Reward: 49.34
Iteration: 3044 | Episodes: 123800 | Median Reward: 46.68 | Max Reward: 49.34
Iteration: 3046 | Episodes: 123900 | Median Reward: 44.00 | Max Reward: 49.34
Iteration: 3049 | Episodes: 124000 | Median Reward: 45.86 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.8       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 3050        |
|    time_elapsed         | 43532       |
|    total_timesteps      | 12492800    |
| train/                  |             |
|    approx_kl            | 0.003688939 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -135        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.74       |
|    n_updates            | 30490       |
|    policy_gradient_loss | -0.0136     |
|    std                  | 10.3        |
|    value_loss           | 0.0959      |
-----------------------------------------
Iteration: 3051 | Episodes: 124100 | Median Reward: 46.29 | Max Reward: 49.34
Iteration: 3054 | Episodes: 124200 | Median Reward: 43.63 | Max Reward: 49.34
Iteration: 3056 | Episodes: 124300 | Median Reward: 46.26 | Max Reward: 49.34
Iteration: 3059 | Episodes: 124400 | Median Reward: 46.66 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -53.3         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 3060          |
|    time_elapsed         | 43675         |
|    total_timesteps      | 12533760      |
| train/                  |               |
|    approx_kl            | 0.00028988597 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -135          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.48         |
|    n_updates            | 30590         |
|    policy_gradient_loss | -0.000971     |
|    std                  | 10.4          |
|    value_loss           | 0.716         |
-------------------------------------------
Iteration: 3061 | Episodes: 124500 | Median Reward: 47.27 | Max Reward: 49.34
Iteration: 3064 | Episodes: 124600 | Median Reward: 44.10 | Max Reward: 49.34
Iteration: 3066 | Episodes: 124700 | Median Reward: 43.64 | Max Reward: 49.34
Iteration: 3068 | Episodes: 124800 | Median Reward: 45.66 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.8         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 3070          |
|    time_elapsed         | 43815         |
|    total_timesteps      | 12574720      |
| train/                  |               |
|    approx_kl            | 2.5498914e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -135          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.69         |
|    n_updates            | 30690         |
|    policy_gradient_loss | 1.3e-05       |
|    std                  | 10.4          |
|    value_loss           | 0.307         |
-------------------------------------------
Iteration: 3071 | Episodes: 124900 | Median Reward: 47.28 | Max Reward: 49.34
Iteration: 3073 | Episodes: 125000 | Median Reward: 43.78 | Max Reward: 49.34
Iteration: 3076 | Episodes: 125100 | Median Reward: 47.25 | Max Reward: 49.34
Iteration: 3078 | Episodes: 125200 | Median Reward: 47.03 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -53.2         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 3080          |
|    time_elapsed         | 43957         |
|    total_timesteps      | 12615680      |
| train/                  |               |
|    approx_kl            | 1.4687073e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -135          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.73         |
|    n_updates            | 30790         |
|    policy_gradient_loss | 3.13e-05      |
|    std                  | 10.5          |
|    value_loss           | 0.188         |
-------------------------------------------
Iteration: 3081 | Episodes: 125300 | Median Reward: 45.23 | Max Reward: 49.34
Iteration: 3083 | Episodes: 125400 | Median Reward: 43.67 | Max Reward: 49.34
Iteration: 3086 | Episodes: 125500 | Median Reward: 46.33 | Max Reward: 49.34
Iteration: 3088 | Episodes: 125600 | Median Reward: 47.38 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.3         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 3090          |
|    time_elapsed         | 44100         |
|    total_timesteps      | 12656640      |
| train/                  |               |
|    approx_kl            | 4.5569614e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.67         |
|    n_updates            | 30890         |
|    policy_gradient_loss | -3.08e-05     |
|    std                  | 10.6          |
|    value_loss           | 0.702         |
-------------------------------------------
Iteration: 3091 | Episodes: 125700 | Median Reward: 46.81 | Max Reward: 49.34
Iteration: 3093 | Episodes: 125800 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 3096 | Episodes: 125900 | Median Reward: 44.26 | Max Reward: 49.34
Iteration: 3098 | Episodes: 126000 | Median Reward: 46.29 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.6         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3100          |
|    time_elapsed         | 44241         |
|    total_timesteps      | 12697600      |
| train/                  |               |
|    approx_kl            | 0.00060462125 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.7          |
|    n_updates            | 30990         |
|    policy_gradient_loss | -0.00206      |
|    std                  | 10.6          |
|    value_loss           | 0.341         |
-------------------------------------------
Iteration: 3101 | Episodes: 126100 | Median Reward: 44.18 | Max Reward: 49.34
Iteration: 3103 | Episodes: 126200 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 3105 | Episodes: 126300 | Median Reward: 46.71 | Max Reward: 49.34
Iteration: 3108 | Episodes: 126400 | Median Reward: 42.89 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.8         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3110          |
|    time_elapsed         | 44384         |
|    total_timesteps      | 12738560      |
| train/                  |               |
|    approx_kl            | 0.00025845045 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.72         |
|    n_updates            | 31090         |
|    policy_gradient_loss | -0.000774     |
|    std                  | 10.7          |
|    value_loss           | 0.302         |
-------------------------------------------
Iteration: 3110 | Episodes: 126500 | Median Reward: 47.25 | Max Reward: 49.34
Iteration: 3113 | Episodes: 126600 | Median Reward: 47.08 | Max Reward: 49.34
Iteration: 3115 | Episodes: 126700 | Median Reward: 46.52 | Max Reward: 49.34
Iteration: 3118 | Episodes: 126800 | Median Reward: 46.35 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.2         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3120          |
|    time_elapsed         | 44526         |
|    total_timesteps      | 12779520      |
| train/                  |               |
|    approx_kl            | 1.7487444e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.61         |
|    n_updates            | 31190         |
|    policy_gradient_loss | -9.41e-05     |
|    std                  | 10.8          |
|    value_loss           | 0.281         |
-------------------------------------------
Iteration: 3120 | Episodes: 126900 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 3123 | Episodes: 127000 | Median Reward: 46.28 | Max Reward: 49.34
Iteration: 3125 | Episodes: 127100 | Median Reward: 45.03 | Max Reward: 49.34
Iteration: 3128 | Episodes: 127200 | Median Reward: 46.84 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.2         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3130          |
|    time_elapsed         | 44669         |
|    total_timesteps      | 12820480      |
| train/                  |               |
|    approx_kl            | 0.00015882851 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.73         |
|    n_updates            | 31290         |
|    policy_gradient_loss | -0.00153      |
|    std                  | 10.9          |
|    value_loss           | 0.248         |
-------------------------------------------
Iteration: 3130 | Episodes: 127300 | Median Reward: 46.63 | Max Reward: 49.34
Iteration: 3133 | Episodes: 127400 | Median Reward: 43.92 | Max Reward: 49.34
Iteration: 3135 | Episodes: 127500 | Median Reward: 42.55 | Max Reward: 49.34
Iteration: 3137 | Episodes: 127600 | Median Reward: 46.50 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.4         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3140          |
|    time_elapsed         | 44812         |
|    total_timesteps      | 12861440      |
| train/                  |               |
|    approx_kl            | 0.00027346285 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.72         |
|    n_updates            | 31390         |
|    policy_gradient_loss | -0.00253      |
|    std                  | 11            |
|    value_loss           | 0.353         |
-------------------------------------------
Iteration: 3140 | Episodes: 127700 | Median Reward: 46.67 | Max Reward: 49.34
Iteration: 3142 | Episodes: 127800 | Median Reward: 47.51 | Max Reward: 49.34
Iteration: 3145 | Episodes: 127900 | Median Reward: 46.35 | Max Reward: 49.34
Iteration: 3147 | Episodes: 128000 | Median Reward: 47.36 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.8         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3150          |
|    time_elapsed         | 44952         |
|    total_timesteps      | 12902400      |
| train/                  |               |
|    approx_kl            | 1.2308243e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.34         |
|    n_updates            | 31490         |
|    policy_gradient_loss | -3.35e-05     |
|    std                  | 11.1          |
|    value_loss           | 0.409         |
-------------------------------------------
Iteration: 3150 | Episodes: 128100 | Median Reward: 47.28 | Max Reward: 49.34
Iteration: 3152 | Episodes: 128200 | Median Reward: 44.75 | Max Reward: 49.34
Iteration: 3155 | Episodes: 128300 | Median Reward: 43.46 | Max Reward: 49.34
Iteration: 3157 | Episodes: 128400 | Median Reward: 46.72 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.3       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3160        |
|    time_elapsed         | 45092       |
|    total_timesteps      | 12943360    |
| train/                  |             |
|    approx_kl            | 0.002067151 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -137        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.71       |
|    n_updates            | 31590       |
|    policy_gradient_loss | -0.00631    |
|    std                  | 11.2        |
|    value_loss           | 0.244       |
-----------------------------------------
Iteration: 3160 | Episodes: 128500 | Median Reward: 44.09 | Max Reward: 49.34
Iteration: 3162 | Episodes: 128600 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 3165 | Episodes: 128700 | Median Reward: 46.24 | Max Reward: 49.34
Iteration: 3167 | Episodes: 128800 | Median Reward: 46.97 | Max Reward: 49.34
Iteration: 3169 | Episodes: 128900 | Median Reward: 46.68 | Max Reward: 49.34
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -54.9      |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 3170       |
|    time_elapsed         | 45233      |
|    total_timesteps      | 12984320   |
| train/                  |            |
|    approx_kl            | 0.00429707 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -137       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -6.75      |
|    n_updates            | 31690      |
|    policy_gradient_loss | -0.0106    |
|    std                  | 11.4       |
|    value_loss           | 0.312      |
----------------------------------------
Iteration: 3172 | Episodes: 129000 | Median Reward: 46.45 | Max Reward: 49.34
Iteration: 3174 | Episodes: 129100 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 3177 | Episodes: 129200 | Median Reward: 47.12 | Max Reward: 49.34
Iteration: 3179 | Episodes: 129300 | Median Reward: 47.55 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.1         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3180          |
|    time_elapsed         | 45373         |
|    total_timesteps      | 13025280      |
| train/                  |               |
|    approx_kl            | 0.00040410162 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.73         |
|    n_updates            | 31790         |
|    policy_gradient_loss | -0.00198      |
|    std                  | 11.5          |
|    value_loss           | 0.324         |
-------------------------------------------
Iteration: 3182 | Episodes: 129400 | Median Reward: 46.97 | Max Reward: 49.34
Iteration: 3184 | Episodes: 129500 | Median Reward: 46.29 | Max Reward: 49.34
Iteration: 3187 | Episodes: 129600 | Median Reward: 45.89 | Max Reward: 49.34
Iteration: 3189 | Episodes: 129700 | Median Reward: 47.01 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -52.7         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3190          |
|    time_elapsed         | 45513         |
|    total_timesteps      | 13066240      |
| train/                  |               |
|    approx_kl            | 9.7989425e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.81         |
|    n_updates            | 31890         |
|    policy_gradient_loss | -0.000366     |
|    std                  | 11.6          |
|    value_loss           | 0.148         |
-------------------------------------------
Iteration: 3192 | Episodes: 129800 | Median Reward: 46.31 | Max Reward: 49.34
Iteration: 3194 | Episodes: 129900 | Median Reward: 46.26 | Max Reward: 49.34
Iteration: 3197 | Episodes: 130000 | Median Reward: 46.42 | Max Reward: 49.34
Iteration: 3199 | Episodes: 130100 | Median Reward: 45.13 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.3         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3200          |
|    time_elapsed         | 45656         |
|    total_timesteps      | 13107200      |
| train/                  |               |
|    approx_kl            | 0.00053828757 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.82         |
|    n_updates            | 31990         |
|    policy_gradient_loss | -0.00295      |
|    std                  | 11.7          |
|    value_loss           | 0.298         |
-------------------------------------------
Iteration: 3201 | Episodes: 130200 | Median Reward: 45.91 | Max Reward: 49.34
Iteration: 3204 | Episodes: 130300 | Median Reward: 46.29 | Max Reward: 49.34
Iteration: 3206 | Episodes: 130400 | Median Reward: 46.24 | Max Reward: 49.34
Iteration: 3209 | Episodes: 130500 | Median Reward: 45.75 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.5       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3210        |
|    time_elapsed         | 45798       |
|    total_timesteps      | 13148160    |
| train/                  |             |
|    approx_kl            | 0.004206267 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -137        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.74       |
|    n_updates            | 32090       |
|    policy_gradient_loss | -0.0133     |
|    std                  | 11.7        |
|    value_loss           | 0.341       |
-----------------------------------------
Iteration: 3211 | Episodes: 130600 | Median Reward: 47.38 | Max Reward: 49.34
Iteration: 3214 | Episodes: 130700 | Median Reward: 44.35 | Max Reward: 49.34
Iteration: 3216 | Episodes: 130800 | Median Reward: 44.05 | Max Reward: 49.34
Iteration: 3219 | Episodes: 130900 | Median Reward: 46.25 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54           |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3220          |
|    time_elapsed         | 45941         |
|    total_timesteps      | 13189120      |
| train/                  |               |
|    approx_kl            | 1.7419457e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -138          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.66         |
|    n_updates            | 32190         |
|    policy_gradient_loss | -3.37e-05     |
|    std                  | 11.8          |
|    value_loss           | 0.606         |
-------------------------------------------
Iteration: 3221 | Episodes: 131000 | Median Reward: 45.13 | Max Reward: 49.34
Iteration: 3224 | Episodes: 131100 | Median Reward: 46.34 | Max Reward: 49.34
Iteration: 3226 | Episodes: 131200 | Median Reward: 46.13 | Max Reward: 49.34
Iteration: 3229 | Episodes: 131300 | Median Reward: 46.67 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.4         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3230          |
|    time_elapsed         | 46085         |
|    total_timesteps      | 13230080      |
| train/                  |               |
|    approx_kl            | 7.2357216e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -138          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.84         |
|    n_updates            | 32290         |
|    policy_gradient_loss | -5.09e-05     |
|    std                  | 11.9          |
|    value_loss           | 0.239         |
-------------------------------------------
Iteration: 3231 | Episodes: 131400 | Median Reward: 46.11 | Max Reward: 49.34
Iteration: 3233 | Episodes: 131500 | Median Reward: 46.43 | Max Reward: 49.34
Iteration: 3236 | Episodes: 131600 | Median Reward: 47.15 | Max Reward: 49.34
Iteration: 3238 | Episodes: 131700 | Median Reward: 46.43 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -53.1        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3240         |
|    time_elapsed         | 46227        |
|    total_timesteps      | 13271040     |
| train/                  |              |
|    approx_kl            | 0.0003416293 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -138         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0005       |
|    loss                 | -2.95        |
|    n_updates            | 32390        |
|    policy_gradient_loss | -0.000799    |
|    std                  | 12           |
|    value_loss           | 7.91         |
------------------------------------------
Iteration: 3241 | Episodes: 131800 | Median Reward: 46.64 | Max Reward: 49.34
Iteration: 3243 | Episodes: 131900 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 3246 | Episodes: 132000 | Median Reward: 47.46 | Max Reward: 49.34
Iteration: 3248 | Episodes: 132100 | Median Reward: 47.13 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.8         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3250          |
|    time_elapsed         | 46370         |
|    total_timesteps      | 13312000      |
| train/                  |               |
|    approx_kl            | 0.00016270619 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -138          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.72         |
|    n_updates            | 32490         |
|    policy_gradient_loss | -0.000701     |
|    std                  | 12.1          |
|    value_loss           | 0.328         |
-------------------------------------------
Iteration: 3251 | Episodes: 132200 | Median Reward: 46.24 | Max Reward: 49.34
Iteration: 3253 | Episodes: 132300 | Median Reward: 46.67 | Max Reward: 49.34
Iteration: 3256 | Episodes: 132400 | Median Reward: 44.28 | Max Reward: 49.34
Iteration: 3258 | Episodes: 132500 | Median Reward: 43.65 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.8         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3260          |
|    time_elapsed         | 46502         |
|    total_timesteps      | 13352960      |
| train/                  |               |
|    approx_kl            | 0.00095215894 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -138          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.64         |
|    n_updates            | 32590         |
|    policy_gradient_loss | -0.00403      |
|    std                  | 12.2          |
|    value_loss           | 0.538         |
-------------------------------------------
Iteration: 3261 | Episodes: 132600 | Median Reward: 46.25 | Max Reward: 49.34
Iteration: 3263 | Episodes: 132700 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 3266 | Episodes: 132800 | Median Reward: 43.68 | Max Reward: 49.34
Iteration: 3268 | Episodes: 132900 | Median Reward: 46.76 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.5       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3270        |
|    time_elapsed         | 46643       |
|    total_timesteps      | 13393920    |
| train/                  |             |
|    approx_kl            | 0.002670257 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -138        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.67       |
|    n_updates            | 32690       |
|    policy_gradient_loss | -0.0128     |
|    std                  | 12.2        |
|    value_loss           | 0.436       |
-----------------------------------------
Iteration: 3270 | Episodes: 133000 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 3273 | Episodes: 133100 | Median Reward: 46.63 | Max Reward: 49.34
Iteration: 3275 | Episodes: 133200 | Median Reward: 44.14 | Max Reward: 49.34
Iteration: 3278 | Episodes: 133300 | Median Reward: 44.36 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.6        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3280         |
|    time_elapsed         | 46787        |
|    total_timesteps      | 13434880     |
| train/                  |              |
|    approx_kl            | 0.0018915979 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -138         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.88        |
|    n_updates            | 32790        |
|    policy_gradient_loss | -0.00603     |
|    std                  | 12.4         |
|    value_loss           | 0.178        |
------------------------------------------
Iteration: 3280 | Episodes: 133400 | Median Reward: 43.58 | Max Reward: 49.34
Iteration: 3283 | Episodes: 133500 | Median Reward: 46.96 | Max Reward: 49.34
Iteration: 3285 | Episodes: 133600 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 3288 | Episodes: 133700 | Median Reward: 47.26 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.2         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3290          |
|    time_elapsed         | 46929         |
|    total_timesteps      | 13475840      |
| train/                  |               |
|    approx_kl            | 2.9975345e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -139          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.78         |
|    n_updates            | 32890         |
|    policy_gradient_loss | -0.000277     |
|    std                  | 12.5          |
|    value_loss           | 0.414         |
-------------------------------------------
Iteration: 3290 | Episodes: 133800 | Median Reward: 43.56 | Max Reward: 49.34
Iteration: 3293 | Episodes: 133900 | Median Reward: 47.17 | Max Reward: 49.34
Iteration: 3295 | Episodes: 134000 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 3298 | Episodes: 134100 | Median Reward: 46.39 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.6        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3300         |
|    time_elapsed         | 47072        |
|    total_timesteps      | 13516800     |
| train/                  |              |
|    approx_kl            | 0.0017976853 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -139         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.8         |
|    n_updates            | 32990        |
|    policy_gradient_loss | -0.011       |
|    std                  | 12.5         |
|    value_loss           | 0.507        |
------------------------------------------
Iteration: 3300 | Episodes: 134200 | Median Reward: 46.62 | Max Reward: 49.34
Iteration: 3302 | Episodes: 134300 | Median Reward: 47.12 | Max Reward: 49.34
Iteration: 3305 | Episodes: 134400 | Median Reward: 46.30 | Max Reward: 49.34
Iteration: 3307 | Episodes: 134500 | Median Reward: 43.85 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3310         |
|    time_elapsed         | 47213        |
|    total_timesteps      | 13557760     |
| train/                  |              |
|    approx_kl            | 0.0010075199 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -139         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.6         |
|    n_updates            | 33090        |
|    policy_gradient_loss | -0.00461     |
|    std                  | 12.6         |
|    value_loss           | 0.497        |
------------------------------------------
Iteration: 3310 | Episodes: 134600 | Median Reward: 46.41 | Max Reward: 49.34
Iteration: 3312 | Episodes: 134700 | Median Reward: 47.01 | Max Reward: 49.34
Iteration: 3315 | Episodes: 134800 | Median Reward: 47.31 | Max Reward: 49.34
Iteration: 3317 | Episodes: 134900 | Median Reward: 45.96 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3320         |
|    time_elapsed         | 47356        |
|    total_timesteps      | 13598720     |
| train/                  |              |
|    approx_kl            | 0.0017571806 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -139         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.76        |
|    n_updates            | 33190        |
|    policy_gradient_loss | -0.00687     |
|    std                  | 12.7         |
|    value_loss           | 0.322        |
------------------------------------------
Iteration: 3320 | Episodes: 135000 | Median Reward: 43.65 | Max Reward: 49.34
Iteration: 3322 | Episodes: 135100 | Median Reward: 46.84 | Max Reward: 49.34
Iteration: 3325 | Episodes: 135200 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 3327 | Episodes: 135300 | Median Reward: 46.11 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.6       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3330        |
|    time_elapsed         | 47500       |
|    total_timesteps      | 13639680    |
| train/                  |             |
|    approx_kl            | 0.004184624 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -139        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.92       |
|    n_updates            | 33290       |
|    policy_gradient_loss | -0.0118     |
|    std                  | 12.9        |
|    value_loss           | 0.124       |
-----------------------------------------
Iteration: 3330 | Episodes: 135400 | Median Reward: 45.96 | Max Reward: 49.34
Iteration: 3332 | Episodes: 135500 | Median Reward: 47.46 | Max Reward: 49.34
Iteration: 3334 | Episodes: 135600 | Median Reward: 47.10 | Max Reward: 49.34
Iteration: 3337 | Episodes: 135700 | Median Reward: 47.23 | Max Reward: 49.34
Iteration: 3339 | Episodes: 135800 | Median Reward: 47.38 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3340          |
|    time_elapsed         | 47641         |
|    total_timesteps      | 13680640      |
| train/                  |               |
|    approx_kl            | 0.00023833185 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -139          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.91         |
|    n_updates            | 33390         |
|    policy_gradient_loss | -0.000411     |
|    std                  | 12.9          |
|    value_loss           | 0.124         |
-------------------------------------------
Iteration: 3342 | Episodes: 135900 | Median Reward: 46.61 | Max Reward: 49.34
Iteration: 3344 | Episodes: 136000 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 3347 | Episodes: 136100 | Median Reward: 46.17 | Max Reward: 49.34
Iteration: 3349 | Episodes: 136200 | Median Reward: 46.65 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -53.9         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3350          |
|    time_elapsed         | 47785         |
|    total_timesteps      | 13721600      |
| train/                  |               |
|    approx_kl            | 2.4031673e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -139          |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.05         |
|    n_updates            | 33490         |
|    policy_gradient_loss | -8.94e-05     |
|    std                  | 13.1          |
|    value_loss           | 5.98          |
-------------------------------------------
Iteration: 3352 | Episodes: 136300 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 3354 | Episodes: 136400 | Median Reward: 46.63 | Max Reward: 49.34
Iteration: 3357 | Episodes: 136500 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 3359 | Episodes: 136600 | Median Reward: 46.91 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54          |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3360         |
|    time_elapsed         | 47927        |
|    total_timesteps      | 13762560     |
| train/                  |              |
|    approx_kl            | 0.0001426132 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -139         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.88        |
|    n_updates            | 33590        |
|    policy_gradient_loss | -0.000651    |
|    std                  | 13.2         |
|    value_loss           | 0.369        |
------------------------------------------
Iteration: 3362 | Episodes: 136700 | Median Reward: 45.69 | Max Reward: 49.34
Iteration: 3364 | Episodes: 136800 | Median Reward: 46.86 | Max Reward: 49.34
Iteration: 3366 | Episodes: 136900 | Median Reward: 46.05 | Max Reward: 49.34
Iteration: 3369 | Episodes: 137000 | Median Reward: 46.90 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.1         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3370          |
|    time_elapsed         | 48071         |
|    total_timesteps      | 13803520      |
| train/                  |               |
|    approx_kl            | 0.00034472073 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -140          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.84         |
|    n_updates            | 33690         |
|    policy_gradient_loss | -0.00189      |
|    std                  | 13.4          |
|    value_loss           | 0.163         |
-------------------------------------------
Iteration: 3371 | Episodes: 137100 | Median Reward: 47.04 | Max Reward: 49.34
Iteration: 3374 | Episodes: 137200 | Median Reward: 46.30 | Max Reward: 49.34
Iteration: 3376 | Episodes: 137300 | Median Reward: 47.33 | Max Reward: 49.34
Iteration: 3379 | Episodes: 137400 | Median Reward: 45.64 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.7       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3380        |
|    time_elapsed         | 48215       |
|    total_timesteps      | 13844480    |
| train/                  |             |
|    approx_kl            | 9.22654e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -140        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -6.9        |
|    n_updates            | 33790       |
|    policy_gradient_loss | 3.48e-06    |
|    std                  | 13.5        |
|    value_loss           | 0.311       |
-----------------------------------------
Iteration: 3381 | Episodes: 137500 | Median Reward: 47.51 | Max Reward: 49.34
Iteration: 3384 | Episodes: 137600 | Median Reward: 46.85 | Max Reward: 49.34
Iteration: 3386 | Episodes: 137700 | Median Reward: 46.32 | Max Reward: 49.34
Iteration: 3389 | Episodes: 137800 | Median Reward: 44.19 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.5        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3390         |
|    time_elapsed         | 48355        |
|    total_timesteps      | 13885440     |
| train/                  |              |
|    approx_kl            | 0.0012742761 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -140         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.91        |
|    n_updates            | 33890        |
|    policy_gradient_loss | -0.00207     |
|    std                  | 13.6         |
|    value_loss           | 0.207        |
------------------------------------------
Iteration: 3391 | Episodes: 137900 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 3394 | Episodes: 138000 | Median Reward: 43.43 | Max Reward: 49.34
Iteration: 3396 | Episodes: 138100 | Median Reward: 46.35 | Max Reward: 49.34
Iteration: 3399 | Episodes: 138200 | Median Reward: 46.72 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.5         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3400          |
|    time_elapsed         | 48499         |
|    total_timesteps      | 13926400      |
| train/                  |               |
|    approx_kl            | 0.00056564587 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -140          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.85         |
|    n_updates            | 33990         |
|    policy_gradient_loss | -0.0012       |
|    std                  | 13.7          |
|    value_loss           | 0.291         |
-------------------------------------------
Iteration: 3401 | Episodes: 138300 | Median Reward: 46.73 | Max Reward: 49.34
Iteration: 3403 | Episodes: 138400 | Median Reward: 46.68 | Max Reward: 49.34
Iteration: 3406 | Episodes: 138500 | Median Reward: 46.84 | Max Reward: 49.34
Iteration: 3408 | Episodes: 138600 | Median Reward: 46.84 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3410         |
|    time_elapsed         | 48642        |
|    total_timesteps      | 13967360     |
| train/                  |              |
|    approx_kl            | 0.0003898531 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -140         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.9         |
|    n_updates            | 34090        |
|    policy_gradient_loss | -0.0018      |
|    std                  | 13.8         |
|    value_loss           | 0.181        |
------------------------------------------
Iteration: 3411 | Episodes: 138700 | Median Reward: 46.84 | Max Reward: 49.34
Iteration: 3413 | Episodes: 138800 | Median Reward: 47.16 | Max Reward: 49.34
Iteration: 3416 | Episodes: 138900 | Median Reward: 46.45 | Max Reward: 49.34
Iteration: 3418 | Episodes: 139000 | Median Reward: 46.39 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.4       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3420        |
|    time_elapsed         | 48784       |
|    total_timesteps      | 14008320    |
| train/                  |             |
|    approx_kl            | 0.014156491 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -140        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.07       |
|    n_updates            | 34190       |
|    policy_gradient_loss | -0.032      |
|    std                  | 13.9        |
|    value_loss           | 0.0912      |
-----------------------------------------
Iteration: 3421 | Episodes: 139100 | Median Reward: 46.16 | Max Reward: 49.34
Iteration: 3423 | Episodes: 139200 | Median Reward: 46.84 | Max Reward: 49.34
Iteration: 3426 | Episodes: 139300 | Median Reward: 44.13 | Max Reward: 49.34
Iteration: 3428 | Episodes: 139400 | Median Reward: 47.51 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3430         |
|    time_elapsed         | 48927        |
|    total_timesteps      | 14049280     |
| train/                  |              |
|    approx_kl            | 0.0004664919 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -141         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.98        |
|    n_updates            | 34290        |
|    policy_gradient_loss | -0.00304     |
|    std                  | 14.1         |
|    value_loss           | 0.176        |
------------------------------------------
Iteration: 3431 | Episodes: 139500 | Median Reward: 45.91 | Max Reward: 49.34
Iteration: 3433 | Episodes: 139600 | Median Reward: 47.00 | Max Reward: 49.34
Iteration: 3436 | Episodes: 139700 | Median Reward: 46.29 | Max Reward: 49.34
Iteration: 3438 | Episodes: 139800 | Median Reward: 44.39 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.1         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3440          |
|    time_elapsed         | 49068         |
|    total_timesteps      | 14090240      |
| train/                  |               |
|    approx_kl            | 0.00080597313 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.96         |
|    n_updates            | 34390         |
|    policy_gradient_loss | -0.0028       |
|    std                  | 14.2          |
|    value_loss           | 0.194         |
-------------------------------------------
Iteration: 3440 | Episodes: 139900 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 3443 | Episodes: 140000 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 3445 | Episodes: 140100 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 3448 | Episodes: 140200 | Median Reward: 46.95 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.4       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3450        |
|    time_elapsed         | 49211       |
|    total_timesteps      | 14131200    |
| train/                  |             |
|    approx_kl            | 0.001469732 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -141        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.01       |
|    n_updates            | 34490       |
|    policy_gradient_loss | -0.00298    |
|    std                  | 14.4        |
|    value_loss           | 0.164       |
-----------------------------------------
Iteration: 3450 | Episodes: 140300 | Median Reward: 44.11 | Max Reward: 49.34
Iteration: 3453 | Episodes: 140400 | Median Reward: 47.33 | Max Reward: 49.34
Iteration: 3455 | Episodes: 140500 | Median Reward: 45.76 | Max Reward: 49.34
Iteration: 3458 | Episodes: 140600 | Median Reward: 46.98 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 99.7        |
|    ep_rew_mean          | -53         |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3460        |
|    time_elapsed         | 49347       |
|    total_timesteps      | 14172160    |
| train/                  |             |
|    approx_kl            | 0.000163436 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -141        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.58       |
|    n_updates            | 34590       |
|    policy_gradient_loss | -0.000636   |
|    std                  | 14.5        |
|    value_loss           | 2.93        |
-----------------------------------------
Iteration: 3460 | Episodes: 140700 | Median Reward: 47.46 | Max Reward: 49.34
Iteration: 3463 | Episodes: 140800 | Median Reward: 45.88 | Max Reward: 49.34
Iteration: 3465 | Episodes: 140900 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 3468 | Episodes: 141000 | Median Reward: 47.15 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.4         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3470          |
|    time_elapsed         | 49485         |
|    total_timesteps      | 14213120      |
| train/                  |               |
|    approx_kl            | 0.00024249511 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.95         |
|    n_updates            | 34690         |
|    policy_gradient_loss | -0.00162      |
|    std                  | 14.5          |
|    value_loss           | 0.185         |
-------------------------------------------
Iteration: 3470 | Episodes: 141100 | Median Reward: 46.28 | Max Reward: 49.34
Iteration: 3472 | Episodes: 141200 | Median Reward: 44.26 | Max Reward: 49.34
Iteration: 3475 | Episodes: 141300 | Median Reward: 44.04 | Max Reward: 49.34
Iteration: 3477 | Episodes: 141400 | Median Reward: 45.71 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.1        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3480         |
|    time_elapsed         | 49622        |
|    total_timesteps      | 14254080     |
| train/                  |              |
|    approx_kl            | 0.0020099794 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -141         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.98        |
|    n_updates            | 34790        |
|    policy_gradient_loss | -0.00774     |
|    std                  | 14.6         |
|    value_loss           | 0.235        |
------------------------------------------
Iteration: 3480 | Episodes: 141500 | Median Reward: 45.92 | Max Reward: 49.34
Iteration: 3482 | Episodes: 141600 | Median Reward: 46.05 | Max Reward: 49.34
Iteration: 3485 | Episodes: 141700 | Median Reward: 44.43 | Max Reward: 49.34
Iteration: 3487 | Episodes: 141800 | Median Reward: 45.56 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.4         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3490          |
|    time_elapsed         | 49759         |
|    total_timesteps      | 14295040      |
| train/                  |               |
|    approx_kl            | 0.00043805235 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.92         |
|    n_updates            | 34890         |
|    policy_gradient_loss | -0.00288      |
|    std                  | 14.7          |
|    value_loss           | 0.162         |
-------------------------------------------
Iteration: 3490 | Episodes: 141900 | Median Reward: 46.26 | Max Reward: 49.34
Iteration: 3492 | Episodes: 142000 | Median Reward: 46.34 | Max Reward: 49.34
Iteration: 3495 | Episodes: 142100 | Median Reward: 45.34 | Max Reward: 49.34
Iteration: 3497 | Episodes: 142200 | Median Reward: 45.68 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99.3          |
|    ep_rew_mean          | -53           |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3500          |
|    time_elapsed         | 49893         |
|    total_timesteps      | 14336000      |
| train/                  |               |
|    approx_kl            | 6.6903303e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 0.989         |
|    learning_rate        | 0.0005        |
|    loss                 | -3.31         |
|    n_updates            | 34990         |
|    policy_gradient_loss | 3.43e-05      |
|    std                  | 14.8          |
|    value_loss           | 7.37          |
-------------------------------------------
Iteration: 3500 | Episodes: 142300 | Median Reward: 47.04 | Max Reward: 49.34
Iteration: 3502 | Episodes: 142400 | Median Reward: 46.96 | Max Reward: 49.34
Iteration: 3504 | Episodes: 142500 | Median Reward: 46.80 | Max Reward: 49.34
Iteration: 3507 | Episodes: 142600 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 3509 | Episodes: 142700 | Median Reward: 46.93 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.4        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3510         |
|    time_elapsed         | 50031        |
|    total_timesteps      | 14376960     |
| train/                  |              |
|    approx_kl            | 0.0014642626 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -141         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7           |
|    n_updates            | 35090        |
|    policy_gradient_loss | -0.00485     |
|    std                  | 14.9         |
|    value_loss           | 0.197        |
------------------------------------------
Iteration: 3512 | Episodes: 142800 | Median Reward: 45.55 | Max Reward: 49.34
Iteration: 3514 | Episodes: 142900 | Median Reward: 43.31 | Max Reward: 49.34
Iteration: 3517 | Episodes: 143000 | Median Reward: 46.82 | Max Reward: 49.34
Iteration: 3519 | Episodes: 143100 | Median Reward: 47.28 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.2         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3520          |
|    time_elapsed         | 50171         |
|    total_timesteps      | 14417920      |
| train/                  |               |
|    approx_kl            | 0.00011940455 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -142          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.05         |
|    n_updates            | 35190         |
|    policy_gradient_loss | -5.8e-05      |
|    std                  | 15.1          |
|    value_loss           | 0.12          |
-------------------------------------------
Iteration: 3522 | Episodes: 143200 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 3524 | Episodes: 143300 | Median Reward: 46.05 | Max Reward: 49.34
Iteration: 3527 | Episodes: 143400 | Median Reward: 46.32 | Max Reward: 49.34
Iteration: 3529 | Episodes: 143500 | Median Reward: 47.21 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.9        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3530         |
|    time_elapsed         | 50308        |
|    total_timesteps      | 14458880     |
| train/                  |              |
|    approx_kl            | 0.0021350244 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -142         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.04        |
|    n_updates            | 35290        |
|    policy_gradient_loss | -0.00782     |
|    std                  | 15.3         |
|    value_loss           | 0.189        |
------------------------------------------
Iteration: 3532 | Episodes: 143600 | Median Reward: 45.51 | Max Reward: 49.34
Iteration: 3534 | Episodes: 143700 | Median Reward: 46.16 | Max Reward: 49.34
Iteration: 3537 | Episodes: 143800 | Median Reward: 46.36 | Max Reward: 49.34
Iteration: 3539 | Episodes: 143900 | Median Reward: 46.75 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.4         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3540          |
|    time_elapsed         | 50449         |
|    total_timesteps      | 14499840      |
| train/                  |               |
|    approx_kl            | 0.00039152274 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -142          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.81         |
|    n_updates            | 35390         |
|    policy_gradient_loss | -0.00319      |
|    std                  | 15.4          |
|    value_loss           | 0.507         |
-------------------------------------------
Iteration: 3541 | Episodes: 144000 | Median Reward: 45.86 | Max Reward: 49.34
Iteration: 3544 | Episodes: 144100 | Median Reward: 45.96 | Max Reward: 49.34
Iteration: 3546 | Episodes: 144200 | Median Reward: 46.62 | Max Reward: 49.34
Iteration: 3549 | Episodes: 144300 | Median Reward: 43.59 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.6        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3550         |
|    time_elapsed         | 50588        |
|    total_timesteps      | 14540800     |
| train/                  |              |
|    approx_kl            | 0.0034803632 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -142         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.09        |
|    n_updates            | 35490        |
|    policy_gradient_loss | -0.00559     |
|    std                  | 15.6         |
|    value_loss           | 0.126        |
------------------------------------------
Iteration: 3551 | Episodes: 144400 | Median Reward: 46.79 | Max Reward: 49.34
Iteration: 3554 | Episodes: 144500 | Median Reward: 46.14 | Max Reward: 49.34
Iteration: 3556 | Episodes: 144600 | Median Reward: 45.51 | Max Reward: 49.34
Iteration: 3559 | Episodes: 144700 | Median Reward: 46.39 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -53.8        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3560         |
|    time_elapsed         | 50726        |
|    total_timesteps      | 14581760     |
| train/                  |              |
|    approx_kl            | 9.094912e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -142         |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0005       |
|    loss                 | -3.57        |
|    n_updates            | 35590        |
|    policy_gradient_loss | -0.00032     |
|    std                  | 15.7         |
|    value_loss           | 7.24         |
------------------------------------------
Iteration: 3561 | Episodes: 144800 | Median Reward: 46.72 | Max Reward: 49.34
Iteration: 3564 | Episodes: 144900 | Median Reward: 46.86 | Max Reward: 49.34
Iteration: 3566 | Episodes: 145000 | Median Reward: 46.29 | Max Reward: 49.34
Iteration: 3569 | Episodes: 145100 | Median Reward: 44.05 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55          |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3570         |
|    time_elapsed         | 50864        |
|    total_timesteps      | 14622720     |
| train/                  |              |
|    approx_kl            | 0.0009629804 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -143         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7           |
|    n_updates            | 35690        |
|    policy_gradient_loss | -0.00331     |
|    std                  | 15.9         |
|    value_loss           | 0.149        |
------------------------------------------
Iteration: 3571 | Episodes: 145200 | Median Reward: 44.08 | Max Reward: 49.34
Iteration: 3573 | Episodes: 145300 | Median Reward: 43.90 | Max Reward: 49.34
Iteration: 3576 | Episodes: 145400 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 3578 | Episodes: 145500 | Median Reward: 46.41 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.2       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3580        |
|    time_elapsed         | 51001       |
|    total_timesteps      | 14663680    |
| train/                  |             |
|    approx_kl            | 0.000381549 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -143        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.03       |
|    n_updates            | 35790       |
|    policy_gradient_loss | -0.00346    |
|    std                  | 16          |
|    value_loss           | 0.232       |
-----------------------------------------
Iteration: 3581 | Episodes: 145600 | Median Reward: 46.17 | Max Reward: 49.34
Iteration: 3583 | Episodes: 145700 | Median Reward: 47.18 | Max Reward: 49.34
Iteration: 3586 | Episodes: 145800 | Median Reward: 47.26 | Max Reward: 49.34
Iteration: 3588 | Episodes: 145900 | Median Reward: 47.02 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.9         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3590          |
|    time_elapsed         | 51141         |
|    total_timesteps      | 14704640      |
| train/                  |               |
|    approx_kl            | 0.00051229156 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -143          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.12         |
|    n_updates            | 35890         |
|    policy_gradient_loss | -0.00337      |
|    std                  | 16.2          |
|    value_loss           | 0.122         |
-------------------------------------------
Iteration: 3591 | Episodes: 146000 | Median Reward: 46.36 | Max Reward: 49.34
Iteration: 3593 | Episodes: 146100 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 3596 | Episodes: 146200 | Median Reward: 47.20 | Max Reward: 49.34
Iteration: 3598 | Episodes: 146300 | Median Reward: 46.65 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.3        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3600         |
|    time_elapsed         | 51277        |
|    total_timesteps      | 14745600     |
| train/                  |              |
|    approx_kl            | 9.535928e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -143         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.52        |
|    n_updates            | 35990        |
|    policy_gradient_loss | -9.59e-05    |
|    std                  | 16.3         |
|    value_loss           | 1.21         |
------------------------------------------
Iteration: 3601 | Episodes: 146400 | Median Reward: 46.26 | Max Reward: 49.34
Iteration: 3603 | Episodes: 146500 | Median Reward: 44.21 | Max Reward: 49.34
Iteration: 3606 | Episodes: 146600 | Median Reward: 44.02 | Max Reward: 49.34
Iteration: 3608 | Episodes: 146700 | Median Reward: 44.28 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.5        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3610         |
|    time_elapsed         | 51411        |
|    total_timesteps      | 14786560     |
| train/                  |              |
|    approx_kl            | 0.0006176549 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -143         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.04        |
|    n_updates            | 36090        |
|    policy_gradient_loss | -0.00349     |
|    std                  | 16.4         |
|    value_loss           | 0.272        |
------------------------------------------
Iteration: 3610 | Episodes: 146800 | Median Reward: 45.69 | Max Reward: 49.34
Iteration: 3613 | Episodes: 146900 | Median Reward: 46.97 | Max Reward: 49.34
Iteration: 3615 | Episodes: 147000 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 3618 | Episodes: 147100 | Median Reward: 43.95 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.5       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3620        |
|    time_elapsed         | 51548       |
|    total_timesteps      | 14827520    |
| train/                  |             |
|    approx_kl            | 1.62613e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -143        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.06       |
|    n_updates            | 36190       |
|    policy_gradient_loss | -0.00018    |
|    std                  | 16.5        |
|    value_loss           | 0.438       |
-----------------------------------------
Iteration: 3620 | Episodes: 147200 | Median Reward: 46.69 | Max Reward: 49.34
Iteration: 3623 | Episodes: 147300 | Median Reward: 45.91 | Max Reward: 49.34
Iteration: 3625 | Episodes: 147400 | Median Reward: 46.35 | Max Reward: 49.34
Iteration: 3628 | Episodes: 147500 | Median Reward: 47.01 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.4        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3630         |
|    time_elapsed         | 51685        |
|    total_timesteps      | 14868480     |
| train/                  |              |
|    approx_kl            | 0.0006669972 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -143         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.91        |
|    n_updates            | 36290        |
|    policy_gradient_loss | -0.0044      |
|    std                  | 16.6         |
|    value_loss           | 0.223        |
------------------------------------------
Iteration: 3630 | Episodes: 147600 | Median Reward: 45.75 | Max Reward: 49.34
Iteration: 3633 | Episodes: 147700 | Median Reward: 44.02 | Max Reward: 49.34
Iteration: 3635 | Episodes: 147800 | Median Reward: 43.52 | Max Reward: 49.34
Iteration: 3638 | Episodes: 147900 | Median Reward: 43.56 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.5        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3640         |
|    time_elapsed         | 51821        |
|    total_timesteps      | 14909440     |
| train/                  |              |
|    approx_kl            | 0.0013895249 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.09        |
|    n_updates            | 36390        |
|    policy_gradient_loss | -0.00785     |
|    std                  | 16.6         |
|    value_loss           | 0.28         |
------------------------------------------
Iteration: 3640 | Episodes: 148000 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 3642 | Episodes: 148100 | Median Reward: 42.79 | Max Reward: 49.34
Iteration: 3645 | Episodes: 148200 | Median Reward: 47.01 | Max Reward: 49.34
Iteration: 3647 | Episodes: 148300 | Median Reward: 46.43 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -53.4        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3650         |
|    time_elapsed         | 51959        |
|    total_timesteps      | 14950400     |
| train/                  |              |
|    approx_kl            | 0.0028198604 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.32        |
|    n_updates            | 36490        |
|    policy_gradient_loss | -0.00163     |
|    std                  | 16.8         |
|    value_loss           | 3.82         |
------------------------------------------
Iteration: 3650 | Episodes: 148400 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 3652 | Episodes: 148500 | Median Reward: 47.27 | Max Reward: 49.34
Iteration: 3655 | Episodes: 148600 | Median Reward: 46.33 | Max Reward: 49.34
Iteration: 3657 | Episodes: 148700 | Median Reward: 42.93 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.5        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3660         |
|    time_elapsed         | 52102        |
|    total_timesteps      | 14991360     |
| train/                  |              |
|    approx_kl            | 9.482261e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.1         |
|    n_updates            | 36590        |
|    policy_gradient_loss | -0.000509    |
|    std                  | 16.9         |
|    value_loss           | 0.242        |
------------------------------------------
Iteration: 3660 | Episodes: 148800 | Median Reward: 46.61 | Max Reward: 49.34
Iteration: 3662 | Episodes: 148900 | Median Reward: 46.39 | Max Reward: 49.34
Iteration: 3665 | Episodes: 149000 | Median Reward: 44.19 | Max Reward: 49.34
Iteration: 3667 | Episodes: 149100 | Median Reward: 46.42 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.5         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3670          |
|    time_elapsed         | 52238         |
|    total_timesteps      | 15032320      |
| train/                  |               |
|    approx_kl            | 6.1734114e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -144          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.94         |
|    n_updates            | 36690         |
|    policy_gradient_loss | -0.000159     |
|    std                  | 17.1          |
|    value_loss           | 0.622         |
-------------------------------------------
Iteration: 3670 | Episodes: 149200 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 3672 | Episodes: 149300 | Median Reward: 46.35 | Max Reward: 49.34
Iteration: 3675 | Episodes: 149400 | Median Reward: 45.62 | Max Reward: 49.34
Iteration: 3677 | Episodes: 149500 | Median Reward: 46.36 | Max Reward: 49.34
Iteration: 3679 | Episodes: 149600 | Median Reward: 44.98 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.5         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3680          |
|    time_elapsed         | 52381         |
|    total_timesteps      | 15073280      |
| train/                  |               |
|    approx_kl            | 0.00032512096 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -144          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.1          |
|    n_updates            | 36790         |
|    policy_gradient_loss | -0.00165      |
|    std                  | 17.2          |
|    value_loss           | 0.265         |
-------------------------------------------
Iteration: 3682 | Episodes: 149700 | Median Reward: 46.06 | Max Reward: 49.34
Iteration: 3684 | Episodes: 149800 | Median Reward: 46.40 | Max Reward: 49.34
Iteration: 3687 | Episodes: 149900 | Median Reward: 45.68 | Max Reward: 49.34
Iteration: 3689 | Episodes: 150000 | Median Reward: 43.59 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.9       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3690        |
|    time_elapsed         | 52521       |
|    total_timesteps      | 15114240    |
| train/                  |             |
|    approx_kl            | 0.006491529 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -144        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.12       |
|    n_updates            | 36890       |
|    policy_gradient_loss | -0.017      |
|    std                  | 17.3        |
|    value_loss           | 0.316       |
-----------------------------------------
Iteration: 3692 | Episodes: 150100 | Median Reward: 43.87 | Max Reward: 49.34
Iteration: 3694 | Episodes: 150200 | Median Reward: 46.35 | Max Reward: 49.34
Iteration: 3697 | Episodes: 150300 | Median Reward: 45.60 | Max Reward: 49.34
Iteration: 3699 | Episodes: 150400 | Median Reward: 46.94 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.1         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3700          |
|    time_elapsed         | 52662         |
|    total_timesteps      | 15155200      |
| train/                  |               |
|    approx_kl            | 0.00083082385 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -144          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.17         |
|    n_updates            | 36990         |
|    policy_gradient_loss | -0.00453      |
|    std                  | 17.4          |
|    value_loss           | 0.23          |
-------------------------------------------
Iteration: 3702 | Episodes: 150500 | Median Reward: 46.63 | Max Reward: 49.34
Iteration: 3704 | Episodes: 150600 | Median Reward: 46.34 | Max Reward: 49.34
Iteration: 3707 | Episodes: 150700 | Median Reward: 46.82 | Max Reward: 49.34
Iteration: 3709 | Episodes: 150800 | Median Reward: 47.12 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.3        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3710         |
|    time_elapsed         | 52804        |
|    total_timesteps      | 15196160     |
| train/                  |              |
|    approx_kl            | 0.0022131167 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.22        |
|    n_updates            | 37090        |
|    policy_gradient_loss | -0.00828     |
|    std                  | 17.5         |
|    value_loss           | 0.063        |
------------------------------------------
Iteration: 3712 | Episodes: 150900 | Median Reward: 47.00 | Max Reward: 49.34
Iteration: 3714 | Episodes: 151000 | Median Reward: 43.30 | Max Reward: 49.34
Iteration: 3716 | Episodes: 151100 | Median Reward: 44.03 | Max Reward: 49.34
Iteration: 3719 | Episodes: 151200 | Median Reward: 43.95 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.9       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3720        |
|    time_elapsed         | 52948       |
|    total_timesteps      | 15237120    |
| train/                  |             |
|    approx_kl            | 0.004207265 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -145        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.08       |
|    n_updates            | 37190       |
|    policy_gradient_loss | -0.00622    |
|    std                  | 17.6        |
|    value_loss           | 0.438       |
-----------------------------------------
Iteration: 3721 | Episodes: 151300 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 3724 | Episodes: 151400 | Median Reward: 44.29 | Max Reward: 49.34
Iteration: 3726 | Episodes: 151500 | Median Reward: 46.31 | Max Reward: 49.34
Iteration: 3729 | Episodes: 151600 | Median Reward: 43.58 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.7        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3730         |
|    time_elapsed         | 53088        |
|    total_timesteps      | 15278080     |
| train/                  |              |
|    approx_kl            | 0.0023189557 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -145         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.06        |
|    n_updates            | 37290        |
|    policy_gradient_loss | -0.00622     |
|    std                  | 17.7         |
|    value_loss           | 0.512        |
------------------------------------------
Iteration: 3731 | Episodes: 151700 | Median Reward: 45.77 | Max Reward: 49.34
Iteration: 3734 | Episodes: 151800 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 3736 | Episodes: 151900 | Median Reward: 45.59 | Max Reward: 49.34
Iteration: 3739 | Episodes: 152000 | Median Reward: 43.42 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3740         |
|    time_elapsed         | 53228        |
|    total_timesteps      | 15319040     |
| train/                  |              |
|    approx_kl            | 0.0006455632 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -145         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.15        |
|    n_updates            | 37390        |
|    policy_gradient_loss | -0.0047      |
|    std                  | 17.8         |
|    value_loss           | 0.23         |
------------------------------------------
Iteration: 3741 | Episodes: 152100 | Median Reward: 43.88 | Max Reward: 49.34
Iteration: 3744 | Episodes: 152200 | Median Reward: 46.30 | Max Reward: 49.34
Iteration: 3746 | Episodes: 152300 | Median Reward: 43.08 | Max Reward: 49.34
Iteration: 3748 | Episodes: 152400 | Median Reward: 47.27 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.6        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3750         |
|    time_elapsed         | 53369        |
|    total_timesteps      | 15360000     |
| train/                  |              |
|    approx_kl            | 0.0010018207 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -145         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.12        |
|    n_updates            | 37490        |
|    policy_gradient_loss | -0.00793     |
|    std                  | 17.8         |
|    value_loss           | 0.433        |
------------------------------------------
Iteration: 3751 | Episodes: 152500 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 3753 | Episodes: 152600 | Median Reward: 47.04 | Max Reward: 49.34
Iteration: 3756 | Episodes: 152700 | Median Reward: 45.85 | Max Reward: 49.34
Iteration: 3758 | Episodes: 152800 | Median Reward: 45.11 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55           |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3760          |
|    time_elapsed         | 53510         |
|    total_timesteps      | 15400960      |
| train/                  |               |
|    approx_kl            | 0.00023049103 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -145          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.08         |
|    n_updates            | 37590         |
|    policy_gradient_loss | -0.00222      |
|    std                  | 17.9          |
|    value_loss           | 0.521         |
-------------------------------------------
Iteration: 3761 | Episodes: 152900 | Median Reward: 47.46 | Max Reward: 49.34
Iteration: 3763 | Episodes: 153000 | Median Reward: 45.14 | Max Reward: 49.34
Iteration: 3766 | Episodes: 153100 | Median Reward: 46.31 | Max Reward: 49.34
Iteration: 3768 | Episodes: 153200 | Median Reward: 46.31 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55           |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3770          |
|    time_elapsed         | 53651         |
|    total_timesteps      | 15441920      |
| train/                  |               |
|    approx_kl            | 0.00015044032 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -145          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.19         |
|    n_updates            | 37690         |
|    policy_gradient_loss | -0.000307     |
|    std                  | 18            |
|    value_loss           | 0.226         |
-------------------------------------------
Iteration: 3771 | Episodes: 153300 | Median Reward: 43.84 | Max Reward: 49.34
Iteration: 3773 | Episodes: 153400 | Median Reward: 46.16 | Max Reward: 49.34
Iteration: 3776 | Episodes: 153500 | Median Reward: 44.66 | Max Reward: 49.34
Iteration: 3778 | Episodes: 153600 | Median Reward: 46.04 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.1         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3780          |
|    time_elapsed         | 53790         |
|    total_timesteps      | 15482880      |
| train/                  |               |
|    approx_kl            | 0.00024411271 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -145          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.89         |
|    n_updates            | 37790         |
|    policy_gradient_loss | -0.00159      |
|    std                  | 18.1          |
|    value_loss           | 0.387         |
-------------------------------------------
Iteration: 3780 | Episodes: 153700 | Median Reward: 44.91 | Max Reward: 49.34
Iteration: 3783 | Episodes: 153800 | Median Reward: 45.86 | Max Reward: 49.34
Iteration: 3785 | Episodes: 153900 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 3788 | Episodes: 154000 | Median Reward: 46.01 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55           |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3790          |
|    time_elapsed         | 53933         |
|    total_timesteps      | 15523840      |
| train/                  |               |
|    approx_kl            | 0.00019795723 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -145          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.21         |
|    n_updates            | 37890         |
|    policy_gradient_loss | -0.00133      |
|    std                  | 18.4          |
|    value_loss           | 0.132         |
-------------------------------------------
Iteration: 3790 | Episodes: 154100 | Median Reward: 45.64 | Max Reward: 49.34
Iteration: 3793 | Episodes: 154200 | Median Reward: 47.22 | Max Reward: 49.34
Iteration: 3795 | Episodes: 154300 | Median Reward: 45.74 | Max Reward: 49.34
Iteration: 3798 | Episodes: 154400 | Median Reward: 45.40 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.7         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3800          |
|    time_elapsed         | 54072         |
|    total_timesteps      | 15564800      |
| train/                  |               |
|    approx_kl            | 0.00038137374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -145          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.23         |
|    n_updates            | 37990         |
|    policy_gradient_loss | -0.00326      |
|    std                  | 18.6          |
|    value_loss           | 0.237         |
-------------------------------------------
Iteration: 3800 | Episodes: 154500 | Median Reward: 46.13 | Max Reward: 49.34
Iteration: 3803 | Episodes: 154600 | Median Reward: 43.98 | Max Reward: 49.34
Iteration: 3805 | Episodes: 154700 | Median Reward: 46.27 | Max Reward: 49.34
Iteration: 3808 | Episodes: 154800 | Median Reward: 43.70 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.5         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3810          |
|    time_elapsed         | 54211         |
|    total_timesteps      | 15605760      |
| train/                  |               |
|    approx_kl            | 0.00056813384 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -146          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.51         |
|    n_updates            | 38090         |
|    policy_gradient_loss | -0.0031       |
|    std                  | 18.7          |
|    value_loss           | 0.513         |
-------------------------------------------
Iteration: 3810 | Episodes: 154900 | Median Reward: 43.85 | Max Reward: 49.34
Iteration: 3812 | Episodes: 155000 | Median Reward: 43.97 | Max Reward: 49.34
Iteration: 3815 | Episodes: 155100 | Median Reward: 46.92 | Max Reward: 49.34
Iteration: 3817 | Episodes: 155200 | Median Reward: 43.42 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3820         |
|    time_elapsed         | 54353        |
|    total_timesteps      | 15646720     |
| train/                  |              |
|    approx_kl            | 8.170378e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -146         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.72        |
|    n_updates            | 38190        |
|    policy_gradient_loss | -0.000744    |
|    std                  | 18.8         |
|    value_loss           | 1.86         |
------------------------------------------
Iteration: 3820 | Episodes: 155300 | Median Reward: 45.16 | Max Reward: 49.34
Iteration: 3822 | Episodes: 155400 | Median Reward: 43.54 | Max Reward: 49.34
Iteration: 3825 | Episodes: 155500 | Median Reward: 46.10 | Max Reward: 49.34
Iteration: 3827 | Episodes: 155600 | Median Reward: 43.00 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.9       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3830        |
|    time_elapsed         | 54488       |
|    total_timesteps      | 15687680    |
| train/                  |             |
|    approx_kl            | 0.000299294 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -146        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.08       |
|    n_updates            | 38290       |
|    policy_gradient_loss | -0.00167    |
|    std                  | 18.9        |
|    value_loss           | 0.466       |
-----------------------------------------
Iteration: 3830 | Episodes: 155700 | Median Reward: 45.72 | Max Reward: 49.34
Iteration: 3832 | Episodes: 155800 | Median Reward: 43.32 | Max Reward: 49.34
Iteration: 3835 | Episodes: 155900 | Median Reward: 44.09 | Max Reward: 49.34
Iteration: 3837 | Episodes: 156000 | Median Reward: 46.24 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.7       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3840        |
|    time_elapsed         | 54630       |
|    total_timesteps      | 15728640    |
| train/                  |             |
|    approx_kl            | 0.004508191 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -146        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.27       |
|    n_updates            | 38390       |
|    policy_gradient_loss | -0.0208     |
|    std                  | 19          |
|    value_loss           | 0.258       |
-----------------------------------------
Iteration: 3840 | Episodes: 156100 | Median Reward: 47.01 | Max Reward: 49.34
Iteration: 3842 | Episodes: 156200 | Median Reward: 46.31 | Max Reward: 49.34
Iteration: 3845 | Episodes: 156300 | Median Reward: 46.53 | Max Reward: 49.34
Iteration: 3847 | Episodes: 156400 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 3849 | Episodes: 156500 | Median Reward: 46.26 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.9         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3850          |
|    time_elapsed         | 54772         |
|    total_timesteps      | 15769600      |
| train/                  |               |
|    approx_kl            | 0.00011950449 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -146          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.24         |
|    n_updates            | 38490         |
|    policy_gradient_loss | -0.000662     |
|    std                  | 19.2          |
|    value_loss           | 0.184         |
-------------------------------------------
Iteration: 3852 | Episodes: 156600 | Median Reward: 46.88 | Max Reward: 49.34
Iteration: 3854 | Episodes: 156700 | Median Reward: 45.08 | Max Reward: 49.34
Iteration: 3857 | Episodes: 156800 | Median Reward: 43.08 | Max Reward: 49.34
Iteration: 3859 | Episodes: 156900 | Median Reward: 45.73 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.2       |
| time/                   |             |
|    fps                  | 287         |
|    iterations           | 3860        |
|    time_elapsed         | 54915       |
|    total_timesteps      | 15810560    |
| train/                  |             |
|    approx_kl            | 0.002688551 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -146        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.21       |
|    n_updates            | 38590       |
|    policy_gradient_loss | -0.00795    |
|    std                  | 19.4        |
|    value_loss           | 0.247       |
-----------------------------------------
Iteration: 3862 | Episodes: 157000 | Median Reward: 45.64 | Max Reward: 49.34
Iteration: 3864 | Episodes: 157100 | Median Reward: 44.07 | Max Reward: 49.34
Iteration: 3867 | Episodes: 157200 | Median Reward: 46.72 | Max Reward: 49.34
Iteration: 3869 | Episodes: 157300 | Median Reward: 46.23 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.6         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3870          |
|    time_elapsed         | 55055         |
|    total_timesteps      | 15851520      |
| train/                  |               |
|    approx_kl            | 4.2589832e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -146          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.23         |
|    n_updates            | 38690         |
|    policy_gradient_loss | -0.000178     |
|    std                  | 19.5          |
|    value_loss           | 0.238         |
-------------------------------------------
Iteration: 3872 | Episodes: 157400 | Median Reward: 44.57 | Max Reward: 49.34
Iteration: 3874 | Episodes: 157500 | Median Reward: 45.85 | Max Reward: 49.34
Iteration: 3877 | Episodes: 157600 | Median Reward: 43.70 | Max Reward: 49.34
Iteration: 3879 | Episodes: 157700 | Median Reward: 46.90 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.7        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3880         |
|    time_elapsed         | 55204        |
|    total_timesteps      | 15892480     |
| train/                  |              |
|    approx_kl            | 0.0043858164 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -146         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.26        |
|    n_updates            | 38790        |
|    policy_gradient_loss | -0.0175      |
|    std                  | 19.8         |
|    value_loss           | 0.247        |
------------------------------------------
Iteration: 3882 | Episodes: 157800 | Median Reward: 46.85 | Max Reward: 49.34
Iteration: 3884 | Episodes: 157900 | Median Reward: 43.61 | Max Reward: 49.34
Iteration: 3886 | Episodes: 158000 | Median Reward: 46.11 | Max Reward: 49.34
Iteration: 3889 | Episodes: 158100 | Median Reward: 44.24 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3890         |
|    time_elapsed         | 55345        |
|    total_timesteps      | 15933440     |
| train/                  |              |
|    approx_kl            | 4.197615e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -147         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.23        |
|    n_updates            | 38890        |
|    policy_gradient_loss | -0.000363    |
|    std                  | 20           |
|    value_loss           | 0.473        |
------------------------------------------
Iteration: 3891 | Episodes: 158200 | Median Reward: 47.13 | Max Reward: 49.34
Iteration: 3894 | Episodes: 158300 | Median Reward: 44.11 | Max Reward: 49.34
Iteration: 3896 | Episodes: 158400 | Median Reward: 46.01 | Max Reward: 49.34
Iteration: 3899 | Episodes: 158500 | Median Reward: 46.11 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -53.6        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3900         |
|    time_elapsed         | 55482        |
|    total_timesteps      | 15974400     |
| train/                  |              |
|    approx_kl            | 7.915724e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -147         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.11        |
|    n_updates            | 38990        |
|    policy_gradient_loss | -0.000383    |
|    std                  | 20.2         |
|    value_loss           | 0.226        |
------------------------------------------
Iteration: 3901 | Episodes: 158600 | Median Reward: 46.22 | Max Reward: 49.34
Iteration: 3904 | Episodes: 158700 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 3906 | Episodes: 158800 | Median Reward: 46.14 | Max Reward: 49.34
Iteration: 3909 | Episodes: 158900 | Median Reward: 46.78 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.7         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3910          |
|    time_elapsed         | 55623         |
|    total_timesteps      | 16015360      |
| train/                  |               |
|    approx_kl            | 0.00060587376 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -147          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.22         |
|    n_updates            | 39090         |
|    policy_gradient_loss | -0.0011       |
|    std                  | 20.4          |
|    value_loss           | 0.159         |
-------------------------------------------
Iteration: 3911 | Episodes: 159000 | Median Reward: 45.22 | Max Reward: 49.34
Iteration: 3914 | Episodes: 159100 | Median Reward: 46.27 | Max Reward: 49.34
Iteration: 3916 | Episodes: 159200 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 3918 | Episodes: 159300 | Median Reward: 46.33 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -53.4        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3920         |
|    time_elapsed         | 55759        |
|    total_timesteps      | 16056320     |
| train/                  |              |
|    approx_kl            | 7.015592e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -147         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -5.43        |
|    n_updates            | 39190        |
|    policy_gradient_loss | -0.000945    |
|    std                  | 20.4         |
|    value_loss           | 1.54         |
------------------------------------------
Iteration: 3921 | Episodes: 159400 | Median Reward: 45.20 | Max Reward: 49.34
Iteration: 3923 | Episodes: 159500 | Median Reward: 44.35 | Max Reward: 49.34
Iteration: 3926 | Episodes: 159600 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 3928 | Episodes: 159700 | Median Reward: 47.15 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.7        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3930         |
|    time_elapsed         | 55900        |
|    total_timesteps      | 16097280     |
| train/                  |              |
|    approx_kl            | 0.0008147552 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -147         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.15        |
|    n_updates            | 39290        |
|    policy_gradient_loss | -0.00458     |
|    std                  | 20.6         |
|    value_loss           | 0.292        |
------------------------------------------
Iteration: 3931 | Episodes: 159800 | Median Reward: 46.26 | Max Reward: 49.34
Iteration: 3933 | Episodes: 159900 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 3936 | Episodes: 160000 | Median Reward: 43.65 | Max Reward: 49.34
Iteration: 3938 | Episodes: 160100 | Median Reward: 45.67 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.3         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 3940          |
|    time_elapsed         | 56042         |
|    total_timesteps      | 16138240      |
| train/                  |               |
|    approx_kl            | 0.00028874853 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -147          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.29         |
|    n_updates            | 39390         |
|    policy_gradient_loss | -0.00223      |
|    std                  | 20.7          |
|    value_loss           | 0.168         |
-------------------------------------------
Iteration: 3941 | Episodes: 160200 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 3943 | Episodes: 160300 | Median Reward: 46.52 | Max Reward: 49.34
Iteration: 3946 | Episodes: 160400 | Median Reward: 42.83 | Max Reward: 49.34
Iteration: 3948 | Episodes: 160500 | Median Reward: 44.37 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55          |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3950         |
|    time_elapsed         | 56182        |
|    total_timesteps      | 16179200     |
| train/                  |              |
|    approx_kl            | 3.134273e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -147         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.32        |
|    n_updates            | 39490        |
|    policy_gradient_loss | 0.000174     |
|    std                  | 20.8         |
|    value_loss           | 0.193        |
------------------------------------------
Iteration: 3950 | Episodes: 160600 | Median Reward: 45.05 | Max Reward: 49.34
Iteration: 3953 | Episodes: 160700 | Median Reward: 46.71 | Max Reward: 49.34
Iteration: 3955 | Episodes: 160800 | Median Reward: 46.62 | Max Reward: 49.34
Iteration: 3958 | Episodes: 160900 | Median Reward: 44.33 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.6        |
| time/                   |              |
|    fps                  | 287          |
|    iterations           | 3960         |
|    time_elapsed         | 56323        |
|    total_timesteps      | 16220160     |
| train/                  |              |
|    approx_kl            | 0.0033822318 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -147         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.27        |
|    n_updates            | 39590        |
|    policy_gradient_loss | -0.0078      |
|    std                  | 21           |
|    value_loss           | 0.403        |
------------------------------------------
Iteration: 3960 | Episodes: 161000 | Median Reward: 43.73 | Max Reward: 49.34
Iteration: 3963 | Episodes: 161100 | Median Reward: 46.27 | Max Reward: 49.34
Iteration: 3965 | Episodes: 161200 | Median Reward: 44.56 | Max Reward: 49.34
Iteration: 3968 | Episodes: 161300 | Median Reward: 46.91 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.6        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 3970         |
|    time_elapsed         | 56460        |
|    total_timesteps      | 16261120     |
| train/                  |              |
|    approx_kl            | 5.632188e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -148         |
|    explained_variance   | 0.883        |
|    learning_rate        | 0.0005       |
|    loss                 | 37.1         |
|    n_updates            | 39690        |
|    policy_gradient_loss | -0.000442    |
|    std                  | 21.2         |
|    value_loss           | 87.9         |
------------------------------------------
Iteration: 3970 | Episodes: 161400 | Median Reward: 46.98 | Max Reward: 49.34
Iteration: 3973 | Episodes: 161500 | Median Reward: 46.10 | Max Reward: 49.34
Iteration: 3975 | Episodes: 161600 | Median Reward: 46.13 | Max Reward: 49.34
Iteration: 3978 | Episodes: 161700 | Median Reward: 46.95 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.4        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 3980         |
|    time_elapsed         | 56599        |
|    total_timesteps      | 16302080     |
| train/                  |              |
|    approx_kl            | 0.0026952499 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -148         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.26        |
|    n_updates            | 39790        |
|    policy_gradient_loss | -0.00881     |
|    std                  | 21.3         |
|    value_loss           | 0.231        |
------------------------------------------
Iteration: 3980 | Episodes: 161800 | Median Reward: 45.59 | Max Reward: 49.34
Iteration: 3983 | Episodes: 161900 | Median Reward: 47.21 | Max Reward: 49.34
Iteration: 3985 | Episodes: 162000 | Median Reward: 45.74 | Max Reward: 49.34
Iteration: 3987 | Episodes: 162100 | Median Reward: 46.89 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.9         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 3990          |
|    time_elapsed         | 56739         |
|    total_timesteps      | 16343040      |
| train/                  |               |
|    approx_kl            | 0.00018212287 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -148          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.23         |
|    n_updates            | 39890         |
|    policy_gradient_loss | -0.0019       |
|    std                  | 21.6          |
|    value_loss           | 0.402         |
-------------------------------------------
Iteration: 3990 | Episodes: 162200 | Median Reward: 44.48 | Max Reward: 49.34
Iteration: 3992 | Episodes: 162300 | Median Reward: 43.43 | Max Reward: 49.34
Iteration: 3995 | Episodes: 162400 | Median Reward: 46.36 | Max Reward: 49.34
Iteration: 3997 | Episodes: 162500 | Median Reward: 45.23 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.8       |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 4000        |
|    time_elapsed         | 56877       |
|    total_timesteps      | 16384000    |
| train/                  |             |
|    approx_kl            | 0.000809961 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -148        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.22       |
|    n_updates            | 39990       |
|    policy_gradient_loss | -0.00643    |
|    std                  | 21.7        |
|    value_loss           | 0.428       |
-----------------------------------------
Iteration: 4000 | Episodes: 162600 | Median Reward: 46.27 | Max Reward: 49.34
Iteration: 4002 | Episodes: 162700 | Median Reward: 43.61 | Max Reward: 49.34
Iteration: 4005 | Episodes: 162800 | Median Reward: 46.37 | Max Reward: 49.34
Iteration: 4007 | Episodes: 162900 | Median Reward: 43.78 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4010          |
|    time_elapsed         | 57016         |
|    total_timesteps      | 16424960      |
| train/                  |               |
|    approx_kl            | 3.9607432e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -148          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.34         |
|    n_updates            | 40090         |
|    policy_gradient_loss | -0.000544     |
|    std                  | 21.9          |
|    value_loss           | 0.59          |
-------------------------------------------
Iteration: 4010 | Episodes: 163000 | Median Reward: 44.05 | Max Reward: 49.34
Iteration: 4012 | Episodes: 163100 | Median Reward: 46.69 | Max Reward: 49.34
Iteration: 4015 | Episodes: 163200 | Median Reward: 45.67 | Max Reward: 49.34
Iteration: 4017 | Episodes: 163300 | Median Reward: 43.80 | Max Reward: 49.34
Iteration: 4019 | Episodes: 163400 | Median Reward: 44.78 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.6         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4020          |
|    time_elapsed         | 57154         |
|    total_timesteps      | 16465920      |
| train/                  |               |
|    approx_kl            | 0.00022912383 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -148          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.28         |
|    n_updates            | 40190         |
|    policy_gradient_loss | -0.00132      |
|    std                  | 22.1          |
|    value_loss           | 0.362         |
-------------------------------------------
Iteration: 4022 | Episodes: 163500 | Median Reward: 44.35 | Max Reward: 49.34
Iteration: 4024 | Episodes: 163600 | Median Reward: 45.18 | Max Reward: 49.34
Iteration: 4027 | Episodes: 163700 | Median Reward: 44.32 | Max Reward: 49.34
Iteration: 4029 | Episodes: 163800 | Median Reward: 46.34 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.7         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4030          |
|    time_elapsed         | 57294         |
|    total_timesteps      | 16506880      |
| train/                  |               |
|    approx_kl            | 0.00030242844 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -148          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.35         |
|    n_updates            | 40290         |
|    policy_gradient_loss | -0.00237      |
|    std                  | 22.2          |
|    value_loss           | 0.25          |
-------------------------------------------
Iteration: 4032 | Episodes: 163900 | Median Reward: 46.39 | Max Reward: 49.34
Iteration: 4034 | Episodes: 164000 | Median Reward: 47.00 | Max Reward: 49.34
Iteration: 4037 | Episodes: 164100 | Median Reward: 45.75 | Max Reward: 49.34
Iteration: 4039 | Episodes: 164200 | Median Reward: 46.53 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54           |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4040          |
|    time_elapsed         | 57434         |
|    total_timesteps      | 16547840      |
| train/                  |               |
|    approx_kl            | 0.00013390154 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -149          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.04         |
|    n_updates            | 40390         |
|    policy_gradient_loss | -0.000553     |
|    std                  | 22.4          |
|    value_loss           | 0.851         |
-------------------------------------------
Iteration: 4042 | Episodes: 164300 | Median Reward: 47.01 | Max Reward: 49.34
Iteration: 4044 | Episodes: 164400 | Median Reward: 47.16 | Max Reward: 49.34
Iteration: 4047 | Episodes: 164500 | Median Reward: 43.83 | Max Reward: 49.34
Iteration: 4049 | Episodes: 164600 | Median Reward: 44.40 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.5       |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 4050        |
|    time_elapsed         | 57575       |
|    total_timesteps      | 16588800    |
| train/                  |             |
|    approx_kl            | 0.000479376 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -149        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.25       |
|    n_updates            | 40490       |
|    policy_gradient_loss | 0.000348    |
|    std                  | 22.4        |
|    value_loss           | 0.483       |
-----------------------------------------
Iteration: 4051 | Episodes: 164700 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 4054 | Episodes: 164800 | Median Reward: 46.63 | Max Reward: 49.34
Iteration: 4056 | Episodes: 164900 | Median Reward: 46.74 | Max Reward: 49.34
Iteration: 4059 | Episodes: 165000 | Median Reward: 44.34 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.4        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4060         |
|    time_elapsed         | 57715        |
|    total_timesteps      | 16629760     |
| train/                  |              |
|    approx_kl            | 6.253843e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -149         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.19        |
|    n_updates            | 40590        |
|    policy_gradient_loss | -0.000741    |
|    std                  | 22.5         |
|    value_loss           | 0.656        |
------------------------------------------
Iteration: 4061 | Episodes: 165100 | Median Reward: 46.08 | Max Reward: 49.34
Iteration: 4064 | Episodes: 165200 | Median Reward: 43.71 | Max Reward: 49.34
Iteration: 4066 | Episodes: 165300 | Median Reward: 46.23 | Max Reward: 49.34
Iteration: 4069 | Episodes: 165400 | Median Reward: 45.74 | Max Reward: 49.34
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -56.4      |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 4070       |
|    time_elapsed         | 57851      |
|    total_timesteps      | 16670720   |
| train/                  |            |
|    approx_kl            | 0.00566351 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -149       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0005     |
|    loss                 | -7.15      |
|    n_updates            | 40690      |
|    policy_gradient_loss | -0.00857   |
|    std                  | 22.8       |
|    value_loss           | 0.905      |
----------------------------------------
Iteration: 4071 | Episodes: 165500 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 4074 | Episodes: 165600 | Median Reward: 42.76 | Max Reward: 49.34
Iteration: 4076 | Episodes: 165700 | Median Reward: 46.13 | Max Reward: 49.34
Iteration: 4079 | Episodes: 165800 | Median Reward: 42.45 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.7         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4080          |
|    time_elapsed         | 57991         |
|    total_timesteps      | 16711680      |
| train/                  |               |
|    approx_kl            | 0.00081952685 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -149          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.33         |
|    n_updates            | 40790         |
|    policy_gradient_loss | -0.00193      |
|    std                  | 23.1          |
|    value_loss           | 0.257         |
-------------------------------------------
Iteration: 4081 | Episodes: 165900 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 4084 | Episodes: 166000 | Median Reward: 46.86 | Max Reward: 49.34
Iteration: 4086 | Episodes: 166100 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 4088 | Episodes: 166200 | Median Reward: 45.24 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -54.2         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4090          |
|    time_elapsed         | 58132         |
|    total_timesteps      | 16752640      |
| train/                  |               |
|    approx_kl            | 3.8340775e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -149          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.36         |
|    n_updates            | 40890         |
|    policy_gradient_loss | -0.000206     |
|    std                  | 23.3          |
|    value_loss           | 0.341         |
-------------------------------------------
Iteration: 4091 | Episodes: 166300 | Median Reward: 46.78 | Max Reward: 49.34
Iteration: 4093 | Episodes: 166400 | Median Reward: 43.71 | Max Reward: 49.34
Iteration: 4096 | Episodes: 166500 | Median Reward: 43.81 | Max Reward: 49.34
Iteration: 4098 | Episodes: 166600 | Median Reward: 43.48 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.5         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4100          |
|    time_elapsed         | 58272         |
|    total_timesteps      | 16793600      |
| train/                  |               |
|    approx_kl            | 0.00097636157 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -149          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.44         |
|    n_updates            | 40990         |
|    policy_gradient_loss | -0.00398      |
|    std                  | 23.5          |
|    value_loss           | 0.124         |
-------------------------------------------
Iteration: 4101 | Episodes: 166700 | Median Reward: 46.86 | Max Reward: 49.34
Iteration: 4103 | Episodes: 166800 | Median Reward: 43.20 | Max Reward: 49.34
Iteration: 4106 | Episodes: 166900 | Median Reward: 44.20 | Max Reward: 49.34
Iteration: 4108 | Episodes: 167000 | Median Reward: 46.00 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.5       |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 4110        |
|    time_elapsed         | 58410       |
|    total_timesteps      | 16834560    |
| train/                  |             |
|    approx_kl            | 0.021743234 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -150        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.35       |
|    n_updates            | 41090       |
|    policy_gradient_loss | -0.0476     |
|    std                  | 23.6        |
|    value_loss           | 0.46        |
-----------------------------------------
Iteration: 4111 | Episodes: 167100 | Median Reward: 45.74 | Max Reward: 49.34
Iteration: 4113 | Episodes: 167200 | Median Reward: 44.00 | Max Reward: 49.34
Iteration: 4116 | Episodes: 167300 | Median Reward: 44.17 | Max Reward: 49.34
Iteration: 4118 | Episodes: 167400 | Median Reward: 45.97 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.4        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4120         |
|    time_elapsed         | 58550        |
|    total_timesteps      | 16875520     |
| train/                  |              |
|    approx_kl            | 0.0001100198 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -150         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.38        |
|    n_updates            | 41190        |
|    policy_gradient_loss | -0.000751    |
|    std                  | 23.7         |
|    value_loss           | 0.42         |
------------------------------------------
Iteration: 4120 | Episodes: 167500 | Median Reward: 45.28 | Max Reward: 49.34
Iteration: 4123 | Episodes: 167600 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 4125 | Episodes: 167700 | Median Reward: 43.85 | Max Reward: 49.34
Iteration: 4128 | Episodes: 167800 | Median Reward: 43.62 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4130         |
|    time_elapsed         | 58696        |
|    total_timesteps      | 16916480     |
| train/                  |              |
|    approx_kl            | 0.0014148518 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -150         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.43        |
|    n_updates            | 41290        |
|    policy_gradient_loss | -0.0051      |
|    std                  | 23.9         |
|    value_loss           | 0.187        |
------------------------------------------
Iteration: 4130 | Episodes: 167900 | Median Reward: 46.27 | Max Reward: 49.34
Iteration: 4133 | Episodes: 168000 | Median Reward: 45.19 | Max Reward: 49.34
Iteration: 4135 | Episodes: 168100 | Median Reward: 46.87 | Max Reward: 49.34
Iteration: 4138 | Episodes: 168200 | Median Reward: 46.33 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.1         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4140          |
|    time_elapsed         | 58838         |
|    total_timesteps      | 16957440      |
| train/                  |               |
|    approx_kl            | 0.00020526805 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -150          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.42         |
|    n_updates            | 41390         |
|    policy_gradient_loss | -0.000392     |
|    std                  | 24            |
|    value_loss           | 0.176         |
-------------------------------------------
Iteration: 4140 | Episodes: 168300 | Median Reward: 46.37 | Max Reward: 49.34
Iteration: 4143 | Episodes: 168400 | Median Reward: 46.99 | Max Reward: 49.34
Iteration: 4145 | Episodes: 168500 | Median Reward: 46.07 | Max Reward: 49.34
Iteration: 4148 | Episodes: 168600 | Median Reward: 45.19 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.8        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4150         |
|    time_elapsed         | 58977        |
|    total_timesteps      | 16998400     |
| train/                  |              |
|    approx_kl            | 0.0016168258 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -150         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.44        |
|    n_updates            | 41490        |
|    policy_gradient_loss | -0.00626     |
|    std                  | 24.4         |
|    value_loss           | 0.205        |
------------------------------------------
Iteration: 4150 | Episodes: 168700 | Median Reward: 45.74 | Max Reward: 49.34
Iteration: 4153 | Episodes: 168800 | Median Reward: 46.09 | Max Reward: 49.34
Iteration: 4155 | Episodes: 168900 | Median Reward: 43.76 | Max Reward: 49.34
Iteration: 4157 | Episodes: 169000 | Median Reward: 43.67 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.8        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4160         |
|    time_elapsed         | 59118        |
|    total_timesteps      | 17039360     |
| train/                  |              |
|    approx_kl            | 8.038551e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -150         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.43        |
|    n_updates            | 41590        |
|    policy_gradient_loss | -4.94e-05    |
|    std                  | 24.7         |
|    value_loss           | 0.494        |
------------------------------------------
Iteration: 4160 | Episodes: 169100 | Median Reward: 46.63 | Max Reward: 49.34
Iteration: 4162 | Episodes: 169200 | Median Reward: 46.89 | Max Reward: 49.34
Iteration: 4165 | Episodes: 169300 | Median Reward: 44.30 | Max Reward: 49.34
Iteration: 4167 | Episodes: 169400 | Median Reward: 46.13 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.5       |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 4170        |
|    time_elapsed         | 59257       |
|    total_timesteps      | 17080320    |
| train/                  |             |
|    approx_kl            | 0.007409867 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -150        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.42       |
|    n_updates            | 41690       |
|    policy_gradient_loss | -0.0244     |
|    std                  | 24.9        |
|    value_loss           | 0.481       |
-----------------------------------------
Iteration: 4170 | Episodes: 169500 | Median Reward: 43.39 | Max Reward: 49.34
Iteration: 4172 | Episodes: 169600 | Median Reward: 46.00 | Max Reward: 49.34
Iteration: 4175 | Episodes: 169700 | Median Reward: 46.38 | Max Reward: 49.34
Iteration: 4177 | Episodes: 169800 | Median Reward: 44.20 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.9       |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 4180        |
|    time_elapsed         | 59399       |
|    total_timesteps      | 17121280    |
| train/                  |             |
|    approx_kl            | 0.002573261 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -150        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.38       |
|    n_updates            | 41790       |
|    policy_gradient_loss | -0.00817    |
|    std                  | 24.9        |
|    value_loss           | 0.27        |
-----------------------------------------
Iteration: 4180 | Episodes: 169900 | Median Reward: 47.24 | Max Reward: 49.34
Iteration: 4182 | Episodes: 170000 | Median Reward: 44.22 | Max Reward: 49.34
Iteration: 4185 | Episodes: 170100 | Median Reward: 44.24 | Max Reward: 49.34
Iteration: 4187 | Episodes: 170200 | Median Reward: 44.23 | Max Reward: 49.34
Iteration: 4189 | Episodes: 170300 | Median Reward: 42.77 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4190         |
|    time_elapsed         | 59538        |
|    total_timesteps      | 17162240     |
| train/                  |              |
|    approx_kl            | 0.0005546977 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -150         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.32        |
|    n_updates            | 41890        |
|    policy_gradient_loss | -0.00239     |
|    std                  | 25           |
|    value_loss           | 0.337        |
------------------------------------------
Iteration: 4192 | Episodes: 170400 | Median Reward: 43.06 | Max Reward: 49.34
Iteration: 4194 | Episodes: 170500 | Median Reward: 43.76 | Max Reward: 49.34
Iteration: 4197 | Episodes: 170600 | Median Reward: 43.96 | Max Reward: 49.34
Iteration: 4199 | Episodes: 170700 | Median Reward: 43.51 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.1         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4200          |
|    time_elapsed         | 59675         |
|    total_timesteps      | 17203200      |
| train/                  |               |
|    approx_kl            | 0.00014793954 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -151          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.35         |
|    n_updates            | 41990         |
|    policy_gradient_loss | -0.000808     |
|    std                  | 25.2          |
|    value_loss           | 0.301         |
-------------------------------------------
Iteration: 4202 | Episodes: 170800 | Median Reward: 47.48 | Max Reward: 49.34
Iteration: 4204 | Episodes: 170900 | Median Reward: 46.73 | Max Reward: 49.34
Iteration: 4207 | Episodes: 171000 | Median Reward: 44.24 | Max Reward: 49.34
Iteration: 4209 | Episodes: 171100 | Median Reward: 45.65 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55          |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4210         |
|    time_elapsed         | 59816        |
|    total_timesteps      | 17244160     |
| train/                  |              |
|    approx_kl            | 0.0038808575 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -151         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.49        |
|    n_updates            | 42090        |
|    policy_gradient_loss | -0.011       |
|    std                  | 25.5         |
|    value_loss           | 0.14         |
------------------------------------------
Iteration: 4212 | Episodes: 171200 | Median Reward: 44.33 | Max Reward: 49.34
Iteration: 4214 | Episodes: 171300 | Median Reward: 46.17 | Max Reward: 49.34
Iteration: 4217 | Episodes: 171400 | Median Reward: 43.35 | Max Reward: 49.34
Iteration: 4219 | Episodes: 171500 | Median Reward: 44.61 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.6       |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 4220        |
|    time_elapsed         | 59956       |
|    total_timesteps      | 17285120    |
| train/                  |             |
|    approx_kl            | 0.005751688 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -151        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.52       |
|    n_updates            | 42190       |
|    policy_gradient_loss | -0.0137     |
|    std                  | 25.8        |
|    value_loss           | 0.162       |
-----------------------------------------
Iteration: 4222 | Episodes: 171600 | Median Reward: 45.97 | Max Reward: 49.34
Iteration: 4224 | Episodes: 171700 | Median Reward: 46.68 | Max Reward: 49.34
Iteration: 4226 | Episodes: 171800 | Median Reward: 47.28 | Max Reward: 49.34
Iteration: 4229 | Episodes: 171900 | Median Reward: 42.96 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.9         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4230          |
|    time_elapsed         | 60096         |
|    total_timesteps      | 17326080      |
| train/                  |               |
|    approx_kl            | 1.3279117e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -151          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.31         |
|    n_updates            | 42290         |
|    policy_gradient_loss | -8.49e-05     |
|    std                  | 26            |
|    value_loss           | 0.397         |
-------------------------------------------
Iteration: 4231 | Episodes: 172000 | Median Reward: 46.82 | Max Reward: 49.34
Iteration: 4234 | Episodes: 172100 | Median Reward: 44.58 | Max Reward: 49.34
Iteration: 4236 | Episodes: 172200 | Median Reward: 44.21 | Max Reward: 49.34
Iteration: 4239 | Episodes: 172300 | Median Reward: 46.67 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55           |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4240          |
|    time_elapsed         | 60236         |
|    total_timesteps      | 17367040      |
| train/                  |               |
|    approx_kl            | 0.00015356278 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -151          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.45         |
|    n_updates            | 42390         |
|    policy_gradient_loss | -0.000792     |
|    std                  | 26.1          |
|    value_loss           | 0.23          |
-------------------------------------------
Iteration: 4241 | Episodes: 172400 | Median Reward: 45.74 | Max Reward: 49.34
Iteration: 4244 | Episodes: 172500 | Median Reward: 43.73 | Max Reward: 49.34
Iteration: 4246 | Episodes: 172600 | Median Reward: 46.04 | Max Reward: 49.34
Iteration: 4249 | Episodes: 172700 | Median Reward: 46.39 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.2        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4250         |
|    time_elapsed         | 60377        |
|    total_timesteps      | 17408000     |
| train/                  |              |
|    approx_kl            | 0.0014335375 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -151         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.41        |
|    n_updates            | 42490        |
|    policy_gradient_loss | -0.00548     |
|    std                  | 26.2         |
|    value_loss           | 0.336        |
------------------------------------------
Iteration: 4251 | Episodes: 172800 | Median Reward: 46.91 | Max Reward: 49.34
Iteration: 4254 | Episodes: 172900 | Median Reward: 44.06 | Max Reward: 49.34
Iteration: 4256 | Episodes: 173000 | Median Reward: 46.17 | Max Reward: 49.34
Iteration: 4258 | Episodes: 173100 | Median Reward: 46.62 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.3         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4260          |
|    time_elapsed         | 60517         |
|    total_timesteps      | 17448960      |
| train/                  |               |
|    approx_kl            | 4.9349823e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -151          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.51         |
|    n_updates            | 42590         |
|    policy_gradient_loss | -6.8e-05      |
|    std                  | 26.2          |
|    value_loss           | 2.27          |
-------------------------------------------
Iteration: 4261 | Episodes: 173200 | Median Reward: 44.31 | Max Reward: 49.34
Iteration: 4263 | Episodes: 173300 | Median Reward: 46.24 | Max Reward: 49.34
Iteration: 4266 | Episodes: 173400 | Median Reward: 46.27 | Max Reward: 49.34
Iteration: 4268 | Episodes: 173500 | Median Reward: 46.27 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.8        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4270         |
|    time_elapsed         | 60659        |
|    total_timesteps      | 17489920     |
| train/                  |              |
|    approx_kl            | 2.127052e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -151         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.05        |
|    n_updates            | 42690        |
|    policy_gradient_loss | -0.000189    |
|    std                  | 26.5         |
|    value_loss           | 0.414        |
------------------------------------------
Iteration: 4271 | Episodes: 173600 | Median Reward: 43.71 | Max Reward: 49.34
Iteration: 4273 | Episodes: 173700 | Median Reward: 44.26 | Max Reward: 49.34
Iteration: 4276 | Episodes: 173800 | Median Reward: 44.94 | Max Reward: 49.34
Iteration: 4278 | Episodes: 173900 | Median Reward: 46.73 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.3         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4280          |
|    time_elapsed         | 60798         |
|    total_timesteps      | 17530880      |
| train/                  |               |
|    approx_kl            | 0.00011274441 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -151          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.47         |
|    n_updates            | 42790         |
|    policy_gradient_loss | -0.000719     |
|    std                  | 26.6          |
|    value_loss           | 0.423         |
-------------------------------------------
Iteration: 4281 | Episodes: 174000 | Median Reward: 44.37 | Max Reward: 49.34
Iteration: 4283 | Episodes: 174100 | Median Reward: 45.38 | Max Reward: 49.34
Iteration: 4286 | Episodes: 174200 | Median Reward: 47.57 | Max Reward: 49.34
Iteration: 4288 | Episodes: 174300 | Median Reward: 43.68 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.7        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4290         |
|    time_elapsed         | 60933        |
|    total_timesteps      | 17571840     |
| train/                  |              |
|    approx_kl            | 7.758528e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -151         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.49        |
|    n_updates            | 42890        |
|    policy_gradient_loss | -0.000267    |
|    std                  | 26.8         |
|    value_loss           | 0.183        |
------------------------------------------
Iteration: 4291 | Episodes: 174400 | Median Reward: 46.45 | Max Reward: 49.34
Iteration: 4293 | Episodes: 174500 | Median Reward: 43.53 | Max Reward: 49.34
Iteration: 4295 | Episodes: 174600 | Median Reward: 47.48 | Max Reward: 49.34
Iteration: 4298 | Episodes: 174700 | Median Reward: 45.08 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.5         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4300          |
|    time_elapsed         | 61071         |
|    total_timesteps      | 17612800      |
| train/                  |               |
|    approx_kl            | 0.00013512315 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -151          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.46         |
|    n_updates            | 42990         |
|    policy_gradient_loss | -0.000426     |
|    std                  | 26.9          |
|    value_loss           | 0.32          |
-------------------------------------------
Iteration: 4300 | Episodes: 174800 | Median Reward: 43.67 | Max Reward: 49.34
Iteration: 4303 | Episodes: 174900 | Median Reward: 46.14 | Max Reward: 49.34
Iteration: 4305 | Episodes: 175000 | Median Reward: 43.96 | Max Reward: 49.34
Iteration: 4308 | Episodes: 175100 | Median Reward: 46.61 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.5       |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 4310        |
|    time_elapsed         | 61209       |
|    total_timesteps      | 17653760    |
| train/                  |             |
|    approx_kl            | 0.000293861 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -151        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.52       |
|    n_updates            | 43090       |
|    policy_gradient_loss | -0.00267    |
|    std                  | 26.9        |
|    value_loss           | 0.169       |
-----------------------------------------
Iteration: 4310 | Episodes: 175200 | Median Reward: 44.41 | Max Reward: 49.34
Iteration: 4313 | Episodes: 175300 | Median Reward: 46.69 | Max Reward: 49.34
Iteration: 4315 | Episodes: 175400 | Median Reward: 43.97 | Max Reward: 49.34
Iteration: 4318 | Episodes: 175500 | Median Reward: 44.32 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.5         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4320          |
|    time_elapsed         | 61347         |
|    total_timesteps      | 17694720      |
| train/                  |               |
|    approx_kl            | 0.00012535206 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.46         |
|    n_updates            | 43190         |
|    policy_gradient_loss | -0.000865     |
|    std                  | 27.1          |
|    value_loss           | 0.248         |
-------------------------------------------
Iteration: 4320 | Episodes: 175600 | Median Reward: 46.44 | Max Reward: 49.34
Iteration: 4323 | Episodes: 175700 | Median Reward: 43.68 | Max Reward: 49.34
Iteration: 4325 | Episodes: 175800 | Median Reward: 45.38 | Max Reward: 49.34
Iteration: 4327 | Episodes: 175900 | Median Reward: 44.52 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55           |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4330          |
|    time_elapsed         | 61484         |
|    total_timesteps      | 17735680      |
| train/                  |               |
|    approx_kl            | 3.5675563e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.55         |
|    n_updates            | 43290         |
|    policy_gradient_loss | -0.000421     |
|    std                  | 27.3          |
|    value_loss           | 0.213         |
-------------------------------------------
Iteration: 4330 | Episodes: 176000 | Median Reward: 43.68 | Max Reward: 49.34
Iteration: 4332 | Episodes: 176100 | Median Reward: 43.78 | Max Reward: 49.34
Iteration: 4335 | Episodes: 176200 | Median Reward: 44.47 | Max Reward: 49.34
Iteration: 4337 | Episodes: 176300 | Median Reward: 43.37 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.3        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4340         |
|    time_elapsed         | 61624        |
|    total_timesteps      | 17776640     |
| train/                  |              |
|    approx_kl            | 1.579232e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -152         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.52        |
|    n_updates            | 43390        |
|    policy_gradient_loss | 9.12e-06     |
|    std                  | 27.5         |
|    value_loss           | 0.352        |
------------------------------------------
Iteration: 4340 | Episodes: 176400 | Median Reward: 46.13 | Max Reward: 49.34
Iteration: 4342 | Episodes: 176500 | Median Reward: 46.35 | Max Reward: 49.34
Iteration: 4345 | Episodes: 176600 | Median Reward: 46.64 | Max Reward: 49.34
Iteration: 4347 | Episodes: 176700 | Median Reward: 46.89 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.6         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4350          |
|    time_elapsed         | 61764         |
|    total_timesteps      | 17817600      |
| train/                  |               |
|    approx_kl            | 0.00033239892 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.52         |
|    n_updates            | 43490         |
|    policy_gradient_loss | -0.00066      |
|    std                  | 27.8          |
|    value_loss           | 0.22          |
-------------------------------------------
Iteration: 4350 | Episodes: 176800 | Median Reward: 43.98 | Max Reward: 49.34
Iteration: 4352 | Episodes: 176900 | Median Reward: 43.74 | Max Reward: 49.34
Iteration: 4355 | Episodes: 177000 | Median Reward: 46.35 | Max Reward: 49.34
Iteration: 4357 | Episodes: 177100 | Median Reward: 45.09 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.9         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4360          |
|    time_elapsed         | 61901         |
|    total_timesteps      | 17858560      |
| train/                  |               |
|    approx_kl            | 0.00030578405 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.76         |
|    n_updates            | 43590         |
|    policy_gradient_loss | -0.001        |
|    std                  | 27.9          |
|    value_loss           | 1.77          |
-------------------------------------------
Iteration: 4360 | Episodes: 177200 | Median Reward: 42.97 | Max Reward: 49.34
Iteration: 4362 | Episodes: 177300 | Median Reward: 45.74 | Max Reward: 49.34
Iteration: 4364 | Episodes: 177400 | Median Reward: 45.08 | Max Reward: 49.34
Iteration: 4367 | Episodes: 177500 | Median Reward: 45.26 | Max Reward: 49.34
Iteration: 4369 | Episodes: 177600 | Median Reward: 46.25 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.4        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4370         |
|    time_elapsed         | 62047        |
|    total_timesteps      | 17899520     |
| train/                  |              |
|    approx_kl            | 0.0033876477 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -152         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.29        |
|    n_updates            | 43690        |
|    policy_gradient_loss | -0.0156      |
|    std                  | 28.1         |
|    value_loss           | 0.363        |
------------------------------------------
Iteration: 4372 | Episodes: 177700 | Median Reward: 46.51 | Max Reward: 49.34
Iteration: 4374 | Episodes: 177800 | Median Reward: 46.30 | Max Reward: 49.34
Iteration: 4377 | Episodes: 177900 | Median Reward: 44.26 | Max Reward: 49.34
Iteration: 4379 | Episodes: 178000 | Median Reward: 46.33 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.2        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4380         |
|    time_elapsed         | 62192        |
|    total_timesteps      | 17940480     |
| train/                  |              |
|    approx_kl            | 0.0001285946 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -152         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.48        |
|    n_updates            | 43790        |
|    policy_gradient_loss | -0.00102     |
|    std                  | 28.4         |
|    value_loss           | 0.343        |
------------------------------------------
Iteration: 4382 | Episodes: 178100 | Median Reward: 44.20 | Max Reward: 49.34
Iteration: 4384 | Episodes: 178200 | Median Reward: 46.72 | Max Reward: 49.34
Iteration: 4387 | Episodes: 178300 | Median Reward: 46.33 | Max Reward: 49.34
Iteration: 4389 | Episodes: 178400 | Median Reward: 43.50 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -56.2         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4390          |
|    time_elapsed         | 62331         |
|    total_timesteps      | 17981440      |
| train/                  |               |
|    approx_kl            | 1.5977828e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.39         |
|    n_updates            | 43890         |
|    policy_gradient_loss | 9e-05         |
|    std                  | 28.6          |
|    value_loss           | 4.98          |
-------------------------------------------
Iteration: 4392 | Episodes: 178500 | Median Reward: 44.87 | Max Reward: 49.34
Iteration: 4394 | Episodes: 178600 | Median Reward: 46.39 | Max Reward: 49.34
Iteration: 4396 | Episodes: 178700 | Median Reward: 46.22 | Max Reward: 49.34
Iteration: 4399 | Episodes: 178800 | Median Reward: 46.89 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.6         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4400          |
|    time_elapsed         | 62468         |
|    total_timesteps      | 18022400      |
| train/                  |               |
|    approx_kl            | 0.00011155772 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -153          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.58         |
|    n_updates            | 43990         |
|    policy_gradient_loss | -0.000316     |
|    std                  | 28.9          |
|    value_loss           | 0.103         |
-------------------------------------------
Iteration: 4401 | Episodes: 178900 | Median Reward: 43.97 | Max Reward: 49.34
Iteration: 4404 | Episodes: 179000 | Median Reward: 45.69 | Max Reward: 49.34
Iteration: 4406 | Episodes: 179100 | Median Reward: 46.31 | Max Reward: 49.34
Iteration: 4409 | Episodes: 179200 | Median Reward: 46.94 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.4        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4410         |
|    time_elapsed         | 62609        |
|    total_timesteps      | 18063360     |
| train/                  |              |
|    approx_kl            | 0.0018028448 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -153         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.55        |
|    n_updates            | 44090        |
|    policy_gradient_loss | -0.00706     |
|    std                  | 29.3         |
|    value_loss           | 0.179        |
------------------------------------------
Iteration: 4411 | Episodes: 179300 | Median Reward: 46.09 | Max Reward: 49.34
Iteration: 4414 | Episodes: 179400 | Median Reward: 46.66 | Max Reward: 49.34
Iteration: 4416 | Episodes: 179500 | Median Reward: 46.52 | Max Reward: 49.34
Iteration: 4419 | Episodes: 179600 | Median Reward: 45.63 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.3         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4420          |
|    time_elapsed         | 62748         |
|    total_timesteps      | 18104320      |
| train/                  |               |
|    approx_kl            | 0.00070592476 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -153          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.56         |
|    n_updates            | 44190         |
|    policy_gradient_loss | -0.00445      |
|    std                  | 29.5          |
|    value_loss           | 0.255         |
-------------------------------------------
Iteration: 4421 | Episodes: 179700 | Median Reward: 46.14 | Max Reward: 49.34
Iteration: 4424 | Episodes: 179800 | Median Reward: 43.44 | Max Reward: 49.34
Iteration: 4426 | Episodes: 179900 | Median Reward: 46.61 | Max Reward: 49.34
Iteration: 4429 | Episodes: 180000 | Median Reward: 46.67 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4430         |
|    time_elapsed         | 62889        |
|    total_timesteps      | 18145280     |
| train/                  |              |
|    approx_kl            | 0.0027915956 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -153         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.54        |
|    n_updates            | 44290        |
|    policy_gradient_loss | -0.00777     |
|    std                  | 29.7         |
|    value_loss           | 0.348        |
------------------------------------------
Iteration: 4431 | Episodes: 180100 | Median Reward: 46.25 | Max Reward: 49.34
Iteration: 4433 | Episodes: 180200 | Median Reward: 46.37 | Max Reward: 49.34
Iteration: 4436 | Episodes: 180300 | Median Reward: 46.67 | Max Reward: 49.34
Iteration: 4438 | Episodes: 180400 | Median Reward: 44.84 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.8        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4440         |
|    time_elapsed         | 63028        |
|    total_timesteps      | 18186240     |
| train/                  |              |
|    approx_kl            | 0.0004111543 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -153         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.21        |
|    n_updates            | 44390        |
|    policy_gradient_loss | -0.00236     |
|    std                  | 29.9         |
|    value_loss           | 0.444        |
------------------------------------------
Iteration: 4441 | Episodes: 180500 | Median Reward: 43.67 | Max Reward: 49.34
Iteration: 4443 | Episodes: 180600 | Median Reward: 46.09 | Max Reward: 49.34
Iteration: 4446 | Episodes: 180700 | Median Reward: 45.72 | Max Reward: 49.34
Iteration: 4448 | Episodes: 180800 | Median Reward: 46.92 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54          |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4450         |
|    time_elapsed         | 63171        |
|    total_timesteps      | 18227200     |
| train/                  |              |
|    approx_kl            | 8.478746e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -153         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.62        |
|    n_updates            | 44490        |
|    policy_gradient_loss | -0.00119     |
|    std                  | 30.2         |
|    value_loss           | 0.233        |
------------------------------------------
Iteration: 4451 | Episodes: 180900 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 4453 | Episodes: 181000 | Median Reward: 46.28 | Max Reward: 49.34
Iteration: 4456 | Episodes: 181100 | Median Reward: 46.95 | Max Reward: 49.34
Iteration: 4458 | Episodes: 181200 | Median Reward: 46.91 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.3        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4460         |
|    time_elapsed         | 63311        |
|    total_timesteps      | 18268160     |
| train/                  |              |
|    approx_kl            | 0.0003438693 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -154         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.62        |
|    n_updates            | 44590        |
|    policy_gradient_loss | -0.001       |
|    std                  | 30.5         |
|    value_loss           | 0.0988       |
------------------------------------------
Iteration: 4461 | Episodes: 181300 | Median Reward: 46.57 | Max Reward: 49.34
Iteration: 4463 | Episodes: 181400 | Median Reward: 43.51 | Max Reward: 49.34
Iteration: 4466 | Episodes: 181500 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 4468 | Episodes: 181600 | Median Reward: 46.34 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.5        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4470         |
|    time_elapsed         | 63453        |
|    total_timesteps      | 18309120     |
| train/                  |              |
|    approx_kl            | 0.0006347877 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -154         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.61        |
|    n_updates            | 44690        |
|    policy_gradient_loss | -0.00453     |
|    std                  | 30.9         |
|    value_loss           | 0.124        |
------------------------------------------
Iteration: 4470 | Episodes: 181700 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 4473 | Episodes: 181800 | Median Reward: 44.62 | Max Reward: 49.34
Iteration: 4475 | Episodes: 181900 | Median Reward: 45.82 | Max Reward: 49.34
Iteration: 4478 | Episodes: 182000 | Median Reward: 46.41 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.3         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4480          |
|    time_elapsed         | 63594         |
|    total_timesteps      | 18350080      |
| train/                  |               |
|    approx_kl            | 0.00032921424 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -154          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.62         |
|    n_updates            | 44790         |
|    policy_gradient_loss | -0.00174      |
|    std                  | 31.1          |
|    value_loss           | 0.232         |
-------------------------------------------
Iteration: 4480 | Episodes: 182100 | Median Reward: 44.44 | Max Reward: 49.34
Iteration: 4483 | Episodes: 182200 | Median Reward: 46.14 | Max Reward: 49.34
Iteration: 4485 | Episodes: 182300 | Median Reward: 43.37 | Max Reward: 49.34
Iteration: 4488 | Episodes: 182400 | Median Reward: 43.45 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.6       |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 4490        |
|    time_elapsed         | 63734       |
|    total_timesteps      | 18391040    |
| train/                  |             |
|    approx_kl            | 0.005045928 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -154        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.62       |
|    n_updates            | 44890       |
|    policy_gradient_loss | -0.0164     |
|    std                  | 31.2        |
|    value_loss           | 0.242       |
-----------------------------------------
Iteration: 4490 | Episodes: 182500 | Median Reward: 44.47 | Max Reward: 49.34
Iteration: 4493 | Episodes: 182600 | Median Reward: 43.74 | Max Reward: 49.34
Iteration: 4495 | Episodes: 182700 | Median Reward: 45.85 | Max Reward: 49.34
Iteration: 4498 | Episodes: 182800 | Median Reward: 46.90 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4500         |
|    time_elapsed         | 63875        |
|    total_timesteps      | 18432000     |
| train/                  |              |
|    approx_kl            | 0.0006573548 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -154         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.38        |
|    n_updates            | 44990        |
|    policy_gradient_loss | -0.00526     |
|    std                  | 31.3         |
|    value_loss           | 1.33         |
------------------------------------------
Iteration: 4500 | Episodes: 182900 | Median Reward: 43.59 | Max Reward: 49.34
Iteration: 4503 | Episodes: 183000 | Median Reward: 43.53 | Max Reward: 49.34
Iteration: 4505 | Episodes: 183100 | Median Reward: 44.81 | Max Reward: 49.34
Iteration: 4507 | Episodes: 183200 | Median Reward: 45.42 | Max Reward: 49.34
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -56        |
| time/                   |            |
|    fps                  | 288        |
|    iterations           | 4510       |
|    time_elapsed         | 64017      |
|    total_timesteps      | 18472960   |
| train/                  |            |
|    approx_kl            | 0.00025849 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -154       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -7.5       |
|    n_updates            | 45090      |
|    policy_gradient_loss | -0.00172   |
|    std                  | 31.6       |
|    value_loss           | 0.315      |
----------------------------------------
Iteration: 4510 | Episodes: 183300 | Median Reward: 44.55 | Max Reward: 49.34
Iteration: 4512 | Episodes: 183400 | Median Reward: 46.17 | Max Reward: 49.34
Iteration: 4515 | Episodes: 183500 | Median Reward: 46.69 | Max Reward: 49.34
Iteration: 4517 | Episodes: 183600 | Median Reward: 46.04 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.5         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4520          |
|    time_elapsed         | 64157         |
|    total_timesteps      | 18513920      |
| train/                  |               |
|    approx_kl            | 0.00039293332 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -154          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.46         |
|    n_updates            | 45190         |
|    policy_gradient_loss | -0.00163      |
|    std                  | 31.8          |
|    value_loss           | 0.26          |
-------------------------------------------
Iteration: 4520 | Episodes: 183700 | Median Reward: 44.63 | Max Reward: 49.34
Iteration: 4522 | Episodes: 183800 | Median Reward: 47.02 | Max Reward: 49.34
Iteration: 4525 | Episodes: 183900 | Median Reward: 45.65 | Max Reward: 49.34
Iteration: 4527 | Episodes: 184000 | Median Reward: 46.30 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.4         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4530          |
|    time_elapsed         | 64300         |
|    total_timesteps      | 18554880      |
| train/                  |               |
|    approx_kl            | 0.00029665043 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -154          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.63         |
|    n_updates            | 45290         |
|    policy_gradient_loss | -0.00106      |
|    std                  | 32            |
|    value_loss           | 0.336         |
-------------------------------------------
Iteration: 4530 | Episodes: 184100 | Median Reward: 44.64 | Max Reward: 49.34
Iteration: 4532 | Episodes: 184200 | Median Reward: 46.17 | Max Reward: 49.34
Iteration: 4535 | Episodes: 184300 | Median Reward: 46.15 | Max Reward: 49.34
Iteration: 4537 | Episodes: 184400 | Median Reward: 46.08 | Max Reward: 49.34
Iteration: 4539 | Episodes: 184500 | Median Reward: 42.96 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.9         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4540          |
|    time_elapsed         | 64437         |
|    total_timesteps      | 18595840      |
| train/                  |               |
|    approx_kl            | 0.00031866296 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.54         |
|    n_updates            | 45390         |
|    policy_gradient_loss | -0.00142      |
|    std                  | 32.2          |
|    value_loss           | 0.409         |
-------------------------------------------
Iteration: 4542 | Episodes: 184600 | Median Reward: 43.85 | Max Reward: 49.34
Iteration: 4544 | Episodes: 184700 | Median Reward: 43.49 | Max Reward: 49.34
Iteration: 4547 | Episodes: 184800 | Median Reward: 46.33 | Max Reward: 49.34
Iteration: 4549 | Episodes: 184900 | Median Reward: 45.63 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.7         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4550          |
|    time_elapsed         | 64578         |
|    total_timesteps      | 18636800      |
| train/                  |               |
|    approx_kl            | 3.8215032e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.7          |
|    n_updates            | 45490         |
|    policy_gradient_loss | -0.00029      |
|    std                  | 32.5          |
|    value_loss           | 0.179         |
-------------------------------------------
Iteration: 4552 | Episodes: 185000 | Median Reward: 42.74 | Max Reward: 49.34
Iteration: 4554 | Episodes: 185100 | Median Reward: 44.11 | Max Reward: 49.34
Iteration: 4557 | Episodes: 185200 | Median Reward: 43.36 | Max Reward: 49.34
Iteration: 4559 | Episodes: 185300 | Median Reward: 45.44 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.2         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4560          |
|    time_elapsed         | 64722         |
|    total_timesteps      | 18677760      |
| train/                  |               |
|    approx_kl            | 2.4213368e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.68         |
|    n_updates            | 45590         |
|    policy_gradient_loss | -4.66e-05     |
|    std                  | 32.8          |
|    value_loss           | 0.385         |
-------------------------------------------
Iteration: 4562 | Episodes: 185400 | Median Reward: 45.32 | Max Reward: 49.34
Iteration: 4564 | Episodes: 185500 | Median Reward: 44.27 | Max Reward: 49.34
Iteration: 4567 | Episodes: 185600 | Median Reward: 44.53 | Max Reward: 49.34
Iteration: 4569 | Episodes: 185700 | Median Reward: 44.62 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.9         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4570          |
|    time_elapsed         | 64871         |
|    total_timesteps      | 18718720      |
| train/                  |               |
|    approx_kl            | 0.00080829964 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.65         |
|    n_updates            | 45690         |
|    policy_gradient_loss | -0.00347      |
|    std                  | 33.2          |
|    value_loss           | 0.269         |
-------------------------------------------
Iteration: 4571 | Episodes: 185800 | Median Reward: 46.35 | Max Reward: 49.34
Iteration: 4574 | Episodes: 185900 | Median Reward: 46.31 | Max Reward: 49.34
Iteration: 4576 | Episodes: 186000 | Median Reward: 46.28 | Max Reward: 49.34
Iteration: 4579 | Episodes: 186100 | Median Reward: 46.42 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.4         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4580          |
|    time_elapsed         | 65012         |
|    total_timesteps      | 18759680      |
| train/                  |               |
|    approx_kl            | 0.00071652466 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.65         |
|    n_updates            | 45790         |
|    policy_gradient_loss | -0.00341      |
|    std                  | 33.5          |
|    value_loss           | 0.255         |
-------------------------------------------
Iteration: 4581 | Episodes: 186200 | Median Reward: 43.97 | Max Reward: 49.34
Iteration: 4584 | Episodes: 186300 | Median Reward: 44.02 | Max Reward: 49.34
Iteration: 4586 | Episodes: 186400 | Median Reward: 43.71 | Max Reward: 49.34
Iteration: 4589 | Episodes: 186500 | Median Reward: 44.32 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.8        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4590         |
|    time_elapsed         | 65151        |
|    total_timesteps      | 18800640     |
| train/                  |              |
|    approx_kl            | 0.0010680731 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -155         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.53        |
|    n_updates            | 45890        |
|    policy_gradient_loss | -0.00182     |
|    std                  | 33.8         |
|    value_loss           | 0.25         |
------------------------------------------
Iteration: 4591 | Episodes: 186600 | Median Reward: 46.73 | Max Reward: 49.34
Iteration: 4594 | Episodes: 186700 | Median Reward: 43.42 | Max Reward: 49.34
Iteration: 4596 | Episodes: 186800 | Median Reward: 43.19 | Max Reward: 49.34
Iteration: 4599 | Episodes: 186900 | Median Reward: 43.21 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.1         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4600          |
|    time_elapsed         | 65292         |
|    total_timesteps      | 18841600      |
| train/                  |               |
|    approx_kl            | 3.1968448e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.61         |
|    n_updates            | 45990         |
|    policy_gradient_loss | -0.000273     |
|    std                  | 33.9          |
|    value_loss           | 0.41          |
-------------------------------------------
Iteration: 4601 | Episodes: 187000 | Median Reward: 46.94 | Max Reward: 49.34
Iteration: 4604 | Episodes: 187100 | Median Reward: 46.93 | Max Reward: 49.34
Iteration: 4606 | Episodes: 187200 | Median Reward: 46.03 | Max Reward: 49.34
Iteration: 4608 | Episodes: 187300 | Median Reward: 45.65 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.4         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4610          |
|    time_elapsed         | 65433         |
|    total_timesteps      | 18882560      |
| train/                  |               |
|    approx_kl            | 0.00029014228 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.73         |
|    n_updates            | 46090         |
|    policy_gradient_loss | -0.00151      |
|    std                  | 34.2          |
|    value_loss           | 0.271         |
-------------------------------------------
Iteration: 4611 | Episodes: 187400 | Median Reward: 43.51 | Max Reward: 49.34
Iteration: 4613 | Episodes: 187500 | Median Reward: 42.64 | Max Reward: 49.34
Iteration: 4616 | Episodes: 187600 | Median Reward: 46.96 | Max Reward: 49.34
Iteration: 4618 | Episodes: 187700 | Median Reward: 46.34 | Max Reward: 49.34
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -55            |
| time/                   |                |
|    fps                  | 288            |
|    iterations           | 4620           |
|    time_elapsed         | 65573          |
|    total_timesteps      | 18923520       |
| train/                  |                |
|    approx_kl            | 0.000105352636 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -156           |
|    explained_variance   | 1              |
|    learning_rate        | 0.0005         |
|    loss                 | -7.59          |
|    n_updates            | 46190          |
|    policy_gradient_loss | -0.000423      |
|    std                  | 34.5           |
|    value_loss           | 0.495          |
--------------------------------------------
Iteration: 4621 | Episodes: 187800 | Median Reward: 43.89 | Max Reward: 49.34
Iteration: 4623 | Episodes: 187900 | Median Reward: 45.67 | Max Reward: 49.34
Iteration: 4626 | Episodes: 188000 | Median Reward: 43.95 | Max Reward: 49.34
Iteration: 4628 | Episodes: 188100 | Median Reward: 43.25 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4630          |
|    time_elapsed         | 65713         |
|    total_timesteps      | 18964480      |
| train/                  |               |
|    approx_kl            | 0.00017507319 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -156          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.68         |
|    n_updates            | 46290         |
|    policy_gradient_loss | -0.000897     |
|    std                  | 34.6          |
|    value_loss           | 0.313         |
-------------------------------------------
Iteration: 4631 | Episodes: 188200 | Median Reward: 44.19 | Max Reward: 49.34
Iteration: 4633 | Episodes: 188300 | Median Reward: 44.18 | Max Reward: 49.34
Iteration: 4636 | Episodes: 188400 | Median Reward: 44.48 | Max Reward: 49.34
Iteration: 4638 | Episodes: 188500 | Median Reward: 43.74 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.2         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4640          |
|    time_elapsed         | 65852         |
|    total_timesteps      | 19005440      |
| train/                  |               |
|    approx_kl            | 1.7183367e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -156          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.77         |
|    n_updates            | 46390         |
|    policy_gradient_loss | -0.000119     |
|    std                  | 35.1          |
|    value_loss           | 0.12          |
-------------------------------------------
Iteration: 4641 | Episodes: 188600 | Median Reward: 43.54 | Max Reward: 49.34
Iteration: 4643 | Episodes: 188700 | Median Reward: 44.43 | Max Reward: 49.34
Iteration: 4645 | Episodes: 188800 | Median Reward: 45.05 | Max Reward: 49.34
Iteration: 4648 | Episodes: 188900 | Median Reward: 45.88 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.8         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4650          |
|    time_elapsed         | 65991         |
|    total_timesteps      | 19046400      |
| train/                  |               |
|    approx_kl            | 0.00012545346 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -156          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.74         |
|    n_updates            | 46490         |
|    policy_gradient_loss | -0.000415     |
|    std                  | 35.4          |
|    value_loss           | 0.285         |
-------------------------------------------
Iteration: 4650 | Episodes: 189000 | Median Reward: 46.03 | Max Reward: 49.34
Iteration: 4653 | Episodes: 189100 | Median Reward: 43.60 | Max Reward: 49.34
Iteration: 4655 | Episodes: 189200 | Median Reward: 43.06 | Max Reward: 49.34
Iteration: 4658 | Episodes: 189300 | Median Reward: 46.11 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.5         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4660          |
|    time_elapsed         | 66131         |
|    total_timesteps      | 19087360      |
| train/                  |               |
|    approx_kl            | 6.8387395e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -156          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.7          |
|    n_updates            | 46590         |
|    policy_gradient_loss | -0.000359     |
|    std                  | 35.6          |
|    value_loss           | 0.18          |
-------------------------------------------
Iteration: 4660 | Episodes: 189400 | Median Reward: 43.52 | Max Reward: 49.34
Iteration: 4663 | Episodes: 189500 | Median Reward: 46.40 | Max Reward: 49.34
Iteration: 4665 | Episodes: 189600 | Median Reward: 44.59 | Max Reward: 49.34
Iteration: 4668 | Episodes: 189700 | Median Reward: 43.33 | Max Reward: 49.34
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -55.8          |
| time/                   |                |
|    fps                  | 288            |
|    iterations           | 4670           |
|    time_elapsed         | 66268          |
|    total_timesteps      | 19128320       |
| train/                  |                |
|    approx_kl            | 0.000118038326 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -156           |
|    explained_variance   | 0.999          |
|    learning_rate        | 0.0005         |
|    loss                 | -7.69          |
|    n_updates            | 46690          |
|    policy_gradient_loss | -0.000339      |
|    std                  | 36             |
|    value_loss           | 0.323          |
--------------------------------------------
Iteration: 4670 | Episodes: 189800 | Median Reward: 46.03 | Max Reward: 49.34
Iteration: 4673 | Episodes: 189900 | Median Reward: 46.37 | Max Reward: 49.34
Iteration: 4675 | Episodes: 190000 | Median Reward: 43.53 | Max Reward: 49.34
Iteration: 4678 | Episodes: 190100 | Median Reward: 45.21 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.8         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4680          |
|    time_elapsed         | 66409         |
|    total_timesteps      | 19169280      |
| train/                  |               |
|    approx_kl            | 0.00010514152 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -157          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.69         |
|    n_updates            | 46790         |
|    policy_gradient_loss | -0.000503     |
|    std                  | 36.3          |
|    value_loss           | 0.445         |
-------------------------------------------
Iteration: 4680 | Episodes: 190200 | Median Reward: 46.13 | Max Reward: 49.34
Iteration: 4682 | Episodes: 190300 | Median Reward: 46.80 | Max Reward: 49.34
Iteration: 4685 | Episodes: 190400 | Median Reward: 43.17 | Max Reward: 49.34
Iteration: 4687 | Episodes: 190500 | Median Reward: 46.35 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.8         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4690          |
|    time_elapsed         | 66548         |
|    total_timesteps      | 19210240      |
| train/                  |               |
|    approx_kl            | 3.8689977e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -157          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.59         |
|    n_updates            | 46890         |
|    policy_gradient_loss | -2.95e-05     |
|    std                  | 36.8          |
|    value_loss           | 0.319         |
-------------------------------------------
Iteration: 4690 | Episodes: 190600 | Median Reward: 46.19 | Max Reward: 49.34
Iteration: 4692 | Episodes: 190700 | Median Reward: 44.29 | Max Reward: 49.34
Iteration: 4695 | Episodes: 190800 | Median Reward: 44.55 | Max Reward: 49.34
Iteration: 4697 | Episodes: 190900 | Median Reward: 45.71 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.9        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4700         |
|    time_elapsed         | 66690        |
|    total_timesteps      | 19251200     |
| train/                  |              |
|    approx_kl            | 0.0021215861 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -157         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.74        |
|    n_updates            | 46990        |
|    policy_gradient_loss | -0.00639     |
|    std                  | 37.1         |
|    value_loss           | 0.189        |
------------------------------------------
Iteration: 4700 | Episodes: 191000 | Median Reward: 45.63 | Max Reward: 49.34
Iteration: 4702 | Episodes: 191100 | Median Reward: 43.00 | Max Reward: 49.34
Iteration: 4705 | Episodes: 191200 | Median Reward: 46.59 | Max Reward: 49.34
Iteration: 4707 | Episodes: 191300 | Median Reward: 42.44 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.2         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4710          |
|    time_elapsed         | 66828         |
|    total_timesteps      | 19292160      |
| train/                  |               |
|    approx_kl            | 0.00020449734 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -157          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.6          |
|    n_updates            | 47090         |
|    policy_gradient_loss | -0.00187      |
|    std                  | 37.4          |
|    value_loss           | 0.237         |
-------------------------------------------
Iteration: 4710 | Episodes: 191400 | Median Reward: 46.10 | Max Reward: 49.34
Iteration: 4712 | Episodes: 191500 | Median Reward: 42.84 | Max Reward: 49.34
Iteration: 4715 | Episodes: 191600 | Median Reward: 43.74 | Max Reward: 49.34
Iteration: 4717 | Episodes: 191700 | Median Reward: 42.31 | Max Reward: 49.34
Iteration: 4719 | Episodes: 191800 | Median Reward: 45.65 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.2         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4720          |
|    time_elapsed         | 66969         |
|    total_timesteps      | 19333120      |
| train/                  |               |
|    approx_kl            | 0.00018414049 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -157          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.77         |
|    n_updates            | 47190         |
|    policy_gradient_loss | -0.00068      |
|    std                  | 37.8          |
|    value_loss           | 0.139         |
-------------------------------------------
Iteration: 4722 | Episodes: 191900 | Median Reward: 45.21 | Max Reward: 49.34
Iteration: 4724 | Episodes: 192000 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 4727 | Episodes: 192100 | Median Reward: 43.36 | Max Reward: 49.34
Iteration: 4729 | Episodes: 192200 | Median Reward: 43.84 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.9         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4730          |
|    time_elapsed         | 67108         |
|    total_timesteps      | 19374080      |
| train/                  |               |
|    approx_kl            | 0.00036186015 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -157          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.79         |
|    n_updates            | 47290         |
|    policy_gradient_loss | -0.00125      |
|    std                  | 38            |
|    value_loss           | 0.247         |
-------------------------------------------
Iteration: 4732 | Episodes: 192300 | Median Reward: 44.32 | Max Reward: 49.34
Iteration: 4734 | Episodes: 192400 | Median Reward: 43.44 | Max Reward: 49.34
Iteration: 4737 | Episodes: 192500 | Median Reward: 44.45 | Max Reward: 49.34
Iteration: 4739 | Episodes: 192600 | Median Reward: 45.12 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.5         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4740          |
|    time_elapsed         | 67249         |
|    total_timesteps      | 19415040      |
| train/                  |               |
|    approx_kl            | 2.4381196e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -157          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.83         |
|    n_updates            | 47390         |
|    policy_gradient_loss | -0.000255     |
|    std                  | 38.2          |
|    value_loss           | 0.403         |
-------------------------------------------
Iteration: 4742 | Episodes: 192700 | Median Reward: 42.27 | Max Reward: 49.34
Iteration: 4744 | Episodes: 192800 | Median Reward: 44.24 | Max Reward: 49.34
Iteration: 4747 | Episodes: 192900 | Median Reward: 46.05 | Max Reward: 49.34
Iteration: 4749 | Episodes: 193000 | Median Reward: 46.29 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.6        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4750         |
|    time_elapsed         | 67387        |
|    total_timesteps      | 19456000     |
| train/                  |              |
|    approx_kl            | 9.864569e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -157         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.77        |
|    n_updates            | 47490        |
|    policy_gradient_loss | -9.67e-05    |
|    std                  | 38.5         |
|    value_loss           | 0.282        |
------------------------------------------
Iteration: 4752 | Episodes: 193100 | Median Reward: 43.13 | Max Reward: 49.34
Iteration: 4754 | Episodes: 193200 | Median Reward: 44.26 | Max Reward: 49.34
Iteration: 4756 | Episodes: 193300 | Median Reward: 46.25 | Max Reward: 49.34
Iteration: 4759 | Episodes: 193400 | Median Reward: 42.57 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.5         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4760          |
|    time_elapsed         | 67527         |
|    total_timesteps      | 19496960      |
| train/                  |               |
|    approx_kl            | 5.7277502e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -158          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.71         |
|    n_updates            | 47590         |
|    policy_gradient_loss | -0.000141     |
|    std                  | 39            |
|    value_loss           | 0.45          |
-------------------------------------------
Iteration: 4761 | Episodes: 193500 | Median Reward: 43.35 | Max Reward: 49.34
Iteration: 4764 | Episodes: 193600 | Median Reward: 45.05 | Max Reward: 49.34
Iteration: 4766 | Episodes: 193700 | Median Reward: 45.26 | Max Reward: 49.34
Iteration: 4769 | Episodes: 193800 | Median Reward: 44.12 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.5       |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 4770        |
|    time_elapsed         | 67670       |
|    total_timesteps      | 19537920    |
| train/                  |             |
|    approx_kl            | 0.000638557 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -158        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -7.81       |
|    n_updates            | 47690       |
|    policy_gradient_loss | -0.0019     |
|    std                  | 39.3        |
|    value_loss           | 0.141       |
-----------------------------------------
Iteration: 4771 | Episodes: 193900 | Median Reward: 43.47 | Max Reward: 49.34
Iteration: 4774 | Episodes: 194000 | Median Reward: 46.10 | Max Reward: 49.34
Iteration: 4776 | Episodes: 194100 | Median Reward: 43.42 | Max Reward: 49.34
Iteration: 4779 | Episodes: 194200 | Median Reward: 43.87 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.1         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4780          |
|    time_elapsed         | 67816         |
|    total_timesteps      | 19578880      |
| train/                  |               |
|    approx_kl            | 2.7065427e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -158          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.73         |
|    n_updates            | 47790         |
|    policy_gradient_loss | -6.91e-05     |
|    std                  | 39.7          |
|    value_loss           | 0.453         |
-------------------------------------------
Iteration: 4781 | Episodes: 194300 | Median Reward: 43.98 | Max Reward: 49.34
Iteration: 4784 | Episodes: 194400 | Median Reward: 43.72 | Max Reward: 49.34
Iteration: 4786 | Episodes: 194500 | Median Reward: 46.10 | Max Reward: 49.34
Iteration: 4788 | Episodes: 194600 | Median Reward: 43.48 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.5         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4790          |
|    time_elapsed         | 67953         |
|    total_timesteps      | 19619840      |
| train/                  |               |
|    approx_kl            | 0.00026682092 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -158          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.72         |
|    n_updates            | 47890         |
|    policy_gradient_loss | -0.000929     |
|    std                  | 40.1          |
|    value_loss           | 0.527         |
-------------------------------------------
Iteration: 4791 | Episodes: 194700 | Median Reward: 42.71 | Max Reward: 49.34
Iteration: 4793 | Episodes: 194800 | Median Reward: 45.64 | Max Reward: 49.34
Iteration: 4796 | Episodes: 194900 | Median Reward: 46.86 | Max Reward: 49.34
Iteration: 4798 | Episodes: 195000 | Median Reward: 45.07 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55           |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4800          |
|    time_elapsed         | 68091         |
|    total_timesteps      | 19660800      |
| train/                  |               |
|    approx_kl            | 1.6078062e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -158          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.37         |
|    n_updates            | 47990         |
|    policy_gradient_loss | -0.000249     |
|    std                  | 40.6          |
|    value_loss           | 0.553         |
-------------------------------------------
Iteration: 4801 | Episodes: 195100 | Median Reward: 46.30 | Max Reward: 49.34
Iteration: 4803 | Episodes: 195200 | Median Reward: 45.62 | Max Reward: 49.34
Iteration: 4806 | Episodes: 195300 | Median Reward: 43.29 | Max Reward: 49.34
Iteration: 4808 | Episodes: 195400 | Median Reward: 46.26 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.7         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4810          |
|    time_elapsed         | 68233         |
|    total_timesteps      | 19701760      |
| train/                  |               |
|    approx_kl            | 0.00046384815 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -159          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.78         |
|    n_updates            | 48090         |
|    policy_gradient_loss | -0.00123      |
|    std                  | 41.2          |
|    value_loss           | 0.181         |
-------------------------------------------
Iteration: 4811 | Episodes: 195500 | Median Reward: 43.40 | Max Reward: 49.34
Iteration: 4813 | Episodes: 195600 | Median Reward: 43.46 | Max Reward: 49.34
Iteration: 4816 | Episodes: 195700 | Median Reward: 42.50 | Max Reward: 49.34
Iteration: 4818 | Episodes: 195800 | Median Reward: 45.96 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.9         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4820          |
|    time_elapsed         | 68370         |
|    total_timesteps      | 19742720      |
| train/                  |               |
|    approx_kl            | 0.00057583244 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -159          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.72         |
|    n_updates            | 48190         |
|    policy_gradient_loss | -0.00129      |
|    std                  | 41.7          |
|    value_loss           | 0.263         |
-------------------------------------------
Iteration: 4821 | Episodes: 195900 | Median Reward: 42.44 | Max Reward: 49.34
Iteration: 4823 | Episodes: 196000 | Median Reward: 46.63 | Max Reward: 49.34
Iteration: 4825 | Episodes: 196100 | Median Reward: 43.44 | Max Reward: 49.34
Iteration: 4828 | Episodes: 196200 | Median Reward: 43.79 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.5         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4830          |
|    time_elapsed         | 68507         |
|    total_timesteps      | 19783680      |
| train/                  |               |
|    approx_kl            | 4.7430978e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -159          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.9          |
|    n_updates            | 48290         |
|    policy_gradient_loss | -8.02e-05     |
|    std                  | 42.1          |
|    value_loss           | 0.326         |
-------------------------------------------
Iteration: 4830 | Episodes: 196300 | Median Reward: 43.64 | Max Reward: 49.34
Iteration: 4833 | Episodes: 196400 | Median Reward: 43.50 | Max Reward: 49.34
Iteration: 4835 | Episodes: 196500 | Median Reward: 46.28 | Max Reward: 49.34
Iteration: 4838 | Episodes: 196600 | Median Reward: 43.61 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.5        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4840         |
|    time_elapsed         | 68648        |
|    total_timesteps      | 19824640     |
| train/                  |              |
|    approx_kl            | 0.0019451547 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -159         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.9         |
|    n_updates            | 48390        |
|    policy_gradient_loss | -0.00506     |
|    std                  | 42.4         |
|    value_loss           | 0.134        |
------------------------------------------
Iteration: 4840 | Episodes: 196700 | Median Reward: 44.59 | Max Reward: 49.34
Iteration: 4843 | Episodes: 196800 | Median Reward: 46.12 | Max Reward: 49.34
Iteration: 4845 | Episodes: 196900 | Median Reward: 42.81 | Max Reward: 49.34
Iteration: 4848 | Episodes: 197000 | Median Reward: 43.08 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.2         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4850          |
|    time_elapsed         | 68787         |
|    total_timesteps      | 19865600      |
| train/                  |               |
|    approx_kl            | 7.0104725e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -159          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.85         |
|    n_updates            | 48490         |
|    policy_gradient_loss | 0.00014       |
|    std                  | 43            |
|    value_loss           | 0.415         |
-------------------------------------------
Iteration: 4850 | Episodes: 197100 | Median Reward: 46.21 | Max Reward: 49.34
Iteration: 4853 | Episodes: 197200 | Median Reward: 44.66 | Max Reward: 49.34
Iteration: 4855 | Episodes: 197300 | Median Reward: 46.30 | Max Reward: 49.34
Iteration: 4858 | Episodes: 197400 | Median Reward: 40.18 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4860         |
|    time_elapsed         | 68926        |
|    total_timesteps      | 19906560     |
| train/                  |              |
|    approx_kl            | 0.0010855077 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -160         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.87        |
|    n_updates            | 48590        |
|    policy_gradient_loss | -0.00545     |
|    std                  | 43.8         |
|    value_loss           | 0.288        |
------------------------------------------
Iteration: 4860 | Episodes: 197500 | Median Reward: 44.13 | Max Reward: 49.34
Iteration: 4862 | Episodes: 197600 | Median Reward: 46.11 | Max Reward: 49.34
Iteration: 4865 | Episodes: 197700 | Median Reward: 44.25 | Max Reward: 49.34
Iteration: 4867 | Episodes: 197800 | Median Reward: 46.44 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.3         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4870          |
|    time_elapsed         | 69065         |
|    total_timesteps      | 19947520      |
| train/                  |               |
|    approx_kl            | 0.00078706106 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -160          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.8          |
|    n_updates            | 48690         |
|    policy_gradient_loss | -0.00352      |
|    std                  | 44.2          |
|    value_loss           | 0.401         |
-------------------------------------------
Iteration: 4870 | Episodes: 197900 | Median Reward: 42.91 | Max Reward: 49.34
Iteration: 4872 | Episodes: 198000 | Median Reward: 45.02 | Max Reward: 49.34
Iteration: 4875 | Episodes: 198100 | Median Reward: 43.40 | Max Reward: 49.34
Iteration: 4877 | Episodes: 198200 | Median Reward: 41.05 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.2         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4880          |
|    time_elapsed         | 69207         |
|    total_timesteps      | 19988480      |
| train/                  |               |
|    approx_kl            | 0.00032942067 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -160          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.9          |
|    n_updates            | 48790         |
|    policy_gradient_loss | -2.22e-05     |
|    std                  | 44.7          |
|    value_loss           | 0.208         |
-------------------------------------------
Iteration: 4880 | Episodes: 198300 | Median Reward: 43.73 | Max Reward: 49.34
Iteration: 4882 | Episodes: 198400 | Median Reward: 46.10 | Max Reward: 49.34
Iteration: 4885 | Episodes: 198500 | Median Reward: 46.57 | Max Reward: 49.34
Iteration: 4887 | Episodes: 198600 | Median Reward: 45.91 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.6        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4890         |
|    time_elapsed         | 69346        |
|    total_timesteps      | 20029440     |
| train/                  |              |
|    approx_kl            | 0.0052209394 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -160         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.88        |
|    n_updates            | 48890        |
|    policy_gradient_loss | -0.0095      |
|    std                  | 45.3         |
|    value_loss           | 0.337        |
------------------------------------------
Iteration: 4890 | Episodes: 198700 | Median Reward: 43.41 | Max Reward: 49.34
Iteration: 4892 | Episodes: 198800 | Median Reward: 44.43 | Max Reward: 49.34
Iteration: 4894 | Episodes: 198900 | Median Reward: 42.75 | Max Reward: 49.34
Iteration: 4897 | Episodes: 199000 | Median Reward: 44.21 | Max Reward: 49.34
Iteration: 4899 | Episodes: 199100 | Median Reward: 42.99 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.5        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4900         |
|    time_elapsed         | 69491        |
|    total_timesteps      | 20070400     |
| train/                  |              |
|    approx_kl            | 0.0004337567 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -160         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.92        |
|    n_updates            | 48990        |
|    policy_gradient_loss | -0.00116     |
|    std                  | 45.7         |
|    value_loss           | 0.271        |
------------------------------------------
Iteration: 4902 | Episodes: 199200 | Median Reward: 43.49 | Max Reward: 49.34
Iteration: 4904 | Episodes: 199300 | Median Reward: 45.24 | Max Reward: 49.34
Iteration: 4907 | Episodes: 199400 | Median Reward: 46.21 | Max Reward: 49.34
Iteration: 4909 | Episodes: 199500 | Median Reward: 43.60 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57           |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4910          |
|    time_elapsed         | 69628         |
|    total_timesteps      | 20111360      |
| train/                  |               |
|    approx_kl            | 0.00038240495 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -161          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.98         |
|    n_updates            | 49090         |
|    policy_gradient_loss | -0.00189      |
|    std                  | 46.3          |
|    value_loss           | 0.172         |
-------------------------------------------
Iteration: 4912 | Episodes: 199600 | Median Reward: 45.75 | Max Reward: 49.34
Iteration: 4914 | Episodes: 199700 | Median Reward: 43.96 | Max Reward: 49.34
Iteration: 4917 | Episodes: 199800 | Median Reward: 45.70 | Max Reward: 49.34
Iteration: 4919 | Episodes: 199900 | Median Reward: 43.14 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.5         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4920          |
|    time_elapsed         | 69769         |
|    total_timesteps      | 20152320      |
| train/                  |               |
|    approx_kl            | 3.2490352e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -161          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.98         |
|    n_updates            | 49190         |
|    policy_gradient_loss | -2.6e-05      |
|    std                  | 46.5          |
|    value_loss           | 0.212         |
-------------------------------------------
Iteration: 4922 | Episodes: 200000 | Median Reward: 43.55 | Max Reward: 49.34
Iteration: 4924 | Episodes: 200100 | Median Reward: 46.25 | Max Reward: 49.34
Iteration: 4927 | Episodes: 200200 | Median Reward: 43.42 | Max Reward: 49.34
Iteration: 4929 | Episodes: 200300 | Median Reward: 45.89 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.3         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4930          |
|    time_elapsed         | 69910         |
|    total_timesteps      | 20193280      |
| train/                  |               |
|    approx_kl            | 0.00011564279 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -161          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.85         |
|    n_updates            | 49290         |
|    policy_gradient_loss | -0.000392     |
|    std                  | 47.1          |
|    value_loss           | 0.503         |
-------------------------------------------
Iteration: 4931 | Episodes: 200400 | Median Reward: 43.81 | Max Reward: 49.34
Iteration: 4934 | Episodes: 200500 | Median Reward: 43.44 | Max Reward: 49.34
Iteration: 4936 | Episodes: 200600 | Median Reward: 46.90 | Max Reward: 49.34
Iteration: 4939 | Episodes: 200700 | Median Reward: 42.64 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.1         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4940          |
|    time_elapsed         | 70050         |
|    total_timesteps      | 20234240      |
| train/                  |               |
|    approx_kl            | 0.00054831064 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -161          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.95         |
|    n_updates            | 49390         |
|    policy_gradient_loss | -0.00318      |
|    std                  | 47.3          |
|    value_loss           | 0.185         |
-------------------------------------------
Iteration: 4941 | Episodes: 200800 | Median Reward: 45.97 | Max Reward: 49.34
Iteration: 4944 | Episodes: 200900 | Median Reward: 43.60 | Max Reward: 49.34
Iteration: 4946 | Episodes: 201000 | Median Reward: 43.32 | Max Reward: 49.34
Iteration: 4949 | Episodes: 201100 | Median Reward: 45.09 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.8        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4950         |
|    time_elapsed         | 70189        |
|    total_timesteps      | 20275200     |
| train/                  |              |
|    approx_kl            | 5.291945e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -161         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.79        |
|    n_updates            | 49490        |
|    policy_gradient_loss | -0.000608    |
|    std                  | 47.4         |
|    value_loss           | 0.705        |
------------------------------------------
Iteration: 4951 | Episodes: 201200 | Median Reward: 42.06 | Max Reward: 49.34
Iteration: 4954 | Episodes: 201300 | Median Reward: 43.04 | Max Reward: 49.34
Iteration: 4956 | Episodes: 201400 | Median Reward: 45.61 | Max Reward: 49.34
Iteration: 4959 | Episodes: 201500 | Median Reward: 42.36 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.8        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4960         |
|    time_elapsed         | 70328        |
|    total_timesteps      | 20316160     |
| train/                  |              |
|    approx_kl            | 0.0013673368 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -161         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.77        |
|    n_updates            | 49590        |
|    policy_gradient_loss | -0.00995     |
|    std                  | 47.6         |
|    value_loss           | 0.76         |
------------------------------------------
Iteration: 4961 | Episodes: 201600 | Median Reward: 42.75 | Max Reward: 49.34
Iteration: 4963 | Episodes: 201700 | Median Reward: 43.20 | Max Reward: 49.34
Iteration: 4966 | Episodes: 201800 | Median Reward: 46.34 | Max Reward: 49.34
Iteration: 4968 | Episodes: 201900 | Median Reward: 43.60 | Max Reward: 49.34
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.1       |
| time/                   |             |
|    fps                  | 288         |
|    iterations           | 4970        |
|    time_elapsed         | 70468       |
|    total_timesteps      | 20357120    |
| train/                  |             |
|    approx_kl            | 0.000997392 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -161        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -8.05       |
|    n_updates            | 49690       |
|    policy_gradient_loss | -0.00309    |
|    std                  | 48          |
|    value_loss           | 0.0686      |
-----------------------------------------
Iteration: 4971 | Episodes: 202000 | Median Reward: 45.70 | Max Reward: 49.34
Iteration: 4973 | Episodes: 202100 | Median Reward: 43.57 | Max Reward: 49.34
Iteration: 4976 | Episodes: 202200 | Median Reward: 46.10 | Max Reward: 49.34
Iteration: 4978 | Episodes: 202300 | Median Reward: 45.15 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.8        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 4980         |
|    time_elapsed         | 70607        |
|    total_timesteps      | 20398080     |
| train/                  |              |
|    approx_kl            | 0.0004852977 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -162         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.97        |
|    n_updates            | 49790        |
|    policy_gradient_loss | -0.00343     |
|    std                  | 48.6         |
|    value_loss           | 0.354        |
------------------------------------------
Iteration: 4981 | Episodes: 202400 | Median Reward: 46.39 | Max Reward: 49.34
Iteration: 4983 | Episodes: 202500 | Median Reward: 43.90 | Max Reward: 49.34
Iteration: 4986 | Episodes: 202600 | Median Reward: 44.38 | Max Reward: 49.34
Iteration: 4988 | Episodes: 202700 | Median Reward: 43.70 | Max Reward: 49.34
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.2         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 4990          |
|    time_elapsed         | 70755         |
|    total_timesteps      | 20439040      |
| train/                  |               |
|    approx_kl            | 0.00014897347 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -162          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -8.07         |
|    n_updates            | 49890         |
|    policy_gradient_loss | -0.00163      |
|    std                  | 49.2          |
|    value_loss           | 0.164         |
-------------------------------------------
Iteration: 4991 | Episodes: 202800 | Median Reward: 44.29 | Max Reward: 49.34
Iteration: 4993 | Episodes: 202900 | Median Reward: 42.84 | Max Reward: 49.34
Iteration: 4996 | Episodes: 203000 | Median Reward: 42.79 | Max Reward: 49.34
Iteration: 4998 | Episodes: 203100 | Median Reward: 42.34 | Max Reward: 49.34
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.3        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 5000         |
|    time_elapsed         | 70895        |
|    total_timesteps      | 20480000     |
| train/                  |              |
|    approx_kl            | 0.0038366811 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -162         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.98        |
|    n_updates            | 49990        |
|    policy_gradient_loss | -0.0174      |
|    std                  | 49.5         |
|    value_loss           | 0.546        |
------------------------------------------
Training End | Episodes: 203160 | Median Reward: 42.86 | Max Reward: 49.34
Plot saved as fig_g3d_2_weighted.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> a[Kcd ..
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand[0m> cd agents
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/agents[0m> ls
agent_g3b_0_weighted.zip  agent_g3d_3_50int_weighted.zip  agent_g3d_6_weighted.zip	     hand_pose_ppo_gpu_0_g3d_grace.zip	hand_pose_ppo_gpu_0.zip      hand_pose_ppo_gpu_5.zip
agent_g3c_5_weighted.zip  agent_g3d_4_2_weighted.zip	  hand_pose_ppo_gpu_0_g3b_1e5lr.zip  hand_pose_ppo_gpu_0_g3d.zip	hand_pose_ppo_gpu_5_g3c.zip
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/agents[0m> exit()
exit

Script done on 2024-10-17 00:05:18-04:00 [COMMAND_EXIT_CODE="0"]
