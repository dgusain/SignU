Script started on 2024-10-17 04:19:23-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/24" COLUMNS="211" LINES="55"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> conda acitv[K[K[Ktivate mujc[Koco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python code2_1.py
GPU 2: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
Iteration: 2 | Episodes: 100 | Median Reward: 18.59 | Max Reward: 31.09
Iteration: 4 | Episodes: 200 | Median Reward: 15.40 | Max Reward: 37.90
Iteration: 7 | Episodes: 300 | Median Reward: 13.18 | Max Reward: 37.90
Iteration: 9 | Episodes: 400 | Median Reward: 11.53 | Max Reward: 37.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -88.5       |
| time/                   |             |
|    fps                  | 274         |
|    iterations           | 10          |
|    time_elapsed         | 148         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.045840487 |
|    clip_fraction        | 0.0948      |
|    clip_range           | 0.4         |
|    entropy_loss         | -62.5       |
|    explained_variance   | -0.0267     |
|    learning_rate        | 0.0005      |
|    loss                 | 78.6        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0182     |
|    std                  | 1.01        |
|    value_loss           | 167         |
-----------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 18.70 | Max Reward: 37.90
Iteration: 14 | Episodes: 600 | Median Reward: 13.40 | Max Reward: 37.90
Iteration: 17 | Episodes: 700 | Median Reward: 15.91 | Max Reward: 37.90
Iteration: 19 | Episodes: 800 | Median Reward: 9.47 | Max Reward: 37.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -91.4       |
| time/                   |             |
|    fps                  | 272         |
|    iterations           | 20          |
|    time_elapsed         | 300         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.043741852 |
|    clip_fraction        | 0.0709      |
|    clip_range           | 0.4         |
|    entropy_loss         | -71.7       |
|    explained_variance   | -0.013      |
|    learning_rate        | 0.0005      |
|    loss                 | 70.2        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00688    |
|    std                  | 1.01        |
|    value_loss           | 150         |
-----------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 14.72 | Max Reward: 37.90
Iteration: 24 | Episodes: 1000 | Median Reward: 14.35 | Max Reward: 37.90
Iteration: 27 | Episodes: 1100 | Median Reward: 13.65 | Max Reward: 37.90
Iteration: 29 | Episodes: 1200 | Median Reward: 18.82 | Max Reward: 37.90
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.7        |
| time/                   |              |
|    fps                  | 271          |
|    iterations           | 30           |
|    time_elapsed         | 452          |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0051267734 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -78.8        |
|    explained_variance   | -0.00339     |
|    learning_rate        | 0.0005       |
|    loss                 | 82.2         |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00375     |
|    std                  | 1.02         |
|    value_loss           | 174          |
------------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 27.89 | Max Reward: 37.90
Iteration: 34 | Episodes: 1400 | Median Reward: 16.63 | Max Reward: 37.90
Iteration: 36 | Episodes: 1500 | Median Reward: 17.38 | Max Reward: 37.90
Iteration: 39 | Episodes: 1600 | Median Reward: 24.98 | Max Reward: 38.70
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -77.5      |
| time/                   |            |
|    fps                  | 271        |
|    iterations           | 40         |
|    time_elapsed         | 603        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.04681419 |
|    clip_fraction        | 0.0517     |
|    clip_range           | 0.4        |
|    entropy_loss         | -81.8      |
|    explained_variance   | -0.00131   |
|    learning_rate        | 0.0005     |
|    loss                 | 82.7       |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0154    |
|    std                  | 1.04       |
|    value_loss           | 175        |
----------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 24.28 | Max Reward: 38.70
Iteration: 44 | Episodes: 1800 | Median Reward: 17.28 | Max Reward: 38.70
Iteration: 46 | Episodes: 1900 | Median Reward: 20.36 | Max Reward: 38.70
Iteration: 49 | Episodes: 2000 | Median Reward: 21.95 | Max Reward: 38.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -81.3       |
| time/                   |             |
|    fps                  | 271         |
|    iterations           | 50          |
|    time_elapsed         | 754         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.033245333 |
|    clip_fraction        | 0.00708     |
|    clip_range           | 0.4         |
|    entropy_loss         | -80.6       |
|    explained_variance   | -0.000512   |
|    learning_rate        | 0.0005      |
|    loss                 | 97.1        |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.016      |
|    std                  | 1.06        |
|    value_loss           | 203         |
-----------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 19.37 | Max Reward: 41.30
Iteration: 54 | Episodes: 2200 | Median Reward: 27.13 | Max Reward: 41.30
Iteration: 56 | Episodes: 2300 | Median Reward: 25.11 | Max Reward: 44.87
Iteration: 59 | Episodes: 2400 | Median Reward: 21.99 | Max Reward: 44.87
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -92.3     |
| time/                   |           |
|    fps                  | 267       |
|    iterations           | 60        |
|    time_elapsed         | 919       |
|    total_timesteps      | 245760    |
| train/                  |           |
|    approx_kl            | 0.2542591 |
|    clip_fraction        | 0.373     |
|    clip_range           | 0.4       |
|    entropy_loss         | -68.1     |
|    explained_variance   | -0.000237 |
|    learning_rate        | 0.0005    |
|    loss                 | 67.6      |
|    n_updates            | 590       |
|    policy_gradient_loss | 0.041     |
|    std                  | 1.09      |
|    value_loss           | 143       |
---------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 0.58 | Max Reward: 44.87
Iteration: 64 | Episodes: 2600 | Median Reward: 1.75 | Max Reward: 44.87
Iteration: 66 | Episodes: 2700 | Median Reward: 9.66 | Max Reward: 44.87
Iteration: 69 | Episodes: 2800 | Median Reward: 8.22 | Max Reward: 44.87
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -90.1      |
| time/                   |            |
|    fps                  | 267        |
|    iterations           | 70         |
|    time_elapsed         | 1070       |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.05804296 |
|    clip_fraction        | 0.0651     |
|    clip_range           | 0.4        |
|    entropy_loss         | -67.1      |
|    explained_variance   | -0.000142  |
|    learning_rate        | 0.0005     |
|    loss                 | 68.4       |
|    n_updates            | 690        |
|    policy_gradient_loss | 0.0118     |
|    std                  | 1.11       |
|    value_loss           | 145        |
----------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 15.50 | Max Reward: 44.87
Iteration: 73 | Episodes: 3000 | Median Reward: 16.39 | Max Reward: 44.87
Iteration: 76 | Episodes: 3100 | Median Reward: -8.95 | Max Reward: 44.87
Iteration: 78 | Episodes: 3200 | Median Reward: -6.62 | Max Reward: 44.87
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -101       |
| time/                   |            |
|    fps                  | 268        |
|    iterations           | 80         |
|    time_elapsed         | 1221       |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.10566268 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.4        |
|    entropy_loss         | -71.1      |
|    explained_variance   | -0.00011   |
|    learning_rate        | 0.0005     |
|    loss                 | 57.1       |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0185    |
|    std                  | 1.12       |
|    value_loss           | 123        |
----------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: -2.33 | Max Reward: 44.87
Iteration: 83 | Episodes: 3400 | Median Reward: 6.51 | Max Reward: 44.87
Iteration: 86 | Episodes: 3500 | Median Reward: 9.67 | Max Reward: 44.87
Iteration: 88 | Episodes: 3600 | Median Reward: 10.49 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -81         |
| time/                   |             |
|    fps                  | 268         |
|    iterations           | 90          |
|    time_elapsed         | 1373        |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.033727713 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.4         |
|    entropy_loss         | -79.7       |
|    explained_variance   | -5.42e-05   |
|    learning_rate        | 0.0005      |
|    loss                 | 136         |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.0231     |
|    std                  | 1.14        |
|    value_loss           | 280         |
-----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 17.08 | Max Reward: 46.52
Iteration: 93 | Episodes: 3800 | Median Reward: 15.94 | Max Reward: 46.52
Iteration: 96 | Episodes: 3900 | Median Reward: 15.94 | Max Reward: 46.52
Iteration: 98 | Episodes: 4000 | Median Reward: 15.65 | Max Reward: 46.52
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.7     |
| time/                   |           |
|    fps                  | 268       |
|    iterations           | 100       |
|    time_elapsed         | 1525      |
|    total_timesteps      | 409600    |
| train/                  |           |
|    approx_kl            | 0.6787145 |
|    clip_fraction        | 0.549     |
|    clip_range           | 0.4       |
|    entropy_loss         | -81.4     |
|    explained_variance   | -2.11e-05 |
|    learning_rate        | 0.0005    |
|    loss                 | 115       |
|    n_updates            | 990       |
|    policy_gradient_loss | 0.136     |
|    std                  | 1.19      |
|    value_loss           | 238       |
---------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 16.41 | Max Reward: 46.52
Iteration: 103 | Episodes: 4200 | Median Reward: 22.70 | Max Reward: 46.52
Iteration: 106 | Episodes: 4300 | Median Reward: 22.69 | Max Reward: 46.52
Iteration: 108 | Episodes: 4400 | Median Reward: 15.21 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.7         |
| time/                   |               |
|    fps                  | 268           |
|    iterations           | 110           |
|    time_elapsed         | 1675          |
|    total_timesteps      | 450560        |
| train/                  |               |
|    approx_kl            | 0.00090800016 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -87.4         |
|    explained_variance   | -1.22e-05     |
|    learning_rate        | 0.0005        |
|    loss                 | 98.9          |
|    n_updates            | 1090          |
|    policy_gradient_loss | 1.26e-05      |
|    std                  | 1.22          |
|    value_loss           | 207           |
-------------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 22.79 | Max Reward: 46.52
Iteration: 113 | Episodes: 4600 | Median Reward: 17.45 | Max Reward: 46.52
Iteration: 115 | Episodes: 4700 | Median Reward: 15.22 | Max Reward: 46.52
Iteration: 118 | Episodes: 4800 | Median Reward: 17.82 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -83.9       |
| time/                   |             |
|    fps                  | 268         |
|    iterations           | 120         |
|    time_elapsed         | 1827        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.004208589 |
|    clip_fraction        | 0.00122     |
|    clip_range           | 0.4         |
|    entropy_loss         | -88.4       |
|    explained_variance   | -7.75e-06   |
|    learning_rate        | 0.0005      |
|    loss                 | 138         |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.00244    |
|    std                  | 1.24        |
|    value_loss           | 284         |
-----------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 16.79 | Max Reward: 46.52
Iteration: 123 | Episodes: 5000 | Median Reward: 31.17 | Max Reward: 46.52
Iteration: 125 | Episodes: 5100 | Median Reward: 23.97 | Max Reward: 46.52
Iteration: 128 | Episodes: 5200 | Median Reward: 23.83 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.9        |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 130          |
|    time_elapsed         | 1977         |
|    total_timesteps      | 532480       |
| train/                  |              |
|    approx_kl            | 0.0020683077 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -88.9        |
|    explained_variance   | -6.79e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 111          |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.00275     |
|    std                  | 1.26         |
|    value_loss           | 231          |
------------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 21.41 | Max Reward: 46.52
Iteration: 133 | Episodes: 5400 | Median Reward: 22.48 | Max Reward: 46.52
Iteration: 135 | Episodes: 5500 | Median Reward: 21.92 | Max Reward: 46.52
Iteration: 138 | Episodes: 5600 | Median Reward: 23.51 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -85.1       |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 140         |
|    time_elapsed         | 2127        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.026177797 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -89.7       |
|    explained_variance   | -4.53e-06   |
|    learning_rate        | 0.0005      |
|    loss                 | 131         |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.00698    |
|    std                  | 1.29        |
|    value_loss           | 270         |
-----------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 17.21 | Max Reward: 46.52
Iteration: 143 | Episodes: 5800 | Median Reward: 21.59 | Max Reward: 46.52
Iteration: 145 | Episodes: 5900 | Median Reward: 22.41 | Max Reward: 46.52
Iteration: 147 | Episodes: 6000 | Median Reward: 24.71 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.2        |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 150          |
|    time_elapsed         | 2280         |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0062615555 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.4          |
|    entropy_loss         | -90.3        |
|    explained_variance   | -3.46e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 123          |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.0028      |
|    std                  | 1.33         |
|    value_loss           | 254          |
------------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 23.46 | Max Reward: 46.52
Iteration: 152 | Episodes: 6200 | Median Reward: 25.99 | Max Reward: 46.52
Iteration: 155 | Episodes: 6300 | Median Reward: 26.15 | Max Reward: 46.52
Iteration: 157 | Episodes: 6400 | Median Reward: 26.12 | Max Reward: 46.52
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -76.8     |
| time/                   |           |
|    fps                  | 269       |
|    iterations           | 160       |
|    time_elapsed         | 2431      |
|    total_timesteps      | 655360    |
| train/                  |           |
|    approx_kl            | 0.0546145 |
|    clip_fraction        | 0.0384    |
|    clip_range           | 0.4       |
|    entropy_loss         | -90.9     |
|    explained_variance   | -2.74e-06 |
|    learning_rate        | 0.0005    |
|    loss                 | 133       |
|    n_updates            | 1590      |
|    policy_gradient_loss | 0.00286   |
|    std                  | 1.37      |
|    value_loss           | 276       |
---------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 22.91 | Max Reward: 46.52
Iteration: 162 | Episodes: 6600 | Median Reward: 21.84 | Max Reward: 46.52
Iteration: 165 | Episodes: 6700 | Median Reward: 26.74 | Max Reward: 46.52
Iteration: 167 | Episodes: 6800 | Median Reward: 17.31 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -72.6       |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 170         |
|    time_elapsed         | 2583        |
|    total_timesteps      | 696320      |
| train/                  |             |
|    approx_kl            | 0.001229937 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -91.8       |
|    explained_variance   | -2.03e-06   |
|    learning_rate        | 0.0005      |
|    loss                 | 173         |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.000865   |
|    std                  | 1.4         |
|    value_loss           | 355         |
-----------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 30.14 | Max Reward: 46.52
Iteration: 172 | Episodes: 7000 | Median Reward: 21.78 | Max Reward: 46.52
Iteration: 175 | Episodes: 7100 | Median Reward: 30.18 | Max Reward: 46.52
Iteration: 177 | Episodes: 7200 | Median Reward: 27.95 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.8        |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 180          |
|    time_elapsed         | 2733         |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.0003559158 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -92.2        |
|    explained_variance   | -1.67e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 177          |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.000429    |
|    std                  | 1.42         |
|    value_loss           | 363          |
------------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 27.50 | Max Reward: 46.52
Iteration: 182 | Episodes: 7400 | Median Reward: 30.07 | Max Reward: 46.52
Iteration: 184 | Episodes: 7500 | Median Reward: 29.53 | Max Reward: 46.52
Iteration: 187 | Episodes: 7600 | Median Reward: 28.24 | Max Reward: 46.52
Iteration: 189 | Episodes: 7700 | Median Reward: 28.00 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74.7       |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 190         |
|    time_elapsed         | 2886        |
|    total_timesteps      | 778240      |
| train/                  |             |
|    approx_kl            | 0.000683731 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -92.5       |
|    explained_variance   | -1.91e-06   |
|    learning_rate        | 0.0005      |
|    loss                 | 115         |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00161    |
|    std                  | 1.45        |
|    value_loss           | 239         |
-----------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 27.73 | Max Reward: 46.52
Iteration: 194 | Episodes: 7900 | Median Reward: 32.21 | Max Reward: 46.52
Iteration: 197 | Episodes: 8000 | Median Reward: 27.34 | Max Reward: 46.52
Iteration: 199 | Episodes: 8100 | Median Reward: 27.71 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.5       |
| time/                   |             |
|    fps                  | 268         |
|    iterations           | 200         |
|    time_elapsed         | 3046        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.030307323 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.4         |
|    entropy_loss         | -93.1       |
|    explained_variance   | -1.43e-06   |
|    learning_rate        | 0.0005      |
|    loss                 | 139         |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00851    |
|    std                  | 1.49        |
|    value_loss           | 287         |
-----------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 27.71 | Max Reward: 46.52
Iteration: 204 | Episodes: 8300 | Median Reward: 31.54 | Max Reward: 46.52
Iteration: 207 | Episodes: 8400 | Median Reward: 30.48 | Max Reward: 46.52
Iteration: 209 | Episodes: 8500 | Median Reward: 28.27 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.1        |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 210          |
|    time_elapsed         | 3196         |
|    total_timesteps      | 860160       |
| train/                  |              |
|    approx_kl            | 0.0010220113 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.3        |
|    explained_variance   | -1.19e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 143          |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.000111    |
|    std                  | 1.52         |
|    value_loss           | 295          |
------------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 26.07 | Max Reward: 46.52
Iteration: 214 | Episodes: 8700 | Median Reward: 26.88 | Max Reward: 46.52
Iteration: 216 | Episodes: 8800 | Median Reward: 30.05 | Max Reward: 46.52
Iteration: 219 | Episodes: 8900 | Median Reward: 26.16 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.3         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 220           |
|    time_elapsed         | 3348          |
|    total_timesteps      | 901120        |
| train/                  |               |
|    approx_kl            | 0.00030542037 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -94.4         |
|    explained_variance   | -1.07e-06     |
|    learning_rate        | 0.0005        |
|    loss                 | 151           |
|    n_updates            | 2190          |
|    policy_gradient_loss | -0.000754     |
|    std                  | 1.57          |
|    value_loss           | 311           |
-------------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 33.89 | Max Reward: 46.52
Iteration: 224 | Episodes: 9100 | Median Reward: 31.76 | Max Reward: 46.52
Iteration: 226 | Episodes: 9200 | Median Reward: 31.81 | Max Reward: 46.52
Iteration: 229 | Episodes: 9300 | Median Reward: 31.60 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.3       |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 230         |
|    time_elapsed         | 3500        |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.043680575 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.4         |
|    entropy_loss         | -95         |
|    explained_variance   | -1.07e-06   |
|    learning_rate        | 0.0005      |
|    loss                 | 145         |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00783    |
|    std                  | 1.6         |
|    value_loss           | 299         |
-----------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 28.59 | Max Reward: 46.52
Iteration: 234 | Episodes: 9500 | Median Reward: 32.33 | Max Reward: 46.52
Iteration: 236 | Episodes: 9600 | Median Reward: 31.90 | Max Reward: 46.52
Iteration: 239 | Episodes: 9700 | Median Reward: 32.20 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.8         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 240           |
|    time_elapsed         | 3650          |
|    total_timesteps      | 983040        |
| train/                  |               |
|    approx_kl            | 0.00013094721 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95.7         |
|    explained_variance   | -8.34e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 160           |
|    n_updates            | 2390          |
|    policy_gradient_loss | 0.00013       |
|    std                  | 1.64          |
|    value_loss           | 329           |
-------------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 31.50 | Max Reward: 46.52
Iteration: 244 | Episodes: 9900 | Median Reward: 29.29 | Max Reward: 46.52
Iteration: 246 | Episodes: 10000 | Median Reward: 36.62 | Max Reward: 46.52
Iteration: 249 | Episodes: 10100 | Median Reward: 34.43 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.9        |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 250          |
|    time_elapsed         | 3800         |
|    total_timesteps      | 1024000      |
| train/                  |              |
|    approx_kl            | 0.0013833208 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -96.2        |
|    explained_variance   | -7.15e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 156          |
|    n_updates            | 2490         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 1.68         |
|    value_loss           | 322          |
------------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 27.76 | Max Reward: 46.52
Iteration: 253 | Episodes: 10300 | Median Reward: 32.95 | Max Reward: 46.52
Iteration: 256 | Episodes: 10400 | Median Reward: 36.16 | Max Reward: 46.52
Iteration: 258 | Episodes: 10500 | Median Reward: 32.12 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.8        |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 260          |
|    time_elapsed         | 3950         |
|    total_timesteps      | 1064960      |
| train/                  |              |
|    approx_kl            | 0.0010804767 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.4          |
|    entropy_loss         | -95.9        |
|    explained_variance   | -7.15e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 135          |
|    n_updates            | 2590         |
|    policy_gradient_loss | 0.000588     |
|    std                  | 1.71         |
|    value_loss           | 279          |
------------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 27.10 | Max Reward: 46.52
Iteration: 263 | Episodes: 10700 | Median Reward: 30.89 | Max Reward: 46.52
Iteration: 266 | Episodes: 10800 | Median Reward: 32.25 | Max Reward: 46.52
Iteration: 268 | Episodes: 10900 | Median Reward: 24.88 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.8       |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 270         |
|    time_elapsed         | 4101        |
|    total_timesteps      | 1105920     |
| train/                  |             |
|    approx_kl            | 0.008747701 |
|    clip_fraction        | 0.00879     |
|    clip_range           | 0.4         |
|    entropy_loss         | -96.6       |
|    explained_variance   | -5.96e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 126         |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.00699    |
|    std                  | 1.73        |
|    value_loss           | 261         |
-----------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 34.58 | Max Reward: 46.52
Iteration: 273 | Episodes: 11100 | Median Reward: 28.48 | Max Reward: 46.52
Iteration: 276 | Episodes: 11200 | Median Reward: 33.31 | Max Reward: 46.52
Iteration: 278 | Episodes: 11300 | Median Reward: 33.32 | Max Reward: 46.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -68.4      |
| time/                   |            |
|    fps                  | 269        |
|    iterations           | 280        |
|    time_elapsed         | 4252       |
|    total_timesteps      | 1146880    |
| train/                  |            |
|    approx_kl            | 0.22075084 |
|    clip_fraction        | 0.043      |
|    clip_range           | 0.4        |
|    entropy_loss         | -96.2      |
|    explained_variance   | -5.96e-07  |
|    learning_rate        | 0.0005     |
|    loss                 | 147        |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.017     |
|    std                  | 1.76       |
|    value_loss           | 304        |
----------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 34.75 | Max Reward: 46.52
Iteration: 283 | Episodes: 11500 | Median Reward: -12.46 | Max Reward: 46.52
Iteration: 286 | Episodes: 11600 | Median Reward: 26.59 | Max Reward: 46.52
Iteration: 288 | Episodes: 11700 | Median Reward: -22.64 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -128         |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 290          |
|    time_elapsed         | 4402         |
|    total_timesteps      | 1187840      |
| train/                  |              |
|    approx_kl            | 9.298034e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.7        |
|    explained_variance   | -1.91e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 44.7         |
|    n_updates            | 2890         |
|    policy_gradient_loss | -3.83e-05    |
|    std                  | 1.79         |
|    value_loss           | 102          |
------------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: -26.07 | Max Reward: 46.52
Iteration: 293 | Episodes: 11900 | Median Reward: -28.81 | Max Reward: 46.52
Iteration: 295 | Episodes: 12000 | Median Reward: -21.41 | Max Reward: 46.52
Iteration: 298 | Episodes: 12100 | Median Reward: -19.93 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -122          |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 300           |
|    time_elapsed         | 4554          |
|    total_timesteps      | 1228800       |
| train/                  |               |
|    approx_kl            | 1.3872894e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -97.7         |
|    explained_variance   | -1.67e-06     |
|    learning_rate        | 0.0005        |
|    loss                 | 36.3          |
|    n_updates            | 2990          |
|    policy_gradient_loss | -0.00045      |
|    std                  | 1.79          |
|    value_loss           | 84.5          |
-------------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: -26.57 | Max Reward: 46.52
Iteration: 303 | Episodes: 12300 | Median Reward: -25.55 | Max Reward: 46.52
Iteration: 305 | Episodes: 12400 | Median Reward: -24.83 | Max Reward: 46.52
Iteration: 308 | Episodes: 12500 | Median Reward: -28.06 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -125         |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 310          |
|    time_elapsed         | 4703         |
|    total_timesteps      | 1269760      |
| train/                  |              |
|    approx_kl            | 2.629678e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.7        |
|    explained_variance   | -1.43e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 36           |
|    n_updates            | 3090         |
|    policy_gradient_loss | -0.000644    |
|    std                  | 1.79         |
|    value_loss           | 83.7         |
------------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: -17.39 | Max Reward: 46.52
Iteration: 313 | Episodes: 12700 | Median Reward: -24.56 | Max Reward: 46.52
Iteration: 315 | Episodes: 12800 | Median Reward: -25.25 | Max Reward: 46.52
Iteration: 318 | Episodes: 12900 | Median Reward: -21.22 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -120         |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 320          |
|    time_elapsed         | 4856         |
|    total_timesteps      | 1310720      |
| train/                  |              |
|    approx_kl            | 2.476963e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.7        |
|    explained_variance   | -1.55e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 32.1         |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.000671    |
|    std                  | 1.79         |
|    value_loss           | 75.8         |
------------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: -21.16 | Max Reward: 46.52
Iteration: 323 | Episodes: 13100 | Median Reward: -17.47 | Max Reward: 46.52
Iteration: 325 | Episodes: 13200 | Median Reward: -22.16 | Max Reward: 46.52
Iteration: 327 | Episodes: 13300 | Median Reward: -22.22 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -118         |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 330          |
|    time_elapsed         | 5007         |
|    total_timesteps      | 1351680      |
| train/                  |              |
|    approx_kl            | 2.171463e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.8        |
|    explained_variance   | -1.55e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 31.9         |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.000437    |
|    std                  | 1.79         |
|    value_loss           | 74.9         |
------------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: -18.56 | Max Reward: 46.52
Iteration: 332 | Episodes: 13500 | Median Reward: -17.44 | Max Reward: 46.52
Iteration: 335 | Episodes: 13600 | Median Reward: -23.36 | Max Reward: 46.52
Iteration: 337 | Episodes: 13700 | Median Reward: -25.15 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -120          |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 340           |
|    time_elapsed         | 5164          |
|    total_timesteps      | 1392640       |
| train/                  |               |
|    approx_kl            | 2.0858366e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -97.8         |
|    explained_variance   | -1.31e-06     |
|    learning_rate        | 0.0005        |
|    loss                 | 40.8          |
|    n_updates            | 3390          |
|    policy_gradient_loss | -0.000196     |
|    std                  | 1.8           |
|    value_loss           | 92.1          |
-------------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: -20.99 | Max Reward: 46.52
Iteration: 342 | Episodes: 13900 | Median Reward: -16.35 | Max Reward: 46.52
Iteration: 345 | Episodes: 14000 | Median Reward: -19.90 | Max Reward: 46.52
Iteration: 347 | Episodes: 14100 | Median Reward: -22.58 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -126          |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 350           |
|    time_elapsed         | 5320          |
|    total_timesteps      | 1433600       |
| train/                  |               |
|    approx_kl            | 0.00015699078 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -97.9         |
|    explained_variance   | -1.43e-06     |
|    learning_rate        | 0.0005        |
|    loss                 | 31.2          |
|    n_updates            | 3490          |
|    policy_gradient_loss | -0.00173      |
|    std                  | 1.81          |
|    value_loss           | 73            |
-------------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: -22.68 | Max Reward: 46.52
Iteration: 352 | Episodes: 14300 | Median Reward: -18.04 | Max Reward: 46.52
Iteration: 355 | Episodes: 14400 | Median Reward: -22.59 | Max Reward: 46.52
Iteration: 357 | Episodes: 14500 | Median Reward: -23.57 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -121         |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 360          |
|    time_elapsed         | 5472         |
|    total_timesteps      | 1474560      |
| train/                  |              |
|    approx_kl            | 0.0010544148 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -98.1        |
|    explained_variance   | -1.43e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 32           |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.0034      |
|    std                  | 1.82         |
|    value_loss           | 74.2         |
------------------------------------------
Iteration: 360 | Episodes: 14600 | Median Reward: -18.32 | Max Reward: 46.52
Iteration: 362 | Episodes: 14700 | Median Reward: -24.14 | Max Reward: 46.52
Iteration: 364 | Episodes: 14800 | Median Reward: -18.89 | Max Reward: 46.52
Iteration: 367 | Episodes: 14900 | Median Reward: -20.30 | Max Reward: 46.52
Iteration: 369 | Episodes: 15000 | Median Reward: -28.33 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -125         |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 370          |
|    time_elapsed         | 5622         |
|    total_timesteps      | 1515520      |
| train/                  |              |
|    approx_kl            | 0.0038547423 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -98.6        |
|    explained_variance   | -8.34e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 69.4         |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00549     |
|    std                  | 1.85         |
|    value_loss           | 149          |
------------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: -16.17 | Max Reward: 46.52
Iteration: 374 | Episodes: 15200 | Median Reward: -19.54 | Max Reward: 46.52
Iteration: 377 | Episodes: 15300 | Median Reward: -13.20 | Max Reward: 46.52
Iteration: 379 | Episodes: 15400 | Median Reward: -13.66 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -118        |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 380         |
|    time_elapsed         | 5772        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.018259887 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -99.2       |
|    explained_variance   | -9.54e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 60.4        |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.0151     |
|    std                  | 1.9         |
|    value_loss           | 131         |
-----------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: -15.72 | Max Reward: 46.52
Iteration: 384 | Episodes: 15600 | Median Reward: -12.20 | Max Reward: 46.52
Iteration: 387 | Episodes: 15700 | Median Reward: -19.37 | Max Reward: 46.52
Iteration: 389 | Episodes: 15800 | Median Reward: -13.59 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -119         |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 390          |
|    time_elapsed         | 5925         |
|    total_timesteps      | 1597440      |
| train/                  |              |
|    approx_kl            | 0.0017069664 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -99.6        |
|    explained_variance   | -9.54e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 46.9         |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.00326     |
|    std                  | 1.93         |
|    value_loss           | 104          |
------------------------------------------
Iteration: 392 | Episodes: 15900 | Median Reward: -15.27 | Max Reward: 46.52
Iteration: 394 | Episodes: 16000 | Median Reward: -13.20 | Max Reward: 46.52
Iteration: 396 | Episodes: 16100 | Median Reward: -9.20 | Max Reward: 46.52
Iteration: 399 | Episodes: 16200 | Median Reward: -13.95 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -117          |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 400           |
|    time_elapsed         | 6077          |
|    total_timesteps      | 1638400       |
| train/                  |               |
|    approx_kl            | 0.00040719734 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -100          |
|    explained_variance   | -7.15e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 88.2          |
|    n_updates            | 3990          |
|    policy_gradient_loss | -0.00151      |
|    std                  | 1.98          |
|    value_loss           | 187           |
-------------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: -21.59 | Max Reward: 46.52
Iteration: 404 | Episodes: 16400 | Median Reward: -10.12 | Max Reward: 46.52
Iteration: 406 | Episodes: 16500 | Median Reward: -13.00 | Max Reward: 46.52
Iteration: 409 | Episodes: 16600 | Median Reward: -16.65 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -116        |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 410         |
|    time_elapsed         | 6228        |
|    total_timesteps      | 1679360     |
| train/                  |             |
|    approx_kl            | 0.003816855 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -101        |
|    explained_variance   | -4.77e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 92.2        |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.00893    |
|    std                  | 2.01        |
|    value_loss           | 194         |
-----------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: -11.75 | Max Reward: 46.52
Iteration: 414 | Episodes: 16800 | Median Reward: -14.96 | Max Reward: 46.52
Iteration: 416 | Episodes: 16900 | Median Reward: -14.52 | Max Reward: 46.52
Iteration: 419 | Episodes: 17000 | Median Reward: -14.70 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -113        |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 420         |
|    time_elapsed         | 6380        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.010447868 |
|    clip_fraction        | 0.000537    |
|    clip_range           | 0.4         |
|    entropy_loss         | -101        |
|    explained_variance   | -7.15e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 57.7        |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00763    |
|    std                  | 2.06        |
|    value_loss           | 126         |
-----------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: -10.22 | Max Reward: 46.52
Iteration: 424 | Episodes: 17200 | Median Reward: -8.68 | Max Reward: 46.52
Iteration: 426 | Episodes: 17300 | Median Reward: -9.64 | Max Reward: 46.52
Iteration: 429 | Episodes: 17400 | Median Reward: -11.23 | Max Reward: 46.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -111       |
| time/                   |            |
|    fps                  | 269        |
|    iterations           | 430        |
|    time_elapsed         | 6529       |
|    total_timesteps      | 1761280    |
| train/                  |            |
|    approx_kl            | 0.08039892 |
|    clip_fraction        | 0.0575     |
|    clip_range           | 0.4        |
|    entropy_loss         | -100       |
|    explained_variance   | -5.96e-07  |
|    learning_rate        | 0.0005     |
|    loss                 | 67.9       |
|    n_updates            | 4290       |
|    policy_gradient_loss | -0.015     |
|    std                  | 2.1        |
|    value_loss           | 146        |
----------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: -12.54 | Max Reward: 46.52
Iteration: 433 | Episodes: 17600 | Median Reward: -1.72 | Max Reward: 46.52
Iteration: 436 | Episodes: 17700 | Median Reward: -9.03 | Max Reward: 46.52
Iteration: 438 | Episodes: 17800 | Median Reward: 8.04 | Max Reward: 46.52
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -96.1     |
| time/                   |           |
|    fps                  | 269       |
|    iterations           | 440       |
|    time_elapsed         | 6679      |
|    total_timesteps      | 1802240   |
| train/                  |           |
|    approx_kl            | 0.0690725 |
|    clip_fraction        | 0.0657    |
|    clip_range           | 0.4       |
|    entropy_loss         | -96.1     |
|    explained_variance   | -5.96e-07 |
|    learning_rate        | 0.0005    |
|    loss                 | 99.7      |
|    n_updates            | 4390      |
|    policy_gradient_loss | -0.00741  |
|    std                  | 2.12      |
|    value_loss           | 209       |
---------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 14.47 | Max Reward: 46.52
Iteration: 443 | Episodes: 18000 | Median Reward: 18.45 | Max Reward: 46.52
Iteration: 446 | Episodes: 18100 | Median Reward: 18.94 | Max Reward: 46.52
Iteration: 448 | Episodes: 18200 | Median Reward: 24.73 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77          |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 450          |
|    time_elapsed         | 6830         |
|    total_timesteps      | 1843200      |
| train/                  |              |
|    approx_kl            | 0.0042905174 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -95.3        |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 214          |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 2.13         |
|    value_loss           | 438          |
------------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 23.40 | Max Reward: 46.52
Iteration: 453 | Episodes: 18400 | Median Reward: 23.02 | Max Reward: 46.52
Iteration: 456 | Episodes: 18500 | Median Reward: 32.97 | Max Reward: 46.52
Iteration: 458 | Episodes: 18600 | Median Reward: 22.73 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79          |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 460          |
|    time_elapsed         | 6981         |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 0.0071962066 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -98          |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 165          |
|    n_updates            | 4590         |
|    policy_gradient_loss | 0.00144      |
|    std                  | 2.13         |
|    value_loss           | 341          |
------------------------------------------
Iteration: 461 | Episodes: 18700 | Median Reward: 18.40 | Max Reward: 46.52
Iteration: 463 | Episodes: 18800 | Median Reward: 19.82 | Max Reward: 46.52
Iteration: 466 | Episodes: 18900 | Median Reward: 26.07 | Max Reward: 46.52
Iteration: 468 | Episodes: 19000 | Median Reward: 22.82 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76.8       |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 470         |
|    time_elapsed         | 7130        |
|    total_timesteps      | 1925120     |
| train/                  |             |
|    approx_kl            | 0.001263377 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -99.5       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 160         |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.000288   |
|    std                  | 2.14        |
|    value_loss           | 330         |
-----------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 29.18 | Max Reward: 46.52
Iteration: 473 | Episodes: 19200 | Median Reward: 24.29 | Max Reward: 46.52
Iteration: 475 | Episodes: 19300 | Median Reward: 24.58 | Max Reward: 46.52
Iteration: 478 | Episodes: 19400 | Median Reward: 22.88 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.9        |
| time/                   |              |
|    fps                  | 270          |
|    iterations           | 480          |
|    time_elapsed         | 7281         |
|    total_timesteps      | 1966080      |
| train/                  |              |
|    approx_kl            | 0.0070123603 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.4          |
|    entropy_loss         | -101         |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 109          |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.00331     |
|    std                  | 2.15         |
|    value_loss           | 229          |
------------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 28.97 | Max Reward: 46.52
Iteration: 483 | Episodes: 19600 | Median Reward: 26.76 | Max Reward: 46.52
Iteration: 485 | Episodes: 19700 | Median Reward: 20.51 | Max Reward: 46.52
Iteration: 488 | Episodes: 19800 | Median Reward: 21.90 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.6         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 490           |
|    time_elapsed         | 7441          |
|    total_timesteps      | 2007040       |
| train/                  |               |
|    approx_kl            | 0.00059018226 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | -3.58e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 132           |
|    n_updates            | 4890          |
|    policy_gradient_loss | -0.00148      |
|    std                  | 2.18          |
|    value_loss           | 273           |
-------------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 22.47 | Max Reward: 46.52
Iteration: 493 | Episodes: 20000 | Median Reward: 24.79 | Max Reward: 46.52
Iteration: 495 | Episodes: 20100 | Median Reward: 28.18 | Max Reward: 46.52
Iteration: 498 | Episodes: 20200 | Median Reward: 28.86 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -72           |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 500           |
|    time_elapsed         | 7591          |
|    total_timesteps      | 2048000       |
| train/                  |               |
|    approx_kl            | 0.00025828317 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | -4.77e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 141           |
|    n_updates            | 4990          |
|    policy_gradient_loss | 2.96e-05      |
|    std                  | 2.22          |
|    value_loss           | 292           |
-------------------------------------------
Iteration: 500 | Episodes: 20300 | Median Reward: 24.98 | Max Reward: 46.52
Iteration: 503 | Episodes: 20400 | Median Reward: 15.98 | Max Reward: 46.52
Iteration: 505 | Episodes: 20500 | Median Reward: 27.50 | Max Reward: 46.52
Iteration: 507 | Episodes: 20600 | Median Reward: 24.67 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.1        |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 510          |
|    time_elapsed         | 7745         |
|    total_timesteps      | 2088960      |
| train/                  |              |
|    approx_kl            | 0.0012195497 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -103         |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 120          |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.00175     |
|    std                  | 2.26         |
|    value_loss           | 251          |
------------------------------------------
Iteration: 510 | Episodes: 20700 | Median Reward: 24.72 | Max Reward: 46.52
Iteration: 512 | Episodes: 20800 | Median Reward: 21.14 | Max Reward: 46.52
Iteration: 515 | Episodes: 20900 | Median Reward: 21.19 | Max Reward: 46.52
Iteration: 517 | Episodes: 21000 | Median Reward: 24.37 | Max Reward: 46.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -72.2      |
| time/                   |            |
|    fps                  | 269        |
|    iterations           | 520        |
|    time_elapsed         | 7897       |
|    total_timesteps      | 2129920    |
| train/                  |            |
|    approx_kl            | 0.02595123 |
|    clip_fraction        | 0.00488    |
|    clip_range           | 0.4        |
|    entropy_loss         | -104       |
|    explained_variance   | -3.58e-07  |
|    learning_rate        | 0.0005     |
|    loss                 | 142        |
|    n_updates            | 5190       |
|    policy_gradient_loss | -0.00429   |
|    std                  | 2.31       |
|    value_loss           | 295        |
----------------------------------------
Iteration: 520 | Episodes: 21100 | Median Reward: 26.32 | Max Reward: 46.52
Iteration: 522 | Episodes: 21200 | Median Reward: 32.74 | Max Reward: 46.52
Iteration: 525 | Episodes: 21300 | Median Reward: 24.32 | Max Reward: 46.52
Iteration: 527 | Episodes: 21400 | Median Reward: 31.80 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.5       |
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 530         |
|    time_elapsed         | 8047        |
|    total_timesteps      | 2170880     |
| train/                  |             |
|    approx_kl            | 0.018994616 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -104        |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 137         |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.00714    |
|    std                  | 2.37        |
|    value_loss           | 284         |
-----------------------------------------
Iteration: 530 | Episodes: 21500 | Median Reward: 27.43 | Max Reward: 46.52
Iteration: 532 | Episodes: 21600 | Median Reward: 28.26 | Max Reward: 46.52
Iteration: 535 | Episodes: 21700 | Median Reward: 22.99 | Max Reward: 46.52
Iteration: 537 | Episodes: 21800 | Median Reward: 28.05 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.6         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 540           |
|    time_elapsed         | 8199          |
|    total_timesteps      | 2211840       |
| train/                  |               |
|    approx_kl            | 0.00042301507 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -104          |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 151           |
|    n_updates            | 5390          |
|    policy_gradient_loss | -0.00102      |
|    std                  | 2.41          |
|    value_loss           | 312           |
-------------------------------------------
Iteration: 540 | Episodes: 21900 | Median Reward: 26.72 | Max Reward: 46.52
Iteration: 542 | Episodes: 22000 | Median Reward: 26.92 | Max Reward: 46.52
Iteration: 544 | Episodes: 22100 | Median Reward: 29.67 | Max Reward: 46.52
Iteration: 547 | Episodes: 22200 | Median Reward: 26.76 | Max Reward: 46.52
Iteration: 549 | Episodes: 22300 | Median Reward: 26.46 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.9         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 550           |
|    time_elapsed         | 8351          |
|    total_timesteps      | 2252800       |
| train/                  |               |
|    approx_kl            | 0.00042107265 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -105          |
|    explained_variance   | -3.58e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 124           |
|    n_updates            | 5490          |
|    policy_gradient_loss | -0.000659     |
|    std                  | 2.47          |
|    value_loss           | 260           |
-------------------------------------------
Iteration: 552 | Episodes: 22400 | Median Reward: 31.11 | Max Reward: 46.52
Iteration: 554 | Episodes: 22500 | Median Reward: 30.60 | Max Reward: 46.52
Iteration: 557 | Episodes: 22600 | Median Reward: 30.31 | Max Reward: 46.52
Iteration: 559 | Episodes: 22700 | Median Reward: 29.28 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.1        |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 560          |
|    time_elapsed         | 8499         |
|    total_timesteps      | 2293760      |
| train/                  |              |
|    approx_kl            | 0.0037871676 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -106         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 134          |
|    n_updates            | 5590         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 2.54         |
|    value_loss           | 279          |
------------------------------------------
Iteration: 562 | Episodes: 22800 | Median Reward: 30.74 | Max Reward: 46.52
Iteration: 564 | Episodes: 22900 | Median Reward: 26.22 | Max Reward: 46.52
Iteration: 567 | Episodes: 23000 | Median Reward: 34.46 | Max Reward: 46.52
Iteration: 569 | Episodes: 23100 | Median Reward: 30.72 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.4         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 570           |
|    time_elapsed         | 8651          |
|    total_timesteps      | 2334720       |
| train/                  |               |
|    approx_kl            | 0.00035183044 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -107          |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 138           |
|    n_updates            | 5690          |
|    policy_gradient_loss | -0.000903     |
|    std                  | 2.58          |
|    value_loss           | 286           |
-------------------------------------------
Iteration: 572 | Episodes: 23200 | Median Reward: 28.16 | Max Reward: 46.52
Iteration: 574 | Episodes: 23300 | Median Reward: 28.49 | Max Reward: 46.52
Iteration: 577 | Episodes: 23400 | Median Reward: 29.79 | Max Reward: 46.52
Iteration: 579 | Episodes: 23500 | Median Reward: 33.16 | Max Reward: 46.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -67        |
| time/                   |            |
|    fps                  | 269        |
|    iterations           | 580        |
|    time_elapsed         | 8804       |
|    total_timesteps      | 2375680    |
| train/                  |            |
|    approx_kl            | 0.00962056 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0005     |
|    loss                 | 150        |
|    n_updates            | 5790       |
|    policy_gradient_loss | -0.0049    |
|    std                  | 2.65       |
|    value_loss           | 311        |
----------------------------------------
Iteration: 581 | Episodes: 23600 | Median Reward: 24.99 | Max Reward: 46.52
Iteration: 584 | Episodes: 23700 | Median Reward: 25.49 | Max Reward: 46.52
Iteration: 586 | Episodes: 23800 | Median Reward: 32.61 | Max Reward: 46.52
Iteration: 589 | Episodes: 23900 | Median Reward: 29.00 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.6         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 590           |
|    time_elapsed         | 8952          |
|    total_timesteps      | 2416640       |
| train/                  |               |
|    approx_kl            | 0.00013776349 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -108          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 120           |
|    n_updates            | 5890          |
|    policy_gradient_loss | -0.000112     |
|    std                  | 2.7           |
|    value_loss           | 252           |
-------------------------------------------
Iteration: 591 | Episodes: 24000 | Median Reward: 31.62 | Max Reward: 46.52
Iteration: 594 | Episodes: 24100 | Median Reward: 29.61 | Max Reward: 46.52
Iteration: 596 | Episodes: 24200 | Median Reward: 26.38 | Max Reward: 46.52
Iteration: 599 | Episodes: 24300 | Median Reward: 26.38 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72.7        |
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 600          |
|    time_elapsed         | 9104         |
|    total_timesteps      | 2457600      |
| train/                  |              |
|    approx_kl            | 0.0004636647 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -108         |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 128          |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.001       |
|    std                  | 2.76         |
|    value_loss           | 267          |
------------------------------------------
Iteration: 601 | Episodes: 24400 | Median Reward: 30.16 | Max Reward: 46.52
Iteration: 604 | Episodes: 24500 | Median Reward: 22.43 | Max Reward: 46.52
Iteration: 606 | Episodes: 24600 | Median Reward: 28.01 | Max Reward: 46.52
Iteration: 609 | Episodes: 24700 | Median Reward: 26.27 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -72.5         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 610           |
|    time_elapsed         | 9255          |
|    total_timesteps      | 2498560       |
| train/                  |               |
|    approx_kl            | 1.0910444e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -109          |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 149           |
|    n_updates            | 6090          |
|    policy_gradient_loss | 1.71e-05      |
|    std                  | 2.8           |
|    value_loss           | 310           |
-------------------------------------------
Iteration: 611 | Episodes: 24800 | Median Reward: 27.51 | Max Reward: 46.52
Iteration: 613 | Episodes: 24900 | Median Reward: 33.86 | Max Reward: 46.52
Iteration: 616 | Episodes: 25000 | Median Reward: 29.69 | Max Reward: 46.52
Iteration: 618 | Episodes: 25100 | Median Reward: 25.15 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.3         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 620           |
|    time_elapsed         | 9407          |
|    total_timesteps      | 2539520       |
| train/                  |               |
|    approx_kl            | 0.00048818605 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -109          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 124           |
|    n_updates            | 6190          |
|    policy_gradient_loss | 0.00057       |
|    std                  | 2.87          |
|    value_loss           | 259           |
-------------------------------------------
Iteration: 621 | Episodes: 25200 | Median Reward: 31.18 | Max Reward: 46.52
Iteration: 623 | Episodes: 25300 | Median Reward: 30.82 | Max Reward: 46.52
Iteration: 626 | Episodes: 25400 | Median Reward: 31.01 | Max Reward: 46.52
Iteration: 628 | Episodes: 25500 | Median Reward: 24.21 | Max Reward: 46.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -75.6      |
| time/                   |            |
|    fps                  | 269        |
|    iterations           | 630        |
|    time_elapsed         | 9565       |
|    total_timesteps      | 2580480    |
| train/                  |            |
|    approx_kl            | 0.00976748 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0005     |
|    loss                 | 137        |
|    n_updates            | 6290       |
|    policy_gradient_loss | -0.00472   |
|    std                  | 2.97       |
|    value_loss           | 284        |
----------------------------------------
Iteration: 631 | Episodes: 25600 | Median Reward: 29.08 | Max Reward: 46.52
Iteration: 633 | Episodes: 25700 | Median Reward: 28.30 | Max Reward: 46.52
Iteration: 636 | Episodes: 25800 | Median Reward: 29.88 | Max Reward: 46.52
Iteration: 638 | Episodes: 25900 | Median Reward: 27.69 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.7         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 640           |
|    time_elapsed         | 9713          |
|    total_timesteps      | 2621440       |
| train/                  |               |
|    approx_kl            | 9.6954114e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -110          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 121           |
|    n_updates            | 6390          |
|    policy_gradient_loss | -0.000469     |
|    std                  | 3.01          |
|    value_loss           | 253           |
-------------------------------------------
Iteration: 641 | Episodes: 26000 | Median Reward: 31.49 | Max Reward: 46.52
Iteration: 643 | Episodes: 26100 | Median Reward: 33.27 | Max Reward: 46.52
Iteration: 646 | Episodes: 26200 | Median Reward: 28.34 | Max Reward: 46.52
Iteration: 648 | Episodes: 26300 | Median Reward: 27.69 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.2         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 650           |
|    time_elapsed         | 9865          |
|    total_timesteps      | 2662400       |
| train/                  |               |
|    approx_kl            | 5.4141623e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -111          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 134           |
|    n_updates            | 6490          |
|    policy_gradient_loss | 5.4e-05       |
|    std                  | 3.06          |
|    value_loss           | 279           |
-------------------------------------------
Iteration: 650 | Episodes: 26400 | Median Reward: 32.57 | Max Reward: 46.52
Iteration: 653 | Episodes: 26500 | Median Reward: 31.98 | Max Reward: 46.52
Iteration: 655 | Episodes: 26600 | Median Reward: 33.43 | Max Reward: 46.52
Iteration: 658 | Episodes: 26700 | Median Reward: 26.98 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.7         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 660           |
|    time_elapsed         | 10015         |
|    total_timesteps      | 2703360       |
| train/                  |               |
|    approx_kl            | 5.7946643e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -111          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 160           |
|    n_updates            | 6590          |
|    policy_gradient_loss | -9.95e-05     |
|    std                  | 3.13          |
|    value_loss           | 331           |
-------------------------------------------
Iteration: 660 | Episodes: 26800 | Median Reward: 28.14 | Max Reward: 46.52
Iteration: 663 | Episodes: 26900 | Median Reward: 26.62 | Max Reward: 46.52
Iteration: 665 | Episodes: 27000 | Median Reward: 30.63 | Max Reward: 46.52
Iteration: 668 | Episodes: 27100 | Median Reward: 29.20 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.9         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 670           |
|    time_elapsed         | 10167         |
|    total_timesteps      | 2744320       |
| train/                  |               |
|    approx_kl            | 0.00055873394 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 148           |
|    n_updates            | 6690          |
|    policy_gradient_loss | -0.000659     |
|    std                  | 3.2           |
|    value_loss           | 307           |
-------------------------------------------
Iteration: 670 | Episodes: 27200 | Median Reward: 28.87 | Max Reward: 46.52
Iteration: 673 | Episodes: 27300 | Median Reward: 31.31 | Max Reward: 46.52
Iteration: 675 | Episodes: 27400 | Median Reward: 29.07 | Max Reward: 46.52
Iteration: 678 | Episodes: 27500 | Median Reward: 32.99 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.7         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 680           |
|    time_elapsed         | 10320         |
|    total_timesteps      | 2785280       |
| train/                  |               |
|    approx_kl            | 0.00023398617 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 165           |
|    n_updates            | 6790          |
|    policy_gradient_loss | -0.000479     |
|    std                  | 3.28          |
|    value_loss           | 342           |
-------------------------------------------
Iteration: 680 | Episodes: 27600 | Median Reward: 32.29 | Max Reward: 46.52
Iteration: 683 | Episodes: 27700 | Median Reward: 28.26 | Max Reward: 46.52
Iteration: 685 | Episodes: 27800 | Median Reward: 35.56 | Max Reward: 46.52
Iteration: 687 | Episodes: 27900 | Median Reward: 30.27 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.1         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 690           |
|    time_elapsed         | 10468         |
|    total_timesteps      | 2826240       |
| train/                  |               |
|    approx_kl            | 0.00073271175 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 129           |
|    n_updates            | 6890          |
|    policy_gradient_loss | -0.00134      |
|    std                  | 3.34          |
|    value_loss           | 269           |
-------------------------------------------
Iteration: 690 | Episodes: 28000 | Median Reward: 30.76 | Max Reward: 46.52
Iteration: 692 | Episodes: 28100 | Median Reward: 29.16 | Max Reward: 46.52
Iteration: 695 | Episodes: 28200 | Median Reward: 31.00 | Max Reward: 46.52
Iteration: 697 | Episodes: 28300 | Median Reward: 27.46 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.2         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 700           |
|    time_elapsed         | 10619         |
|    total_timesteps      | 2867200       |
| train/                  |               |
|    approx_kl            | 0.00027359312 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 173           |
|    n_updates            | 6990          |
|    policy_gradient_loss | 0.000134      |
|    std                  | 3.44          |
|    value_loss           | 357           |
-------------------------------------------
Iteration: 700 | Episodes: 28400 | Median Reward: 31.95 | Max Reward: 46.52
Iteration: 702 | Episodes: 28500 | Median Reward: 27.45 | Max Reward: 46.52
Iteration: 705 | Episodes: 28600 | Median Reward: 27.14 | Max Reward: 46.52
Iteration: 707 | Episodes: 28700 | Median Reward: 36.89 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.3         |
| time/                   |               |
|    fps                  | 270           |
|    iterations           | 710           |
|    time_elapsed         | 10770         |
|    total_timesteps      | 2908160       |
| train/                  |               |
|    approx_kl            | 0.00086183473 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.4           |
|    entropy_loss         | -114          |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 115           |
|    n_updates            | 7090          |
|    policy_gradient_loss | 0.00133       |
|    std                  | 3.57          |
|    value_loss           | 241           |
-------------------------------------------
Iteration: 710 | Episodes: 28800 | Median Reward: 23.55 | Max Reward: 46.52
Iteration: 712 | Episodes: 28900 | Median Reward: 30.78 | Max Reward: 46.52
Iteration: 715 | Episodes: 29000 | Median Reward: 31.87 | Max Reward: 46.52
Iteration: 717 | Episodes: 29100 | Median Reward: 26.59 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69           |
| time/                   |               |
|    fps                  | 270           |
|    iterations           | 720           |
|    time_elapsed         | 10921         |
|    total_timesteps      | 2949120       |
| train/                  |               |
|    approx_kl            | 5.1598297e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -115          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 157           |
|    n_updates            | 7190          |
|    policy_gradient_loss | -0.000136     |
|    std                  | 3.62          |
|    value_loss           | 325           |
-------------------------------------------
Iteration: 720 | Episodes: 29200 | Median Reward: 30.83 | Max Reward: 46.52
Iteration: 722 | Episodes: 29300 | Median Reward: 30.78 | Max Reward: 46.52
Iteration: 724 | Episodes: 29400 | Median Reward: 32.54 | Max Reward: 46.52
Iteration: 727 | Episodes: 29500 | Median Reward: 30.20 | Max Reward: 46.52
Iteration: 729 | Episodes: 29600 | Median Reward: 32.14 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.2       |
| time/                   |             |
|    fps                  | 270         |
|    iterations           | 730         |
|    time_elapsed         | 11073       |
|    total_timesteps      | 2990080     |
| train/                  |             |
|    approx_kl            | 0.008140826 |
|    clip_fraction        | 0.00591     |
|    clip_range           | 0.4         |
|    entropy_loss         | -115        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 159         |
|    n_updates            | 7290        |
|    policy_gradient_loss | 0.00094     |
|    std                  | 3.69        |
|    value_loss           | 330         |
-----------------------------------------
Iteration: 732 | Episodes: 29700 | Median Reward: 30.03 | Max Reward: 46.52
Iteration: 734 | Episodes: 29800 | Median Reward: 30.24 | Max Reward: 46.52
Iteration: 737 | Episodes: 29900 | Median Reward: 29.70 | Max Reward: 46.52
Iteration: 739 | Episodes: 30000 | Median Reward: 30.72 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70         |
| time/                   |             |
|    fps                  | 270         |
|    iterations           | 740         |
|    time_elapsed         | 11223       |
|    total_timesteps      | 3031040     |
| train/                  |             |
|    approx_kl            | 0.007997689 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -116        |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 142         |
|    n_updates            | 7390        |
|    policy_gradient_loss | -0.00443    |
|    std                  | 3.78        |
|    value_loss           | 296         |
-----------------------------------------
Iteration: 742 | Episodes: 30100 | Median Reward: 37.31 | Max Reward: 46.52
Iteration: 744 | Episodes: 30200 | Median Reward: 28.74 | Max Reward: 46.52
Iteration: 747 | Episodes: 30300 | Median Reward: 29.29 | Max Reward: 46.52
Iteration: 749 | Episodes: 30400 | Median Reward: 35.09 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.5        |
| time/                   |              |
|    fps                  | 270          |
|    iterations           | 750          |
|    time_elapsed         | 11372        |
|    total_timesteps      | 3072000      |
| train/                  |              |
|    approx_kl            | 0.0037328603 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 168          |
|    n_updates            | 7490         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 3.88         |
|    value_loss           | 348          |
------------------------------------------
Iteration: 752 | Episodes: 30500 | Median Reward: 21.61 | Max Reward: 46.52
Iteration: 754 | Episodes: 30600 | Median Reward: 26.57 | Max Reward: 46.52
Iteration: 757 | Episodes: 30700 | Median Reward: 31.80 | Max Reward: 46.52
Iteration: 759 | Episodes: 30800 | Median Reward: 31.14 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.2       |
| time/                   |             |
|    fps                  | 270         |
|    iterations           | 760         |
|    time_elapsed         | 11521       |
|    total_timesteps      | 3112960     |
| train/                  |             |
|    approx_kl            | 0.082263455 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.4         |
|    entropy_loss         | -116        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 177         |
|    n_updates            | 7590        |
|    policy_gradient_loss | 0.0357      |
|    std                  | 3.93        |
|    value_loss           | 365         |
-----------------------------------------
Iteration: 761 | Episodes: 30900 | Median Reward: 29.15 | Max Reward: 46.52
Iteration: 764 | Episodes: 31000 | Median Reward: 30.36 | Max Reward: 46.52
Iteration: 766 | Episodes: 31100 | Median Reward: 32.79 | Max Reward: 46.52
Iteration: 769 | Episodes: 31200 | Median Reward: 33.22 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68         |
| time/                   |             |
|    fps                  | 270         |
|    iterations           | 770         |
|    time_elapsed         | 11676       |
|    total_timesteps      | 3153920     |
| train/                  |             |
|    approx_kl            | 5.88689e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -117        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 158         |
|    n_updates            | 7690        |
|    policy_gradient_loss | 5.19e-05    |
|    std                  | 3.98        |
|    value_loss           | 329         |
-----------------------------------------
Iteration: 771 | Episodes: 31300 | Median Reward: 31.41 | Max Reward: 46.52
Iteration: 774 | Episodes: 31400 | Median Reward: 33.32 | Max Reward: 46.52
Iteration: 776 | Episodes: 31500 | Median Reward: 33.00 | Max Reward: 46.52
Iteration: 779 | Episodes: 31600 | Median Reward: 32.97 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.3         |
| time/                   |               |
|    fps                  | 270           |
|    iterations           | 780           |
|    time_elapsed         | 11832         |
|    total_timesteps      | 3194880       |
| train/                  |               |
|    approx_kl            | 4.8476388e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -117          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 143           |
|    n_updates            | 7790          |
|    policy_gradient_loss | -0.00021      |
|    std                  | 4.05          |
|    value_loss           | 297           |
-------------------------------------------
Iteration: 781 | Episodes: 31700 | Median Reward: 32.44 | Max Reward: 46.52
Iteration: 784 | Episodes: 31800 | Median Reward: 31.97 | Max Reward: 46.52
Iteration: 786 | Episodes: 31900 | Median Reward: 31.57 | Max Reward: 46.52
Iteration: 789 | Episodes: 32000 | Median Reward: 32.20 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.3        |
| time/                   |              |
|    fps                  | 270          |
|    iterations           | 790          |
|    time_elapsed         | 11983        |
|    total_timesteps      | 3235840      |
| train/                  |              |
|    approx_kl            | 0.0001246437 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -118         |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 163          |
|    n_updates            | 7890         |
|    policy_gradient_loss | 8.38e-05     |
|    std                  | 4.17         |
|    value_loss           | 337          |
------------------------------------------
Iteration: 791 | Episodes: 32100 | Median Reward: 30.87 | Max Reward: 46.52
Iteration: 793 | Episodes: 32200 | Median Reward: 33.10 | Max Reward: 46.52
Iteration: 796 | Episodes: 32300 | Median Reward: 31.83 | Max Reward: 46.52
Iteration: 798 | Episodes: 32400 | Median Reward: 30.70 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -77.6       |
| time/                   |             |
|    fps                  | 270         |
|    iterations           | 800         |
|    time_elapsed         | 12134       |
|    total_timesteps      | 3276800     |
| train/                  |             |
|    approx_kl            | 0.000434155 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -119        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 108         |
|    n_updates            | 7990        |
|    policy_gradient_loss | 0.000852    |
|    std                  | 4.27        |
|    value_loss           | 228         |
-----------------------------------------
Iteration: 801 | Episodes: 32500 | Median Reward: 27.83 | Max Reward: 46.52
Iteration: 803 | Episodes: 32600 | Median Reward: 31.46 | Max Reward: 46.52
Iteration: 806 | Episodes: 32700 | Median Reward: 32.52 | Max Reward: 46.52
Iteration: 808 | Episodes: 32800 | Median Reward: 32.24 | Max Reward: 46.52
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -67.7          |
| time/                   |                |
|    fps                  | 270            |
|    iterations           | 810            |
|    time_elapsed         | 12286          |
|    total_timesteps      | 3317760        |
| train/                  |                |
|    approx_kl            | 0.000121265286 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -119           |
|    explained_variance   | -1.19e-07      |
|    learning_rate        | 0.0005         |
|    loss                 | 130            |
|    n_updates            | 8090           |
|    policy_gradient_loss | -0.000173      |
|    std                  | 4.38           |
|    value_loss           | 273            |
--------------------------------------------
Iteration: 811 | Episodes: 32900 | Median Reward: 32.67 | Max Reward: 46.52
Iteration: 813 | Episodes: 33000 | Median Reward: 29.89 | Max Reward: 46.52
Iteration: 816 | Episodes: 33100 | Median Reward: 28.50 | Max Reward: 46.52
Iteration: 818 | Episodes: 33200 | Median Reward: 24.85 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.1        |
| time/                   |              |
|    fps                  | 270          |
|    iterations           | 820          |
|    time_elapsed         | 12438        |
|    total_timesteps      | 3358720      |
| train/                  |              |
|    approx_kl            | 0.0024158917 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 135          |
|    n_updates            | 8190         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 4.47         |
|    value_loss           | 283          |
------------------------------------------
Iteration: 821 | Episodes: 33300 | Median Reward: 28.34 | Max Reward: 46.52
Iteration: 823 | Episodes: 33400 | Median Reward: 32.23 | Max Reward: 46.52
Iteration: 826 | Episodes: 33500 | Median Reward: 25.70 | Max Reward: 46.52
Iteration: 828 | Episodes: 33600 | Median Reward: 26.70 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -73.4        |
| time/                   |              |
|    fps                  | 270          |
|    iterations           | 830          |
|    time_elapsed         | 12588        |
|    total_timesteps      | 3399680      |
| train/                  |              |
|    approx_kl            | 7.304545e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 134          |
|    n_updates            | 8290         |
|    policy_gradient_loss | -8.32e-05    |
|    std                  | 4.57         |
|    value_loss           | 280          |
------------------------------------------
Iteration: 830 | Episodes: 33700 | Median Reward: 29.45 | Max Reward: 46.52
Iteration: 833 | Episodes: 33800 | Median Reward: 30.21 | Max Reward: 46.52
Iteration: 835 | Episodes: 33900 | Median Reward: 33.24 | Max Reward: 46.52
Iteration: 838 | Episodes: 34000 | Median Reward: 31.75 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.8        |
| time/                   |              |
|    fps                  | 270          |
|    iterations           | 840          |
|    time_elapsed         | 12736        |
|    total_timesteps      | 3440640      |
| train/                  |              |
|    approx_kl            | 0.0002201156 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -121         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 129          |
|    n_updates            | 8390         |
|    policy_gradient_loss | -0.000452    |
|    std                  | 4.71         |
|    value_loss           | 270          |
------------------------------------------
Iteration: 840 | Episodes: 34100 | Median Reward: 26.27 | Max Reward: 46.52
Iteration: 843 | Episodes: 34200 | Median Reward: 32.49 | Max Reward: 46.52
Iteration: 845 | Episodes: 34300 | Median Reward: 26.00 | Max Reward: 46.52
Iteration: 848 | Episodes: 34400 | Median Reward: 36.10 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.4        |
| time/                   |              |
|    fps                  | 270          |
|    iterations           | 850          |
|    time_elapsed         | 12886        |
|    total_timesteps      | 3481600      |
| train/                  |              |
|    approx_kl            | 9.011435e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -121         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 155          |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.000353    |
|    std                  | 4.79         |
|    value_loss           | 323          |
------------------------------------------
Iteration: 850 | Episodes: 34500 | Median Reward: 33.89 | Max Reward: 46.52
Iteration: 853 | Episodes: 34600 | Median Reward: 35.75 | Max Reward: 46.52
Iteration: 855 | Episodes: 34700 | Median Reward: 32.02 | Max Reward: 46.52
Iteration: 858 | Episodes: 34800 | Median Reward: 32.26 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.9         |
| time/                   |               |
|    fps                  | 270           |
|    iterations           | 860           |
|    time_elapsed         | 13036         |
|    total_timesteps      | 3522560       |
| train/                  |               |
|    approx_kl            | 4.9381822e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -122          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 165           |
|    n_updates            | 8590          |
|    policy_gradient_loss | 4.35e-05      |
|    std                  | 4.93          |
|    value_loss           | 343           |
-------------------------------------------
Iteration: 860 | Episodes: 34900 | Median Reward: 28.03 | Max Reward: 46.52
Iteration: 863 | Episodes: 35000 | Median Reward: 26.79 | Max Reward: 46.52
Iteration: 865 | Episodes: 35100 | Median Reward: 29.94 | Max Reward: 46.52
Iteration: 867 | Episodes: 35200 | Median Reward: 31.54 | Max Reward: 46.52
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -66            |
| time/                   |                |
|    fps                  | 270            |
|    iterations           | 870            |
|    time_elapsed         | 13187          |
|    total_timesteps      | 3563520        |
| train/                  |                |
|    approx_kl            | 0.000100823585 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -123           |
|    explained_variance   | 0              |
|    learning_rate        | 0.0005         |
|    loss                 | 165            |
|    n_updates            | 8690           |
|    policy_gradient_loss | 0.000122       |
|    std                  | 5.05           |
|    value_loss           | 343            |
--------------------------------------------
Iteration: 870 | Episodes: 35300 | Median Reward: 32.97 | Max Reward: 46.52
Iteration: 872 | Episodes: 35400 | Median Reward: 28.61 | Max Reward: 46.52
Iteration: 875 | Episodes: 35500 | Median Reward: 31.97 | Max Reward: 46.52
Iteration: 877 | Episodes: 35600 | Median Reward: 30.85 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.5         |
| time/                   |               |
|    fps                  | 270           |
|    iterations           | 880           |
|    time_elapsed         | 13337         |
|    total_timesteps      | 3604480       |
| train/                  |               |
|    approx_kl            | 0.00021979587 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 143           |
|    n_updates            | 8790          |
|    policy_gradient_loss | -0.000456     |
|    std                  | 5.17          |
|    value_loss           | 299           |
-------------------------------------------
Iteration: 880 | Episodes: 35700 | Median Reward: 31.77 | Max Reward: 46.52
Iteration: 882 | Episodes: 35800 | Median Reward: 31.63 | Max Reward: 46.52
Iteration: 885 | Episodes: 35900 | Median Reward: 34.49 | Max Reward: 46.52
Iteration: 887 | Episodes: 36000 | Median Reward: 31.17 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.3         |
| time/                   |               |
|    fps                  | 270           |
|    iterations           | 890           |
|    time_elapsed         | 13490         |
|    total_timesteps      | 3645440       |
| train/                  |               |
|    approx_kl            | 3.4393583e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -124          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 167           |
|    n_updates            | 8890          |
|    policy_gradient_loss | -0.000145     |
|    std                  | 5.28          |
|    value_loss           | 347           |
-------------------------------------------
Iteration: 890 | Episodes: 36100 | Median Reward: 32.10 | Max Reward: 46.52
Iteration: 892 | Episodes: 36200 | Median Reward: 22.73 | Max Reward: 46.52
Iteration: 895 | Episodes: 36300 | Median Reward: 31.02 | Max Reward: 46.52
Iteration: 897 | Episodes: 36400 | Median Reward: 27.78 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.4         |
| time/                   |               |
|    fps                  | 270           |
|    iterations           | 900           |
|    time_elapsed         | 13651         |
|    total_timesteps      | 3686400       |
| train/                  |               |
|    approx_kl            | 0.00035248935 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -124          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 145           |
|    n_updates            | 8990          |
|    policy_gradient_loss | -0.00074      |
|    std                  | 5.36          |
|    value_loss           | 303           |
-------------------------------------------
Iteration: 900 | Episodes: 36500 | Median Reward: 26.42 | Max Reward: 46.52
Iteration: 902 | Episodes: 36600 | Median Reward: 28.00 | Max Reward: 46.52
Iteration: 904 | Episodes: 36700 | Median Reward: 29.67 | Max Reward: 46.52
Iteration: 907 | Episodes: 36800 | Median Reward: 31.56 | Max Reward: 46.52
Iteration: 909 | Episodes: 36900 | Median Reward: 28.21 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -73          |
| time/                   |              |
|    fps                  | 270          |
|    iterations           | 910          |
|    time_elapsed         | 13801        |
|    total_timesteps      | 3727360      |
| train/                  |              |
|    approx_kl            | 0.0016430069 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -125         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 135          |
|    n_updates            | 9090         |
|    policy_gradient_loss | -0.000748    |
|    std                  | 5.54         |
|    value_loss           | 282          |
------------------------------------------
Iteration: 912 | Episodes: 37000 | Median Reward: 32.71 | Max Reward: 46.52
Iteration: 914 | Episodes: 37100 | Median Reward: 33.10 | Max Reward: 46.52
Iteration: 917 | Episodes: 37200 | Median Reward: 31.60 | Max Reward: 46.52
Iteration: 919 | Episodes: 37300 | Median Reward: 35.54 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.8        |
| time/                   |              |
|    fps                  | 270          |
|    iterations           | 920          |
|    time_elapsed         | 13951        |
|    total_timesteps      | 3768320      |
| train/                  |              |
|    approx_kl            | 4.027976e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -125         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 154          |
|    n_updates            | 9190         |
|    policy_gradient_loss | 4.47e-05     |
|    std                  | 5.7          |
|    value_loss           | 321          |
------------------------------------------
Iteration: 922 | Episodes: 37400 | Median Reward: 30.76 | Max Reward: 46.52
Iteration: 924 | Episodes: 37500 | Median Reward: 29.72 | Max Reward: 46.52
Iteration: 927 | Episodes: 37600 | Median Reward: 31.81 | Max Reward: 46.52
Iteration: 929 | Episodes: 37700 | Median Reward: 30.39 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.1         |
| time/                   |               |
|    fps                  | 270           |
|    iterations           | 930           |
|    time_elapsed         | 14071         |
|    total_timesteps      | 3809280       |
| train/                  |               |
|    approx_kl            | 2.7488451e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -126          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 134           |
|    n_updates            | 9290          |
|    policy_gradient_loss | -0.000257     |
|    std                  | 5.82          |
|    value_loss           | 280           |
-------------------------------------------
Iteration: 932 | Episodes: 37800 | Median Reward: 27.57 | Max Reward: 46.52
Iteration: 934 | Episodes: 37900 | Median Reward: 29.60 | Max Reward: 46.52
Iteration: 937 | Episodes: 38000 | Median Reward: 28.81 | Max Reward: 46.52
Iteration: 939 | Episodes: 38100 | Median Reward: 27.32 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.2         |
| time/                   |               |
|    fps                  | 272           |
|    iterations           | 940           |
|    time_elapsed         | 14111         |
|    total_timesteps      | 3850240       |
| train/                  |               |
|    approx_kl            | 0.00021507285 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -127          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 111           |
|    n_updates            | 9390          |
|    policy_gradient_loss | 8.2e-05       |
|    std                  | 6             |
|    value_loss           | 235           |
-------------------------------------------
Iteration: 941 | Episodes: 38200 | Median Reward: 26.99 | Max Reward: 46.52
Iteration: 944 | Episodes: 38300 | Median Reward: 32.39 | Max Reward: 46.52
Iteration: 946 | Episodes: 38400 | Median Reward: 28.36 | Max Reward: 46.52
Iteration: 949 | Episodes: 38500 | Median Reward: 28.63 | Max Reward: 46.52
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -71.7      |
| time/                   |            |
|    fps                  | 274        |
|    iterations           | 950        |
|    time_elapsed         | 14151      |
|    total_timesteps      | 3891200    |
| train/                  |            |
|    approx_kl            | 0.00411817 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -127       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0005     |
|    loss                 | 138        |
|    n_updates            | 9490       |
|    policy_gradient_loss | -0.00157   |
|    std                  | 6.17       |
|    value_loss           | 289        |
----------------------------------------
Iteration: 951 | Episodes: 38600 | Median Reward: 27.00 | Max Reward: 46.52
Iteration: 954 | Episodes: 38700 | Median Reward: 32.45 | Max Reward: 46.52
Iteration: 956 | Episodes: 38800 | Median Reward: 29.58 | Max Reward: 46.52
Iteration: 959 | Episodes: 38900 | Median Reward: 28.39 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.9         |
| time/                   |               |
|    fps                  | 277           |
|    iterations           | 960           |
|    time_elapsed         | 14191         |
|    total_timesteps      | 3932160       |
| train/                  |               |
|    approx_kl            | 3.7617792e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -128          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 130           |
|    n_updates            | 9590          |
|    policy_gradient_loss | -0.000103     |
|    std                  | 6.24          |
|    value_loss           | 273           |
-------------------------------------------
Iteration: 961 | Episodes: 39000 | Median Reward: 31.74 | Max Reward: 46.52
Iteration: 964 | Episodes: 39100 | Median Reward: 32.63 | Max Reward: 46.52
Iteration: 966 | Episodes: 39200 | Median Reward: 28.48 | Max Reward: 46.52
Iteration: 969 | Episodes: 39300 | Median Reward: 30.76 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.8        |
| time/                   |              |
|    fps                  | 279          |
|    iterations           | 970          |
|    time_elapsed         | 14231        |
|    total_timesteps      | 3973120      |
| train/                  |              |
|    approx_kl            | 0.0068260585 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 143          |
|    n_updates            | 9690         |
|    policy_gradient_loss | -0.00354     |
|    std                  | 6.43         |
|    value_loss           | 300          |
------------------------------------------
Iteration: 971 | Episodes: 39400 | Median Reward: 30.71 | Max Reward: 46.52
Iteration: 973 | Episodes: 39500 | Median Reward: 35.81 | Max Reward: 46.52
Iteration: 976 | Episodes: 39600 | Median Reward: 30.55 | Max Reward: 46.52
Iteration: 978 | Episodes: 39700 | Median Reward: 36.25 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.3         |
| time/                   |               |
|    fps                  | 281           |
|    iterations           | 980           |
|    time_elapsed         | 14271         |
|    total_timesteps      | 4014080       |
| train/                  |               |
|    approx_kl            | 0.00030081253 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -129          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 160           |
|    n_updates            | 9790          |
|    policy_gradient_loss | -0.000456     |
|    std                  | 6.58          |
|    value_loss           | 333           |
-------------------------------------------
Iteration: 981 | Episodes: 39800 | Median Reward: 31.38 | Max Reward: 46.52
Iteration: 983 | Episodes: 39900 | Median Reward: 25.95 | Max Reward: 46.52
Iteration: 986 | Episodes: 40000 | Median Reward: 28.87 | Max Reward: 46.52
Iteration: 988 | Episodes: 40100 | Median Reward: 24.91 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.9         |
| time/                   |               |
|    fps                  | 283           |
|    iterations           | 990           |
|    time_elapsed         | 14311         |
|    total_timesteps      | 4055040       |
| train/                  |               |
|    approx_kl            | 0.00031801927 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -130          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 130           |
|    n_updates            | 9890          |
|    policy_gradient_loss | -0.000693     |
|    std                  | 6.77          |
|    value_loss           | 272           |
-------------------------------------------
Iteration: 991 | Episodes: 40200 | Median Reward: 31.34 | Max Reward: 46.52
Iteration: 993 | Episodes: 40300 | Median Reward: 33.62 | Max Reward: 46.52
Iteration: 996 | Episodes: 40400 | Median Reward: 32.23 | Max Reward: 46.52
Iteration: 998 | Episodes: 40500 | Median Reward: 31.35 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.5         |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 1000          |
|    time_elapsed         | 14352         |
|    total_timesteps      | 4096000       |
| train/                  |               |
|    approx_kl            | 0.00012388143 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -130          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 126           |
|    n_updates            | 9990          |
|    policy_gradient_loss | 0.000203      |
|    std                  | 6.95          |
|    value_loss           | 265           |
-------------------------------------------
Iteration: 1001 | Episodes: 40600 | Median Reward: 32.71 | Max Reward: 46.52
Iteration: 1003 | Episodes: 40700 | Median Reward: 28.12 | Max Reward: 46.52
Iteration: 1006 | Episodes: 40800 | Median Reward: 26.73 | Max Reward: 46.52
Iteration: 1008 | Episodes: 40900 | Median Reward: 29.43 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.8         |
| time/                   |               |
|    fps                  | 287           |
|    iterations           | 1010          |
|    time_elapsed         | 14392         |
|    total_timesteps      | 4136960       |
| train/                  |               |
|    approx_kl            | 0.00020058878 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 131           |
|    n_updates            | 10090         |
|    policy_gradient_loss | -0.000312     |
|    std                  | 7.09          |
|    value_loss           | 275           |
-------------------------------------------
Iteration: 1010 | Episodes: 41000 | Median Reward: 27.06 | Max Reward: 46.52
Iteration: 1013 | Episodes: 41100 | Median Reward: 30.75 | Max Reward: 46.52
Iteration: 1015 | Episodes: 41200 | Median Reward: 32.12 | Max Reward: 46.52
Iteration: 1018 | Episodes: 41300 | Median Reward: 34.87 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -72.6         |
| time/                   |               |
|    fps                  | 289           |
|    iterations           | 1020          |
|    time_elapsed         | 14432         |
|    total_timesteps      | 4177920       |
| train/                  |               |
|    approx_kl            | 4.8789792e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 172           |
|    n_updates            | 10190         |
|    policy_gradient_loss | -0.000112     |
|    std                  | 7.21          |
|    value_loss           | 357           |
-------------------------------------------
Iteration: 1020 | Episodes: 41400 | Median Reward: 25.50 | Max Reward: 46.52
Iteration: 1023 | Episodes: 41500 | Median Reward: 30.69 | Max Reward: 46.52
Iteration: 1025 | Episodes: 41600 | Median Reward: 31.86 | Max Reward: 46.52
Iteration: 1028 | Episodes: 41700 | Median Reward: 27.11 | Max Reward: 46.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72.6        |
| time/                   |              |
|    fps                  | 291          |
|    iterations           | 1030         |
|    time_elapsed         | 14472        |
|    total_timesteps      | 4218880      |
| train/                  |              |
|    approx_kl            | 0.0006169571 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -132         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 141          |
|    n_updates            | 10290        |
|    policy_gradient_loss | -0.000362    |
|    std                  | 7.36         |
|    value_loss           | 294          |
------------------------------------------
Iteration: 1030 | Episodes: 41800 | Median Reward: 27.35 | Max Reward: 46.52
Iteration: 1033 | Episodes: 41900 | Median Reward: 34.46 | Max Reward: 46.52
Iteration: 1035 | Episodes: 42000 | Median Reward: 32.50 | Max Reward: 46.52
Iteration: 1038 | Episodes: 42100 | Median Reward: 30.80 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69           |
| time/                   |               |
|    fps                  | 293           |
|    iterations           | 1040          |
|    time_elapsed         | 14513         |
|    total_timesteps      | 4259840       |
| train/                  |               |
|    approx_kl            | 0.00048521964 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -132          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 151           |
|    n_updates            | 10390         |
|    policy_gradient_loss | -0.00114      |
|    std                  | 7.55          |
|    value_loss           | 315           |
-------------------------------------------
Iteration: 1040 | Episodes: 42200 | Median Reward: 32.29 | Max Reward: 46.52
Iteration: 1043 | Episodes: 42300 | Median Reward: 31.69 | Max Reward: 46.52
Iteration: 1045 | Episodes: 42400 | Median Reward: 30.99 | Max Reward: 46.52
Iteration: 1047 | Episodes: 42500 | Median Reward: 27.05 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.7         |
| time/                   |               |
|    fps                  | 295           |
|    iterations           | 1050          |
|    time_elapsed         | 14553         |
|    total_timesteps      | 4300800       |
| train/                  |               |
|    approx_kl            | 5.0327915e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 124           |
|    n_updates            | 10490         |
|    policy_gradient_loss | 1.76e-06      |
|    std                  | 7.76          |
|    value_loss           | 262           |
-------------------------------------------
Iteration: 1050 | Episodes: 42600 | Median Reward: 27.45 | Max Reward: 46.52
Iteration: 1052 | Episodes: 42700 | Median Reward: 28.99 | Max Reward: 46.52
Iteration: 1055 | Episodes: 42800 | Median Reward: 31.43 | Max Reward: 46.52
Iteration: 1057 | Episodes: 42900 | Median Reward: 28.38 | Max Reward: 46.52
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74.3       |
| time/                   |             |
|    fps                  | 297         |
|    iterations           | 1060        |
|    time_elapsed         | 14593       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 1.53242e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -133        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 120         |
|    n_updates            | 10590       |
|    policy_gradient_loss | 2.77e-05    |
|    std                  | 7.95        |
|    value_loss           | 253         |
-----------------------------------------
Iteration: 1060 | Episodes: 43000 | Median Reward: 25.78 | Max Reward: 46.52
Iteration: 1062 | Episodes: 43100 | Median Reward: 27.00 | Max Reward: 46.52
Iteration: 1065 | Episodes: 43200 | Median Reward: 25.74 | Max Reward: 46.52
Iteration: 1067 | Episodes: 43300 | Median Reward: 25.12 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.9         |
| time/                   |               |
|    fps                  | 299           |
|    iterations           | 1070          |
|    time_elapsed         | 14633         |
|    total_timesteps      | 4382720       |
| train/                  |               |
|    approx_kl            | 5.0148636e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 121           |
|    n_updates            | 10690         |
|    policy_gradient_loss | -7.36e-05     |
|    std                  | 8.14          |
|    value_loss           | 256           |
-------------------------------------------
Iteration: 1070 | Episodes: 43400 | Median Reward: 22.36 | Max Reward: 46.52
Iteration: 1072 | Episodes: 43500 | Median Reward: 27.73 | Max Reward: 46.52
Iteration: 1075 | Episodes: 43600 | Median Reward: 27.73 | Max Reward: 46.52
Iteration: 1077 | Episodes: 43700 | Median Reward: 27.16 | Max Reward: 46.52
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.1         |
| time/                   |               |
|    fps                  | 301           |
|    iterations           | 1080          |
|    time_elapsed         | 14673         |
|    total_timesteps      | 4423680       |
| train/                  |               |
|    approx_kl            | 7.6328506e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 154           |
|    n_updates            | 10790         |
|    policy_gradient_loss | -0.000301     |
|    std                  | 8.23          |
|    value_loss           | 321           |
-------------------------------------------
Iteration: 1080 | Episodes: 43800 | Median Reward: 32.04 | Max Reward: 46.52
Iteration: 1082 | Episodes: 43900 | Median Reward: 32.93 | Max Reward: 46.56
Iteration: 1084 | Episodes: 44000 | Median Reward: 31.91 | Max Reward: 46.56
Iteration: 1087 | Episodes: 44100 | Median Reward: 28.46 | Max Reward: 46.56
Iteration: 1089 | Episodes: 44200 | Median Reward: 24.95 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.6         |
| time/                   |               |
|    fps                  | 303           |
|    iterations           | 1090          |
|    time_elapsed         | 14713         |
|    total_timesteps      | 4464640       |
| train/                  |               |
|    approx_kl            | 4.2841653e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -135          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 120           |
|    n_updates            | 10890         |
|    policy_gradient_loss | -2.84e-05     |
|    std                  | 8.46          |
|    value_loss           | 254           |
-------------------------------------------
Iteration: 1092 | Episodes: 44300 | Median Reward: 30.87 | Max Reward: 46.56
Iteration: 1094 | Episodes: 44400 | Median Reward: 22.61 | Max Reward: 46.56
Iteration: 1097 | Episodes: 44500 | Median Reward: 26.58 | Max Reward: 46.56
Iteration: 1099 | Episodes: 44600 | Median Reward: 27.06 | Max Reward: 46.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.9       |
| time/                   |             |
|    fps                  | 305         |
|    iterations           | 1100        |
|    time_elapsed         | 14753       |
|    total_timesteps      | 4505600     |
| train/                  |             |
|    approx_kl            | 0.003680969 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -135        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 130         |
|    n_updates            | 10990       |
|    policy_gradient_loss | -0.00243    |
|    std                  | 8.63        |
|    value_loss           | 273         |
-----------------------------------------
Iteration: 1102 | Episodes: 44700 | Median Reward: 30.72 | Max Reward: 46.56
Iteration: 1104 | Episodes: 44800 | Median Reward: 26.98 | Max Reward: 46.56
Iteration: 1107 | Episodes: 44900 | Median Reward: 30.72 | Max Reward: 46.56
Iteration: 1109 | Episodes: 45000 | Median Reward: 31.63 | Max Reward: 46.56
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -69.9      |
| time/                   |            |
|    fps                  | 307        |
|    iterations           | 1110       |
|    time_elapsed         | 14793      |
|    total_timesteps      | 4546560    |
| train/                  |            |
|    approx_kl            | 0.00124101 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -136       |
|    explained_variance   | 1.79e-07   |
|    learning_rate        | 0.0005     |
|    loss                 | 144        |
|    n_updates            | 11090      |
|    policy_gradient_loss | -0.00132   |
|    std                  | 8.88       |
|    value_loss           | 302        |
----------------------------------------
Iteration: 1112 | Episodes: 45100 | Median Reward: 29.98 | Max Reward: 46.56
Iteration: 1114 | Episodes: 45200 | Median Reward: 35.14 | Max Reward: 46.56
Iteration: 1117 | Episodes: 45300 | Median Reward: 33.04 | Max Reward: 46.56
Iteration: 1119 | Episodes: 45400 | Median Reward: 26.43 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -72.8         |
| time/                   |               |
|    fps                  | 309           |
|    iterations           | 1120          |
|    time_elapsed         | 14834         |
|    total_timesteps      | 4587520       |
| train/                  |               |
|    approx_kl            | 1.4779376e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 120           |
|    n_updates            | 11190         |
|    policy_gradient_loss | 2.6e-05       |
|    std                  | 9.07          |
|    value_loss           | 253           |
-------------------------------------------
Iteration: 1121 | Episodes: 45500 | Median Reward: 27.43 | Max Reward: 46.56
Iteration: 1124 | Episodes: 45600 | Median Reward: 31.26 | Max Reward: 46.56
Iteration: 1126 | Episodes: 45700 | Median Reward: 28.61 | Max Reward: 46.56
Iteration: 1129 | Episodes: 45800 | Median Reward: 26.59 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -73          |
| time/                   |              |
|    fps                  | 311          |
|    iterations           | 1130         |
|    time_elapsed         | 14874        |
|    total_timesteps      | 4628480      |
| train/                  |              |
|    approx_kl            | 0.0010785969 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -137         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 132          |
|    n_updates            | 11290        |
|    policy_gradient_loss | -0.00137     |
|    std                  | 9.23         |
|    value_loss           | 278          |
------------------------------------------
Iteration: 1131 | Episodes: 45900 | Median Reward: 30.45 | Max Reward: 46.56
Iteration: 1134 | Episodes: 46000 | Median Reward: 28.61 | Max Reward: 46.56
Iteration: 1136 | Episodes: 46100 | Median Reward: 31.10 | Max Reward: 46.56
Iteration: 1139 | Episodes: 46200 | Median Reward: 25.03 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.4         |
| time/                   |               |
|    fps                  | 313           |
|    iterations           | 1140          |
|    time_elapsed         | 14914         |
|    total_timesteps      | 4669440       |
| train/                  |               |
|    approx_kl            | 0.00013431365 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -138          |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 119           |
|    n_updates            | 11390         |
|    policy_gradient_loss | 0.000723      |
|    std                  | 9.51          |
|    value_loss           | 252           |
-------------------------------------------
Iteration: 1141 | Episodes: 46300 | Median Reward: 27.38 | Max Reward: 46.56
Iteration: 1144 | Episodes: 46400 | Median Reward: 25.84 | Max Reward: 46.56
Iteration: 1146 | Episodes: 46500 | Median Reward: 26.72 | Max Reward: 46.56
Iteration: 1149 | Episodes: 46600 | Median Reward: 26.50 | Max Reward: 46.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74.4       |
| time/                   |             |
|    fps                  | 314         |
|    iterations           | 1150        |
|    time_elapsed         | 14954       |
|    total_timesteps      | 4710400     |
| train/                  |             |
|    approx_kl            | 0.054492455 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.4         |
|    entropy_loss         | -138        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 130         |
|    n_updates            | 11490       |
|    policy_gradient_loss | -0.00445    |
|    std                  | 9.8         |
|    value_loss           | 274         |
-----------------------------------------
Iteration: 1151 | Episodes: 46700 | Median Reward: 29.39 | Max Reward: 46.56
Iteration: 1154 | Episodes: 46800 | Median Reward: 31.90 | Max Reward: 46.56
Iteration: 1156 | Episodes: 46900 | Median Reward: 25.67 | Max Reward: 46.56
Iteration: 1158 | Episodes: 47000 | Median Reward: 25.94 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.6         |
| time/                   |               |
|    fps                  | 316           |
|    iterations           | 1160          |
|    time_elapsed         | 14994         |
|    total_timesteps      | 4751360       |
| train/                  |               |
|    approx_kl            | 0.00036925977 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -139          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 126           |
|    n_updates            | 11590         |
|    policy_gradient_loss | 0.000437      |
|    std                  | 10.1          |
|    value_loss           | 266           |
-------------------------------------------
Iteration: 1161 | Episodes: 47100 | Median Reward: 28.29 | Max Reward: 46.56
Iteration: 1163 | Episodes: 47200 | Median Reward: 27.92 | Max Reward: 46.56
Iteration: 1166 | Episodes: 47300 | Median Reward: 26.03 | Max Reward: 46.56
Iteration: 1168 | Episodes: 47400 | Median Reward: 34.98 | Max Reward: 46.56
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70           |
| time/                   |               |
|    fps                  | 318           |
|    iterations           | 1170          |
|    time_elapsed         | 15034         |
|    total_timesteps      | 4792320       |
| train/                  |               |
|    approx_kl            | 0.00013619718 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -139          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 159           |
|    n_updates            | 11690         |
|    policy_gradient_loss | -0.000692     |
|    std                  | 10.2          |
|    value_loss           | 331           |
-------------------------------------------
Iteration: 1171 | Episodes: 47500 | Median Reward: 31.46 | Max Reward: 46.56
Iteration: 1173 | Episodes: 47600 | Median Reward: 33.42 | Max Reward: 46.56
Iteration: 1176 | Episodes: 47700 | Median Reward: 22.00 | Max Reward: 46.56
Iteration: 1178 | Episodes: 47800 | Median Reward: 28.78 | Max Reward: 46.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -75.8       |
| time/                   |             |
|    fps                  | 320         |
|    iterations           | 1180        |
|    time_elapsed         | 15074       |
|    total_timesteps      | 4833280     |
| train/                  |             |
|    approx_kl            | 0.009965775 |
|    clip_fraction        | 0.00269     |
|    clip_range           | 0.4         |
|    entropy_loss         | -140        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 140         |
|    n_updates            | 11790       |
|    policy_gradient_loss | -0.00146    |
|    std                  | 10.5        |
|    value_loss           | 294         |
-----------------------------------------
Iteration: 1181 | Episodes: 47900 | Median Reward: 18.76 | Max Reward: 46.56
Iteration: 1183 | Episodes: 48000 | Median Reward: 26.89 | Max Reward: 46.56
Iteration: 1186 | Episodes: 48100 | Median Reward: 28.97 | Max Reward: 46.56
Iteration: 1188 | Episodes: 48200 | Median Reward: 25.50 | Max Reward: 46.56
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -75.6       |
| time/                   |             |
|    fps                  | 322         |
|    iterations           | 1190        |
|    time_elapsed         | 15114       |
|    total_timesteps      | 4874240     |
| train/                  |             |
|    approx_kl            | 0.006498259 |
|    clip_fraction        | 0.00391     |
|    clip_range           | 0.4         |
|    entropy_loss         | -140        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 126         |
|    n_updates            | 11890       |
|    policy_gradient_loss | -0.00194    |
|    std                  | 10.6        |
|    value_loss           | 265         |
-----------------------------------------
Iteration: 1190 | Episodes: 48300 | Median Reward: 24.58 | Max Reward: 46.56
Iteration: 1193 | Episodes: 48400 | Median Reward: 31.98 | Max Reward: 46.56
Iteration: 1195 | Episodes: 48500 | Median Reward: 30.65 | Max Reward: 46.56
Iteration: 1198 | Episodes: 48600 | Median Reward: 28.64 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78          |
| time/                   |              |
|    fps                  | 324          |
|    iterations           | 1200         |
|    time_elapsed         | 15154        |
|    total_timesteps      | 4915200      |
| train/                  |              |
|    approx_kl            | 0.0153643545 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.4          |
|    entropy_loss         | -140         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 97.2         |
|    n_updates            | 11990        |
|    policy_gradient_loss | -0.00327     |
|    std                  | 10.8         |
|    value_loss           | 209          |
------------------------------------------
Iteration: 1200 | Episodes: 48700 | Median Reward: 18.87 | Max Reward: 46.56
Iteration: 1203 | Episodes: 48800 | Median Reward: 24.22 | Max Reward: 46.56
Iteration: 1205 | Episodes: 48900 | Median Reward: 26.49 | Max Reward: 46.56
Iteration: 1208 | Episodes: 49000 | Median Reward: 25.02 | Max Reward: 46.56
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.5        |
| time/                   |              |
|    fps                  | 326          |
|    iterations           | 1210         |
|    time_elapsed         | 15195        |
|    total_timesteps      | 4956160      |
| train/                  |              |
|    approx_kl            | 0.0004239218 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -141         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 119          |
|    n_updates            | 12090        |
|    policy_gradient_loss | -0.000623    |
|    std                  | 11           |
|    value_loss           | 251          |
------------------------------------------
Iteration: 1210 | Episodes: 49100 | Median Reward: 23.64 | Max Reward: 46.56
Iteration: 1213 | Episodes: 49200 | Median Reward: 31.29 | Max Reward: 46.56
Iteration: 1215 | Episodes: 49300 | Median Reward: 44.27 | Max Reward: 46.56
Iteration: 1218 | Episodes: 49400 | Median Reward: 45.16 | Max Reward: 46.72
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -56       |
| time/                   |           |
|    fps                  | 328       |
|    iterations           | 1220      |
|    time_elapsed         | 15234     |
|    total_timesteps      | 4997120   |
| train/                  |           |
|    approx_kl            | 3.8733315 |
|    clip_fraction        | 0.551     |
|    clip_range           | 0.4       |
|    entropy_loss         | -138      |
|    explained_variance   | 0         |
|    learning_rate        | 0.0005    |
|    loss                 | 212       |
|    n_updates            | 12190     |
|    policy_gradient_loss | 0.232     |
|    std                  | 11.2      |
|    value_loss           | 438       |
---------------------------------------
Iteration: 1220 | Episodes: 49500 | Median Reward: 43.84 | Max Reward: 46.72
Iteration: 1223 | Episodes: 49600 | Median Reward: 43.88 | Max Reward: 46.72
Iteration: 1225 | Episodes: 49700 | Median Reward: 44.71 | Max Reward: 46.72
Iteration: 1227 | Episodes: 49800 | Median Reward: 43.26 | Max Reward: 46.72
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 101      |
|    ep_rew_mean          | -55.8    |
| time/                   |          |
|    fps                  | 329      |
|    iterations           | 1230     |
|    time_elapsed         | 15274    |
|    total_timesteps      | 5038080  |
| train/                  |          |
|    approx_kl            | 5.550544 |
|    clip_fraction        | 0.383    |
|    clip_range           | 0.4      |
|    entropy_loss         | -139     |
|    explained_variance   | 0        |
|    learning_rate        | 0.0005   |
|    loss                 | 194      |
|    n_updates            | 12290    |
|    policy_gradient_loss | 0.238    |
|    std                  | 11.2     |
|    value_loss           | 401      |
--------------------------------------
Iteration: 1230 | Episodes: 49900 | Median Reward: 44.89 | Max Reward: 46.94
Iteration: 1232 | Episodes: 50000 | Median Reward: 43.30 | Max Reward: 46.94
Iteration: 1235 | Episodes: 50100 | Median Reward: 43.16 | Max Reward: 46.94
Iteration: 1237 | Episodes: 50200 | Median Reward: 25.18 | Max Reward: 46.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -106        |
| time/                   |             |
|    fps                  | 331         |
|    iterations           | 1240        |
|    time_elapsed         | 15314       |
|    total_timesteps      | 5079040     |
| train/                  |             |
|    approx_kl            | 0.038109362 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.4         |
|    entropy_loss         | -140        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 52.7        |
|    n_updates            | 12390       |
|    policy_gradient_loss | 0.00355     |
|    std                  | 11.3        |
|    value_loss           | 121         |
-----------------------------------------
Iteration: 1240 | Episodes: 50300 | Median Reward: -9.04 | Max Reward: 46.94
Iteration: 1242 | Episodes: 50400 | Median Reward: -2.56 | Max Reward: 46.94
Iteration: 1245 | Episodes: 50500 | Median Reward: -8.27 | Max Reward: 46.94
Iteration: 1247 | Episodes: 50600 | Median Reward: -6.04 | Max Reward: 46.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -111        |
| time/                   |             |
|    fps                  | 333         |
|    iterations           | 1250        |
|    time_elapsed         | 15354       |
|    total_timesteps      | 5120000     |
| train/                  |             |
|    approx_kl            | 0.004517904 |
|    clip_fraction        | 0.0244      |
|    clip_range           | 0.4         |
|    entropy_loss         | -140        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 42.5        |
|    n_updates            | 12490       |
|    policy_gradient_loss | -0.00517    |
|    std                  | 11.3        |
|    value_loss           | 102         |
-----------------------------------------
Iteration: 1250 | Episodes: 50700 | Median Reward: -10.26 | Max Reward: 46.94
Iteration: 1252 | Episodes: 50800 | Median Reward: -5.40 | Max Reward: 46.94
Iteration: 1255 | Episodes: 50900 | Median Reward: 19.14 | Max Reward: 46.94
Iteration: 1257 | Episodes: 51000 | Median Reward: 9.90 | Max Reward: 46.94
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -112         |
| time/                   |              |
|    fps                  | 335          |
|    iterations           | 1260         |
|    time_elapsed         | 15394        |
|    total_timesteps      | 5160960      |
| train/                  |              |
|    approx_kl            | 0.0029516811 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.4          |
|    entropy_loss         | -141         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 42.6         |
|    n_updates            | 12590        |
|    policy_gradient_loss | 0.00115      |
|    std                  | 11.3         |
|    value_loss           | 101          |
------------------------------------------
Iteration: 1260 | Episodes: 51100 | Median Reward: -19.40 | Max Reward: 46.94
Iteration: 1262 | Episodes: 51200 | Median Reward: -24.74 | Max Reward: 46.94
Iteration: 1264 | Episodes: 51300 | Median Reward: -23.33 | Max Reward: 46.94
Iteration: 1267 | Episodes: 51400 | Median Reward: -16.33 | Max Reward: 46.94
Iteration: 1269 | Episodes: 51500 | Median Reward: -17.76 | Max Reward: 46.94
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -114         |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 1270         |
|    time_elapsed         | 15434        |
|    total_timesteps      | 5201920      |
| train/                  |              |
|    approx_kl            | 0.0011665046 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.4          |
|    entropy_loss         | -141         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 47           |
|    n_updates            | 12690        |
|    policy_gradient_loss | -0.000943    |
|    std                  | 11.4         |
|    value_loss           | 110          |
------------------------------------------
Iteration: 1272 | Episodes: 51600 | Median Reward: -11.96 | Max Reward: 46.94
Iteration: 1274 | Episodes: 51700 | Median Reward: -17.24 | Max Reward: 46.94
Iteration: 1277 | Episodes: 51800 | Median Reward: -15.70 | Max Reward: 46.94
Iteration: 1279 | Episodes: 51900 | Median Reward: -11.89 | Max Reward: 46.94
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -112         |
| time/                   |              |
|    fps                  | 338          |
|    iterations           | 1280         |
|    time_elapsed         | 15474        |
|    total_timesteps      | 5242880      |
| train/                  |              |
|    approx_kl            | 0.0077555114 |
|    clip_fraction        | 0.00952      |
|    clip_range           | 0.4          |
|    entropy_loss         | -141         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 39.5         |
|    n_updates            | 12790        |
|    policy_gradient_loss | 0.00175      |
|    std                  | 11.4         |
|    value_loss           | 94.5         |
------------------------------------------
Iteration: 1282 | Episodes: 52000 | Median Reward: -16.65 | Max Reward: 46.94
Iteration: 1283 | Episodes: 52100 | Median Reward: 48.13 | Max Reward: 48.13
Iteration: 1284 | Episodes: 52200 | Median Reward: -3.49 | Max Reward: 48.13
Iteration: 1287 | Episodes: 52300 | Median Reward: -12.89 | Max Reward: 48.13
Iteration: 1289 | Episodes: 52400 | Median Reward: -10.81 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -110        |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 1290        |
|    time_elapsed         | 15514       |
|    total_timesteps      | 5283840     |
| train/                  |             |
|    approx_kl            | 0.011482852 |
|    clip_fraction        | 0.0073      |
|    clip_range           | 0.4         |
|    entropy_loss         | -141        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 44.4        |
|    n_updates            | 12890       |
|    policy_gradient_loss | -0.00202    |
|    std                  | 11.4        |
|    value_loss           | 104         |
-----------------------------------------
Iteration: 1292 | Episodes: 52500 | Median Reward: -19.48 | Max Reward: 48.13
Iteration: 1294 | Episodes: 52600 | Median Reward: -21.82 | Max Reward: 48.13
Iteration: 1297 | Episodes: 52700 | Median Reward: -7.87 | Max Reward: 48.13
Iteration: 1299 | Episodes: 52800 | Median Reward: -15.79 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -122        |
| time/                   |             |
|    fps                  | 342         |
|    iterations           | 1300        |
|    time_elapsed         | 15554       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.005570326 |
|    clip_fraction        | 0.00537     |
|    clip_range           | 0.4         |
|    entropy_loss         | -142        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 36.4        |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.00312    |
|    std                  | 11.4        |
|    value_loss           | 88.2        |
-----------------------------------------
Iteration: 1302 | Episodes: 52900 | Median Reward: -14.28 | Max Reward: 48.13
Iteration: 1304 | Episodes: 53000 | Median Reward: -7.87 | Max Reward: 48.13
Iteration: 1307 | Episodes: 53100 | Median Reward: -22.26 | Max Reward: 48.13
Iteration: 1309 | Episodes: 53200 | Median Reward: -11.02 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -111          |
| time/                   |               |
|    fps                  | 344           |
|    iterations           | 1310          |
|    time_elapsed         | 15594         |
|    total_timesteps      | 5365760       |
| train/                  |               |
|    approx_kl            | 0.00045537174 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.4           |
|    entropy_loss         | -142          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 56.1          |
|    n_updates            | 13090         |
|    policy_gradient_loss | 0.000345      |
|    std                  | 11.5          |
|    value_loss           | 127           |
-------------------------------------------
Iteration: 1312 | Episodes: 53300 | Median Reward: -13.01 | Max Reward: 48.13
Iteration: 1314 | Episodes: 53400 | Median Reward: -12.44 | Max Reward: 48.13
Iteration: 1317 | Episodes: 53500 | Median Reward: -15.57 | Max Reward: 48.13
Iteration: 1319 | Episodes: 53600 | Median Reward: -9.76 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -114         |
| time/                   |              |
|    fps                  | 345          |
|    iterations           | 1320         |
|    time_elapsed         | 15634        |
|    total_timesteps      | 5406720      |
| train/                  |              |
|    approx_kl            | 0.0012045279 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -142         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 56.7         |
|    n_updates            | 13190        |
|    policy_gradient_loss | -0.00117     |
|    std                  | 11.6         |
|    value_loss           | 128          |
------------------------------------------
Iteration: 1321 | Episodes: 53700 | Median Reward: -6.22 | Max Reward: 48.13
Iteration: 1324 | Episodes: 53800 | Median Reward: -16.36 | Max Reward: 48.13
Iteration: 1326 | Episodes: 53900 | Median Reward: -3.63 | Max Reward: 48.13
Iteration: 1329 | Episodes: 54000 | Median Reward: -10.29 | Max Reward: 48.13
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -114       |
| time/                   |            |
|    fps                  | 347        |
|    iterations           | 1330       |
|    time_elapsed         | 15675      |
|    total_timesteps      | 5447680    |
| train/                  |            |
|    approx_kl            | 0.07314354 |
|    clip_fraction        | 0.0172     |
|    clip_range           | 0.4        |
|    entropy_loss         | -142       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0005     |
|    loss                 | 57.7       |
|    n_updates            | 13290      |
|    policy_gradient_loss | -0.00596   |
|    std                  | 11.7       |
|    value_loss           | 130        |
----------------------------------------
Iteration: 1331 | Episodes: 54100 | Median Reward: -3.33 | Max Reward: 48.13
Iteration: 1334 | Episodes: 54200 | Median Reward: -9.85 | Max Reward: 48.13
Iteration: 1336 | Episodes: 54300 | Median Reward: -1.68 | Max Reward: 48.13
Iteration: 1339 | Episodes: 54400 | Median Reward: -6.02 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -109        |
| time/                   |             |
|    fps                  | 349         |
|    iterations           | 1340        |
|    time_elapsed         | 15715       |
|    total_timesteps      | 5488640     |
| train/                  |             |
|    approx_kl            | 0.009010766 |
|    clip_fraction        | 0.00415     |
|    clip_range           | 0.4         |
|    entropy_loss         | -143        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 56.8        |
|    n_updates            | 13390       |
|    policy_gradient_loss | -0.00196    |
|    std                  | 12          |
|    value_loss           | 128         |
-----------------------------------------
Iteration: 1341 | Episodes: 54500 | Median Reward: -4.80 | Max Reward: 48.13
Iteration: 1344 | Episodes: 54600 | Median Reward: -4.38 | Max Reward: 48.13
Iteration: 1346 | Episodes: 54700 | Median Reward: -5.15 | Max Reward: 48.13
Iteration: 1349 | Episodes: 54800 | Median Reward: -11.24 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -114          |
| time/                   |               |
|    fps                  | 350           |
|    iterations           | 1350          |
|    time_elapsed         | 15755         |
|    total_timesteps      | 5529600       |
| train/                  |               |
|    approx_kl            | 0.00012469557 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -144          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 57.8          |
|    n_updates            | 13490         |
|    policy_gradient_loss | -0.000808     |
|    std                  | 12.3          |
|    value_loss           | 130           |
-------------------------------------------
Iteration: 1351 | Episodes: 54900 | Median Reward: -13.73 | Max Reward: 48.13
Iteration: 1354 | Episodes: 55000 | Median Reward: -10.64 | Max Reward: 48.13
Iteration: 1356 | Episodes: 55100 | Median Reward: 1.16 | Max Reward: 48.13
Iteration: 1358 | Episodes: 55200 | Median Reward: -2.95 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -111         |
| time/                   |              |
|    fps                  | 352          |
|    iterations           | 1360         |
|    time_elapsed         | 15795        |
|    total_timesteps      | 5570560      |
| train/                  |              |
|    approx_kl            | 0.0094369855 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 67.2         |
|    n_updates            | 13590        |
|    policy_gradient_loss | -0.00559     |
|    std                  | 12.6         |
|    value_loss           | 149          |
------------------------------------------
Iteration: 1361 | Episodes: 55300 | Median Reward: -6.77 | Max Reward: 48.13
Iteration: 1363 | Episodes: 55400 | Median Reward: 10.47 | Max Reward: 48.13
Iteration: 1366 | Episodes: 55500 | Median Reward: -4.48 | Max Reward: 48.13
Iteration: 1368 | Episodes: 55600 | Median Reward: 0.21 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -101         |
| time/                   |              |
|    fps                  | 354          |
|    iterations           | 1370         |
|    time_elapsed         | 15836        |
|    total_timesteps      | 5611520      |
| train/                  |              |
|    approx_kl            | 0.0014840289 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -143         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 97.7         |
|    n_updates            | 13690        |
|    policy_gradient_loss | -0.00171     |
|    std                  | 13           |
|    value_loss           | 210          |
------------------------------------------
Iteration: 1371 | Episodes: 55700 | Median Reward: -0.71 | Max Reward: 48.13
Iteration: 1373 | Episodes: 55800 | Median Reward: -2.26 | Max Reward: 48.13
Iteration: 1376 | Episodes: 55900 | Median Reward: 1.60 | Max Reward: 48.13
Iteration: 1378 | Episodes: 56000 | Median Reward: 17.74 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -90.4       |
| time/                   |             |
|    fps                  | 356         |
|    iterations           | 1380        |
|    time_elapsed         | 15876       |
|    total_timesteps      | 5652480     |
| train/                  |             |
|    approx_kl            | 0.014222081 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -144        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 123         |
|    n_updates            | 13790       |
|    policy_gradient_loss | -0.00887    |
|    std                  | 13.1        |
|    value_loss           | 260         |
-----------------------------------------
Iteration: 1381 | Episodes: 56100 | Median Reward: 9.24 | Max Reward: 48.13
Iteration: 1383 | Episodes: 56200 | Median Reward: 18.45 | Max Reward: 48.13
Iteration: 1386 | Episodes: 56300 | Median Reward: 11.35 | Max Reward: 48.13
Iteration: 1388 | Episodes: 56400 | Median Reward: 19.23 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.9        |
| time/                   |              |
|    fps                  | 357          |
|    iterations           | 1390         |
|    time_elapsed         | 15917        |
|    total_timesteps      | 5693440      |
| train/                  |              |
|    approx_kl            | 0.0014948996 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 151          |
|    n_updates            | 13890        |
|    policy_gradient_loss | -0.00112     |
|    std                  | 13.3         |
|    value_loss           | 318          |
------------------------------------------
Iteration: 1391 | Episodes: 56500 | Median Reward: 13.81 | Max Reward: 48.13
Iteration: 1393 | Episodes: 56600 | Median Reward: 18.73 | Max Reward: 48.13
Iteration: 1395 | Episodes: 56700 | Median Reward: 22.10 | Max Reward: 48.13
Iteration: 1398 | Episodes: 56800 | Median Reward: 26.87 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.6        |
| time/                   |              |
|    fps                  | 359          |
|    iterations           | 1400         |
|    time_elapsed         | 15957        |
|    total_timesteps      | 5734400      |
| train/                  |              |
|    approx_kl            | 9.492623e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -145         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 141          |
|    n_updates            | 13990        |
|    policy_gradient_loss | 1.6e-05      |
|    std                  | 13.4         |
|    value_loss           | 298          |
------------------------------------------
Iteration: 1400 | Episodes: 56900 | Median Reward: 14.54 | Max Reward: 48.13
Iteration: 1403 | Episodes: 57000 | Median Reward: 21.54 | Max Reward: 48.13
Iteration: 1405 | Episodes: 57100 | Median Reward: 25.60 | Max Reward: 48.13
Iteration: 1408 | Episodes: 57200 | Median Reward: 25.05 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.1       |
| time/                   |             |
|    fps                  | 361         |
|    iterations           | 1410        |
|    time_elapsed         | 15997       |
|    total_timesteps      | 5775360     |
| train/                  |             |
|    approx_kl            | 1.86892e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -146        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 190         |
|    n_updates            | 14090       |
|    policy_gradient_loss | -0.000104   |
|    std                  | 13.5        |
|    value_loss           | 396         |
-----------------------------------------
Iteration: 1410 | Episodes: 57300 | Median Reward: 26.95 | Max Reward: 48.13
Iteration: 1413 | Episodes: 57400 | Median Reward: 27.10 | Max Reward: 48.13
Iteration: 1415 | Episodes: 57500 | Median Reward: 20.77 | Max Reward: 48.13
Iteration: 1418 | Episodes: 57600 | Median Reward: 26.89 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.9        |
| time/                   |              |
|    fps                  | 362          |
|    iterations           | 1420         |
|    time_elapsed         | 16037        |
|    total_timesteps      | 5816320      |
| train/                  |              |
|    approx_kl            | 0.0008154579 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -146         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 116          |
|    n_updates            | 14190        |
|    policy_gradient_loss | -0.00111     |
|    std                  | 13.7         |
|    value_loss           | 247          |
------------------------------------------
Iteration: 1420 | Episodes: 57700 | Median Reward: 13.41 | Max Reward: 48.13
Iteration: 1423 | Episodes: 57800 | Median Reward: 23.03 | Max Reward: 48.13
Iteration: 1425 | Episodes: 57900 | Median Reward: 17.47 | Max Reward: 48.13
Iteration: 1428 | Episodes: 58000 | Median Reward: 21.05 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.1         |
| time/                   |               |
|    fps                  | 364           |
|    iterations           | 1430          |
|    time_elapsed         | 16077         |
|    total_timesteps      | 5857280       |
| train/                  |               |
|    approx_kl            | 0.00019862207 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -147          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 104           |
|    n_updates            | 14290         |
|    policy_gradient_loss | 0.000385      |
|    std                  | 14.1          |
|    value_loss           | 223           |
-------------------------------------------
Iteration: 1430 | Episodes: 58100 | Median Reward: 16.81 | Max Reward: 48.13
Iteration: 1432 | Episodes: 58200 | Median Reward: 19.10 | Max Reward: 48.13
Iteration: 1435 | Episodes: 58300 | Median Reward: 21.73 | Max Reward: 48.13
Iteration: 1437 | Episodes: 58400 | Median Reward: 23.50 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.1        |
| time/                   |              |
|    fps                  | 365          |
|    iterations           | 1440         |
|    time_elapsed         | 16118        |
|    total_timesteps      | 5898240      |
| train/                  |              |
|    approx_kl            | 0.0022939565 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -148         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 122          |
|    n_updates            | 14390        |
|    policy_gradient_loss | -0.00168     |
|    std                  | 14.4         |
|    value_loss           | 258          |
------------------------------------------
Iteration: 1440 | Episodes: 58500 | Median Reward: 25.03 | Max Reward: 48.13
Iteration: 1442 | Episodes: 58600 | Median Reward: 17.61 | Max Reward: 48.13
Iteration: 1445 | Episodes: 58700 | Median Reward: 22.80 | Max Reward: 48.13
Iteration: 1447 | Episodes: 58800 | Median Reward: 22.69 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.5        |
| time/                   |              |
|    fps                  | 367          |
|    iterations           | 1450         |
|    time_elapsed         | 16158        |
|    total_timesteps      | 5939200      |
| train/                  |              |
|    approx_kl            | 6.749958e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -148         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 121          |
|    n_updates            | 14490        |
|    policy_gradient_loss | 7.61e-06     |
|    std                  | 14.7         |
|    value_loss           | 257          |
------------------------------------------
Iteration: 1450 | Episodes: 58900 | Median Reward: 24.03 | Max Reward: 48.13
Iteration: 1452 | Episodes: 59000 | Median Reward: 25.89 | Max Reward: 48.13
Iteration: 1455 | Episodes: 59100 | Median Reward: 24.90 | Max Reward: 48.13
Iteration: 1457 | Episodes: 59200 | Median Reward: 21.85 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.6        |
| time/                   |              |
|    fps                  | 369          |
|    iterations           | 1460         |
|    time_elapsed         | 16198        |
|    total_timesteps      | 5980160      |
| train/                  |              |
|    approx_kl            | 0.0010962146 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -149         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 117          |
|    n_updates            | 14590        |
|    policy_gradient_loss | -0.00156     |
|    std                  | 15           |
|    value_loss           | 249          |
------------------------------------------
Iteration: 1460 | Episodes: 59300 | Median Reward: 22.80 | Max Reward: 48.13
Iteration: 1462 | Episodes: 59400 | Median Reward: 24.11 | Max Reward: 48.13
Iteration: 1464 | Episodes: 59500 | Median Reward: 20.20 | Max Reward: 48.13
Iteration: 1467 | Episodes: 59600 | Median Reward: 23.12 | Max Reward: 48.13
Iteration: 1469 | Episodes: 59700 | Median Reward: 25.51 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.2         |
| time/                   |               |
|    fps                  | 370           |
|    iterations           | 1470          |
|    time_elapsed         | 16238         |
|    total_timesteps      | 6021120       |
| train/                  |               |
|    approx_kl            | 5.4093834e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -149          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 148           |
|    n_updates            | 14690         |
|    policy_gradient_loss | -4.05e-05     |
|    std                  | 15.4          |
|    value_loss           | 310           |
-------------------------------------------
Iteration: 1472 | Episodes: 59800 | Median Reward: 24.69 | Max Reward: 48.13
Iteration: 1474 | Episodes: 59900 | Median Reward: 19.33 | Max Reward: 48.13
Iteration: 1477 | Episodes: 60000 | Median Reward: 19.86 | Max Reward: 48.13
Iteration: 1479 | Episodes: 60100 | Median Reward: 24.98 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -79.9       |
| time/                   |             |
|    fps                  | 372         |
|    iterations           | 1480        |
|    time_elapsed         | 16279       |
|    total_timesteps      | 6062080     |
| train/                  |             |
|    approx_kl            | 0.000629851 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -150        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 129         |
|    n_updates            | 14790       |
|    policy_gradient_loss | -0.00129    |
|    std                  | 15.8        |
|    value_loss           | 273         |
-----------------------------------------
Iteration: 1482 | Episodes: 60200 | Median Reward: 24.33 | Max Reward: 48.13
Iteration: 1484 | Episodes: 60300 | Median Reward: 21.89 | Max Reward: 48.13
Iteration: 1487 | Episodes: 60400 | Median Reward: 24.08 | Max Reward: 48.13
Iteration: 1489 | Episodes: 60500 | Median Reward: 18.84 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.5       |
| time/                   |             |
|    fps                  | 373         |
|    iterations           | 1490        |
|    time_elapsed         | 16319       |
|    total_timesteps      | 6103040     |
| train/                  |             |
|    approx_kl            | 0.005457051 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -150        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 124         |
|    n_updates            | 14890       |
|    policy_gradient_loss | -0.00438    |
|    std                  | 16.2        |
|    value_loss           | 262         |
-----------------------------------------
Iteration: 1492 | Episodes: 60600 | Median Reward: 26.79 | Max Reward: 48.13
Iteration: 1494 | Episodes: 60700 | Median Reward: 22.83 | Max Reward: 48.13
Iteration: 1497 | Episodes: 60800 | Median Reward: 21.66 | Max Reward: 48.13
Iteration: 1499 | Episodes: 60900 | Median Reward: 15.02 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.5         |
| time/                   |               |
|    fps                  | 375           |
|    iterations           | 1500          |
|    time_elapsed         | 16359         |
|    total_timesteps      | 6144000       |
| train/                  |               |
|    approx_kl            | 0.00034516296 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -151          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 88.6          |
|    n_updates            | 14990         |
|    policy_gradient_loss | -0.000543     |
|    std                  | 16.8          |
|    value_loss           | 193           |
-------------------------------------------
Iteration: 1501 | Episodes: 61000 | Median Reward: 21.41 | Max Reward: 48.13
Iteration: 1504 | Episodes: 61100 | Median Reward: 25.15 | Max Reward: 48.13
Iteration: 1506 | Episodes: 61200 | Median Reward: 23.22 | Max Reward: 48.13
Iteration: 1509 | Episodes: 61300 | Median Reward: 22.53 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -79.4         |
| time/                   |               |
|    fps                  | 377           |
|    iterations           | 1510          |
|    time_elapsed         | 16400         |
|    total_timesteps      | 6184960       |
| train/                  |               |
|    approx_kl            | 0.00027034996 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 126           |
|    n_updates            | 15090         |
|    policy_gradient_loss | -0.000424     |
|    std                  | 17.1          |
|    value_loss           | 267           |
-------------------------------------------
Iteration: 1511 | Episodes: 61400 | Median Reward: 16.13 | Max Reward: 48.13
Iteration: 1514 | Episodes: 61500 | Median Reward: 15.14 | Max Reward: 48.13
Iteration: 1516 | Episodes: 61600 | Median Reward: 23.14 | Max Reward: 48.13
Iteration: 1519 | Episodes: 61700 | Median Reward: 27.31 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.3        |
| time/                   |              |
|    fps                  | 378          |
|    iterations           | 1520         |
|    time_elapsed         | 16440        |
|    total_timesteps      | 6225920      |
| train/                  |              |
|    approx_kl            | 0.0006923316 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -152         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 134          |
|    n_updates            | 15190        |
|    policy_gradient_loss | -0.00132     |
|    std                  | 17.4         |
|    value_loss           | 283          |
------------------------------------------
Iteration: 1521 | Episodes: 61800 | Median Reward: 21.62 | Max Reward: 48.13
Iteration: 1524 | Episodes: 61900 | Median Reward: 30.99 | Max Reward: 48.13
Iteration: 1526 | Episodes: 62000 | Median Reward: 7.30 | Max Reward: 48.13
Iteration: 1529 | Episodes: 62100 | Median Reward: 11.13 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.7         |
| time/                   |               |
|    fps                  | 380           |
|    iterations           | 1530          |
|    time_elapsed         | 16480         |
|    total_timesteps      | 6266880       |
| train/                  |               |
|    approx_kl            | 0.00038101827 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 119           |
|    n_updates            | 15290         |
|    policy_gradient_loss | -0.00123      |
|    std                  | 17.5          |
|    value_loss           | 254           |
-------------------------------------------
Iteration: 1531 | Episodes: 62200 | Median Reward: 14.66 | Max Reward: 48.13
Iteration: 1534 | Episodes: 62300 | Median Reward: 15.46 | Max Reward: 48.13
Iteration: 1536 | Episodes: 62400 | Median Reward: 22.92 | Max Reward: 48.13
Iteration: 1538 | Episodes: 62500 | Median Reward: 10.76 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80          |
| time/                   |              |
|    fps                  | 381          |
|    iterations           | 1540         |
|    time_elapsed         | 16520        |
|    total_timesteps      | 6307840      |
| train/                  |              |
|    approx_kl            | 0.0001453213 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -153         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 131          |
|    n_updates            | 15390        |
|    policy_gradient_loss | -0.000697    |
|    std                  | 17.9         |
|    value_loss           | 278          |
------------------------------------------
Iteration: 1541 | Episodes: 62600 | Median Reward: 20.63 | Max Reward: 48.13
Iteration: 1543 | Episodes: 62700 | Median Reward: 21.24 | Max Reward: 48.13
Iteration: 1546 | Episodes: 62800 | Median Reward: 28.15 | Max Reward: 48.13
Iteration: 1548 | Episodes: 62900 | Median Reward: 25.19 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.5         |
| time/                   |               |
|    fps                  | 383           |
|    iterations           | 1550          |
|    time_elapsed         | 16561         |
|    total_timesteps      | 6348800       |
| train/                  |               |
|    approx_kl            | 0.00046364177 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -153          |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 117           |
|    n_updates            | 15490         |
|    policy_gradient_loss | -0.000728     |
|    std                  | 18.1          |
|    value_loss           | 249           |
-------------------------------------------
Iteration: 1551 | Episodes: 63000 | Median Reward: 22.63 | Max Reward: 48.13
Iteration: 1553 | Episodes: 63100 | Median Reward: 21.54 | Max Reward: 48.13
Iteration: 1556 | Episodes: 63200 | Median Reward: 30.07 | Max Reward: 48.13
Iteration: 1558 | Episodes: 63300 | Median Reward: 25.50 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -79.5       |
| time/                   |             |
|    fps                  | 384         |
|    iterations           | 1560        |
|    time_elapsed         | 16601       |
|    total_timesteps      | 6389760     |
| train/                  |             |
|    approx_kl            | 0.001361667 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -153        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 130         |
|    n_updates            | 15590       |
|    policy_gradient_loss | -0.00109    |
|    std                  | 18.3        |
|    value_loss           | 276         |
-----------------------------------------
Iteration: 1561 | Episodes: 63400 | Median Reward: 22.26 | Max Reward: 48.13
Iteration: 1563 | Episodes: 63500 | Median Reward: 22.03 | Max Reward: 48.13
Iteration: 1566 | Episodes: 63600 | Median Reward: 17.87 | Max Reward: 48.13
Iteration: 1568 | Episodes: 63700 | Median Reward: 18.36 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.7         |
| time/                   |               |
|    fps                  | 386           |
|    iterations           | 1570          |
|    time_elapsed         | 16641         |
|    total_timesteps      | 6430720       |
| train/                  |               |
|    approx_kl            | 0.00023228033 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -154          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 129           |
|    n_updates            | 15690         |
|    policy_gradient_loss | -0.00094      |
|    std                  | 18.6          |
|    value_loss           | 274           |
-------------------------------------------
Iteration: 1571 | Episodes: 63800 | Median Reward: 19.57 | Max Reward: 48.13
Iteration: 1573 | Episodes: 63900 | Median Reward: 15.54 | Max Reward: 48.13
Iteration: 1575 | Episodes: 64000 | Median Reward: 26.04 | Max Reward: 48.13
Iteration: 1578 | Episodes: 64100 | Median Reward: 16.52 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.4        |
| time/                   |              |
|    fps                  | 387          |
|    iterations           | 1580         |
|    time_elapsed         | 16681        |
|    total_timesteps      | 6471680      |
| train/                  |              |
|    approx_kl            | 6.321893e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -154         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 113          |
|    n_updates            | 15790        |
|    policy_gradient_loss | -0.000202    |
|    std                  | 19           |
|    value_loss           | 242          |
------------------------------------------
Iteration: 1580 | Episodes: 64200 | Median Reward: 21.75 | Max Reward: 48.13
Iteration: 1583 | Episodes: 64300 | Median Reward: 21.43 | Max Reward: 48.13
Iteration: 1585 | Episodes: 64400 | Median Reward: 27.44 | Max Reward: 48.13
Iteration: 1588 | Episodes: 64500 | Median Reward: 19.84 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.6        |
| time/                   |              |
|    fps                  | 389          |
|    iterations           | 1590         |
|    time_elapsed         | 16722        |
|    total_timesteps      | 6512640      |
| train/                  |              |
|    approx_kl            | 0.0005322029 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -155         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 133          |
|    n_updates            | 15890        |
|    policy_gradient_loss | -0.00145     |
|    std                  | 19.4         |
|    value_loss           | 281          |
------------------------------------------
Iteration: 1590 | Episodes: 64600 | Median Reward: 27.60 | Max Reward: 48.13
Iteration: 1593 | Episodes: 64700 | Median Reward: 14.93 | Max Reward: 48.13
Iteration: 1595 | Episodes: 64800 | Median Reward: 17.21 | Max Reward: 48.13
Iteration: 1598 | Episodes: 64900 | Median Reward: 20.33 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.9         |
| time/                   |               |
|    fps                  | 390           |
|    iterations           | 1600          |
|    time_elapsed         | 16762         |
|    total_timesteps      | 6553600       |
| train/                  |               |
|    approx_kl            | 5.6807112e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 139           |
|    n_updates            | 15990         |
|    policy_gradient_loss | -0.000112     |
|    std                  | 19.7          |
|    value_loss           | 293           |
-------------------------------------------
Iteration: 1600 | Episodes: 65000 | Median Reward: 20.20 | Max Reward: 48.13
Iteration: 1603 | Episodes: 65100 | Median Reward: 30.13 | Max Reward: 48.13
Iteration: 1605 | Episodes: 65200 | Median Reward: 26.95 | Max Reward: 48.13
Iteration: 1608 | Episodes: 65300 | Median Reward: 23.40 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.7        |
| time/                   |              |
|    fps                  | 392          |
|    iterations           | 1610         |
|    time_elapsed         | 16802        |
|    total_timesteps      | 6594560      |
| train/                  |              |
|    approx_kl            | 4.008376e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -155         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 102          |
|    n_updates            | 16090        |
|    policy_gradient_loss | -0.000226    |
|    std                  | 20           |
|    value_loss           | 220          |
------------------------------------------
Iteration: 1610 | Episodes: 65400 | Median Reward: 12.24 | Max Reward: 48.13
Iteration: 1612 | Episodes: 65500 | Median Reward: 20.09 | Max Reward: 48.13
Iteration: 1615 | Episodes: 65600 | Median Reward: 19.38 | Max Reward: 48.13
Iteration: 1617 | Episodes: 65700 | Median Reward: 16.06 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.9         |
| time/                   |               |
|    fps                  | 393           |
|    iterations           | 1620          |
|    time_elapsed         | 16842         |
|    total_timesteps      | 6635520       |
| train/                  |               |
|    approx_kl            | 0.00023852571 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -156          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 123           |
|    n_updates            | 16190         |
|    policy_gradient_loss | -0.000585     |
|    std                  | 20.5          |
|    value_loss           | 261           |
-------------------------------------------
Iteration: 1620 | Episodes: 65800 | Median Reward: 18.26 | Max Reward: 48.13
Iteration: 1622 | Episodes: 65900 | Median Reward: 14.00 | Max Reward: 48.13
Iteration: 1625 | Episodes: 66000 | Median Reward: 19.06 | Max Reward: 48.13
Iteration: 1627 | Episodes: 66100 | Median Reward: 20.61 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.1         |
| time/                   |               |
|    fps                  | 395           |
|    iterations           | 1630          |
|    time_elapsed         | 16882         |
|    total_timesteps      | 6676480       |
| train/                  |               |
|    approx_kl            | 2.0104446e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -156          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 164           |
|    n_updates            | 16290         |
|    policy_gradient_loss | -0.000136     |
|    std                  | 20.8          |
|    value_loss           | 344           |
-------------------------------------------
Iteration: 1630 | Episodes: 66200 | Median Reward: 23.50 | Max Reward: 48.13
Iteration: 1632 | Episodes: 66300 | Median Reward: 27.61 | Max Reward: 48.13
Iteration: 1635 | Episodes: 66400 | Median Reward: 20.87 | Max Reward: 48.13
Iteration: 1637 | Episodes: 66500 | Median Reward: 13.47 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.4         |
| time/                   |               |
|    fps                  | 396           |
|    iterations           | 1640          |
|    time_elapsed         | 16928         |
|    total_timesteps      | 6717440       |
| train/                  |               |
|    approx_kl            | 2.0143212e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -157          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 123           |
|    n_updates            | 16390         |
|    policy_gradient_loss | 0.000113      |
|    std                  | 21.1          |
|    value_loss           | 263           |
-------------------------------------------
Iteration: 1640 | Episodes: 66600 | Median Reward: 22.07 | Max Reward: 48.13
Iteration: 1642 | Episodes: 66700 | Median Reward: 18.51 | Max Reward: 48.13
Iteration: 1644 | Episodes: 66800 | Median Reward: 11.12 | Max Reward: 48.13
Iteration: 1647 | Episodes: 66900 | Median Reward: 24.81 | Max Reward: 48.13
Iteration: 1649 | Episodes: 67000 | Median Reward: 25.95 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.2        |
| time/                   |              |
|    fps                  | 395          |
|    iterations           | 1650         |
|    time_elapsed         | 17080        |
|    total_timesteps      | 6758400      |
| train/                  |              |
|    approx_kl            | 5.209059e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -157         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 147          |
|    n_updates            | 16490        |
|    policy_gradient_loss | -0.000287    |
|    std                  | 21.3         |
|    value_loss           | 310          |
------------------------------------------
Iteration: 1652 | Episodes: 67100 | Median Reward: 15.26 | Max Reward: 48.13
Iteration: 1654 | Episodes: 67200 | Median Reward: 18.12 | Max Reward: 48.13
Iteration: 1657 | Episodes: 67300 | Median Reward: 17.01 | Max Reward: 48.13
Iteration: 1659 | Episodes: 67400 | Median Reward: 26.74 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.9         |
| time/                   |               |
|    fps                  | 394           |
|    iterations           | 1660          |
|    time_elapsed         | 17231         |
|    total_timesteps      | 6799360       |
| train/                  |               |
|    approx_kl            | 2.3390356e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -158          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 141           |
|    n_updates            | 16590         |
|    policy_gradient_loss | 3.22e-05      |
|    std                  | 22            |
|    value_loss           | 298           |
-------------------------------------------
Iteration: 1662 | Episodes: 67500 | Median Reward: 28.60 | Max Reward: 48.13
Iteration: 1664 | Episodes: 67600 | Median Reward: 26.09 | Max Reward: 48.13
Iteration: 1667 | Episodes: 67700 | Median Reward: 20.60 | Max Reward: 48.13
Iteration: 1669 | Episodes: 67800 | Median Reward: 18.57 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -81.2        |
| time/                   |              |
|    fps                  | 393          |
|    iterations           | 1670         |
|    time_elapsed         | 17384        |
|    total_timesteps      | 6840320      |
| train/                  |              |
|    approx_kl            | 0.0004318073 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -158         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 113          |
|    n_updates            | 16690        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 22.4         |
|    value_loss           | 243          |
------------------------------------------
Iteration: 1672 | Episodes: 67900 | Median Reward: 19.53 | Max Reward: 48.13
Iteration: 1674 | Episodes: 68000 | Median Reward: 23.35 | Max Reward: 48.13
Iteration: 1677 | Episodes: 68100 | Median Reward: 23.13 | Max Reward: 48.13
Iteration: 1679 | Episodes: 68200 | Median Reward: 15.20 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.2        |
| time/                   |              |
|    fps                  | 392          |
|    iterations           | 1680         |
|    time_elapsed         | 17539        |
|    total_timesteps      | 6881280      |
| train/                  |              |
|    approx_kl            | 0.0002865666 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -159         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 128          |
|    n_updates            | 16790        |
|    policy_gradient_loss | -0.000164    |
|    std                  | 23.1         |
|    value_loss           | 271          |
------------------------------------------
Iteration: 1681 | Episodes: 68300 | Median Reward: 22.59 | Max Reward: 48.13
Iteration: 1684 | Episodes: 68400 | Median Reward: 25.94 | Max Reward: 48.13
Iteration: 1686 | Episodes: 68500 | Median Reward: 20.82 | Max Reward: 48.13
Iteration: 1689 | Episodes: 68600 | Median Reward: 25.08 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.4        |
| time/                   |              |
|    fps                  | 391          |
|    iterations           | 1690         |
|    time_elapsed         | 17690        |
|    total_timesteps      | 6922240      |
| train/                  |              |
|    approx_kl            | 9.828262e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -159         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 159          |
|    n_updates            | 16890        |
|    policy_gradient_loss | -6.02e-05    |
|    std                  | 23.5         |
|    value_loss           | 335          |
------------------------------------------
Iteration: 1691 | Episodes: 68700 | Median Reward: 17.36 | Max Reward: 48.13
Iteration: 1694 | Episodes: 68800 | Median Reward: 23.78 | Max Reward: 48.13
Iteration: 1696 | Episodes: 68900 | Median Reward: 19.78 | Max Reward: 48.13
Iteration: 1699 | Episodes: 69000 | Median Reward: 23.12 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.4        |
| time/                   |              |
|    fps                  | 390          |
|    iterations           | 1700         |
|    time_elapsed         | 17846        |
|    total_timesteps      | 6963200      |
| train/                  |              |
|    approx_kl            | 6.317567e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -160         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 147          |
|    n_updates            | 16990        |
|    policy_gradient_loss | -2.58e-06    |
|    std                  | 23.9         |
|    value_loss           | 310          |
------------------------------------------
Iteration: 1701 | Episodes: 69100 | Median Reward: 23.62 | Max Reward: 48.13
Iteration: 1704 | Episodes: 69200 | Median Reward: 21.70 | Max Reward: 48.13
Iteration: 1706 | Episodes: 69300 | Median Reward: 19.74 | Max Reward: 48.13
Iteration: 1709 | Episodes: 69400 | Median Reward: 13.82 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -85         |
| time/                   |             |
|    fps                  | 389         |
|    iterations           | 1710        |
|    time_elapsed         | 18002       |
|    total_timesteps      | 7004160     |
| train/                  |             |
|    approx_kl            | 3.87482e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -160        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 83.1        |
|    n_updates            | 17090       |
|    policy_gradient_loss | -0.000336   |
|    std                  | 24.4        |
|    value_loss           | 183         |
-----------------------------------------
Iteration: 1711 | Episodes: 69500 | Median Reward: 25.09 | Max Reward: 48.13
Iteration: 1714 | Episodes: 69600 | Median Reward: 13.81 | Max Reward: 48.13
Iteration: 1716 | Episodes: 69700 | Median Reward: 21.60 | Max Reward: 48.13
Iteration: 1718 | Episodes: 69800 | Median Reward: 13.55 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.7         |
| time/                   |               |
|    fps                  | 387           |
|    iterations           | 1720          |
|    time_elapsed         | 18159         |
|    total_timesteps      | 7045120       |
| train/                  |               |
|    approx_kl            | 0.00034889622 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -160          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 95.9          |
|    n_updates            | 17190         |
|    policy_gradient_loss | -0.000597     |
|    std                  | 24.7          |
|    value_loss           | 208           |
-------------------------------------------
Iteration: 1721 | Episodes: 69900 | Median Reward: 21.69 | Max Reward: 48.13
Iteration: 1723 | Episodes: 70000 | Median Reward: 24.17 | Max Reward: 48.13
Iteration: 1726 | Episodes: 70100 | Median Reward: 18.18 | Max Reward: 48.13
Iteration: 1728 | Episodes: 70200 | Median Reward: 18.08 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -77.9       |
| time/                   |             |
|    fps                  | 386         |
|    iterations           | 1730        |
|    time_elapsed         | 18317       |
|    total_timesteps      | 7086080     |
| train/                  |             |
|    approx_kl            | 0.005329284 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -161        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 114         |
|    n_updates            | 17290       |
|    policy_gradient_loss | -0.00142    |
|    std                  | 25.2        |
|    value_loss           | 244         |
-----------------------------------------
Iteration: 1731 | Episodes: 70300 | Median Reward: 26.95 | Max Reward: 48.13
Iteration: 1733 | Episodes: 70400 | Median Reward: 14.82 | Max Reward: 48.13
Iteration: 1736 | Episodes: 70500 | Median Reward: 15.33 | Max Reward: 48.13
Iteration: 1738 | Episodes: 70600 | Median Reward: 19.60 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.5         |
| time/                   |               |
|    fps                  | 385           |
|    iterations           | 1740          |
|    time_elapsed         | 18477         |
|    total_timesteps      | 7127040       |
| train/                  |               |
|    approx_kl            | 0.00035436533 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -161          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 109           |
|    n_updates            | 17390         |
|    policy_gradient_loss | -0.00097      |
|    std                  | 26            |
|    value_loss           | 235           |
-------------------------------------------
Iteration: 1741 | Episodes: 70700 | Median Reward: 19.01 | Max Reward: 48.13
Iteration: 1743 | Episodes: 70800 | Median Reward: 16.77 | Max Reward: 48.13
Iteration: 1746 | Episodes: 70900 | Median Reward: 16.77 | Max Reward: 48.13
Iteration: 1748 | Episodes: 71000 | Median Reward: 22.40 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.3         |
| time/                   |               |
|    fps                  | 384           |
|    iterations           | 1750          |
|    time_elapsed         | 18635         |
|    total_timesteps      | 7168000       |
| train/                  |               |
|    approx_kl            | 2.8361188e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -162          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 99.7          |
|    n_updates            | 17490         |
|    policy_gradient_loss | -0.00037      |
|    std                  | 26.2          |
|    value_loss           | 216           |
-------------------------------------------
Iteration: 1751 | Episodes: 71100 | Median Reward: 16.37 | Max Reward: 48.13
Iteration: 1753 | Episodes: 71200 | Median Reward: 26.56 | Max Reward: 48.13
Iteration: 1755 | Episodes: 71300 | Median Reward: 13.50 | Max Reward: 48.13
Iteration: 1758 | Episodes: 71400 | Median Reward: 20.74 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.7         |
| time/                   |               |
|    fps                  | 383           |
|    iterations           | 1760          |
|    time_elapsed         | 18799         |
|    total_timesteps      | 7208960       |
| train/                  |               |
|    approx_kl            | 0.00011624112 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -162          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 135           |
|    n_updates            | 17590         |
|    policy_gradient_loss | -0.000529     |
|    std                  | 26.7          |
|    value_loss           | 287           |
-------------------------------------------
Iteration: 1760 | Episodes: 71500 | Median Reward: 18.20 | Max Reward: 48.13
Iteration: 1763 | Episodes: 71600 | Median Reward: 16.85 | Max Reward: 48.13
Iteration: 1765 | Episodes: 71700 | Median Reward: 19.80 | Max Reward: 48.13
Iteration: 1768 | Episodes: 71800 | Median Reward: 17.99 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.4        |
| time/                   |              |
|    fps                  | 382          |
|    iterations           | 1770         |
|    time_elapsed         | 18954        |
|    total_timesteps      | 7249920      |
| train/                  |              |
|    approx_kl            | 0.0006948095 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -163         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 124          |
|    n_updates            | 17690        |
|    policy_gradient_loss | -0.000601    |
|    std                  | 27.7         |
|    value_loss           | 264          |
------------------------------------------
Iteration: 1770 | Episodes: 71900 | Median Reward: 19.14 | Max Reward: 48.13
Iteration: 1773 | Episodes: 72000 | Median Reward: 22.80 | Max Reward: 48.13
Iteration: 1775 | Episodes: 72100 | Median Reward: 20.44 | Max Reward: 48.13
Iteration: 1778 | Episodes: 72200 | Median Reward: 18.76 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84.7        |
| time/                   |              |
|    fps                  | 381          |
|    iterations           | 1780         |
|    time_elapsed         | 19109        |
|    total_timesteps      | 7290880      |
| train/                  |              |
|    approx_kl            | 0.0023175334 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -163         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 119          |
|    n_updates            | 17790        |
|    policy_gradient_loss | -0.00073     |
|    std                  | 27.9         |
|    value_loss           | 255          |
------------------------------------------
Iteration: 1780 | Episodes: 72300 | Median Reward: 17.61 | Max Reward: 48.13
Iteration: 1783 | Episodes: 72400 | Median Reward: 23.11 | Max Reward: 48.13
Iteration: 1785 | Episodes: 72500 | Median Reward: 21.62 | Max Reward: 48.13
Iteration: 1788 | Episodes: 72600 | Median Reward: 20.12 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.1        |
| time/                   |              |
|    fps                  | 380          |
|    iterations           | 1790         |
|    time_elapsed         | 19266        |
|    total_timesteps      | 7331840      |
| train/                  |              |
|    approx_kl            | 0.0001231087 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -163         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 143          |
|    n_updates            | 17890        |
|    policy_gradient_loss | -0.000552    |
|    std                  | 28.2         |
|    value_loss           | 302          |
------------------------------------------
Iteration: 1790 | Episodes: 72700 | Median Reward: 18.93 | Max Reward: 48.13
Iteration: 1792 | Episodes: 72800 | Median Reward: 24.10 | Max Reward: 48.13
Iteration: 1795 | Episodes: 72900 | Median Reward: 13.31 | Max Reward: 48.13
Iteration: 1797 | Episodes: 73000 | Median Reward: 17.55 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -85.4       |
| time/                   |             |
|    fps                  | 379         |
|    iterations           | 1800        |
|    time_elapsed         | 19421       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 3.51248e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -164        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 89.9        |
|    n_updates            | 17990       |
|    policy_gradient_loss | -7.84e-05   |
|    std                  | 28.6        |
|    value_loss           | 197         |
-----------------------------------------
Iteration: 1800 | Episodes: 73100 | Median Reward: 23.65 | Max Reward: 48.13
Iteration: 1802 | Episodes: 73200 | Median Reward: 17.26 | Max Reward: 48.13
Iteration: 1805 | Episodes: 73300 | Median Reward: 14.96 | Max Reward: 48.13
Iteration: 1807 | Episodes: 73400 | Median Reward: 18.20 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.5        |
| time/                   |              |
|    fps                  | 378          |
|    iterations           | 1810         |
|    time_elapsed         | 19577        |
|    total_timesteps      | 7413760      |
| train/                  |              |
|    approx_kl            | 4.457333e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -164         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 128          |
|    n_updates            | 18090        |
|    policy_gradient_loss | -0.00013     |
|    std                  | 29.4         |
|    value_loss           | 272          |
------------------------------------------
Iteration: 1810 | Episodes: 73500 | Median Reward: 18.43 | Max Reward: 48.13
Iteration: 1812 | Episodes: 73600 | Median Reward: 23.95 | Max Reward: 48.13
Iteration: 1815 | Episodes: 73700 | Median Reward: 6.10 | Max Reward: 48.13
Iteration: 1817 | Episodes: 73800 | Median Reward: 18.37 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.2         |
| time/                   |               |
|    fps                  | 377           |
|    iterations           | 1820          |
|    time_elapsed         | 19731         |
|    total_timesteps      | 7454720       |
| train/                  |               |
|    approx_kl            | 4.7854483e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -165          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 102           |
|    n_updates            | 18190         |
|    policy_gradient_loss | -0.000678     |
|    std                  | 29.9          |
|    value_loss           | 220           |
-------------------------------------------
Iteration: 1820 | Episodes: 73900 | Median Reward: 17.23 | Max Reward: 48.13
Iteration: 1822 | Episodes: 74000 | Median Reward: 20.10 | Max Reward: 48.13
Iteration: 1825 | Episodes: 74100 | Median Reward: 15.87 | Max Reward: 48.13
Iteration: 1827 | Episodes: 74200 | Median Reward: 17.20 | Max Reward: 48.13
Iteration: 1829 | Episodes: 74300 | Median Reward: 20.44 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.7       |
| time/                   |             |
|    fps                  | 376         |
|    iterations           | 1830        |
|    time_elapsed         | 19884       |
|    total_timesteps      | 7495680     |
| train/                  |             |
|    approx_kl            | 0.000623183 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -165        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 119         |
|    n_updates            | 18290       |
|    policy_gradient_loss | -0.000543   |
|    std                  | 30.4        |
|    value_loss           | 254         |
-----------------------------------------
Iteration: 1832 | Episodes: 74400 | Median Reward: 24.39 | Max Reward: 48.13
Iteration: 1834 | Episodes: 74500 | Median Reward: 27.74 | Max Reward: 48.13
Iteration: 1837 | Episodes: 74600 | Median Reward: 19.64 | Max Reward: 48.13
Iteration: 1839 | Episodes: 74700 | Median Reward: 9.96 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87.5        |
| time/                   |              |
|    fps                  | 376          |
|    iterations           | 1840         |
|    time_elapsed         | 20038        |
|    total_timesteps      | 7536640      |
| train/                  |              |
|    approx_kl            | 1.649736e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -166         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 96.9         |
|    n_updates            | 18390        |
|    policy_gradient_loss | -0.0002      |
|    std                  | 31.1         |
|    value_loss           | 211          |
------------------------------------------
Iteration: 1842 | Episodes: 74800 | Median Reward: 22.16 | Max Reward: 48.13
Iteration: 1844 | Episodes: 74900 | Median Reward: 20.38 | Max Reward: 48.13
Iteration: 1847 | Episodes: 75000 | Median Reward: 20.28 | Max Reward: 48.13
Iteration: 1849 | Episodes: 75100 | Median Reward: 20.16 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.2        |
| time/                   |              |
|    fps                  | 375          |
|    iterations           | 1850         |
|    time_elapsed         | 20196        |
|    total_timesteps      | 7577600      |
| train/                  |              |
|    approx_kl            | 0.0003433546 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -166         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 121          |
|    n_updates            | 18490        |
|    policy_gradient_loss | -0.00111     |
|    std                  | 31.8         |
|    value_loss           | 259          |
------------------------------------------
Iteration: 1852 | Episodes: 75200 | Median Reward: 23.84 | Max Reward: 48.13
Iteration: 1854 | Episodes: 75300 | Median Reward: 18.85 | Max Reward: 48.13
Iteration: 1857 | Episodes: 75400 | Median Reward: 18.73 | Max Reward: 48.13
Iteration: 1859 | Episodes: 75500 | Median Reward: 22.09 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76          |
| time/                   |              |
|    fps                  | 374          |
|    iterations           | 1860         |
|    time_elapsed         | 20359        |
|    total_timesteps      | 7618560      |
| train/                  |              |
|    approx_kl            | 0.0009823537 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.4          |
|    entropy_loss         | -167         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 138          |
|    n_updates            | 18590        |
|    policy_gradient_loss | 0.000979     |
|    std                  | 32.5         |
|    value_loss           | 293          |
------------------------------------------
Iteration: 1861 | Episodes: 75600 | Median Reward: 12.72 | Max Reward: 48.13
Iteration: 1864 | Episodes: 75700 | Median Reward: 14.18 | Max Reward: 48.13
Iteration: 1866 | Episodes: 75800 | Median Reward: 13.65 | Max Reward: 48.13
Iteration: 1869 | Episodes: 75900 | Median Reward: 19.95 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.4        |
| time/                   |              |
|    fps                  | 373          |
|    iterations           | 1870         |
|    time_elapsed         | 20511        |
|    total_timesteps      | 7659520      |
| train/                  |              |
|    approx_kl            | 0.0010513884 |
|    clip_fraction        | 0.000806     |
|    clip_range           | 0.4          |
|    entropy_loss         | -166         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 125          |
|    n_updates            | 18690        |
|    policy_gradient_loss | -0.000191    |
|    std                  | 32.8         |
|    value_loss           | 268          |
------------------------------------------
Iteration: 1871 | Episodes: 76000 | Median Reward: 14.87 | Max Reward: 48.13
Iteration: 1874 | Episodes: 76100 | Median Reward: 22.19 | Max Reward: 48.13
Iteration: 1876 | Episodes: 76200 | Median Reward: 13.44 | Max Reward: 48.13
Iteration: 1879 | Episodes: 76300 | Median Reward: 17.49 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.5        |
| time/                   |              |
|    fps                  | 372          |
|    iterations           | 1880         |
|    time_elapsed         | 20667        |
|    total_timesteps      | 7700480      |
| train/                  |              |
|    approx_kl            | 9.850471e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -167         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 126          |
|    n_updates            | 18790        |
|    policy_gradient_loss | -7.53e-05    |
|    std                  | 33.5         |
|    value_loss           | 269          |
------------------------------------------
Iteration: 1881 | Episodes: 76400 | Median Reward: 10.95 | Max Reward: 48.13
Iteration: 1884 | Episodes: 76500 | Median Reward: 20.35 | Max Reward: 48.13
Iteration: 1886 | Episodes: 76600 | Median Reward: 11.57 | Max Reward: 48.13
Iteration: 1889 | Episodes: 76700 | Median Reward: 20.64 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.2         |
| time/                   |               |
|    fps                  | 371           |
|    iterations           | 1890          |
|    time_elapsed         | 20822         |
|    total_timesteps      | 7741440       |
| train/                  |               |
|    approx_kl            | 0.00044937924 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -168          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 120           |
|    n_updates            | 18890         |
|    policy_gradient_loss | 8.73e-05      |
|    std                  | 34.2          |
|    value_loss           | 258           |
-------------------------------------------
Iteration: 1891 | Episodes: 76800 | Median Reward: 14.25 | Max Reward: 48.13
Iteration: 1894 | Episodes: 76900 | Median Reward: 17.94 | Max Reward: 48.13
Iteration: 1896 | Episodes: 77000 | Median Reward: 16.33 | Max Reward: 48.13
Iteration: 1898 | Episodes: 77100 | Median Reward: 17.85 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.5        |
| time/                   |              |
|    fps                  | 371          |
|    iterations           | 1900         |
|    time_elapsed         | 20974        |
|    total_timesteps      | 7782400      |
| train/                  |              |
|    approx_kl            | 0.0003497396 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -168         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 104          |
|    n_updates            | 18990        |
|    policy_gradient_loss | -0.000106    |
|    std                  | 34.7         |
|    value_loss           | 225          |
------------------------------------------
Iteration: 1901 | Episodes: 77200 | Median Reward: 16.20 | Max Reward: 48.13
Iteration: 1903 | Episodes: 77300 | Median Reward: 17.24 | Max Reward: 48.13
Iteration: 1906 | Episodes: 77400 | Median Reward: 18.30 | Max Reward: 48.13
Iteration: 1908 | Episodes: 77500 | Median Reward: 14.75 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.3         |
| time/                   |               |
|    fps                  | 370           |
|    iterations           | 1910          |
|    time_elapsed         | 21127         |
|    total_timesteps      | 7823360       |
| train/                  |               |
|    approx_kl            | 0.00031908636 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -169          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 113           |
|    n_updates            | 19090         |
|    policy_gradient_loss | -0.000539     |
|    std                  | 35.2          |
|    value_loss           | 243           |
-------------------------------------------
Iteration: 1911 | Episodes: 77600 | Median Reward: 18.25 | Max Reward: 48.13
Iteration: 1913 | Episodes: 77700 | Median Reward: 21.71 | Max Reward: 48.13
Iteration: 1916 | Episodes: 77800 | Median Reward: 13.15 | Max Reward: 48.13
Iteration: 1918 | Episodes: 77900 | Median Reward: 25.12 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.6         |
| time/                   |               |
|    fps                  | 369           |
|    iterations           | 1920          |
|    time_elapsed         | 21282         |
|    total_timesteps      | 7864320       |
| train/                  |               |
|    approx_kl            | 3.8723476e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -169          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 146           |
|    n_updates            | 19190         |
|    policy_gradient_loss | -5.38e-05     |
|    std                  | 35.8          |
|    value_loss           | 308           |
-------------------------------------------
Iteration: 1921 | Episodes: 78000 | Median Reward: 11.84 | Max Reward: 48.13
Iteration: 1923 | Episodes: 78100 | Median Reward: 19.84 | Max Reward: 48.13
Iteration: 1926 | Episodes: 78200 | Median Reward: 20.25 | Max Reward: 48.13
Iteration: 1928 | Episodes: 78300 | Median Reward: 17.44 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.7        |
| time/                   |              |
|    fps                  | 368          |
|    iterations           | 1930         |
|    time_elapsed         | 21438        |
|    total_timesteps      | 7905280      |
| train/                  |              |
|    approx_kl            | 0.0002119243 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -169         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 108          |
|    n_updates            | 19290        |
|    policy_gradient_loss | -0.000675    |
|    std                  | 36.5         |
|    value_loss           | 234          |
------------------------------------------
Iteration: 1931 | Episodes: 78400 | Median Reward: 21.64 | Max Reward: 48.13
Iteration: 1933 | Episodes: 78500 | Median Reward: 12.12 | Max Reward: 48.13
Iteration: 1935 | Episodes: 78600 | Median Reward: 21.46 | Max Reward: 48.13
Iteration: 1938 | Episodes: 78700 | Median Reward: 15.61 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.7         |
| time/                   |               |
|    fps                  | 367           |
|    iterations           | 1940          |
|    time_elapsed         | 21594         |
|    total_timesteps      | 7946240       |
| train/                  |               |
|    approx_kl            | 5.9477956e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -170          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 135           |
|    n_updates            | 19390         |
|    policy_gradient_loss | -0.000535     |
|    std                  | 36.8          |
|    value_loss           | 287           |
-------------------------------------------
Iteration: 1940 | Episodes: 78800 | Median Reward: 24.39 | Max Reward: 48.13
Iteration: 1943 | Episodes: 78900 | Median Reward: 14.67 | Max Reward: 48.13
Iteration: 1945 | Episodes: 79000 | Median Reward: 19.54 | Max Reward: 48.13
Iteration: 1948 | Episodes: 79100 | Median Reward: 25.50 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.9         |
| time/                   |               |
|    fps                  | 367           |
|    iterations           | 1950          |
|    time_elapsed         | 21754         |
|    total_timesteps      | 7987200       |
| train/                  |               |
|    approx_kl            | 0.00010755038 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -170          |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0005        |
|    loss                 | 128           |
|    n_updates            | 19490         |
|    policy_gradient_loss | -6.36e-05     |
|    std                  | 37.3          |
|    value_loss           | 272           |
-------------------------------------------
Iteration: 1950 | Episodes: 79200 | Median Reward: 14.61 | Max Reward: 48.13
Iteration: 1953 | Episodes: 79300 | Median Reward: 12.66 | Max Reward: 48.13
Iteration: 1955 | Episodes: 79400 | Median Reward: 21.16 | Max Reward: 48.13
Iteration: 1958 | Episodes: 79500 | Median Reward: 17.64 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.1         |
| time/                   |               |
|    fps                  | 366           |
|    iterations           | 1960          |
|    time_elapsed         | 21917         |
|    total_timesteps      | 8028160       |
| train/                  |               |
|    approx_kl            | 1.3437355e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -170          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 124           |
|    n_updates            | 19590         |
|    policy_gradient_loss | -0.000104     |
|    std                  | 37.6          |
|    value_loss           | 264           |
-------------------------------------------
Iteration: 1960 | Episodes: 79600 | Median Reward: 20.56 | Max Reward: 48.13
Iteration: 1963 | Episodes: 79700 | Median Reward: 3.55 | Max Reward: 48.13
Iteration: 1965 | Episodes: 79800 | Median Reward: 13.33 | Max Reward: 48.13
Iteration: 1968 | Episodes: 79900 | Median Reward: 17.97 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.8        |
| time/                   |              |
|    fps                  | 365          |
|    iterations           | 1970         |
|    time_elapsed         | 22071        |
|    total_timesteps      | 8069120      |
| train/                  |              |
|    approx_kl            | 0.0008708498 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -170         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 114          |
|    n_updates            | 19690        |
|    policy_gradient_loss | -0.00261     |
|    std                  | 37.8         |
|    value_loss           | 245          |
------------------------------------------
Iteration: 1970 | Episodes: 80000 | Median Reward: 13.40 | Max Reward: 48.13
Iteration: 1972 | Episodes: 80100 | Median Reward: 9.24 | Max Reward: 48.13
Iteration: 1975 | Episodes: 80200 | Median Reward: 19.98 | Max Reward: 48.13
Iteration: 1977 | Episodes: 80300 | Median Reward: 14.46 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.7         |
| time/                   |               |
|    fps                  | 364           |
|    iterations           | 1980          |
|    time_elapsed         | 22225         |
|    total_timesteps      | 8110080       |
| train/                  |               |
|    approx_kl            | 0.00066219695 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -171          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 108           |
|    n_updates            | 19790         |
|    policy_gradient_loss | -0.000734     |
|    std                  | 38.6          |
|    value_loss           | 233           |
-------------------------------------------
Iteration: 1980 | Episodes: 80400 | Median Reward: 19.17 | Max Reward: 48.13
Iteration: 1982 | Episodes: 80500 | Median Reward: 13.15 | Max Reward: 48.13
Iteration: 1985 | Episodes: 80600 | Median Reward: 20.52 | Max Reward: 48.13
Iteration: 1987 | Episodes: 80700 | Median Reward: 14.38 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -90.4         |
| time/                   |               |
|    fps                  | 364           |
|    iterations           | 1990          |
|    time_elapsed         | 22377         |
|    total_timesteps      | 8151040       |
| train/                  |               |
|    approx_kl            | 0.00021838422 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -172          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 86.3          |
|    n_updates            | 19890         |
|    policy_gradient_loss | 0.00129       |
|    std                  | 40.1          |
|    value_loss           | 190           |
-------------------------------------------
Iteration: 1990 | Episodes: 80800 | Median Reward: 11.05 | Max Reward: 48.13
Iteration: 1992 | Episodes: 80900 | Median Reward: 12.87 | Max Reward: 48.13
Iteration: 1995 | Episodes: 81000 | Median Reward: 19.30 | Max Reward: 48.13
Iteration: 1997 | Episodes: 81100 | Median Reward: 21.64 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85           |
| time/                   |               |
|    fps                  | 363           |
|    iterations           | 2000          |
|    time_elapsed         | 22530         |
|    total_timesteps      | 8192000       |
| train/                  |               |
|    approx_kl            | 1.3062323e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -172          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 115           |
|    n_updates            | 19990         |
|    policy_gradient_loss | -5.16e-05     |
|    std                  | 41.1          |
|    value_loss           | 248           |
-------------------------------------------
Iteration: 2000 | Episodes: 81200 | Median Reward: 14.46 | Max Reward: 48.13
Iteration: 2002 | Episodes: 81300 | Median Reward: 15.03 | Max Reward: 48.13
Iteration: 2005 | Episodes: 81400 | Median Reward: 14.52 | Max Reward: 48.13
Iteration: 2007 | Episodes: 81500 | Median Reward: 15.70 | Max Reward: 48.13
Iteration: 2009 | Episodes: 81600 | Median Reward: 12.00 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.1        |
| time/                   |              |
|    fps                  | 362          |
|    iterations           | 2010         |
|    time_elapsed         | 22685        |
|    total_timesteps      | 8232960      |
| train/                  |              |
|    approx_kl            | 4.450543e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -173         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 76.9         |
|    n_updates            | 20090        |
|    policy_gradient_loss | 1.41e-05     |
|    std                  | 41.6         |
|    value_loss           | 171          |
------------------------------------------
Iteration: 2012 | Episodes: 81700 | Median Reward: 21.47 | Max Reward: 48.13
Iteration: 2014 | Episodes: 81800 | Median Reward: 16.45 | Max Reward: 48.13
Iteration: 2017 | Episodes: 81900 | Median Reward: 19.32 | Max Reward: 48.13
Iteration: 2019 | Episodes: 82000 | Median Reward: 13.02 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.2        |
| time/                   |              |
|    fps                  | 362          |
|    iterations           | 2020         |
|    time_elapsed         | 22841        |
|    total_timesteps      | 8273920      |
| train/                  |              |
|    approx_kl            | 0.0003025905 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -173         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 108          |
|    n_updates            | 20190        |
|    policy_gradient_loss | -0.000809    |
|    std                  | 42.4         |
|    value_loss           | 233          |
------------------------------------------
Iteration: 2022 | Episodes: 82100 | Median Reward: 10.01 | Max Reward: 48.13
Iteration: 2024 | Episodes: 82200 | Median Reward: 17.90 | Max Reward: 48.13
Iteration: 2027 | Episodes: 82300 | Median Reward: 17.92 | Max Reward: 48.13
Iteration: 2029 | Episodes: 82400 | Median Reward: 18.15 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.1         |
| time/                   |               |
|    fps                  | 361           |
|    iterations           | 2030          |
|    time_elapsed         | 22995         |
|    total_timesteps      | 8314880       |
| train/                  |               |
|    approx_kl            | 0.00017115613 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -173          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 97.2          |
|    n_updates            | 20290         |
|    policy_gradient_loss | 0.000118      |
|    std                  | 42.8          |
|    value_loss           | 212           |
-------------------------------------------
Iteration: 2032 | Episodes: 82500 | Median Reward: 15.55 | Max Reward: 48.13
Iteration: 2034 | Episodes: 82600 | Median Reward: 11.02 | Max Reward: 48.13
Iteration: 2037 | Episodes: 82700 | Median Reward: 3.97 | Max Reward: 48.13
Iteration: 2039 | Episodes: 82800 | Median Reward: 14.40 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.9        |
| time/                   |              |
|    fps                  | 360          |
|    iterations           | 2040         |
|    time_elapsed         | 23151        |
|    total_timesteps      | 8355840      |
| train/                  |              |
|    approx_kl            | 0.0006026946 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -174         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 106          |
|    n_updates            | 20390        |
|    policy_gradient_loss | -0.000319    |
|    std                  | 43.9         |
|    value_loss           | 229          |
------------------------------------------
Iteration: 2041 | Episodes: 82900 | Median Reward: 14.43 | Max Reward: 48.13
Iteration: 2044 | Episodes: 83000 | Median Reward: 18.33 | Max Reward: 48.13
Iteration: 2046 | Episodes: 83100 | Median Reward: 13.16 | Max Reward: 48.13
Iteration: 2049 | Episodes: 83200 | Median Reward: 21.88 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.6        |
| time/                   |              |
|    fps                  | 360          |
|    iterations           | 2050         |
|    time_elapsed         | 23305        |
|    total_timesteps      | 8396800      |
| train/                  |              |
|    approx_kl            | 8.061179e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -174         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 160          |
|    n_updates            | 20490        |
|    policy_gradient_loss | 1.22e-07     |
|    std                  | 44.7         |
|    value_loss           | 338          |
------------------------------------------
Iteration: 2051 | Episodes: 83300 | Median Reward: 15.65 | Max Reward: 48.13
Iteration: 2054 | Episodes: 83400 | Median Reward: 21.08 | Max Reward: 48.13
Iteration: 2056 | Episodes: 83500 | Median Reward: 15.24 | Max Reward: 48.13
Iteration: 2059 | Episodes: 83600 | Median Reward: 19.46 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -79.9         |
| time/                   |               |
|    fps                  | 359           |
|    iterations           | 2060          |
|    time_elapsed         | 23471         |
|    total_timesteps      | 8437760       |
| train/                  |               |
|    approx_kl            | 1.3651472e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -175          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 141           |
|    n_updates            | 20590         |
|    policy_gradient_loss | -0.000132     |
|    std                  | 45.4          |
|    value_loss           | 299           |
-------------------------------------------
Iteration: 2061 | Episodes: 83700 | Median Reward: 16.12 | Max Reward: 48.13
Iteration: 2064 | Episodes: 83800 | Median Reward: 21.59 | Max Reward: 48.13
Iteration: 2066 | Episodes: 83900 | Median Reward: 13.64 | Max Reward: 48.13
Iteration: 2069 | Episodes: 84000 | Median Reward: 5.87 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.7        |
| time/                   |              |
|    fps                  | 358          |
|    iterations           | 2070         |
|    time_elapsed         | 23628        |
|    total_timesteps      | 8478720      |
| train/                  |              |
|    approx_kl            | 2.044675e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -175         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 87.3         |
|    n_updates            | 20690        |
|    policy_gradient_loss | -0.000233    |
|    std                  | 45.9         |
|    value_loss           | 192          |
------------------------------------------
Iteration: 2071 | Episodes: 84100 | Median Reward: 16.69 | Max Reward: 48.13
Iteration: 2074 | Episodes: 84200 | Median Reward: 19.04 | Max Reward: 48.13
Iteration: 2076 | Episodes: 84300 | Median Reward: 16.41 | Max Reward: 48.13
Iteration: 2078 | Episodes: 84400 | Median Reward: 19.77 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.1         |
| time/                   |               |
|    fps                  | 358           |
|    iterations           | 2080          |
|    time_elapsed         | 23782         |
|    total_timesteps      | 8519680       |
| train/                  |               |
|    approx_kl            | 5.8153542e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -175          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 100           |
|    n_updates            | 20790         |
|    policy_gradient_loss | -0.000234     |
|    std                  | 46.6          |
|    value_loss           | 218           |
-------------------------------------------
Iteration: 2081 | Episodes: 84500 | Median Reward: 7.07 | Max Reward: 48.13
Iteration: 2083 | Episodes: 84600 | Median Reward: 18.81 | Max Reward: 48.13
Iteration: 2086 | Episodes: 84700 | Median Reward: 18.55 | Max Reward: 48.13
Iteration: 2088 | Episodes: 84800 | Median Reward: 26.87 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.9         |
| time/                   |               |
|    fps                  | 357           |
|    iterations           | 2090          |
|    time_elapsed         | 23936         |
|    total_timesteps      | 8560640       |
| train/                  |               |
|    approx_kl            | 1.5262951e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -176          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 139           |
|    n_updates            | 20890         |
|    policy_gradient_loss | -0.000144     |
|    std                  | 47.2          |
|    value_loss           | 295           |
-------------------------------------------
Iteration: 2091 | Episodes: 84900 | Median Reward: 22.07 | Max Reward: 48.13
Iteration: 2093 | Episodes: 85000 | Median Reward: 19.20 | Max Reward: 48.13
Iteration: 2096 | Episodes: 85100 | Median Reward: 10.93 | Max Reward: 48.13
Iteration: 2098 | Episodes: 85200 | Median Reward: 19.00 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80           |
| time/                   |               |
|    fps                  | 357           |
|    iterations           | 2100          |
|    time_elapsed         | 24091         |
|    total_timesteps      | 8601600       |
| train/                  |               |
|    approx_kl            | 0.00011315489 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -176          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 131           |
|    n_updates            | 20990         |
|    policy_gradient_loss | -0.000312     |
|    std                  | 47.7          |
|    value_loss           | 279           |
-------------------------------------------
Iteration: 2101 | Episodes: 85300 | Median Reward: 6.33 | Max Reward: 48.13
Iteration: 2103 | Episodes: 85400 | Median Reward: 16.92 | Max Reward: 48.13
Iteration: 2106 | Episodes: 85500 | Median Reward: 18.34 | Max Reward: 48.13
Iteration: 2108 | Episodes: 85600 | Median Reward: 19.64 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.6         |
| time/                   |               |
|    fps                  | 356           |
|    iterations           | 2110          |
|    time_elapsed         | 24247         |
|    total_timesteps      | 8642560       |
| train/                  |               |
|    approx_kl            | 1.0934804e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -176          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 153           |
|    n_updates            | 21090         |
|    policy_gradient_loss | 0.000105      |
|    std                  | 48.6          |
|    value_loss           | 323           |
-------------------------------------------
Iteration: 2111 | Episodes: 85700 | Median Reward: 16.60 | Max Reward: 48.13
Iteration: 2113 | Episodes: 85800 | Median Reward: 21.67 | Max Reward: 48.13
Iteration: 2115 | Episodes: 85900 | Median Reward: 15.17 | Max Reward: 48.13
Iteration: 2118 | Episodes: 86000 | Median Reward: 19.42 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.4         |
| time/                   |               |
|    fps                  | 355           |
|    iterations           | 2120          |
|    time_elapsed         | 24403         |
|    total_timesteps      | 8683520       |
| train/                  |               |
|    approx_kl            | 0.00038173265 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -177          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 88.6          |
|    n_updates            | 21190         |
|    policy_gradient_loss | -0.000168     |
|    std                  | 49.5          |
|    value_loss           | 195           |
-------------------------------------------
Iteration: 2120 | Episodes: 86100 | Median Reward: 14.94 | Max Reward: 48.13
Iteration: 2123 | Episodes: 86200 | Median Reward: 21.37 | Max Reward: 48.13
Iteration: 2125 | Episodes: 86300 | Median Reward: 10.41 | Max Reward: 48.13
Iteration: 2128 | Episodes: 86400 | Median Reward: 26.30 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.7         |
| time/                   |               |
|    fps                  | 355           |
|    iterations           | 2130          |
|    time_elapsed         | 24556         |
|    total_timesteps      | 8724480       |
| train/                  |               |
|    approx_kl            | 1.4893594e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -177          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 141           |
|    n_updates            | 21290         |
|    policy_gradient_loss | -6.23e-06     |
|    std                  | 50.1          |
|    value_loss           | 301           |
-------------------------------------------
Iteration: 2130 | Episodes: 86500 | Median Reward: 15.37 | Max Reward: 48.13
Iteration: 2133 | Episodes: 86600 | Median Reward: 18.07 | Max Reward: 48.13
Iteration: 2135 | Episodes: 86700 | Median Reward: 17.30 | Max Reward: 48.13
Iteration: 2138 | Episodes: 86800 | Median Reward: 10.01 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87.5        |
| time/                   |              |
|    fps                  | 354          |
|    iterations           | 2140         |
|    time_elapsed         | 24710        |
|    total_timesteps      | 8765440      |
| train/                  |              |
|    approx_kl            | 1.541288e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -177         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 98.8         |
|    n_updates            | 21390        |
|    policy_gradient_loss | -9.4e-05     |
|    std                  | 51           |
|    value_loss           | 215          |
------------------------------------------
Iteration: 2140 | Episodes: 86900 | Median Reward: 13.74 | Max Reward: 48.13
Iteration: 2143 | Episodes: 87000 | Median Reward: 17.41 | Max Reward: 48.13
Iteration: 2145 | Episodes: 87100 | Median Reward: 15.16 | Max Reward: 48.13
Iteration: 2148 | Episodes: 87200 | Median Reward: 21.41 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.6         |
| time/                   |               |
|    fps                  | 354           |
|    iterations           | 2150          |
|    time_elapsed         | 24869         |
|    total_timesteps      | 8806400       |
| train/                  |               |
|    approx_kl            | 7.8516416e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -178          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 152           |
|    n_updates            | 21490         |
|    policy_gradient_loss | -0.000134     |
|    std                  | 52.1          |
|    value_loss           | 323           |
-------------------------------------------
Iteration: 2150 | Episodes: 87300 | Median Reward: 22.16 | Max Reward: 48.13
Iteration: 2152 | Episodes: 87400 | Median Reward: 19.58 | Max Reward: 48.13
Iteration: 2155 | Episodes: 87500 | Median Reward: 24.91 | Max Reward: 48.13
Iteration: 2157 | Episodes: 87600 | Median Reward: 18.24 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88.9         |
| time/                   |               |
|    fps                  | 353           |
|    iterations           | 2160          |
|    time_elapsed         | 25031         |
|    total_timesteps      | 8847360       |
| train/                  |               |
|    approx_kl            | 1.7760976e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -179          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 83.6          |
|    n_updates            | 21590         |
|    policy_gradient_loss | -0.000101     |
|    std                  | 53.4          |
|    value_loss           | 185           |
-------------------------------------------
Iteration: 2160 | Episodes: 87700 | Median Reward: 13.67 | Max Reward: 48.13
Iteration: 2162 | Episodes: 87800 | Median Reward: 19.55 | Max Reward: 48.13
Iteration: 2165 | Episodes: 87900 | Median Reward: 12.03 | Max Reward: 48.13
Iteration: 2167 | Episodes: 88000 | Median Reward: 10.92 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.5         |
| time/                   |               |
|    fps                  | 352           |
|    iterations           | 2170          |
|    time_elapsed         | 25185         |
|    total_timesteps      | 8888320       |
| train/                  |               |
|    approx_kl            | 0.00019111302 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -179          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 103           |
|    n_updates            | 21690         |
|    policy_gradient_loss | -0.000513     |
|    std                  | 53.9          |
|    value_loss           | 223           |
-------------------------------------------
Iteration: 2170 | Episodes: 88100 | Median Reward: 15.32 | Max Reward: 48.13
Iteration: 2172 | Episodes: 88200 | Median Reward: 20.66 | Max Reward: 48.13
Iteration: 2175 | Episodes: 88300 | Median Reward: 19.12 | Max Reward: 48.13
Iteration: 2177 | Episodes: 88400 | Median Reward: 17.28 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.6         |
| time/                   |               |
|    fps                  | 352           |
|    iterations           | 2180          |
|    time_elapsed         | 25340         |
|    total_timesteps      | 8929280       |
| train/                  |               |
|    approx_kl            | 0.00039429904 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -179          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 109           |
|    n_updates            | 21790         |
|    policy_gradient_loss | -0.000199     |
|    std                  | 55            |
|    value_loss           | 235           |
-------------------------------------------
Iteration: 2180 | Episodes: 88500 | Median Reward: 14.58 | Max Reward: 48.13
Iteration: 2182 | Episodes: 88600 | Median Reward: 16.81 | Max Reward: 48.13
Iteration: 2185 | Episodes: 88700 | Median Reward: 19.20 | Max Reward: 48.13
Iteration: 2187 | Episodes: 88800 | Median Reward: 12.74 | Max Reward: 48.13
Iteration: 2189 | Episodes: 88900 | Median Reward: 15.81 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.8         |
| time/                   |               |
|    fps                  | 351           |
|    iterations           | 2190          |
|    time_elapsed         | 25495         |
|    total_timesteps      | 8970240       |
| train/                  |               |
|    approx_kl            | 7.3529227e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -180          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 82.8          |
|    n_updates            | 21890         |
|    policy_gradient_loss | -0.000134     |
|    std                  | 56            |
|    value_loss           | 184           |
-------------------------------------------
Iteration: 2192 | Episodes: 89000 | Median Reward: 20.04 | Max Reward: 48.13
Iteration: 2194 | Episodes: 89100 | Median Reward: 12.82 | Max Reward: 48.13
Iteration: 2197 | Episodes: 89200 | Median Reward: 20.99 | Max Reward: 48.13
Iteration: 2199 | Episodes: 89300 | Median Reward: 11.52 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.2         |
| time/                   |               |
|    fps                  | 351           |
|    iterations           | 2200          |
|    time_elapsed         | 25649         |
|    total_timesteps      | 9011200       |
| train/                  |               |
|    approx_kl            | 0.00012612078 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -180          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 101           |
|    n_updates            | 21990         |
|    policy_gradient_loss | -0.000145     |
|    std                  | 56.8          |
|    value_loss           | 220           |
-------------------------------------------
Iteration: 2202 | Episodes: 89400 | Median Reward: 17.24 | Max Reward: 48.13
Iteration: 2204 | Episodes: 89500 | Median Reward: 19.98 | Max Reward: 48.13
Iteration: 2207 | Episodes: 89600 | Median Reward: 17.44 | Max Reward: 48.13
Iteration: 2209 | Episodes: 89700 | Median Reward: 11.40 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -87.7       |
| time/                   |             |
|    fps                  | 350         |
|    iterations           | 2210        |
|    time_elapsed         | 25804       |
|    total_timesteps      | 9052160     |
| train/                  |             |
|    approx_kl            | 0.000112433 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -181        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 94.9        |
|    n_updates            | 22090       |
|    policy_gradient_loss | -2.93e-05   |
|    std                  | 58.1        |
|    value_loss           | 208         |
-----------------------------------------
Iteration: 2212 | Episodes: 89800 | Median Reward: 12.98 | Max Reward: 48.13
Iteration: 2214 | Episodes: 89900 | Median Reward: 11.10 | Max Reward: 48.13
Iteration: 2217 | Episodes: 90000 | Median Reward: 11.46 | Max Reward: 48.13
Iteration: 2219 | Episodes: 90100 | Median Reward: 16.89 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -85.3       |
| time/                   |             |
|    fps                  | 350         |
|    iterations           | 2220        |
|    time_elapsed         | 25960       |
|    total_timesteps      | 9093120     |
| train/                  |             |
|    approx_kl            | 0.002087655 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -181        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 108         |
|    n_updates            | 22190       |
|    policy_gradient_loss | -0.00244    |
|    std                  | 59.5        |
|    value_loss           | 234         |
-----------------------------------------
Iteration: 2221 | Episodes: 90200 | Median Reward: 11.87 | Max Reward: 48.13
Iteration: 2224 | Episodes: 90300 | Median Reward: 12.84 | Max Reward: 48.13
Iteration: 2226 | Episodes: 90400 | Median Reward: 17.39 | Max Reward: 48.13
Iteration: 2229 | Episodes: 90500 | Median Reward: 17.67 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.1        |
| time/                   |              |
|    fps                  | 349          |
|    iterations           | 2230         |
|    time_elapsed         | 26114        |
|    total_timesteps      | 9134080      |
| train/                  |              |
|    approx_kl            | 7.258322e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -182         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 104          |
|    n_updates            | 22290        |
|    policy_gradient_loss | -0.000367    |
|    std                  | 60.7         |
|    value_loss           | 227          |
------------------------------------------
Iteration: 2231 | Episodes: 90600 | Median Reward: 13.37 | Max Reward: 48.13
Iteration: 2234 | Episodes: 90700 | Median Reward: 16.05 | Max Reward: 48.13
Iteration: 2236 | Episodes: 90800 | Median Reward: 14.88 | Max Reward: 48.13
Iteration: 2239 | Episodes: 90900 | Median Reward: 10.76 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.2         |
| time/                   |               |
|    fps                  | 349           |
|    iterations           | 2240          |
|    time_elapsed         | 26269         |
|    total_timesteps      | 9175040       |
| train/                  |               |
|    approx_kl            | 0.00015741945 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -182          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 100           |
|    n_updates            | 22390         |
|    policy_gradient_loss | -0.000433     |
|    std                  | 61.6          |
|    value_loss           | 219           |
-------------------------------------------
Iteration: 2241 | Episodes: 91000 | Median Reward: 20.69 | Max Reward: 48.13
Iteration: 2244 | Episodes: 91100 | Median Reward: 21.35 | Max Reward: 48.13
Iteration: 2246 | Episodes: 91200 | Median Reward: 14.43 | Max Reward: 48.13
Iteration: 2249 | Episodes: 91300 | Median Reward: 16.78 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87.7        |
| time/                   |              |
|    fps                  | 348          |
|    iterations           | 2250         |
|    time_elapsed         | 26425        |
|    total_timesteps      | 9216000      |
| train/                  |              |
|    approx_kl            | 0.0016297421 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -182         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 112          |
|    n_updates            | 22490        |
|    policy_gradient_loss | -0.00248     |
|    std                  | 62.4         |
|    value_loss           | 242          |
------------------------------------------
Iteration: 2251 | Episodes: 91400 | Median Reward: 13.11 | Max Reward: 48.13
Iteration: 2254 | Episodes: 91500 | Median Reward: 13.17 | Max Reward: 48.13
Iteration: 2256 | Episodes: 91600 | Median Reward: 17.34 | Max Reward: 48.13
Iteration: 2258 | Episodes: 91700 | Median Reward: 15.65 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84.3        |
| time/                   |              |
|    fps                  | 348          |
|    iterations           | 2260         |
|    time_elapsed         | 26592        |
|    total_timesteps      | 9256960      |
| train/                  |              |
|    approx_kl            | 0.0020295782 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -182         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 108          |
|    n_updates            | 22590        |
|    policy_gradient_loss | -0.00274     |
|    std                  | 63           |
|    value_loss           | 234          |
------------------------------------------
Iteration: 2261 | Episodes: 91800 | Median Reward: 18.79 | Max Reward: 48.13
Iteration: 2263 | Episodes: 91900 | Median Reward: 17.08 | Max Reward: 48.13
Iteration: 2266 | Episodes: 92000 | Median Reward: 19.16 | Max Reward: 48.13
Iteration: 2268 | Episodes: 92100 | Median Reward: 12.80 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.4         |
| time/                   |               |
|    fps                  | 347           |
|    iterations           | 2270          |
|    time_elapsed         | 26748         |
|    total_timesteps      | 9297920       |
| train/                  |               |
|    approx_kl            | 0.00038773756 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -183          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 118           |
|    n_updates            | 22690         |
|    policy_gradient_loss | -0.00101      |
|    std                  | 63.9          |
|    value_loss           | 254           |
-------------------------------------------
Iteration: 2271 | Episodes: 92200 | Median Reward: 16.09 | Max Reward: 48.13
Iteration: 2273 | Episodes: 92300 | Median Reward: 21.85 | Max Reward: 48.13
Iteration: 2276 | Episodes: 92400 | Median Reward: 9.57 | Max Reward: 48.13
Iteration: 2278 | Episodes: 92500 | Median Reward: 9.08 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.8         |
| time/                   |               |
|    fps                  | 347           |
|    iterations           | 2280          |
|    time_elapsed         | 26900         |
|    total_timesteps      | 9338880       |
| train/                  |               |
|    approx_kl            | 1.5818892e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -183          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 121           |
|    n_updates            | 22790         |
|    policy_gradient_loss | -0.000112     |
|    std                  | 65            |
|    value_loss           | 261           |
-------------------------------------------
Iteration: 2281 | Episodes: 92600 | Median Reward: 13.72 | Max Reward: 48.13
Iteration: 2283 | Episodes: 92700 | Median Reward: 17.96 | Max Reward: 48.13
Iteration: 2286 | Episodes: 92800 | Median Reward: 14.07 | Max Reward: 48.13
Iteration: 2288 | Episodes: 92900 | Median Reward: 16.08 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84.6        |
| time/                   |              |
|    fps                  | 346          |
|    iterations           | 2290         |
|    time_elapsed         | 27055        |
|    total_timesteps      | 9379840      |
| train/                  |              |
|    approx_kl            | 2.690719e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -184         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 129          |
|    n_updates            | 22890        |
|    policy_gradient_loss | 4.9e-05      |
|    std                  | 65.9         |
|    value_loss           | 276          |
------------------------------------------
Iteration: 2291 | Episodes: 93000 | Median Reward: 15.29 | Max Reward: 48.13
Iteration: 2293 | Episodes: 93100 | Median Reward: 7.06 | Max Reward: 48.13
Iteration: 2295 | Episodes: 93200 | Median Reward: 15.89 | Max Reward: 48.13
Iteration: 2298 | Episodes: 93300 | Median Reward: 7.85 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -89.3        |
| time/                   |              |
|    fps                  | 346          |
|    iterations           | 2300         |
|    time_elapsed         | 27209        |
|    total_timesteps      | 9420800      |
| train/                  |              |
|    approx_kl            | 0.0005892103 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -184         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 99.2         |
|    n_updates            | 22990        |
|    policy_gradient_loss | -0.0014      |
|    std                  | 67.5         |
|    value_loss           | 217          |
------------------------------------------
Iteration: 2300 | Episodes: 93400 | Median Reward: 12.61 | Max Reward: 48.13
Iteration: 2303 | Episodes: 93500 | Median Reward: 18.63 | Max Reward: 48.13
Iteration: 2305 | Episodes: 93600 | Median Reward: 22.60 | Max Reward: 48.13
Iteration: 2308 | Episodes: 93700 | Median Reward: 16.04 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.3         |
| time/                   |               |
|    fps                  | 345           |
|    iterations           | 2310          |
|    time_elapsed         | 27364         |
|    total_timesteps      | 9461760       |
| train/                  |               |
|    approx_kl            | 0.00016260275 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -185          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 99.8          |
|    n_updates            | 23090         |
|    policy_gradient_loss | -0.000493     |
|    std                  | 69.5          |
|    value_loss           | 218           |
-------------------------------------------
Iteration: 2310 | Episodes: 93800 | Median Reward: 18.13 | Max Reward: 48.13
Iteration: 2313 | Episodes: 93900 | Median Reward: 12.59 | Max Reward: 48.13
Iteration: 2315 | Episodes: 94000 | Median Reward: 9.82 | Max Reward: 48.13
Iteration: 2318 | Episodes: 94100 | Median Reward: 15.68 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.7         |
| time/                   |               |
|    fps                  | 345           |
|    iterations           | 2320          |
|    time_elapsed         | 27518         |
|    total_timesteps      | 9502720       |
| train/                  |               |
|    approx_kl            | 0.00022617167 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -185          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 110           |
|    n_updates            | 23190         |
|    policy_gradient_loss | -0.000503     |
|    std                  | 70.8          |
|    value_loss           | 238           |
-------------------------------------------
Iteration: 2320 | Episodes: 94200 | Median Reward: 14.20 | Max Reward: 48.13
Iteration: 2323 | Episodes: 94300 | Median Reward: 18.27 | Max Reward: 48.13
Iteration: 2325 | Episodes: 94400 | Median Reward: 14.77 | Max Reward: 48.13
Iteration: 2328 | Episodes: 94500 | Median Reward: 13.25 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.1        |
| time/                   |              |
|    fps                  | 344          |
|    iterations           | 2330         |
|    time_elapsed         | 27673        |
|    total_timesteps      | 9543680      |
| train/                  |              |
|    approx_kl            | 6.179602e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -186         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 150          |
|    n_updates            | 23290        |
|    policy_gradient_loss | -7.05e-05    |
|    std                  | 72.2         |
|    value_loss           | 319          |
------------------------------------------
Iteration: 2330 | Episodes: 94600 | Median Reward: 21.63 | Max Reward: 48.13
Iteration: 2332 | Episodes: 94700 | Median Reward: 13.82 | Max Reward: 48.13
Iteration: 2335 | Episodes: 94800 | Median Reward: 19.48 | Max Reward: 48.13
Iteration: 2337 | Episodes: 94900 | Median Reward: 14.13 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.8         |
| time/                   |               |
|    fps                  | 344           |
|    iterations           | 2340          |
|    time_elapsed         | 27829         |
|    total_timesteps      | 9584640       |
| train/                  |               |
|    approx_kl            | 0.00056328636 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -186          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 106           |
|    n_updates            | 23390         |
|    policy_gradient_loss | -0.000479     |
|    std                  | 72.9          |
|    value_loss           | 231           |
-------------------------------------------
Iteration: 2340 | Episodes: 95000 | Median Reward: 12.71 | Max Reward: 48.13
Iteration: 2342 | Episodes: 95100 | Median Reward: 13.88 | Max Reward: 48.13
Iteration: 2345 | Episodes: 95200 | Median Reward: 18.14 | Max Reward: 48.13
Iteration: 2347 | Episodes: 95300 | Median Reward: 15.93 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.6         |
| time/                   |               |
|    fps                  | 343           |
|    iterations           | 2350          |
|    time_elapsed         | 27996         |
|    total_timesteps      | 9625600       |
| train/                  |               |
|    approx_kl            | 0.00011174314 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -186          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 107           |
|    n_updates            | 23490         |
|    policy_gradient_loss | -4.18e-05     |
|    std                  | 74.3          |
|    value_loss           | 233           |
-------------------------------------------
Iteration: 2350 | Episodes: 95400 | Median Reward: 9.08 | Max Reward: 48.13
Iteration: 2352 | Episodes: 95500 | Median Reward: 9.60 | Max Reward: 48.13
Iteration: 2355 | Episodes: 95600 | Median Reward: 10.88 | Max Reward: 48.13
Iteration: 2357 | Episodes: 95700 | Median Reward: 12.70 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86           |
| time/                   |               |
|    fps                  | 343           |
|    iterations           | 2360          |
|    time_elapsed         | 28152         |
|    total_timesteps      | 9666560       |
| train/                  |               |
|    approx_kl            | 3.0128445e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -187          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 143           |
|    n_updates            | 23590         |
|    policy_gradient_loss | -0.000246     |
|    std                  | 75.8          |
|    value_loss           | 305           |
-------------------------------------------
Iteration: 2360 | Episodes: 95800 | Median Reward: 18.90 | Max Reward: 48.13
Iteration: 2362 | Episodes: 95900 | Median Reward: 14.16 | Max Reward: 48.13
Iteration: 2365 | Episodes: 96000 | Median Reward: 16.36 | Max Reward: 48.13
Iteration: 2367 | Episodes: 96100 | Median Reward: 11.77 | Max Reward: 48.13
Iteration: 2369 | Episodes: 96200 | Median Reward: 17.87 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -85.4       |
| time/                   |             |
|    fps                  | 342         |
|    iterations           | 2370        |
|    time_elapsed         | 28307       |
|    total_timesteps      | 9707520     |
| train/                  |             |
|    approx_kl            | 0.000680014 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -187        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 110         |
|    n_updates            | 23690       |
|    policy_gradient_loss | -0.00159    |
|    std                  | 77          |
|    value_loss           | 239         |
-----------------------------------------
Iteration: 2372 | Episodes: 96300 | Median Reward: 11.91 | Max Reward: 48.13
Iteration: 2374 | Episodes: 96400 | Median Reward: 12.81 | Max Reward: 48.13
Iteration: 2377 | Episodes: 96500 | Median Reward: 22.71 | Max Reward: 48.13
Iteration: 2379 | Episodes: 96600 | Median Reward: 13.05 | Max Reward: 48.13
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -88.3          |
| time/                   |                |
|    fps                  | 342            |
|    iterations           | 2380           |
|    time_elapsed         | 28460          |
|    total_timesteps      | 9748480        |
| train/                  |                |
|    approx_kl            | 1.36274175e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -188           |
|    explained_variance   | 0              |
|    learning_rate        | 0.0005         |
|    loss                 | 75.2           |
|    n_updates            | 23790          |
|    policy_gradient_loss | -0.000131      |
|    std                  | 77.9           |
|    value_loss           | 169            |
--------------------------------------------
Iteration: 2382 | Episodes: 96700 | Median Reward: 12.91 | Max Reward: 48.13
Iteration: 2384 | Episodes: 96800 | Median Reward: 12.22 | Max Reward: 48.13
Iteration: 2387 | Episodes: 96900 | Median Reward: 16.43 | Max Reward: 48.13
Iteration: 2389 | Episodes: 97000 | Median Reward: 14.15 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.4         |
| time/                   |               |
|    fps                  | 342           |
|    iterations           | 2390          |
|    time_elapsed         | 28614         |
|    total_timesteps      | 9789440       |
| train/                  |               |
|    approx_kl            | 0.00017799114 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -188          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 96            |
|    n_updates            | 23890         |
|    policy_gradient_loss | -0.000306     |
|    std                  | 79.6          |
|    value_loss           | 211           |
-------------------------------------------
Iteration: 2392 | Episodes: 97100 | Median Reward: 13.88 | Max Reward: 48.13
Iteration: 2394 | Episodes: 97200 | Median Reward: 14.77 | Max Reward: 48.13
Iteration: 2397 | Episodes: 97300 | Median Reward: 12.32 | Max Reward: 48.13
Iteration: 2399 | Episodes: 97400 | Median Reward: 12.70 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -89.9        |
| time/                   |              |
|    fps                  | 341          |
|    iterations           | 2400         |
|    time_elapsed         | 28768        |
|    total_timesteps      | 9830400      |
| train/                  |              |
|    approx_kl            | 6.269972e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -189         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 91.1         |
|    n_updates            | 23990        |
|    policy_gradient_loss | -2.17e-05    |
|    std                  | 81.7         |
|    value_loss           | 201          |
------------------------------------------
Iteration: 2402 | Episodes: 97500 | Median Reward: 11.41 | Max Reward: 48.13
Iteration: 2404 | Episodes: 97600 | Median Reward: 17.16 | Max Reward: 48.13
Iteration: 2406 | Episodes: 97700 | Median Reward: 12.38 | Max Reward: 48.13
Iteration: 2409 | Episodes: 97800 | Median Reward: 17.78 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -89.6       |
| time/                   |             |
|    fps                  | 341         |
|    iterations           | 2410        |
|    time_elapsed         | 28924       |
|    total_timesteps      | 9871360     |
| train/                  |             |
|    approx_kl            | 6.11898e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -189        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 113         |
|    n_updates            | 24090       |
|    policy_gradient_loss | -4.44e-05   |
|    std                  | 82.4        |
|    value_loss           | 246         |
-----------------------------------------
Iteration: 2411 | Episodes: 97900 | Median Reward: 11.42 | Max Reward: 48.13
Iteration: 2414 | Episodes: 98000 | Median Reward: 16.52 | Max Reward: 48.13
Iteration: 2416 | Episodes: 98100 | Median Reward: 13.07 | Max Reward: 48.13
Iteration: 2419 | Episodes: 98200 | Median Reward: 16.45 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.7        |
| time/                   |              |
|    fps                  | 340          |
|    iterations           | 2420         |
|    time_elapsed         | 29080        |
|    total_timesteps      | 9912320      |
| train/                  |              |
|    approx_kl            | 7.433904e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -189         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 150          |
|    n_updates            | 24190        |
|    policy_gradient_loss | -0.000126    |
|    std                  | 83.3         |
|    value_loss           | 318          |
------------------------------------------
Iteration: 2421 | Episodes: 98300 | Median Reward: 16.41 | Max Reward: 48.13
Iteration: 2424 | Episodes: 98400 | Median Reward: 13.84 | Max Reward: 48.13
Iteration: 2426 | Episodes: 98500 | Median Reward: 7.43 | Max Reward: 48.13
Iteration: 2429 | Episodes: 98600 | Median Reward: 13.96 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -84.2       |
| time/                   |             |
|    fps                  | 340         |
|    iterations           | 2430        |
|    time_elapsed         | 29236       |
|    total_timesteps      | 9953280     |
| train/                  |             |
|    approx_kl            | 0.013094718 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.4         |
|    entropy_loss         | -190        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 110         |
|    n_updates            | 24290       |
|    policy_gradient_loss | -0.00921    |
|    std                  | 85.6        |
|    value_loss           | 239         |
-----------------------------------------
Iteration: 2431 | Episodes: 98700 | Median Reward: 11.75 | Max Reward: 48.13
Iteration: 2434 | Episodes: 98800 | Median Reward: 17.69 | Max Reward: 48.13
Iteration: 2436 | Episodes: 98900 | Median Reward: 11.17 | Max Reward: 48.13
Iteration: 2438 | Episodes: 99000 | Median Reward: 10.73 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91           |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 2440          |
|    time_elapsed         | 29397         |
|    total_timesteps      | 9994240       |
| train/                  |               |
|    approx_kl            | 0.00058153714 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -190          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 101           |
|    n_updates            | 24390         |
|    policy_gradient_loss | -0.000522     |
|    std                  | 87.4          |
|    value_loss           | 220           |
-------------------------------------------
Iteration: 2441 | Episodes: 99100 | Median Reward: 6.67 | Max Reward: 48.13
Iteration: 2443 | Episodes: 99200 | Median Reward: 16.87 | Max Reward: 48.13
Iteration: 2446 | Episodes: 99300 | Median Reward: 14.23 | Max Reward: 48.13
Iteration: 2448 | Episodes: 99400 | Median Reward: 8.21 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.7         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 2450          |
|    time_elapsed         | 29560         |
|    total_timesteps      | 10035200      |
| train/                  |               |
|    approx_kl            | 0.00016318142 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -191          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 96.8          |
|    n_updates            | 24490         |
|    policy_gradient_loss | -0.000559     |
|    std                  | 88.8          |
|    value_loss           | 213           |
-------------------------------------------
Iteration: 2451 | Episodes: 99500 | Median Reward: 17.44 | Max Reward: 48.13
Iteration: 2453 | Episodes: 99600 | Median Reward: 19.49 | Max Reward: 48.13
Iteration: 2456 | Episodes: 99700 | Median Reward: 7.84 | Max Reward: 48.13
Iteration: 2458 | Episodes: 99800 | Median Reward: 9.08 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -90.1         |
| time/                   |               |
|    fps                  | 339           |
|    iterations           | 2460          |
|    time_elapsed         | 29713         |
|    total_timesteps      | 10076160      |
| train/                  |               |
|    approx_kl            | 0.00048573286 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -191          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 110           |
|    n_updates            | 24590         |
|    policy_gradient_loss | -0.0013       |
|    std                  | 90.3          |
|    value_loss           | 239           |
-------------------------------------------
Iteration: 2461 | Episodes: 99900 | Median Reward: 4.85 | Max Reward: 48.13
Iteration: 2463 | Episodes: 100000 | Median Reward: 15.06 | Max Reward: 48.13
Iteration: 2466 | Episodes: 100100 | Median Reward: 15.36 | Max Reward: 48.13
Iteration: 2468 | Episodes: 100200 | Median Reward: 8.49 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.3         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 2470          |
|    time_elapsed         | 29866         |
|    total_timesteps      | 10117120      |
| train/                  |               |
|    approx_kl            | 3.7833524e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -191          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 108           |
|    n_updates            | 24690         |
|    policy_gradient_loss | 1.48e-05      |
|    std                  | 91.9          |
|    value_loss           | 234           |
-------------------------------------------
Iteration: 2471 | Episodes: 100300 | Median Reward: 16.52 | Max Reward: 48.13
Iteration: 2473 | Episodes: 100400 | Median Reward: 11.48 | Max Reward: 48.13
Iteration: 2475 | Episodes: 100500 | Median Reward: 4.42 | Max Reward: 48.13
Iteration: 2478 | Episodes: 100600 | Median Reward: 16.73 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.9         |
| time/                   |               |
|    fps                  | 338           |
|    iterations           | 2480          |
|    time_elapsed         | 30021         |
|    total_timesteps      | 10158080      |
| train/                  |               |
|    approx_kl            | 3.8545404e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -192          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 127           |
|    n_updates            | 24790         |
|    policy_gradient_loss | 1.64e-06      |
|    std                  | 93.3          |
|    value_loss           | 274           |
-------------------------------------------
Iteration: 2480 | Episodes: 100700 | Median Reward: 12.50 | Max Reward: 48.13
Iteration: 2483 | Episodes: 100800 | Median Reward: 7.98 | Max Reward: 48.13
Iteration: 2485 | Episodes: 100900 | Median Reward: 2.73 | Max Reward: 48.13
Iteration: 2488 | Episodes: 101000 | Median Reward: 10.70 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.1         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 2490          |
|    time_elapsed         | 30175         |
|    total_timesteps      | 10199040      |
| train/                  |               |
|    approx_kl            | 8.0324855e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -192          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 139           |
|    n_updates            | 24890         |
|    policy_gradient_loss | -0.000283     |
|    std                  | 94.2          |
|    value_loss           | 297           |
-------------------------------------------
Iteration: 2490 | Episodes: 101100 | Median Reward: 18.27 | Max Reward: 48.13
Iteration: 2493 | Episodes: 101200 | Median Reward: 5.16 | Max Reward: 48.13
Iteration: 2495 | Episodes: 101300 | Median Reward: 16.33 | Max Reward: 48.13
Iteration: 2498 | Episodes: 101400 | Median Reward: 15.51 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.8         |
| time/                   |               |
|    fps                  | 337           |
|    iterations           | 2500          |
|    time_elapsed         | 30328         |
|    total_timesteps      | 10240000      |
| train/                  |               |
|    approx_kl            | 9.2564034e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -192          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 119           |
|    n_updates            | 24990         |
|    policy_gradient_loss | -0.000741     |
|    std                  | 96.3          |
|    value_loss           | 258           |
-------------------------------------------
Iteration: 2500 | Episodes: 101500 | Median Reward: 12.57 | Max Reward: 48.13
Iteration: 2503 | Episodes: 101600 | Median Reward: 9.07 | Max Reward: 48.13
Iteration: 2505 | Episodes: 101700 | Median Reward: 10.76 | Max Reward: 48.13
Iteration: 2508 | Episodes: 101800 | Median Reward: 7.82 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -90          |
| time/                   |              |
|    fps                  | 337          |
|    iterations           | 2510         |
|    time_elapsed         | 30483        |
|    total_timesteps      | 10280960     |
| train/                  |              |
|    approx_kl            | 0.0020186764 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -193         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 103          |
|    n_updates            | 25090        |
|    policy_gradient_loss | -0.000372    |
|    std                  | 98.4         |
|    value_loss           | 226          |
------------------------------------------
Iteration: 2510 | Episodes: 101900 | Median Reward: 11.35 | Max Reward: 48.13
Iteration: 2512 | Episodes: 102000 | Median Reward: 8.44 | Max Reward: 48.13
Iteration: 2515 | Episodes: 102100 | Median Reward: 10.48 | Max Reward: 48.13
Iteration: 2517 | Episodes: 102200 | Median Reward: 19.18 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -94.3       |
| time/                   |             |
|    fps                  | 336         |
|    iterations           | 2520        |
|    time_elapsed         | 30637       |
|    total_timesteps      | 10321920    |
| train/                  |             |
|    approx_kl            | 2.43049e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -193        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 63.7        |
|    n_updates            | 25190       |
|    policy_gradient_loss | -3.37e-05   |
|    std                  | 99.5        |
|    value_loss           | 147         |
-----------------------------------------
Iteration: 2520 | Episodes: 102300 | Median Reward: 4.74 | Max Reward: 48.13
Iteration: 2522 | Episodes: 102400 | Median Reward: 13.74 | Max Reward: 48.13
Iteration: 2525 | Episodes: 102500 | Median Reward: 14.75 | Max Reward: 48.13
Iteration: 2527 | Episodes: 102600 | Median Reward: 17.11 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.2         |
| time/                   |               |
|    fps                  | 336           |
|    iterations           | 2530          |
|    time_elapsed         | 30791         |
|    total_timesteps      | 10362880      |
| train/                  |               |
|    approx_kl            | 0.00038932453 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -193          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 74.8          |
|    n_updates            | 25290         |
|    policy_gradient_loss | -0.000442     |
|    std                  | 100           |
|    value_loss           | 169           |
-------------------------------------------
Iteration: 2530 | Episodes: 102700 | Median Reward: 18.67 | Max Reward: 48.13
Iteration: 2532 | Episodes: 102800 | Median Reward: 20.43 | Max Reward: 48.13
Iteration: 2535 | Episodes: 102900 | Median Reward: 13.41 | Max Reward: 48.13
Iteration: 2537 | Episodes: 103000 | Median Reward: 13.28 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.7        |
| time/                   |              |
|    fps                  | 336          |
|    iterations           | 2540         |
|    time_elapsed         | 30944        |
|    total_timesteps      | 10403840     |
| train/                  |              |
|    approx_kl            | 4.925659e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -194         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 122          |
|    n_updates            | 25390        |
|    policy_gradient_loss | -0.000289    |
|    std                  | 102          |
|    value_loss           | 262          |
------------------------------------------
Iteration: 2540 | Episodes: 103100 | Median Reward: 12.03 | Max Reward: 48.13
Iteration: 2542 | Episodes: 103200 | Median Reward: 15.99 | Max Reward: 48.13
Iteration: 2545 | Episodes: 103300 | Median Reward: 11.44 | Max Reward: 48.13
Iteration: 2547 | Episodes: 103400 | Median Reward: 10.82 | Max Reward: 48.13
Iteration: 2549 | Episodes: 103500 | Median Reward: 13.19 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87.1        |
| time/                   |              |
|    fps                  | 335          |
|    iterations           | 2550         |
|    time_elapsed         | 31111        |
|    total_timesteps      | 10444800     |
| train/                  |              |
|    approx_kl            | 0.0010737288 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -194         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 111          |
|    n_updates            | 25490        |
|    policy_gradient_loss | -0.00159     |
|    std                  | 105          |
|    value_loss           | 241          |
------------------------------------------
Iteration: 2552 | Episodes: 103600 | Median Reward: 12.08 | Max Reward: 48.13
Iteration: 2554 | Episodes: 103700 | Median Reward: 14.53 | Max Reward: 48.13
Iteration: 2557 | Episodes: 103800 | Median Reward: 15.45 | Max Reward: 48.13
Iteration: 2559 | Episodes: 103900 | Median Reward: 19.08 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.5        |
| time/                   |              |
|    fps                  | 335          |
|    iterations           | 2560         |
|    time_elapsed         | 31268        |
|    total_timesteps      | 10485760     |
| train/                  |              |
|    approx_kl            | 6.834752e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -195         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 125          |
|    n_updates            | 25590        |
|    policy_gradient_loss | -0.000344    |
|    std                  | 107          |
|    value_loss           | 270          |
------------------------------------------
Iteration: 2562 | Episodes: 104000 | Median Reward: 15.37 | Max Reward: 48.13
Iteration: 2564 | Episodes: 104100 | Median Reward: 2.82 | Max Reward: 48.13
Iteration: 2567 | Episodes: 104200 | Median Reward: 5.61 | Max Reward: 48.13
Iteration: 2569 | Episodes: 104300 | Median Reward: 16.06 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -90.1         |
| time/                   |               |
|    fps                  | 334           |
|    iterations           | 2570          |
|    time_elapsed         | 31423         |
|    total_timesteps      | 10526720      |
| train/                  |               |
|    approx_kl            | 0.00016752987 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -195          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 86.9          |
|    n_updates            | 25690         |
|    policy_gradient_loss | -0.000308     |
|    std                  | 109           |
|    value_loss           | 194           |
-------------------------------------------
Iteration: 2572 | Episodes: 104400 | Median Reward: 9.69 | Max Reward: 48.13
Iteration: 2574 | Episodes: 104500 | Median Reward: 9.26 | Max Reward: 48.13
Iteration: 2577 | Episodes: 104600 | Median Reward: 13.26 | Max Reward: 48.13
Iteration: 2579 | Episodes: 104700 | Median Reward: 8.36 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.6         |
| time/                   |               |
|    fps                  | 334           |
|    iterations           | 2580          |
|    time_elapsed         | 31577         |
|    total_timesteps      | 10567680      |
| train/                  |               |
|    approx_kl            | 5.4743927e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -196          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 90.6          |
|    n_updates            | 25790         |
|    policy_gradient_loss | 0.000162      |
|    std                  | 111           |
|    value_loss           | 201           |
-------------------------------------------
Iteration: 2582 | Episodes: 104800 | Median Reward: 7.99 | Max Reward: 48.13
Iteration: 2584 | Episodes: 104900 | Median Reward: -1.99 | Max Reward: 48.13
Iteration: 2586 | Episodes: 105000 | Median Reward: 18.77 | Max Reward: 48.13
Iteration: 2589 | Episodes: 105100 | Median Reward: 6.95 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.8         |
| time/                   |               |
|    fps                  | 334           |
|    iterations           | 2590          |
|    time_elapsed         | 31733         |
|    total_timesteps      | 10608640      |
| train/                  |               |
|    approx_kl            | 1.8831182e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -196          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 128           |
|    n_updates            | 25890         |
|    policy_gradient_loss | -0.000196     |
|    std                  | 112           |
|    value_loss           | 277           |
-------------------------------------------
Iteration: 2591 | Episodes: 105200 | Median Reward: 11.80 | Max Reward: 48.13
Iteration: 2594 | Episodes: 105300 | Median Reward: 15.13 | Max Reward: 48.13
Iteration: 2596 | Episodes: 105400 | Median Reward: 13.79 | Max Reward: 48.13
Iteration: 2599 | Episodes: 105500 | Median Reward: 15.27 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.6        |
| time/                   |              |
|    fps                  | 333          |
|    iterations           | 2600         |
|    time_elapsed         | 31889        |
|    total_timesteps      | 10649600     |
| train/                  |              |
|    approx_kl            | 6.116461e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -196         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 130          |
|    n_updates            | 25990        |
|    policy_gradient_loss | -7.02e-05    |
|    std                  | 113          |
|    value_loss           | 281          |
------------------------------------------
Iteration: 2601 | Episodes: 105600 | Median Reward: 16.01 | Max Reward: 48.13
Iteration: 2604 | Episodes: 105700 | Median Reward: 16.07 | Max Reward: 48.13
Iteration: 2606 | Episodes: 105800 | Median Reward: 4.73 | Max Reward: 48.13
Iteration: 2609 | Episodes: 105900 | Median Reward: 5.37 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -94.2         |
| time/                   |               |
|    fps                  | 333           |
|    iterations           | 2610          |
|    time_elapsed         | 32043         |
|    total_timesteps      | 10690560      |
| train/                  |               |
|    approx_kl            | 0.00016047097 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -197          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 84.7          |
|    n_updates            | 26090         |
|    policy_gradient_loss | -0.000439     |
|    std                  | 114           |
|    value_loss           | 189           |
-------------------------------------------
Iteration: 2611 | Episodes: 106000 | Median Reward: 12.03 | Max Reward: 48.13
Iteration: 2614 | Episodes: 106100 | Median Reward: 9.48 | Max Reward: 48.13
Iteration: 2616 | Episodes: 106200 | Median Reward: 17.51 | Max Reward: 48.13
Iteration: 2618 | Episodes: 106300 | Median Reward: 20.96 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -86.3       |
| time/                   |             |
|    fps                  | 333         |
|    iterations           | 2620        |
|    time_elapsed         | 32199       |
|    total_timesteps      | 10731520    |
| train/                  |             |
|    approx_kl            | 0.005774784 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -197        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 118         |
|    n_updates            | 26190       |
|    policy_gradient_loss | -0.00685    |
|    std                  | 117         |
|    value_loss           | 256         |
-----------------------------------------
Iteration: 2621 | Episodes: 106400 | Median Reward: 15.36 | Max Reward: 48.13
Iteration: 2623 | Episodes: 106500 | Median Reward: 16.58 | Max Reward: 48.13
Iteration: 2626 | Episodes: 106600 | Median Reward: 13.13 | Max Reward: 48.13
Iteration: 2628 | Episodes: 106700 | Median Reward: 4.95 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -94.3        |
| time/                   |              |
|    fps                  | 332          |
|    iterations           | 2630         |
|    time_elapsed         | 32354        |
|    total_timesteps      | 10772480     |
| train/                  |              |
|    approx_kl            | 4.028523e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -197         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 66.7         |
|    n_updates            | 26290        |
|    policy_gradient_loss | -5.75e-05    |
|    std                  | 118          |
|    value_loss           | 154          |
------------------------------------------
Iteration: 2631 | Episodes: 106800 | Median Reward: 9.33 | Max Reward: 48.13
Iteration: 2633 | Episodes: 106900 | Median Reward: 16.71 | Max Reward: 48.13
Iteration: 2636 | Episodes: 107000 | Median Reward: 7.12 | Max Reward: 48.13
Iteration: 2638 | Episodes: 107100 | Median Reward: 8.24 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92           |
| time/                   |               |
|    fps                  | 332           |
|    iterations           | 2640          |
|    time_elapsed         | 32517         |
|    total_timesteps      | 10813440      |
| train/                  |               |
|    approx_kl            | 2.3859859e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -198          |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0005        |
|    loss                 | 88.9          |
|    n_updates            | 26390         |
|    policy_gradient_loss | 5.47e-05      |
|    std                  | 120           |
|    value_loss           | 198           |
-------------------------------------------
Iteration: 2641 | Episodes: 107200 | Median Reward: 7.45 | Max Reward: 48.13
Iteration: 2643 | Episodes: 107300 | Median Reward: 6.30 | Max Reward: 48.13
Iteration: 2646 | Episodes: 107400 | Median Reward: 11.06 | Max Reward: 48.13
Iteration: 2648 | Episodes: 107500 | Median Reward: 10.88 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -95.9        |
| time/                   |              |
|    fps                  | 332          |
|    iterations           | 2650         |
|    time_elapsed         | 32671        |
|    total_timesteps      | 10854400     |
| train/                  |              |
|    approx_kl            | 0.0003878576 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -198         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 75.6         |
|    n_updates            | 26490        |
|    policy_gradient_loss | -0.00049     |
|    std                  | 122          |
|    value_loss           | 171          |
------------------------------------------
Iteration: 2651 | Episodes: 107600 | Median Reward: 4.81 | Max Reward: 48.13
Iteration: 2653 | Episodes: 107700 | Median Reward: 16.05 | Max Reward: 48.13
Iteration: 2655 | Episodes: 107800 | Median Reward: 12.09 | Max Reward: 48.13
Iteration: 2658 | Episodes: 107900 | Median Reward: 10.05 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.7        |
| time/                   |              |
|    fps                  | 331          |
|    iterations           | 2660         |
|    time_elapsed         | 32825        |
|    total_timesteps      | 10895360     |
| train/                  |              |
|    approx_kl            | 5.564565e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -198         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 104          |
|    n_updates            | 26590        |
|    policy_gradient_loss | 1.47e-05     |
|    std                  | 124          |
|    value_loss           | 227          |
------------------------------------------
Iteration: 2660 | Episodes: 108000 | Median Reward: 15.07 | Max Reward: 48.13
Iteration: 2663 | Episodes: 108100 | Median Reward: 8.51 | Max Reward: 48.13
Iteration: 2665 | Episodes: 108200 | Median Reward: 7.60 | Max Reward: 48.13
Iteration: 2668 | Episodes: 108300 | Median Reward: 20.20 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.4         |
| time/                   |               |
|    fps                  | 331           |
|    iterations           | 2670          |
|    time_elapsed         | 32979         |
|    total_timesteps      | 10936320      |
| train/                  |               |
|    approx_kl            | 0.00039526515 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -199          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 112           |
|    n_updates            | 26690         |
|    policy_gradient_loss | -0.000906     |
|    std                  | 125           |
|    value_loss           | 245           |
-------------------------------------------
Iteration: 2670 | Episodes: 108400 | Median Reward: 8.12 | Max Reward: 48.13
Iteration: 2673 | Episodes: 108500 | Median Reward: 13.48 | Max Reward: 48.13
Iteration: 2675 | Episodes: 108600 | Median Reward: 3.96 | Max Reward: 48.13
Iteration: 2678 | Episodes: 108700 | Median Reward: 9.17 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -97.5        |
| time/                   |              |
|    fps                  | 331          |
|    iterations           | 2680         |
|    time_elapsed         | 33133        |
|    total_timesteps      | 10977280     |
| train/                  |              |
|    approx_kl            | 7.734579e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -199         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 78.5         |
|    n_updates            | 26790        |
|    policy_gradient_loss | -0.000338    |
|    std                  | 127          |
|    value_loss           | 177          |
------------------------------------------
Iteration: 2680 | Episodes: 108800 | Median Reward: 6.84 | Max Reward: 48.13
Iteration: 2683 | Episodes: 108900 | Median Reward: 14.28 | Max Reward: 48.13
Iteration: 2685 | Episodes: 109000 | Median Reward: 13.73 | Max Reward: 48.13
Iteration: 2688 | Episodes: 109100 | Median Reward: 12.84 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -90.6         |
| time/                   |               |
|    fps                  | 330           |
|    iterations           | 2690          |
|    time_elapsed         | 33287         |
|    total_timesteps      | 11018240      |
| train/                  |               |
|    approx_kl            | 1.8751423e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -199          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 113           |
|    n_updates            | 26890         |
|    policy_gradient_loss | -0.00011      |
|    std                  | 128           |
|    value_loss           | 246           |
-------------------------------------------
Iteration: 2690 | Episodes: 109200 | Median Reward: 10.57 | Max Reward: 48.13
Iteration: 2692 | Episodes: 109300 | Median Reward: 15.51 | Max Reward: 48.13
Iteration: 2695 | Episodes: 109400 | Median Reward: 4.35 | Max Reward: 48.13
Iteration: 2697 | Episodes: 109500 | Median Reward: 8.39 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.1         |
| time/                   |               |
|    fps                  | 330           |
|    iterations           | 2700          |
|    time_elapsed         | 33442         |
|    total_timesteps      | 11059200      |
| train/                  |               |
|    approx_kl            | 3.9145467e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -200          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 117           |
|    n_updates            | 26990         |
|    policy_gradient_loss | -0.000319     |
|    std                  | 129           |
|    value_loss           | 254           |
-------------------------------------------
Iteration: 2700 | Episodes: 109600 | Median Reward: 18.39 | Max Reward: 48.13
Iteration: 2702 | Episodes: 109700 | Median Reward: 8.49 | Max Reward: 48.13
Iteration: 2705 | Episodes: 109800 | Median Reward: 11.86 | Max Reward: 48.13
Iteration: 2707 | Episodes: 109900 | Median Reward: 10.23 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.9         |
| time/                   |               |
|    fps                  | 330           |
|    iterations           | 2710          |
|    time_elapsed         | 33595         |
|    total_timesteps      | 11100160      |
| train/                  |               |
|    approx_kl            | 3.7060658e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -200          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 79.7          |
|    n_updates            | 27090         |
|    policy_gradient_loss | -0.000461     |
|    std                  | 131           |
|    value_loss           | 180           |
-------------------------------------------
Iteration: 2710 | Episodes: 110000 | Median Reward: 7.14 | Max Reward: 48.13
Iteration: 2712 | Episodes: 110100 | Median Reward: 17.74 | Max Reward: 48.13
Iteration: 2715 | Episodes: 110200 | Median Reward: 4.39 | Max Reward: 48.13
Iteration: 2717 | Episodes: 110300 | Median Reward: 12.87 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -102          |
| time/                   |               |
|    fps                  | 330           |
|    iterations           | 2720          |
|    time_elapsed         | 33749         |
|    total_timesteps      | 11141120      |
| train/                  |               |
|    approx_kl            | 0.00017842239 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -200          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 94.7          |
|    n_updates            | 27190         |
|    policy_gradient_loss | -0.00194      |
|    std                  | 133           |
|    value_loss           | 209           |
-------------------------------------------
Iteration: 2720 | Episodes: 110400 | Median Reward: 2.00 | Max Reward: 48.13
Iteration: 2722 | Episodes: 110500 | Median Reward: 16.67 | Max Reward: 48.13
Iteration: 2725 | Episodes: 110600 | Median Reward: 4.03 | Max Reward: 48.13
Iteration: 2727 | Episodes: 110700 | Median Reward: 12.09 | Max Reward: 48.13
Iteration: 2729 | Episodes: 110800 | Median Reward: 14.07 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.2         |
| time/                   |               |
|    fps                  | 329           |
|    iterations           | 2730          |
|    time_elapsed         | 33902         |
|    total_timesteps      | 11182080      |
| train/                  |               |
|    approx_kl            | 0.00025668638 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -200          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 111           |
|    n_updates            | 27290         |
|    policy_gradient_loss | -0.000248     |
|    std                  | 134           |
|    value_loss           | 242           |
-------------------------------------------
Iteration: 2732 | Episodes: 110900 | Median Reward: 14.71 | Max Reward: 48.13
Iteration: 2734 | Episodes: 111000 | Median Reward: 14.97 | Max Reward: 48.13
Iteration: 2737 | Episodes: 111100 | Median Reward: 4.77 | Max Reward: 48.13
Iteration: 2739 | Episodes: 111200 | Median Reward: 12.04 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -93.6        |
| time/                   |              |
|    fps                  | 329          |
|    iterations           | 2740         |
|    time_elapsed         | 34063        |
|    total_timesteps      | 11223040     |
| train/                  |              |
|    approx_kl            | 7.844315e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -201         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 103          |
|    n_updates            | 27390        |
|    policy_gradient_loss | -0.00074     |
|    std                  | 136          |
|    value_loss           | 225          |
------------------------------------------
Iteration: 2742 | Episodes: 111300 | Median Reward: 18.35 | Max Reward: 48.13
Iteration: 2744 | Episodes: 111400 | Median Reward: 13.03 | Max Reward: 48.13
Iteration: 2747 | Episodes: 111500 | Median Reward: 14.40 | Max Reward: 48.13
Iteration: 2749 | Episodes: 111600 | Median Reward: 14.30 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -90.7        |
| time/                   |              |
|    fps                  | 329          |
|    iterations           | 2750         |
|    time_elapsed         | 34218        |
|    total_timesteps      | 11264000     |
| train/                  |              |
|    approx_kl            | 5.535595e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -201         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 125          |
|    n_updates            | 27490        |
|    policy_gradient_loss | 0.000469     |
|    std                  | 138          |
|    value_loss           | 271          |
------------------------------------------
Iteration: 2752 | Episodes: 111700 | Median Reward: 15.41 | Max Reward: 48.13
Iteration: 2754 | Episodes: 111800 | Median Reward: 11.87 | Max Reward: 48.13
Iteration: 2757 | Episodes: 111900 | Median Reward: 13.67 | Max Reward: 48.13
Iteration: 2759 | Episodes: 112000 | Median Reward: 20.00 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.8        |
| time/                   |              |
|    fps                  | 328          |
|    iterations           | 2760         |
|    time_elapsed         | 34368        |
|    total_timesteps      | 11304960     |
| train/                  |              |
|    approx_kl            | 0.0005468429 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -202         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 113          |
|    n_updates            | 27590        |
|    policy_gradient_loss | -0.00173     |
|    std                  | 142          |
|    value_loss           | 245          |
------------------------------------------
Iteration: 2762 | Episodes: 112100 | Median Reward: 15.58 | Max Reward: 48.13
Iteration: 2764 | Episodes: 112200 | Median Reward: 14.07 | Max Reward: 48.13
Iteration: 2766 | Episodes: 112300 | Median Reward: 9.33 | Max Reward: 48.13
Iteration: 2769 | Episodes: 112400 | Median Reward: 13.38 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.2         |
| time/                   |               |
|    fps                  | 328           |
|    iterations           | 2770          |
|    time_elapsed         | 34518         |
|    total_timesteps      | 11345920      |
| train/                  |               |
|    approx_kl            | 0.00026250264 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -202          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 109           |
|    n_updates            | 27690         |
|    policy_gradient_loss | -0.000355     |
|    std                  | 146           |
|    value_loss           | 239           |
-------------------------------------------
Iteration: 2771 | Episodes: 112500 | Median Reward: 10.48 | Max Reward: 48.13
Iteration: 2774 | Episodes: 112600 | Median Reward: 6.96 | Max Reward: 48.13
Iteration: 2776 | Episodes: 112700 | Median Reward: 7.09 | Max Reward: 48.13
Iteration: 2779 | Episodes: 112800 | Median Reward: 16.08 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.6         |
| time/                   |               |
|    fps                  | 328           |
|    iterations           | 2780          |
|    time_elapsed         | 34669         |
|    total_timesteps      | 11386880      |
| train/                  |               |
|    approx_kl            | 0.00012677937 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -203          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 95.3          |
|    n_updates            | 27790         |
|    policy_gradient_loss | -0.0011       |
|    std                  | 148           |
|    value_loss           | 211           |
-------------------------------------------
Iteration: 2781 | Episodes: 112900 | Median Reward: 21.22 | Max Reward: 48.13
Iteration: 2784 | Episodes: 113000 | Median Reward: 13.84 | Max Reward: 48.13
Iteration: 2786 | Episodes: 113100 | Median Reward: 13.16 | Max Reward: 48.13
Iteration: 2789 | Episodes: 113200 | Median Reward: 7.38 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.3         |
| time/                   |               |
|    fps                  | 328           |
|    iterations           | 2790          |
|    time_elapsed         | 34820         |
|    total_timesteps      | 11427840      |
| train/                  |               |
|    approx_kl            | 0.00011184058 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -203          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 76.7          |
|    n_updates            | 27890         |
|    policy_gradient_loss | -0.000636     |
|    std                  | 151           |
|    value_loss           | 174           |
-------------------------------------------
Iteration: 2791 | Episodes: 113300 | Median Reward: 15.47 | Max Reward: 48.13
Iteration: 2794 | Episodes: 113400 | Median Reward: 21.14 | Max Reward: 48.13
Iteration: 2796 | Episodes: 113500 | Median Reward: 8.38 | Max Reward: 48.13
Iteration: 2799 | Episodes: 113600 | Median Reward: 14.76 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.8         |
| time/                   |               |
|    fps                  | 327           |
|    iterations           | 2800          |
|    time_elapsed         | 34970         |
|    total_timesteps      | 11468800      |
| train/                  |               |
|    approx_kl            | 6.8570953e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -203          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 131           |
|    n_updates            | 27990         |
|    policy_gradient_loss | 4.09e-05      |
|    std                  | 152           |
|    value_loss           | 282           |
-------------------------------------------
Iteration: 2801 | Episodes: 113700 | Median Reward: 11.71 | Max Reward: 48.13
Iteration: 2803 | Episodes: 113800 | Median Reward: 20.42 | Max Reward: 48.13
Iteration: 2806 | Episodes: 113900 | Median Reward: 6.47 | Max Reward: 48.13
Iteration: 2808 | Episodes: 114000 | Median Reward: 15.79 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.4         |
| time/                   |               |
|    fps                  | 327           |
|    iterations           | 2810          |
|    time_elapsed         | 35133         |
|    total_timesteps      | 11509760      |
| train/                  |               |
|    approx_kl            | 0.00013705483 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -204          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 106           |
|    n_updates            | 28090         |
|    policy_gradient_loss | -0.000317     |
|    std                  | 154           |
|    value_loss           | 232           |
-------------------------------------------
Iteration: 2811 | Episodes: 114100 | Median Reward: 15.08 | Max Reward: 48.13
Iteration: 2813 | Episodes: 114200 | Median Reward: 4.47 | Max Reward: 48.13
Iteration: 2816 | Episodes: 114300 | Median Reward: 14.01 | Max Reward: 48.13
Iteration: 2818 | Episodes: 114400 | Median Reward: 17.77 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -90.2        |
| time/                   |              |
|    fps                  | 327          |
|    iterations           | 2820         |
|    time_elapsed         | 35284        |
|    total_timesteps      | 11550720     |
| train/                  |              |
|    approx_kl            | 9.497962e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -204         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 99           |
|    n_updates            | 28190        |
|    policy_gradient_loss | -0.000447    |
|    std                  | 156          |
|    value_loss           | 218          |
------------------------------------------
Iteration: 2821 | Episodes: 114500 | Median Reward: 11.06 | Max Reward: 48.13
Iteration: 2823 | Episodes: 114600 | Median Reward: 15.06 | Max Reward: 48.13
Iteration: 2826 | Episodes: 114700 | Median Reward: 8.35 | Max Reward: 48.13
Iteration: 2828 | Episodes: 114800 | Median Reward: 6.82 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87.8        |
| time/                   |              |
|    fps                  | 327          |
|    iterations           | 2830         |
|    time_elapsed         | 35433        |
|    total_timesteps      | 11591680     |
| train/                  |              |
|    approx_kl            | 8.914372e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -204         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 104          |
|    n_updates            | 28290        |
|    policy_gradient_loss | -6.13e-05    |
|    std                  | 158          |
|    value_loss           | 229          |
------------------------------------------
Iteration: 2831 | Episodes: 114900 | Median Reward: 12.98 | Max Reward: 48.13
Iteration: 2833 | Episodes: 115000 | Median Reward: 8.35 | Max Reward: 48.13
Iteration: 2835 | Episodes: 115100 | Median Reward: 13.03 | Max Reward: 48.13
Iteration: 2838 | Episodes: 115200 | Median Reward: 12.31 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.2         |
| time/                   |               |
|    fps                  | 326           |
|    iterations           | 2840          |
|    time_elapsed         | 35584         |
|    total_timesteps      | 11632640      |
| train/                  |               |
|    approx_kl            | 3.3935154e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -205          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 81.8          |
|    n_updates            | 28390         |
|    policy_gradient_loss | 0.000154      |
|    std                  | 161           |
|    value_loss           | 184           |
-------------------------------------------
Iteration: 2840 | Episodes: 115300 | Median Reward: 4.31 | Max Reward: 48.13
Iteration: 2843 | Episodes: 115400 | Median Reward: 11.08 | Max Reward: 48.13
Iteration: 2845 | Episodes: 115500 | Median Reward: 11.28 | Max Reward: 48.13
Iteration: 2848 | Episodes: 115600 | Median Reward: 6.05 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -89.3        |
| time/                   |              |
|    fps                  | 326          |
|    iterations           | 2850         |
|    time_elapsed         | 35737        |
|    total_timesteps      | 11673600     |
| train/                  |              |
|    approx_kl            | 3.258018e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -205         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 68.7         |
|    n_updates            | 28490        |
|    policy_gradient_loss | -8.62e-05    |
|    std                  | 164          |
|    value_loss           | 158          |
------------------------------------------
Iteration: 2850 | Episodes: 115700 | Median Reward: 22.30 | Max Reward: 48.13
Iteration: 2853 | Episodes: 115800 | Median Reward: 6.84 | Max Reward: 48.13
Iteration: 2855 | Episodes: 115900 | Median Reward: 14.08 | Max Reward: 48.13
Iteration: 2858 | Episodes: 116000 | Median Reward: 4.20 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.2         |
| time/                   |               |
|    fps                  | 326           |
|    iterations           | 2860          |
|    time_elapsed         | 35889         |
|    total_timesteps      | 11714560      |
| train/                  |               |
|    approx_kl            | 2.7513364e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -206          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 93.9          |
|    n_updates            | 28590         |
|    policy_gradient_loss | 0.000314      |
|    std                  | 167           |
|    value_loss           | 208           |
-------------------------------------------
Iteration: 2860 | Episodes: 116100 | Median Reward: 14.75 | Max Reward: 48.13
Iteration: 2863 | Episodes: 116200 | Median Reward: 12.55 | Max Reward: 48.13
Iteration: 2865 | Episodes: 116300 | Median Reward: 7.48 | Max Reward: 48.13
Iteration: 2868 | Episodes: 116400 | Median Reward: 10.53 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -93.3        |
| time/                   |              |
|    fps                  | 326          |
|    iterations           | 2870         |
|    time_elapsed         | 36042        |
|    total_timesteps      | 11755520     |
| train/                  |              |
|    approx_kl            | 6.989285e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -206         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 75.4         |
|    n_updates            | 28690        |
|    policy_gradient_loss | 5.61e-05     |
|    std                  | 169          |
|    value_loss           | 172          |
------------------------------------------
Iteration: 2870 | Episodes: 116500 | Median Reward: 8.43 | Max Reward: 48.13
Iteration: 2872 | Episodes: 116600 | Median Reward: 11.59 | Max Reward: 48.13
Iteration: 2875 | Episodes: 116700 | Median Reward: 12.05 | Max Reward: 48.13
Iteration: 2877 | Episodes: 116800 | Median Reward: 5.97 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.5         |
| time/                   |               |
|    fps                  | 325           |
|    iterations           | 2880          |
|    time_elapsed         | 36202         |
|    total_timesteps      | 11796480      |
| train/                  |               |
|    approx_kl            | 2.7289061e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -206          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 147           |
|    n_updates            | 28790         |
|    policy_gradient_loss | -0.00033      |
|    std                  | 172           |
|    value_loss           | 315           |
-------------------------------------------
Iteration: 2880 | Episodes: 116900 | Median Reward: 13.75 | Max Reward: 48.13
Iteration: 2882 | Episodes: 117000 | Median Reward: 12.02 | Max Reward: 48.13
Iteration: 2885 | Episodes: 117100 | Median Reward: 13.06 | Max Reward: 48.13
Iteration: 2887 | Episodes: 117200 | Median Reward: 11.18 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.9        |
| time/                   |              |
|    fps                  | 325          |
|    iterations           | 2890         |
|    time_elapsed         | 36356        |
|    total_timesteps      | 11837440     |
| train/                  |              |
|    approx_kl            | 2.818655e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -207         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 127          |
|    n_updates            | 28890        |
|    policy_gradient_loss | -0.000119    |
|    std                  | 174          |
|    value_loss           | 274          |
------------------------------------------
Iteration: 2890 | Episodes: 117300 | Median Reward: 15.51 | Max Reward: 48.13
Iteration: 2892 | Episodes: 117400 | Median Reward: 9.18 | Max Reward: 48.13
Iteration: 2895 | Episodes: 117500 | Median Reward: 14.69 | Max Reward: 48.13
Iteration: 2897 | Episodes: 117600 | Median Reward: 9.83 | Max Reward: 48.13
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -91.7          |
| time/                   |                |
|    fps                  | 325            |
|    iterations           | 2900           |
|    time_elapsed         | 36507          |
|    total_timesteps      | 11878400       |
| train/                  |                |
|    approx_kl            | 0.000103770115 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -207           |
|    explained_variance   | 0              |
|    learning_rate        | 0.0005         |
|    loss                 | 115            |
|    n_updates            | 28990          |
|    policy_gradient_loss | -0.000303      |
|    std                  | 175            |
|    value_loss           | 251            |
--------------------------------------------
Iteration: 2900 | Episodes: 117700 | Median Reward: 11.39 | Max Reward: 48.13
Iteration: 2902 | Episodes: 117800 | Median Reward: 17.22 | Max Reward: 48.13
Iteration: 2905 | Episodes: 117900 | Median Reward: 16.37 | Max Reward: 48.13
Iteration: 2907 | Episodes: 118000 | Median Reward: 16.25 | Max Reward: 48.13
Iteration: 2909 | Episodes: 118100 | Median Reward: 8.90 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.8         |
| time/                   |               |
|    fps                  | 325           |
|    iterations           | 2910          |
|    time_elapsed         | 36659         |
|    total_timesteps      | 11919360      |
| train/                  |               |
|    approx_kl            | 0.00030162893 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -207          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 78            |
|    n_updates            | 29090         |
|    policy_gradient_loss | 0.000343      |
|    std                  | 178           |
|    value_loss           | 177           |
-------------------------------------------
Iteration: 2912 | Episodes: 118200 | Median Reward: 13.57 | Max Reward: 48.13
Iteration: 2914 | Episodes: 118300 | Median Reward: 0.66 | Max Reward: 48.13
Iteration: 2917 | Episodes: 118400 | Median Reward: -0.00 | Max Reward: 48.13
Iteration: 2919 | Episodes: 118500 | Median Reward: 8.43 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.4         |
| time/                   |               |
|    fps                  | 324           |
|    iterations           | 2920          |
|    time_elapsed         | 36808         |
|    total_timesteps      | 11960320      |
| train/                  |               |
|    approx_kl            | 4.0685758e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -207          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 117           |
|    n_updates            | 29190         |
|    policy_gradient_loss | 0.000102      |
|    std                  | 180           |
|    value_loss           | 255           |
-------------------------------------------
Iteration: 2922 | Episodes: 118600 | Median Reward: 5.79 | Max Reward: 48.13
Iteration: 2924 | Episodes: 118700 | Median Reward: 19.76 | Max Reward: 48.13
Iteration: 2927 | Episodes: 118800 | Median Reward: 6.61 | Max Reward: 48.13
Iteration: 2929 | Episodes: 118900 | Median Reward: 13.52 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88.9         |
| time/                   |               |
|    fps                  | 324           |
|    iterations           | 2930          |
|    time_elapsed         | 36961         |
|    total_timesteps      | 12001280      |
| train/                  |               |
|    approx_kl            | 1.5275553e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -208          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 142           |
|    n_updates            | 29290         |
|    policy_gradient_loss | -0.000291     |
|    std                  | 183           |
|    value_loss           | 306           |
-------------------------------------------
Iteration: 2932 | Episodes: 119000 | Median Reward: 10.39 | Max Reward: 48.13
Iteration: 2934 | Episodes: 119100 | Median Reward: 9.08 | Max Reward: 48.13
Iteration: 2937 | Episodes: 119200 | Median Reward: 14.47 | Max Reward: 48.13
Iteration: 2939 | Episodes: 119300 | Median Reward: 9.63 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -91          |
| time/                   |              |
|    fps                  | 324          |
|    iterations           | 2940         |
|    time_elapsed         | 37117        |
|    total_timesteps      | 12042240     |
| train/                  |              |
|    approx_kl            | 9.841868e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -208         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 104          |
|    n_updates            | 29390        |
|    policy_gradient_loss | -0.000327    |
|    std                  | 186          |
|    value_loss           | 229          |
------------------------------------------
Iteration: 2942 | Episodes: 119400 | Median Reward: 10.94 | Max Reward: 48.13
Iteration: 2944 | Episodes: 119500 | Median Reward: 10.69 | Max Reward: 48.13
Iteration: 2946 | Episodes: 119600 | Median Reward: 3.29 | Max Reward: 48.13
Iteration: 2949 | Episodes: 119700 | Median Reward: 14.91 | Max Reward: 48.13
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -87            |
| time/                   |                |
|    fps                  | 324            |
|    iterations           | 2950           |
|    time_elapsed         | 37273          |
|    total_timesteps      | 12083200       |
| train/                  |                |
|    approx_kl            | 1.29239925e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -208           |
|    explained_variance   | 0              |
|    learning_rate        | 0.0005         |
|    loss                 | 120            |
|    n_updates            | 29490          |
|    policy_gradient_loss | -5.44e-05      |
|    std                  | 188            |
|    value_loss           | 262            |
--------------------------------------------
Iteration: 2951 | Episodes: 119800 | Median Reward: 7.64 | Max Reward: 48.13
Iteration: 2954 | Episodes: 119900 | Median Reward: 6.48 | Max Reward: 48.13
Iteration: 2956 | Episodes: 120000 | Median Reward: 14.07 | Max Reward: 48.13
Iteration: 2959 | Episodes: 120100 | Median Reward: 15.10 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.3        |
| time/                   |              |
|    fps                  | 323          |
|    iterations           | 2960         |
|    time_elapsed         | 37424        |
|    total_timesteps      | 12124160     |
| train/                  |              |
|    approx_kl            | 8.299334e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -209         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 114          |
|    n_updates            | 29590        |
|    policy_gradient_loss | -0.000266    |
|    std                  | 192          |
|    value_loss           | 250          |
------------------------------------------
Iteration: 2961 | Episodes: 120200 | Median Reward: 3.52 | Max Reward: 48.13
Iteration: 2964 | Episodes: 120300 | Median Reward: 2.64 | Max Reward: 48.13
Iteration: 2966 | Episodes: 120400 | Median Reward: 8.65 | Max Reward: 48.13
Iteration: 2969 | Episodes: 120500 | Median Reward: 9.34 | Max Reward: 48.13
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -93.9          |
| time/                   |                |
|    fps                  | 323            |
|    iterations           | 2970           |
|    time_elapsed         | 37575          |
|    total_timesteps      | 12165120       |
| train/                  |                |
|    approx_kl            | 0.000108309265 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -209           |
|    explained_variance   | 0              |
|    learning_rate        | 0.0005         |
|    loss                 | 90.8           |
|    n_updates            | 29690          |
|    policy_gradient_loss | -0.000921      |
|    std                  | 195            |
|    value_loss           | 203            |
--------------------------------------------
Iteration: 2971 | Episodes: 120600 | Median Reward: 7.06 | Max Reward: 48.13
Iteration: 2974 | Episodes: 120700 | Median Reward: 5.89 | Max Reward: 48.13
Iteration: 2976 | Episodes: 120800 | Median Reward: 10.60 | Max Reward: 48.13
Iteration: 2979 | Episodes: 120900 | Median Reward: 9.46 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.1         |
| time/                   |               |
|    fps                  | 323           |
|    iterations           | 2980          |
|    time_elapsed         | 37725         |
|    total_timesteps      | 12206080      |
| train/                  |               |
|    approx_kl            | 2.1357584e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -210          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 128           |
|    n_updates            | 29790         |
|    policy_gradient_loss | 1.12e-05      |
|    std                  | 198           |
|    value_loss           | 277           |
-------------------------------------------
Iteration: 2981 | Episodes: 121000 | Median Reward: 22.34 | Max Reward: 48.13
Iteration: 2983 | Episodes: 121100 | Median Reward: 11.50 | Max Reward: 48.13
Iteration: 2986 | Episodes: 121200 | Median Reward: 4.98 | Max Reward: 48.13
Iteration: 2988 | Episodes: 121300 | Median Reward: 10.00 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.8        |
| time/                   |              |
|    fps                  | 323          |
|    iterations           | 2990         |
|    time_elapsed         | 37877        |
|    total_timesteps      | 12247040     |
| train/                  |              |
|    approx_kl            | 9.692725e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -210         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 70           |
|    n_updates            | 29890        |
|    policy_gradient_loss | -0.000163    |
|    std                  | 199          |
|    value_loss           | 161          |
------------------------------------------
Iteration: 2991 | Episodes: 121400 | Median Reward: 8.34 | Max Reward: 48.13
Iteration: 2993 | Episodes: 121500 | Median Reward: 11.48 | Max Reward: 48.13
Iteration: 2996 | Episodes: 121600 | Median Reward: 10.68 | Max Reward: 48.13
Iteration: 2998 | Episodes: 121700 | Median Reward: 17.34 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84          |
| time/                   |              |
|    fps                  | 323          |
|    iterations           | 3000         |
|    time_elapsed         | 38029        |
|    total_timesteps      | 12288000     |
| train/                  |              |
|    approx_kl            | 3.127764e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -210         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 149          |
|    n_updates            | 29990        |
|    policy_gradient_loss | -0.000258    |
|    std                  | 201          |
|    value_loss           | 319          |
------------------------------------------
Iteration: 3001 | Episodes: 121800 | Median Reward: 11.19 | Max Reward: 48.13
Iteration: 3003 | Episodes: 121900 | Median Reward: 10.18 | Max Reward: 48.13
Iteration: 3006 | Episodes: 122000 | Median Reward: 9.54 | Max Reward: 48.13
Iteration: 3008 | Episodes: 122100 | Median Reward: 15.28 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.2        |
| time/                   |              |
|    fps                  | 322          |
|    iterations           | 3010         |
|    time_elapsed         | 38192        |
|    total_timesteps      | 12328960     |
| train/                  |              |
|    approx_kl            | 6.259608e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -210         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 129          |
|    n_updates            | 30090        |
|    policy_gradient_loss | -5.04e-05    |
|    std                  | 204          |
|    value_loss           | 279          |
------------------------------------------
Iteration: 3011 | Episodes: 122200 | Median Reward: 8.59 | Max Reward: 48.13
Iteration: 3013 | Episodes: 122300 | Median Reward: 1.93 | Max Reward: 48.13
Iteration: 3015 | Episodes: 122400 | Median Reward: 12.45 | Max Reward: 48.13
Iteration: 3018 | Episodes: 122500 | Median Reward: 9.51 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -103          |
| time/                   |               |
|    fps                  | 322           |
|    iterations           | 3020          |
|    time_elapsed         | 38344         |
|    total_timesteps      | 12369920      |
| train/                  |               |
|    approx_kl            | 2.2455235e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -211          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 75.2          |
|    n_updates            | 30190         |
|    policy_gradient_loss | -0.000371     |
|    std                  | 207           |
|    value_loss           | 172           |
-------------------------------------------
Iteration: 3020 | Episodes: 122600 | Median Reward: -6.03 | Max Reward: 48.13
Iteration: 3023 | Episodes: 122700 | Median Reward: 2.82 | Max Reward: 48.13
Iteration: 3025 | Episodes: 122800 | Median Reward: 6.08 | Max Reward: 48.13
Iteration: 3028 | Episodes: 122900 | Median Reward: 7.04 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -95.8         |
| time/                   |               |
|    fps                  | 322           |
|    iterations           | 3030          |
|    time_elapsed         | 38495         |
|    total_timesteps      | 12410880      |
| train/                  |               |
|    approx_kl            | 0.00016554935 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -211          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 113           |
|    n_updates            | 30290         |
|    policy_gradient_loss | -0.000954     |
|    std                  | 211           |
|    value_loss           | 247           |
-------------------------------------------
Iteration: 3030 | Episodes: 123000 | Median Reward: 2.71 | Max Reward: 48.13
Iteration: 3033 | Episodes: 123100 | Median Reward: 10.51 | Max Reward: 48.13
Iteration: 3035 | Episodes: 123200 | Median Reward: 7.63 | Max Reward: 48.13
Iteration: 3038 | Episodes: 123300 | Median Reward: 20.88 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.1        |
| time/                   |              |
|    fps                  | 322          |
|    iterations           | 3040         |
|    time_elapsed         | 38645        |
|    total_timesteps      | 12451840     |
| train/                  |              |
|    approx_kl            | 8.442512e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -211         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 61.2         |
|    n_updates            | 30390        |
|    policy_gradient_loss | -3.69e-05    |
|    std                  | 212          |
|    value_loss           | 144          |
------------------------------------------
Iteration: 3040 | Episodes: 123400 | Median Reward: 0.93 | Max Reward: 48.13
Iteration: 3043 | Episodes: 123500 | Median Reward: 7.84 | Max Reward: 48.13
Iteration: 3045 | Episodes: 123600 | Median Reward: 10.85 | Max Reward: 48.13
Iteration: 3048 | Episodes: 123700 | Median Reward: 12.47 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.5         |
| time/                   |               |
|    fps                  | 322           |
|    iterations           | 3050          |
|    time_elapsed         | 38796         |
|    total_timesteps      | 12492800      |
| train/                  |               |
|    approx_kl            | 2.6108639e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -212          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 145           |
|    n_updates            | 30490         |
|    policy_gradient_loss | 0.00021       |
|    std                  | 216           |
|    value_loss           | 313           |
-------------------------------------------
Iteration: 3050 | Episodes: 123800 | Median Reward: 11.02 | Max Reward: 48.13
Iteration: 3052 | Episodes: 123900 | Median Reward: 10.34 | Max Reward: 48.13
Iteration: 3055 | Episodes: 124000 | Median Reward: 3.10 | Max Reward: 48.13
Iteration: 3057 | Episodes: 124100 | Median Reward: 1.43 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -90.3        |
| time/                   |              |
|    fps                  | 321          |
|    iterations           | 3060         |
|    time_elapsed         | 38944        |
|    total_timesteps      | 12533760     |
| train/                  |              |
|    approx_kl            | 0.0003414399 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -212         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 125          |
|    n_updates            | 30590        |
|    policy_gradient_loss | 0.00116      |
|    std                  | 219          |
|    value_loss           | 271          |
------------------------------------------
Iteration: 3060 | Episodes: 124200 | Median Reward: 3.08 | Max Reward: 48.13
Iteration: 3062 | Episodes: 124300 | Median Reward: 0.94 | Max Reward: 48.13
Iteration: 3065 | Episodes: 124400 | Median Reward: 11.69 | Max Reward: 48.13
Iteration: 3067 | Episodes: 124500 | Median Reward: 3.82 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.1         |
| time/                   |               |
|    fps                  | 321           |
|    iterations           | 3070          |
|    time_elapsed         | 39094         |
|    total_timesteps      | 12574720      |
| train/                  |               |
|    approx_kl            | 0.00013423136 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -212          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 104           |
|    n_updates            | 30690         |
|    policy_gradient_loss | -0.000678     |
|    std                  | 221           |
|    value_loss           | 229           |
-------------------------------------------
Iteration: 3070 | Episodes: 124600 | Median Reward: 5.64 | Max Reward: 48.13
Iteration: 3072 | Episodes: 124700 | Median Reward: 17.26 | Max Reward: 48.13
Iteration: 3075 | Episodes: 124800 | Median Reward: 11.02 | Max Reward: 48.13
Iteration: 3077 | Episodes: 124900 | Median Reward: 8.55 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -93.2        |
| time/                   |              |
|    fps                  | 321          |
|    iterations           | 3080         |
|    time_elapsed         | 39257        |
|    total_timesteps      | 12615680     |
| train/                  |              |
|    approx_kl            | 0.0002264088 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -213         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 105          |
|    n_updates            | 30790        |
|    policy_gradient_loss | 0.000494     |
|    std                  | 226          |
|    value_loss           | 231          |
------------------------------------------
Iteration: 3080 | Episodes: 125000 | Median Reward: 7.49 | Max Reward: 48.13
Iteration: 3082 | Episodes: 125100 | Median Reward: 9.28 | Max Reward: 48.13
Iteration: 3085 | Episodes: 125200 | Median Reward: 15.04 | Max Reward: 48.13
Iteration: 3087 | Episodes: 125300 | Median Reward: 11.11 | Max Reward: 48.13
Iteration: 3089 | Episodes: 125400 | Median Reward: 11.03 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.5         |
| time/                   |               |
|    fps                  | 321           |
|    iterations           | 3090          |
|    time_elapsed         | 39407         |
|    total_timesteps      | 12656640      |
| train/                  |               |
|    approx_kl            | 5.5486467e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -213          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 114           |
|    n_updates            | 30890         |
|    policy_gradient_loss | -0.000278     |
|    std                  | 231           |
|    value_loss           | 250           |
-------------------------------------------
Iteration: 3092 | Episodes: 125500 | Median Reward: 10.46 | Max Reward: 48.13
Iteration: 3094 | Episodes: 125600 | Median Reward: 11.46 | Max Reward: 48.13
Iteration: 3097 | Episodes: 125700 | Median Reward: 3.75 | Max Reward: 48.13
Iteration: 3099 | Episodes: 125800 | Median Reward: 4.77 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -95.9         |
| time/                   |               |
|    fps                  | 320           |
|    iterations           | 3100          |
|    time_elapsed         | 39558         |
|    total_timesteps      | 12697600      |
| train/                  |               |
|    approx_kl            | 4.5429595e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -214          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 110           |
|    n_updates            | 30990         |
|    policy_gradient_loss | -0.000403     |
|    std                  | 236           |
|    value_loss           | 241           |
-------------------------------------------
Iteration: 3102 | Episodes: 125900 | Median Reward: 11.70 | Max Reward: 48.13
Iteration: 3104 | Episodes: 126000 | Median Reward: 10.66 | Max Reward: 48.13
Iteration: 3107 | Episodes: 126100 | Median Reward: 9.96 | Max Reward: 48.13
Iteration: 3109 | Episodes: 126200 | Median Reward: 10.56 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88          |
| time/                   |              |
|    fps                  | 320          |
|    iterations           | 3110         |
|    time_elapsed         | 39707        |
|    total_timesteps      | 12738560     |
| train/                  |              |
|    approx_kl            | 4.535439e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -214         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 128          |
|    n_updates            | 31090        |
|    policy_gradient_loss | -1.18e-05    |
|    std                  | 239          |
|    value_loss           | 277          |
------------------------------------------
Iteration: 3112 | Episodes: 126300 | Median Reward: 6.97 | Max Reward: 48.13
Iteration: 3114 | Episodes: 126400 | Median Reward: 6.74 | Max Reward: 48.13
Iteration: 3117 | Episodes: 126500 | Median Reward: 14.47 | Max Reward: 48.13
Iteration: 3119 | Episodes: 126600 | Median Reward: 2.72 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -99.1         |
| time/                   |               |
|    fps                  | 320           |
|    iterations           | 3120          |
|    time_elapsed         | 39860         |
|    total_timesteps      | 12779520      |
| train/                  |               |
|    approx_kl            | 0.00038306456 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -215          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 93.3          |
|    n_updates            | 31190         |
|    policy_gradient_loss | -0.000337     |
|    std                  | 243           |
|    value_loss           | 208           |
-------------------------------------------
Iteration: 3122 | Episodes: 126700 | Median Reward: 13.23 | Max Reward: 48.13
Iteration: 3124 | Episodes: 126800 | Median Reward: 8.47 | Max Reward: 48.13
Iteration: 3126 | Episodes: 126900 | Median Reward: 4.78 | Max Reward: 48.13
Iteration: 3129 | Episodes: 127000 | Median Reward: 8.74 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -89.7        |
| time/                   |              |
|    fps                  | 320          |
|    iterations           | 3130         |
|    time_elapsed         | 40011        |
|    total_timesteps      | 12820480     |
| train/                  |              |
|    approx_kl            | 4.067138e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -215         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 84           |
|    n_updates            | 31290        |
|    policy_gradient_loss | -4.21e-05    |
|    std                  | 247          |
|    value_loss           | 190          |
------------------------------------------
Iteration: 3131 | Episodes: 127100 | Median Reward: 13.48 | Max Reward: 48.13
Iteration: 3134 | Episodes: 127200 | Median Reward: 14.24 | Max Reward: 48.13
Iteration: 3136 | Episodes: 127300 | Median Reward: 13.11 | Max Reward: 48.13
Iteration: 3139 | Episodes: 127400 | Median Reward: 4.08 | Max Reward: 48.13
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -92.8          |
| time/                   |                |
|    fps                  | 320            |
|    iterations           | 3140           |
|    time_elapsed         | 40163          |
|    total_timesteps      | 12861440       |
| train/                  |                |
|    approx_kl            | 1.14112045e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -215           |
|    explained_variance   | 0              |
|    learning_rate        | 0.0005         |
|    loss                 | 93.8           |
|    n_updates            | 31390          |
|    policy_gradient_loss | -9.86e-05      |
|    std                  | 251            |
|    value_loss           | 209            |
--------------------------------------------
Iteration: 3141 | Episodes: 127500 | Median Reward: 2.18 | Max Reward: 48.13
Iteration: 3144 | Episodes: 127600 | Median Reward: 19.32 | Max Reward: 48.13
Iteration: 3146 | Episodes: 127700 | Median Reward: 9.20 | Max Reward: 48.13
Iteration: 3149 | Episodes: 127800 | Median Reward: 16.82 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.6         |
| time/                   |               |
|    fps                  | 319           |
|    iterations           | 3150          |
|    time_elapsed         | 40323         |
|    total_timesteps      | 12902400      |
| train/                  |               |
|    approx_kl            | 0.00045463812 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -216          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 115           |
|    n_updates            | 31490         |
|    policy_gradient_loss | -0.00185      |
|    std                  | 254           |
|    value_loss           | 252           |
-------------------------------------------
Iteration: 3151 | Episodes: 127900 | Median Reward: 10.39 | Max Reward: 48.13
Iteration: 3154 | Episodes: 128000 | Median Reward: 14.42 | Max Reward: 48.13
Iteration: 3156 | Episodes: 128100 | Median Reward: 20.08 | Max Reward: 48.13
Iteration: 3159 | Episodes: 128200 | Median Reward: 14.56 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88.3         |
| time/                   |               |
|    fps                  | 319           |
|    iterations           | 3160          |
|    time_elapsed         | 40475         |
|    total_timesteps      | 12943360      |
| train/                  |               |
|    approx_kl            | 5.9492333e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -216          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 117           |
|    n_updates            | 31590         |
|    policy_gradient_loss | -8.1e-06      |
|    std                  | 258           |
|    value_loss           | 256           |
-------------------------------------------
Iteration: 3161 | Episodes: 128300 | Median Reward: 5.43 | Max Reward: 48.13
Iteration: 3163 | Episodes: 128400 | Median Reward: 9.82 | Max Reward: 48.13
Iteration: 3166 | Episodes: 128500 | Median Reward: 12.53 | Max Reward: 48.13
Iteration: 3168 | Episodes: 128600 | Median Reward: 15.66 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.5         |
| time/                   |               |
|    fps                  | 319           |
|    iterations           | 3170          |
|    time_elapsed         | 40627         |
|    total_timesteps      | 12984320      |
| train/                  |               |
|    approx_kl            | 0.00071873295 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -217          |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0005        |
|    loss                 | 99            |
|    n_updates            | 31690         |
|    policy_gradient_loss | -0.00229      |
|    std                  | 264           |
|    value_loss           | 220           |
-------------------------------------------
Iteration: 3171 | Episodes: 128700 | Median Reward: 7.25 | Max Reward: 48.13
Iteration: 3173 | Episodes: 128800 | Median Reward: 7.53 | Max Reward: 48.13
Iteration: 3176 | Episodes: 128900 | Median Reward: 11.27 | Max Reward: 48.13
Iteration: 3178 | Episodes: 129000 | Median Reward: 15.69 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.4         |
| time/                   |               |
|    fps                  | 319           |
|    iterations           | 3180          |
|    time_elapsed         | 40777         |
|    total_timesteps      | 13025280      |
| train/                  |               |
|    approx_kl            | 2.5236106e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -217          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 162           |
|    n_updates            | 31790         |
|    policy_gradient_loss | 0.000238      |
|    std                  | 269           |
|    value_loss           | 346           |
-------------------------------------------
Iteration: 3181 | Episodes: 129100 | Median Reward: 9.56 | Max Reward: 48.13
Iteration: 3183 | Episodes: 129200 | Median Reward: 11.77 | Max Reward: 48.13
Iteration: 3186 | Episodes: 129300 | Median Reward: 12.22 | Max Reward: 48.13
Iteration: 3188 | Episodes: 129400 | Median Reward: 12.19 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.7         |
| time/                   |               |
|    fps                  | 319           |
|    iterations           | 3190          |
|    time_elapsed         | 40932         |
|    total_timesteps      | 13066240      |
| train/                  |               |
|    approx_kl            | 0.00014211485 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -217          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 111           |
|    n_updates            | 31890         |
|    policy_gradient_loss | 0.000554      |
|    std                  | 271           |
|    value_loss           | 244           |
-------------------------------------------
Iteration: 3191 | Episodes: 129500 | Median Reward: 13.48 | Max Reward: 48.13
Iteration: 3193 | Episodes: 129600 | Median Reward: 8.89 | Max Reward: 48.13
Iteration: 3195 | Episodes: 129700 | Median Reward: 6.55 | Max Reward: 48.13
Iteration: 3198 | Episodes: 129800 | Median Reward: 12.07 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.1         |
| time/                   |               |
|    fps                  | 319           |
|    iterations           | 3200          |
|    time_elapsed         | 41085         |
|    total_timesteps      | 13107200      |
| train/                  |               |
|    approx_kl            | 2.6260648e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -218          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 83.7          |
|    n_updates            | 31990         |
|    policy_gradient_loss | -0.000279     |
|    std                  | 276           |
|    value_loss           | 189           |
-------------------------------------------
Iteration: 3200 | Episodes: 129900 | Median Reward: 3.90 | Max Reward: 48.13
Iteration: 3203 | Episodes: 130000 | Median Reward: 9.51 | Max Reward: 48.13
Iteration: 3205 | Episodes: 130100 | Median Reward: 13.76 | Max Reward: 48.13
Iteration: 3208 | Episodes: 130200 | Median Reward: 8.95 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.3         |
| time/                   |               |
|    fps                  | 318           |
|    iterations           | 3210          |
|    time_elapsed         | 41236         |
|    total_timesteps      | 13148160      |
| train/                  |               |
|    approx_kl            | 0.00059257494 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -218          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 86.9          |
|    n_updates            | 32090         |
|    policy_gradient_loss | -0.00109      |
|    std                  | 281           |
|    value_loss           | 196           |
-------------------------------------------
Iteration: 3210 | Episodes: 130300 | Median Reward: 10.83 | Max Reward: 48.13
Iteration: 3213 | Episodes: 130400 | Median Reward: 9.13 | Max Reward: 48.13
Iteration: 3215 | Episodes: 130500 | Median Reward: 3.91 | Max Reward: 48.13
Iteration: 3218 | Episodes: 130600 | Median Reward: 5.23 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -94          |
| time/                   |              |
|    fps                  | 318          |
|    iterations           | 3220         |
|    time_elapsed         | 41399        |
|    total_timesteps      | 13189120     |
| train/                  |              |
|    approx_kl            | 0.0009060768 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -219         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 91.6         |
|    n_updates            | 32190        |
|    policy_gradient_loss | -0.00173     |
|    std                  | 288          |
|    value_loss           | 205          |
------------------------------------------
Iteration: 3220 | Episodes: 130700 | Median Reward: 4.97 | Max Reward: 48.13
Iteration: 3223 | Episodes: 130800 | Median Reward: 5.74 | Max Reward: 48.13
Iteration: 3225 | Episodes: 130900 | Median Reward: 14.76 | Max Reward: 48.13
Iteration: 3228 | Episodes: 131000 | Median Reward: 10.94 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -94.4         |
| time/                   |               |
|    fps                  | 318           |
|    iterations           | 3230          |
|    time_elapsed         | 41550         |
|    total_timesteps      | 13230080      |
| train/                  |               |
|    approx_kl            | 0.00015519961 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -219          |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0005        |
|    loss                 | 72.2          |
|    n_updates            | 32290         |
|    policy_gradient_loss | -0.000467     |
|    std                  | 293           |
|    value_loss           | 166           |
-------------------------------------------
Iteration: 3230 | Episodes: 131100 | Median Reward: 5.92 | Max Reward: 48.13
Iteration: 3232 | Episodes: 131200 | Median Reward: 4.66 | Max Reward: 48.13
Iteration: 3235 | Episodes: 131300 | Median Reward: 4.90 | Max Reward: 48.13
Iteration: 3237 | Episodes: 131400 | Median Reward: 7.64 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -94.5         |
| time/                   |               |
|    fps                  | 318           |
|    iterations           | 3240          |
|    time_elapsed         | 41701         |
|    total_timesteps      | 13271040      |
| train/                  |               |
|    approx_kl            | 0.00023563899 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -219          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 85.2          |
|    n_updates            | 32390         |
|    policy_gradient_loss | -0.000846     |
|    std                  | 296           |
|    value_loss           | 193           |
-------------------------------------------
Iteration: 3240 | Episodes: 131500 | Median Reward: 7.18 | Max Reward: 48.13
Iteration: 3242 | Episodes: 131600 | Median Reward: -0.83 | Max Reward: 48.13
Iteration: 3245 | Episodes: 131700 | Median Reward: 19.21 | Max Reward: 48.13
Iteration: 3247 | Episodes: 131800 | Median Reward: 19.70 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87.9        |
| time/                   |              |
|    fps                  | 318          |
|    iterations           | 3250         |
|    time_elapsed         | 41851        |
|    total_timesteps      | 13312000     |
| train/                  |              |
|    approx_kl            | 3.720459e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -219         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 122          |
|    n_updates            | 32490        |
|    policy_gradient_loss | 1.06e-05     |
|    std                  | 298          |
|    value_loss           | 266          |
------------------------------------------
Iteration: 3250 | Episodes: 131900 | Median Reward: 12.73 | Max Reward: 48.13
Iteration: 3252 | Episodes: 132000 | Median Reward: 11.71 | Max Reward: 48.13
Iteration: 3255 | Episodes: 132100 | Median Reward: 14.87 | Max Reward: 48.13
Iteration: 3257 | Episodes: 132200 | Median Reward: 8.64 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.8         |
| time/                   |               |
|    fps                  | 317           |
|    iterations           | 3260          |
|    time_elapsed         | 41999         |
|    total_timesteps      | 13352960      |
| train/                  |               |
|    approx_kl            | 2.4031353e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -220          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 104           |
|    n_updates            | 32590         |
|    policy_gradient_loss | 2.63e-05      |
|    std                  | 303           |
|    value_loss           | 230           |
-------------------------------------------
Iteration: 3260 | Episodes: 132300 | Median Reward: 14.11 | Max Reward: 48.13
Iteration: 3262 | Episodes: 132400 | Median Reward: 9.93 | Max Reward: 48.13
Iteration: 3265 | Episodes: 132500 | Median Reward: 5.35 | Max Reward: 48.13
Iteration: 3267 | Episodes: 132600 | Median Reward: 13.61 | Max Reward: 48.13
Iteration: 3269 | Episodes: 132700 | Median Reward: 10.65 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -98.2         |
| time/                   |               |
|    fps                  | 317           |
|    iterations           | 3270          |
|    time_elapsed         | 42150         |
|    total_timesteps      | 13393920      |
| train/                  |               |
|    approx_kl            | 6.0989973e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -220          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 72.4          |
|    n_updates            | 32690         |
|    policy_gradient_loss | -0.000314     |
|    std                  | 309           |
|    value_loss           | 167           |
-------------------------------------------
Iteration: 3272 | Episodes: 132800 | Median Reward: 15.03 | Max Reward: 48.13
Iteration: 3274 | Episodes: 132900 | Median Reward: 4.84 | Max Reward: 48.13
Iteration: 3277 | Episodes: 133000 | Median Reward: 0.43 | Max Reward: 48.13
Iteration: 3279 | Episodes: 133100 | Median Reward: 12.68 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88.3         |
| time/                   |               |
|    fps                  | 317           |
|    iterations           | 3280          |
|    time_elapsed         | 42302         |
|    total_timesteps      | 13434880      |
| train/                  |               |
|    approx_kl            | 0.00011056212 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -221          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 135           |
|    n_updates            | 32790         |
|    policy_gradient_loss | 0.00031       |
|    std                  | 313           |
|    value_loss           | 293           |
-------------------------------------------
Iteration: 3282 | Episodes: 133200 | Median Reward: 8.98 | Max Reward: 48.13
Iteration: 3284 | Episodes: 133300 | Median Reward: 5.32 | Max Reward: 48.13
Iteration: 3287 | Episodes: 133400 | Median Reward: 3.24 | Max Reward: 48.13
Iteration: 3289 | Episodes: 133500 | Median Reward: 3.88 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -94           |
| time/                   |               |
|    fps                  | 317           |
|    iterations           | 3290          |
|    time_elapsed         | 42455         |
|    total_timesteps      | 13475840      |
| train/                  |               |
|    approx_kl            | 3.3538454e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -221          |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0005        |
|    loss                 | 72.4          |
|    n_updates            | 32890         |
|    policy_gradient_loss | -0.0002       |
|    std                  | 317           |
|    value_loss           | 167           |
-------------------------------------------
Iteration: 3292 | Episodes: 133600 | Median Reward: 7.07 | Max Reward: 48.13
Iteration: 3294 | Episodes: 133700 | Median Reward: 5.04 | Max Reward: 48.13
Iteration: 3297 | Episodes: 133800 | Median Reward: 8.98 | Max Reward: 48.13
Iteration: 3299 | Episodes: 133900 | Median Reward: 6.80 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -95.1        |
| time/                   |              |
|    fps                  | 317          |
|    iterations           | 3300         |
|    time_elapsed         | 42618        |
|    total_timesteps      | 13516800     |
| train/                  |              |
|    approx_kl            | 0.0010970952 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -221         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 92.9         |
|    n_updates            | 32990        |
|    policy_gradient_loss | -0.000756    |
|    std                  | 322          |
|    value_loss           | 208          |
------------------------------------------
Iteration: 3302 | Episodes: 134000 | Median Reward: 3.43 | Max Reward: 48.13
Iteration: 3304 | Episodes: 134100 | Median Reward: 10.99 | Max Reward: 48.13
Iteration: 3306 | Episodes: 134200 | Median Reward: 13.32 | Max Reward: 48.13
Iteration: 3309 | Episodes: 134300 | Median Reward: 5.93 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93           |
| time/                   |               |
|    fps                  | 316           |
|    iterations           | 3310          |
|    time_elapsed         | 42769         |
|    total_timesteps      | 13557760      |
| train/                  |               |
|    approx_kl            | 8.8479035e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -222          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 78.6          |
|    n_updates            | 33090         |
|    policy_gradient_loss | -0.000123     |
|    std                  | 326           |
|    value_loss           | 179           |
-------------------------------------------
Iteration: 3311 | Episodes: 134400 | Median Reward: 7.63 | Max Reward: 48.13
Iteration: 3314 | Episodes: 134500 | Median Reward: 9.68 | Max Reward: 48.13
Iteration: 3316 | Episodes: 134600 | Median Reward: 12.42 | Max Reward: 48.13
Iteration: 3319 | Episodes: 134700 | Median Reward: 10.93 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -86.1        |
| time/                   |              |
|    fps                  | 316          |
|    iterations           | 3320         |
|    time_elapsed         | 42920        |
|    total_timesteps      | 13598720     |
| train/                  |              |
|    approx_kl            | 0.0003406126 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -222         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 117          |
|    n_updates            | 33190        |
|    policy_gradient_loss | 0.000752     |
|    std                  | 333          |
|    value_loss           | 256          |
------------------------------------------
Iteration: 3321 | Episodes: 134800 | Median Reward: 6.12 | Max Reward: 48.13
Iteration: 3324 | Episodes: 134900 | Median Reward: 8.46 | Max Reward: 48.13
Iteration: 3326 | Episodes: 135000 | Median Reward: 11.93 | Max Reward: 48.13
Iteration: 3329 | Episodes: 135100 | Median Reward: 14.51 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -90.3        |
| time/                   |              |
|    fps                  | 316          |
|    iterations           | 3330         |
|    time_elapsed         | 43070        |
|    total_timesteps      | 13639680     |
| train/                  |              |
|    approx_kl            | 7.006258e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -223         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 110          |
|    n_updates            | 33290        |
|    policy_gradient_loss | -0.000502    |
|    std                  | 340          |
|    value_loss           | 243          |
------------------------------------------
Iteration: 3331 | Episodes: 135200 | Median Reward: 6.42 | Max Reward: 48.13
Iteration: 3334 | Episodes: 135300 | Median Reward: 5.84 | Max Reward: 48.13
Iteration: 3336 | Episodes: 135400 | Median Reward: 8.58 | Max Reward: 48.13
Iteration: 3339 | Episodes: 135500 | Median Reward: 8.06 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.2         |
| time/                   |               |
|    fps                  | 316           |
|    iterations           | 3340          |
|    time_elapsed         | 43222         |
|    total_timesteps      | 13680640      |
| train/                  |               |
|    approx_kl            | 1.8202976e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -223          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 72.9          |
|    n_updates            | 33390         |
|    policy_gradient_loss | -0.000308     |
|    std                  | 345           |
|    value_loss           | 168           |
-------------------------------------------
Iteration: 3341 | Episodes: 135600 | Median Reward: 1.42 | Max Reward: 48.13
Iteration: 3343 | Episodes: 135700 | Median Reward: 10.24 | Max Reward: 48.13
Iteration: 3346 | Episodes: 135800 | Median Reward: 6.29 | Max Reward: 48.13
Iteration: 3348 | Episodes: 135900 | Median Reward: 4.37 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -99.6        |
| time/                   |              |
|    fps                  | 316          |
|    iterations           | 3350         |
|    time_elapsed         | 43376        |
|    total_timesteps      | 13721600     |
| train/                  |              |
|    approx_kl            | 7.336357e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -223         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 66.4         |
|    n_updates            | 33490        |
|    policy_gradient_loss | -0.000504    |
|    std                  | 350          |
|    value_loss           | 155          |
------------------------------------------
Iteration: 3351 | Episodes: 136000 | Median Reward: 3.91 | Max Reward: 48.13
Iteration: 3353 | Episodes: 136100 | Median Reward: 14.33 | Max Reward: 48.13
Iteration: 3356 | Episodes: 136200 | Median Reward: -3.20 | Max Reward: 48.13
Iteration: 3358 | Episodes: 136300 | Median Reward: 2.80 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.3         |
| time/                   |               |
|    fps                  | 316           |
|    iterations           | 3360          |
|    time_elapsed         | 43525         |
|    total_timesteps      | 13762560      |
| train/                  |               |
|    approx_kl            | 4.1495747e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -224          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 145           |
|    n_updates            | 33590         |
|    policy_gradient_loss | 0.000109      |
|    std                  | 356           |
|    value_loss           | 313           |
-------------------------------------------
Iteration: 3361 | Episodes: 136400 | Median Reward: 6.13 | Max Reward: 48.13
Iteration: 3363 | Episodes: 136500 | Median Reward: 8.81 | Max Reward: 48.13
Iteration: 3366 | Episodes: 136600 | Median Reward: 7.38 | Max Reward: 48.13
Iteration: 3368 | Episodes: 136700 | Median Reward: 14.12 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -98.8        |
| time/                   |              |
|    fps                  | 315          |
|    iterations           | 3370         |
|    time_elapsed         | 43687        |
|    total_timesteps      | 13803520     |
| train/                  |              |
|    approx_kl            | 0.0049924636 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -224         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 70.3         |
|    n_updates            | 33690        |
|    policy_gradient_loss | -0.00309     |
|    std                  | 360          |
|    value_loss           | 163          |
------------------------------------------
Iteration: 3371 | Episodes: 136800 | Median Reward: -4.40 | Max Reward: 48.13
Iteration: 3373 | Episodes: 136900 | Median Reward: 8.83 | Max Reward: 48.13
Iteration: 3376 | Episodes: 137000 | Median Reward: 13.18 | Max Reward: 48.13
Iteration: 3378 | Episodes: 137100 | Median Reward: 2.26 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -94.4        |
| time/                   |              |
|    fps                  | 315          |
|    iterations           | 3380         |
|    time_elapsed         | 43838        |
|    total_timesteps      | 13844480     |
| train/                  |              |
|    approx_kl            | 4.836562e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -224         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 80           |
|    n_updates            | 33790        |
|    policy_gradient_loss | 0.000691     |
|    std                  | 367          |
|    value_loss           | 183          |
------------------------------------------
Iteration: 3380 | Episodes: 137200 | Median Reward: 16.14 | Max Reward: 48.13
Iteration: 3383 | Episodes: 137300 | Median Reward: 9.64 | Max Reward: 48.13
Iteration: 3385 | Episodes: 137400 | Median Reward: 9.69 | Max Reward: 48.13
Iteration: 3388 | Episodes: 137500 | Median Reward: 8.18 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -94.3        |
| time/                   |              |
|    fps                  | 315          |
|    iterations           | 3390         |
|    time_elapsed         | 43990        |
|    total_timesteps      | 13885440     |
| train/                  |              |
|    approx_kl            | 0.0020750817 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -225         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 57.1         |
|    n_updates            | 33890        |
|    policy_gradient_loss | -0.00318     |
|    std                  | 373          |
|    value_loss           | 137          |
------------------------------------------
Iteration: 3390 | Episodes: 137600 | Median Reward: 4.99 | Max Reward: 48.13
Iteration: 3393 | Episodes: 137700 | Median Reward: 12.95 | Max Reward: 48.13
Iteration: 3395 | Episodes: 137800 | Median Reward: 16.78 | Max Reward: 48.13
Iteration: 3398 | Episodes: 137900 | Median Reward: 6.50 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -91.7        |
| time/                   |              |
|    fps                  | 315          |
|    iterations           | 3400         |
|    time_elapsed         | 44141        |
|    total_timesteps      | 13926400     |
| train/                  |              |
|    approx_kl            | 7.619863e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -225         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 82           |
|    n_updates            | 33990        |
|    policy_gradient_loss | -4.24e-05    |
|    std                  | 380          |
|    value_loss           | 187          |
------------------------------------------
Iteration: 3400 | Episodes: 138000 | Median Reward: 8.13 | Max Reward: 48.13
Iteration: 3403 | Episodes: 138100 | Median Reward: -4.48 | Max Reward: 48.13
Iteration: 3405 | Episodes: 138200 | Median Reward: 0.80 | Max Reward: 48.13
Iteration: 3408 | Episodes: 138300 | Median Reward: 0.49 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -90.7        |
| time/                   |              |
|    fps                  | 315          |
|    iterations           | 3410         |
|    time_elapsed         | 44290        |
|    total_timesteps      | 13967360     |
| train/                  |              |
|    approx_kl            | 2.900543e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -225         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 76.9         |
|    n_updates            | 34090        |
|    policy_gradient_loss | -0.00019     |
|    std                  | 383          |
|    value_loss           | 176          |
------------------------------------------
Iteration: 3410 | Episodes: 138400 | Median Reward: 3.99 | Max Reward: 48.13
Iteration: 3412 | Episodes: 138500 | Median Reward: 4.28 | Max Reward: 48.13
Iteration: 3415 | Episodes: 138600 | Median Reward: 13.94 | Max Reward: 48.13
Iteration: 3417 | Episodes: 138700 | Median Reward: 12.87 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -92.7       |
| time/                   |             |
|    fps                  | 315         |
|    iterations           | 3420        |
|    time_elapsed         | 44439       |
|    total_timesteps      | 14008320    |
| train/                  |             |
|    approx_kl            | 0.005039814 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -226        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 99.9        |
|    n_updates            | 34190       |
|    policy_gradient_loss | -0.00632    |
|    std                  | 386         |
|    value_loss           | 222         |
-----------------------------------------
Iteration: 3420 | Episodes: 138800 | Median Reward: 11.38 | Max Reward: 48.13
Iteration: 3422 | Episodes: 138900 | Median Reward: 17.71 | Max Reward: 48.13
Iteration: 3425 | Episodes: 139000 | Median Reward: -8.31 | Max Reward: 48.13
Iteration: 3427 | Episodes: 139100 | Median Reward: 13.53 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.2         |
| time/                   |               |
|    fps                  | 315           |
|    iterations           | 3430          |
|    time_elapsed         | 44591         |
|    total_timesteps      | 14049280      |
| train/                  |               |
|    approx_kl            | 0.00037175685 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -226          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 102           |
|    n_updates            | 34290         |
|    policy_gradient_loss | -0.000671     |
|    std                  | 392           |
|    value_loss           | 226           |
-------------------------------------------
Iteration: 3430 | Episodes: 139200 | Median Reward: 9.87 | Max Reward: 48.13
Iteration: 3432 | Episodes: 139300 | Median Reward: 7.59 | Max Reward: 48.13
Iteration: 3435 | Episodes: 139400 | Median Reward: 10.84 | Max Reward: 48.13
Iteration: 3437 | Episodes: 139500 | Median Reward: 9.47 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89           |
| time/                   |               |
|    fps                  | 314           |
|    iterations           | 3440          |
|    time_elapsed         | 44751         |
|    total_timesteps      | 14090240      |
| train/                  |               |
|    approx_kl            | 0.00059348665 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -227          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 108           |
|    n_updates            | 34390         |
|    policy_gradient_loss | -0.000282     |
|    std                  | 402           |
|    value_loss           | 239           |
-------------------------------------------
Iteration: 3440 | Episodes: 139600 | Median Reward: 11.13 | Max Reward: 48.13
Iteration: 3442 | Episodes: 139700 | Median Reward: 10.41 | Max Reward: 48.13
Iteration: 3445 | Episodes: 139800 | Median Reward: 11.68 | Max Reward: 48.13
Iteration: 3447 | Episodes: 139900 | Median Reward: 13.07 | Max Reward: 48.13
Iteration: 3449 | Episodes: 140000 | Median Reward: -2.27 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -98.5        |
| time/                   |              |
|    fps                  | 314          |
|    iterations           | 3450         |
|    time_elapsed         | 44906        |
|    total_timesteps      | 14131200     |
| train/                  |              |
|    approx_kl            | 5.475944e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -227         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 63.9         |
|    n_updates            | 34490        |
|    policy_gradient_loss | 7.95e-05     |
|    std                  | 407          |
|    value_loss           | 151          |
------------------------------------------
Iteration: 3452 | Episodes: 140100 | Median Reward: 12.37 | Max Reward: 48.13
Iteration: 3454 | Episodes: 140200 | Median Reward: 11.36 | Max Reward: 48.13
Iteration: 3457 | Episodes: 140300 | Median Reward: 3.33 | Max Reward: 48.13
Iteration: 3459 | Episodes: 140400 | Median Reward: 11.62 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -90.6        |
| time/                   |              |
|    fps                  | 314          |
|    iterations           | 3460         |
|    time_elapsed         | 45055        |
|    total_timesteps      | 14172160     |
| train/                  |              |
|    approx_kl            | 7.727533e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -227         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 123          |
|    n_updates            | 34590        |
|    policy_gradient_loss | 0.000107     |
|    std                  | 409          |
|    value_loss           | 269          |
------------------------------------------
Iteration: 3462 | Episodes: 140500 | Median Reward: 12.18 | Max Reward: 48.13
Iteration: 3464 | Episodes: 140600 | Median Reward: 1.20 | Max Reward: 48.13
Iteration: 3467 | Episodes: 140700 | Median Reward: 14.34 | Max Reward: 48.13
Iteration: 3469 | Episodes: 140800 | Median Reward: 10.94 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87.5        |
| time/                   |              |
|    fps                  | 314          |
|    iterations           | 3470         |
|    time_elapsed         | 45207        |
|    total_timesteps      | 14213120     |
| train/                  |              |
|    approx_kl            | 0.0002957023 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -227         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 115          |
|    n_updates            | 34690        |
|    policy_gradient_loss | 0.000114     |
|    std                  | 414          |
|    value_loss           | 253          |
------------------------------------------
Iteration: 3472 | Episodes: 140900 | Median Reward: 5.33 | Max Reward: 48.13
Iteration: 3474 | Episodes: 141000 | Median Reward: 10.84 | Max Reward: 48.13
Iteration: 3477 | Episodes: 141100 | Median Reward: 11.79 | Max Reward: 48.13
Iteration: 3479 | Episodes: 141200 | Median Reward: 6.81 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -100          |
| time/                   |               |
|    fps                  | 314           |
|    iterations           | 3480          |
|    time_elapsed         | 45358         |
|    total_timesteps      | 14254080      |
| train/                  |               |
|    approx_kl            | 0.00026897364 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -228          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 62.6          |
|    n_updates            | 34790         |
|    policy_gradient_loss | -0.00123      |
|    std                  | 422           |
|    value_loss           | 148           |
-------------------------------------------
Iteration: 3482 | Episodes: 141300 | Median Reward: 1.85 | Max Reward: 48.13
Iteration: 3484 | Episodes: 141400 | Median Reward: 11.50 | Max Reward: 48.13
Iteration: 3486 | Episodes: 141500 | Median Reward: 5.21 | Max Reward: 48.13
Iteration: 3489 | Episodes: 141600 | Median Reward: 6.09 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -97.8        |
| time/                   |              |
|    fps                  | 314          |
|    iterations           | 3490         |
|    time_elapsed         | 45509        |
|    total_timesteps      | 14295040     |
| train/                  |              |
|    approx_kl            | 6.544724e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -228         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 101          |
|    n_updates            | 34890        |
|    policy_gradient_loss | -1.49e-05    |
|    std                  | 427          |
|    value_loss           | 225          |
------------------------------------------
Iteration: 3491 | Episodes: 141700 | Median Reward: 9.07 | Max Reward: 48.13
Iteration: 3494 | Episodes: 141800 | Median Reward: 7.51 | Max Reward: 48.13
Iteration: 3496 | Episodes: 141900 | Median Reward: 8.32 | Max Reward: 48.13
Iteration: 3499 | Episodes: 142000 | Median Reward: 7.28 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.6         |
| time/                   |               |
|    fps                  | 313           |
|    iterations           | 3500          |
|    time_elapsed         | 45664         |
|    total_timesteps      | 14336000      |
| train/                  |               |
|    approx_kl            | 5.5494587e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -228          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 110           |
|    n_updates            | 34990         |
|    policy_gradient_loss | -0.000738     |
|    std                  | 431           |
|    value_loss           | 243           |
-------------------------------------------
Iteration: 3501 | Episodes: 142100 | Median Reward: 10.90 | Max Reward: 48.13
Iteration: 3504 | Episodes: 142200 | Median Reward: 9.67 | Max Reward: 48.13
Iteration: 3506 | Episodes: 142300 | Median Reward: 12.19 | Max Reward: 48.13
Iteration: 3509 | Episodes: 142400 | Median Reward: 8.10 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.4         |
| time/                   |               |
|    fps                  | 313           |
|    iterations           | 3510          |
|    time_elapsed         | 45821         |
|    total_timesteps      | 14376960      |
| train/                  |               |
|    approx_kl            | 0.00022767445 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -228          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 86.3          |
|    n_updates            | 35090         |
|    policy_gradient_loss | -0.00122      |
|    std                  | 435           |
|    value_loss           | 196           |
-------------------------------------------
Iteration: 3511 | Episodes: 142500 | Median Reward: 2.29 | Max Reward: 48.13
Iteration: 3514 | Episodes: 142600 | Median Reward: 16.59 | Max Reward: 48.13
Iteration: 3516 | Episodes: 142700 | Median Reward: 8.94 | Max Reward: 48.13
Iteration: 3519 | Episodes: 142800 | Median Reward: 12.57 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87          |
| time/                   |              |
|    fps                  | 313          |
|    iterations           | 3520         |
|    time_elapsed         | 45971        |
|    total_timesteps      | 14417920     |
| train/                  |              |
|    approx_kl            | 0.0024484461 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -229         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 96.6         |
|    n_updates            | 35190        |
|    policy_gradient_loss | -0.00282     |
|    std                  | 443          |
|    value_loss           | 216          |
------------------------------------------
Iteration: 3521 | Episodes: 142900 | Median Reward: 14.51 | Max Reward: 48.13
Iteration: 3523 | Episodes: 143000 | Median Reward: 9.95 | Max Reward: 48.13
Iteration: 3526 | Episodes: 143100 | Median Reward: 16.67 | Max Reward: 48.13
Iteration: 3528 | Episodes: 143200 | Median Reward: 13.63 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.7         |
| time/                   |               |
|    fps                  | 313           |
|    iterations           | 3530          |
|    time_elapsed         | 46122         |
|    total_timesteps      | 14458880      |
| train/                  |               |
|    approx_kl            | 0.00042881165 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -229          |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0005        |
|    loss                 | 107           |
|    n_updates            | 35290         |
|    policy_gradient_loss | -0.00109      |
|    std                  | 448           |
|    value_loss           | 237           |
-------------------------------------------
Iteration: 3531 | Episodes: 143300 | Median Reward: 3.63 | Max Reward: 48.13
Iteration: 3533 | Episodes: 143400 | Median Reward: 7.51 | Max Reward: 48.13
Iteration: 3536 | Episodes: 143500 | Median Reward: -0.15 | Max Reward: 48.13
Iteration: 3538 | Episodes: 143600 | Median Reward: 7.91 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.1        |
| time/                   |              |
|    fps                  | 313          |
|    iterations           | 3540         |
|    time_elapsed         | 46272        |
|    total_timesteps      | 14499840     |
| train/                  |              |
|    approx_kl            | 0.0022704601 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -230         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 97.7         |
|    n_updates            | 35390        |
|    policy_gradient_loss | -0.00325     |
|    std                  | 457          |
|    value_loss           | 218          |
------------------------------------------
Iteration: 3541 | Episodes: 143700 | Median Reward: 9.06 | Max Reward: 48.13
Iteration: 3543 | Episodes: 143800 | Median Reward: 13.52 | Max Reward: 48.13
Iteration: 3546 | Episodes: 143900 | Median Reward: 8.31 | Max Reward: 48.13
Iteration: 3548 | Episodes: 144000 | Median Reward: 10.03 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.5         |
| time/                   |               |
|    fps                  | 313           |
|    iterations           | 3550          |
|    time_elapsed         | 46422         |
|    total_timesteps      | 14540800      |
| train/                  |               |
|    approx_kl            | 2.0222142e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -230          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 79.4          |
|    n_updates            | 35490         |
|    policy_gradient_loss | -0.000372     |
|    std                  | 467           |
|    value_loss           | 182           |
-------------------------------------------
Iteration: 3551 | Episodes: 144100 | Median Reward: 6.12 | Max Reward: 48.13
Iteration: 3553 | Episodes: 144200 | Median Reward: 6.53 | Max Reward: 48.13
Iteration: 3556 | Episodes: 144300 | Median Reward: 1.98 | Max Reward: 48.13
Iteration: 3558 | Episodes: 144400 | Median Reward: 6.52 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -94.7        |
| time/                   |              |
|    fps                  | 313          |
|    iterations           | 3560         |
|    time_elapsed         | 46572        |
|    total_timesteps      | 14581760     |
| train/                  |              |
|    approx_kl            | 8.636128e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -230         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 103          |
|    n_updates            | 35590        |
|    policy_gradient_loss | -0.000373    |
|    std                  | 475          |
|    value_loss           | 228          |
------------------------------------------
Iteration: 3560 | Episodes: 144500 | Median Reward: 8.34 | Max Reward: 48.13
Iteration: 3563 | Episodes: 144600 | Median Reward: 13.30 | Max Reward: 48.13
Iteration: 3565 | Episodes: 144700 | Median Reward: 2.57 | Max Reward: 48.13
Iteration: 3568 | Episodes: 144800 | Median Reward: 11.88 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -89.6        |
| time/                   |              |
|    fps                  | 312          |
|    iterations           | 3570         |
|    time_elapsed         | 46726        |
|    total_timesteps      | 14622720     |
| train/                  |              |
|    approx_kl            | 0.0017451065 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -231         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 110          |
|    n_updates            | 35690        |
|    policy_gradient_loss | -0.00342     |
|    std                  | 481          |
|    value_loss           | 243          |
------------------------------------------
Iteration: 3570 | Episodes: 144900 | Median Reward: 2.21 | Max Reward: 48.13
Iteration: 3573 | Episodes: 145000 | Median Reward: 10.81 | Max Reward: 48.13
Iteration: 3575 | Episodes: 145100 | Median Reward: 19.73 | Max Reward: 48.13
Iteration: 3578 | Episodes: 145200 | Median Reward: 11.14 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.2        |
| time/                   |              |
|    fps                  | 312          |
|    iterations           | 3580         |
|    time_elapsed         | 46883        |
|    total_timesteps      | 14663680     |
| train/                  |              |
|    approx_kl            | 0.0005991788 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -231         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 100          |
|    n_updates            | 35790        |
|    policy_gradient_loss | -0.000774    |
|    std                  | 486          |
|    value_loss           | 224          |
------------------------------------------
Iteration: 3580 | Episodes: 145300 | Median Reward: 5.04 | Max Reward: 48.13
Iteration: 3583 | Episodes: 145400 | Median Reward: 11.74 | Max Reward: 48.13
Iteration: 3585 | Episodes: 145500 | Median Reward: 21.41 | Max Reward: 48.13
Iteration: 3588 | Episodes: 145600 | Median Reward: 11.56 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -90           |
| time/                   |               |
|    fps                  | 312           |
|    iterations           | 3590          |
|    time_elapsed         | 47041         |
|    total_timesteps      | 14704640      |
| train/                  |               |
|    approx_kl            | 2.3435772e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -232          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 101           |
|    n_updates            | 35890         |
|    policy_gradient_loss | -6.04e-05     |
|    std                  | 497           |
|    value_loss           | 226           |
-------------------------------------------
Iteration: 3590 | Episodes: 145700 | Median Reward: 11.92 | Max Reward: 48.13
Iteration: 3592 | Episodes: 145800 | Median Reward: 10.22 | Max Reward: 48.13
Iteration: 3595 | Episodes: 145900 | Median Reward: 17.07 | Max Reward: 48.13
Iteration: 3597 | Episodes: 146000 | Median Reward: 9.72 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.9         |
| time/                   |               |
|    fps                  | 312           |
|    iterations           | 3600          |
|    time_elapsed         | 47191         |
|    total_timesteps      | 14745600      |
| train/                  |               |
|    approx_kl            | 1.8470353e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -232          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 96.9          |
|    n_updates            | 35990         |
|    policy_gradient_loss | -7.8e-05      |
|    std                  | 505           |
|    value_loss           | 217           |
-------------------------------------------
Iteration: 3600 | Episodes: 146100 | Median Reward: 9.09 | Max Reward: 48.13
Iteration: 3602 | Episodes: 146200 | Median Reward: 11.52 | Max Reward: 48.13
Iteration: 3605 | Episodes: 146300 | Median Reward: 15.89 | Max Reward: 48.13
Iteration: 3607 | Episodes: 146400 | Median Reward: 7.26 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.7         |
| time/                   |               |
|    fps                  | 312           |
|    iterations           | 3610          |
|    time_elapsed         | 47342         |
|    total_timesteps      | 14786560      |
| train/                  |               |
|    approx_kl            | 1.7352635e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -232          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 102           |
|    n_updates            | 36090         |
|    policy_gradient_loss | -0.000202     |
|    std                  | 515           |
|    value_loss           | 228           |
-------------------------------------------
Iteration: 3610 | Episodes: 146500 | Median Reward: 4.76 | Max Reward: 48.13
Iteration: 3612 | Episodes: 146600 | Median Reward: 5.51 | Max Reward: 48.13
Iteration: 3615 | Episodes: 146700 | Median Reward: 13.61 | Max Reward: 48.13
Iteration: 3617 | Episodes: 146800 | Median Reward: 7.46 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -98.8        |
| time/                   |              |
|    fps                  | 312          |
|    iterations           | 3620         |
|    time_elapsed         | 47491        |
|    total_timesteps      | 14827520     |
| train/                  |              |
|    approx_kl            | 1.347842e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -233         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 63.4         |
|    n_updates            | 36190        |
|    policy_gradient_loss | -9.69e-05    |
|    std                  | 524          |
|    value_loss           | 151          |
------------------------------------------
Iteration: 3620 | Episodes: 146900 | Median Reward: 1.44 | Max Reward: 48.13
Iteration: 3622 | Episodes: 147000 | Median Reward: 3.36 | Max Reward: 48.13
Iteration: 3625 | Episodes: 147100 | Median Reward: -3.58 | Max Reward: 48.13
Iteration: 3627 | Episodes: 147200 | Median Reward: 16.71 | Max Reward: 48.13
Iteration: 3629 | Episodes: 147300 | Median Reward: 10.53 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -94.9        |
| time/                   |              |
|    fps                  | 312          |
|    iterations           | 3630         |
|    time_elapsed         | 47641        |
|    total_timesteps      | 14868480     |
| train/                  |              |
|    approx_kl            | 7.730356e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -233         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 67.5         |
|    n_updates            | 36290        |
|    policy_gradient_loss | -0.00028     |
|    std                  | 529          |
|    value_loss           | 158          |
------------------------------------------
Iteration: 3632 | Episodes: 147400 | Median Reward: 1.98 | Max Reward: 48.13
Iteration: 3634 | Episodes: 147500 | Median Reward: 2.77 | Max Reward: 48.13
Iteration: 3637 | Episodes: 147600 | Median Reward: 2.02 | Max Reward: 48.13
Iteration: 3639 | Episodes: 147700 | Median Reward: 10.73 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.1         |
| time/                   |               |
|    fps                  | 311           |
|    iterations           | 3640          |
|    time_elapsed         | 47792         |
|    total_timesteps      | 14909440      |
| train/                  |               |
|    approx_kl            | 2.2216962e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -233          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 74.7          |
|    n_updates            | 36390         |
|    policy_gradient_loss | -0.000204     |
|    std                  | 533           |
|    value_loss           | 173           |
-------------------------------------------
Iteration: 3642 | Episodes: 147800 | Median Reward: 17.67 | Max Reward: 48.13
Iteration: 3644 | Episodes: 147900 | Median Reward: 8.14 | Max Reward: 48.13
Iteration: 3647 | Episodes: 148000 | Median Reward: 10.64 | Max Reward: 48.13
Iteration: 3649 | Episodes: 148100 | Median Reward: 1.33 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -100         |
| time/                   |              |
|    fps                  | 311          |
|    iterations           | 3650         |
|    time_elapsed         | 47947        |
|    total_timesteps      | 14950400     |
| train/                  |              |
|    approx_kl            | 7.451045e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -234         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 86.7         |
|    n_updates            | 36490        |
|    policy_gradient_loss | -0.000583    |
|    std                  | 546          |
|    value_loss           | 197          |
------------------------------------------
Iteration: 3652 | Episodes: 148200 | Median Reward: 18.04 | Max Reward: 48.13
Iteration: 3654 | Episodes: 148300 | Median Reward: 14.77 | Max Reward: 48.13
Iteration: 3657 | Episodes: 148400 | Median Reward: 5.02 | Max Reward: 48.13
Iteration: 3659 | Episodes: 148500 | Median Reward: 8.12 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.5         |
| time/                   |               |
|    fps                  | 311           |
|    iterations           | 3660          |
|    time_elapsed         | 48103         |
|    total_timesteps      | 14991360      |
| train/                  |               |
|    approx_kl            | 2.6356167e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -234          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 77.8          |
|    n_updates            | 36590         |
|    policy_gradient_loss | -2.52e-05     |
|    std                  | 550           |
|    value_loss           | 179           |
-------------------------------------------
Iteration: 3662 | Episodes: 148600 | Median Reward: 15.95 | Max Reward: 48.13
Iteration: 3664 | Episodes: 148700 | Median Reward: 4.38 | Max Reward: 48.13
Iteration: 3666 | Episodes: 148800 | Median Reward: -0.02 | Max Reward: 48.13
Iteration: 3669 | Episodes: 148900 | Median Reward: 4.63 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -93.8        |
| time/                   |              |
|    fps                  | 311          |
|    iterations           | 3670         |
|    time_elapsed         | 48256        |
|    total_timesteps      | 15032320     |
| train/                  |              |
|    approx_kl            | 0.0017812641 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -234         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 90.7         |
|    n_updates            | 36690        |
|    policy_gradient_loss | -0.00192     |
|    std                  | 560          |
|    value_loss           | 205          |
------------------------------------------
Iteration: 3671 | Episodes: 149000 | Median Reward: 2.50 | Max Reward: 48.13
Iteration: 3674 | Episodes: 149100 | Median Reward: 0.21 | Max Reward: 48.13
Iteration: 3676 | Episodes: 149200 | Median Reward: 3.73 | Max Reward: 48.13
Iteration: 3679 | Episodes: 149300 | Median Reward: 4.05 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92           |
| time/                   |               |
|    fps                  | 311           |
|    iterations           | 3680          |
|    time_elapsed         | 48412         |
|    total_timesteps      | 15073280      |
| train/                  |               |
|    approx_kl            | 0.00018525653 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -235          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 71.3          |
|    n_updates            | 36790         |
|    policy_gradient_loss | -8.77e-05     |
|    std                  | 568           |
|    value_loss           | 166           |
-------------------------------------------
Iteration: 3681 | Episodes: 149400 | Median Reward: -1.19 | Max Reward: 48.13
Iteration: 3684 | Episodes: 149500 | Median Reward: 3.67 | Max Reward: 48.13
Iteration: 3686 | Episodes: 149600 | Median Reward: 12.10 | Max Reward: 48.13
Iteration: 3689 | Episodes: 149700 | Median Reward: 6.36 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.1         |
| time/                   |               |
|    fps                  | 311           |
|    iterations           | 3690          |
|    time_elapsed         | 48563         |
|    total_timesteps      | 15114240      |
| train/                  |               |
|    approx_kl            | 5.0353716e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -235          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 98.4          |
|    n_updates            | 36890         |
|    policy_gradient_loss | -0.000429     |
|    std                  | 573           |
|    value_loss           | 220           |
-------------------------------------------
Iteration: 3691 | Episodes: 149800 | Median Reward: 14.53 | Max Reward: 48.13
Iteration: 3694 | Episodes: 149900 | Median Reward: 9.93 | Max Reward: 48.13
Iteration: 3696 | Episodes: 150000 | Median Reward: 2.83 | Max Reward: 48.13
Iteration: 3699 | Episodes: 150100 | Median Reward: 9.20 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.4         |
| time/                   |               |
|    fps                  | 311           |
|    iterations           | 3700          |
|    time_elapsed         | 48715         |
|    total_timesteps      | 15155200      |
| train/                  |               |
|    approx_kl            | 8.2912346e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -235          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 86.7          |
|    n_updates            | 36990         |
|    policy_gradient_loss | -0.000322     |
|    std                  | 584           |
|    value_loss           | 197           |
-------------------------------------------
Iteration: 3701 | Episodes: 150200 | Median Reward: 8.32 | Max Reward: 48.13
Iteration: 3703 | Episodes: 150300 | Median Reward: 9.43 | Max Reward: 48.13
Iteration: 3706 | Episodes: 150400 | Median Reward: 16.76 | Max Reward: 48.13
Iteration: 3708 | Episodes: 150500 | Median Reward: 4.52 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.9        |
| time/                   |              |
|    fps                  | 310          |
|    iterations           | 3710         |
|    time_elapsed         | 48871        |
|    total_timesteps      | 15196160     |
| train/                  |              |
|    approx_kl            | 0.0003674426 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -236         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 117          |
|    n_updates            | 37090        |
|    policy_gradient_loss | -0.000604    |
|    std                  | 597          |
|    value_loss           | 257          |
------------------------------------------
Iteration: 3711 | Episodes: 150600 | Median Reward: 12.94 | Max Reward: 48.13
Iteration: 3713 | Episodes: 150700 | Median Reward: 8.66 | Max Reward: 48.13
Iteration: 3716 | Episodes: 150800 | Median Reward: 11.98 | Max Reward: 48.13
Iteration: 3718 | Episodes: 150900 | Median Reward: -1.04 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -95.9         |
| time/                   |               |
|    fps                  | 310           |
|    iterations           | 3720          |
|    time_elapsed         | 49034         |
|    total_timesteps      | 15237120      |
| train/                  |               |
|    approx_kl            | 0.00019643956 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -236          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 87.2          |
|    n_updates            | 37190         |
|    policy_gradient_loss | -0.00118      |
|    std                  | 603           |
|    value_loss           | 198           |
-------------------------------------------
Iteration: 3721 | Episodes: 151000 | Median Reward: 9.08 | Max Reward: 48.13
Iteration: 3723 | Episodes: 151100 | Median Reward: 1.88 | Max Reward: 48.13
Iteration: 3726 | Episodes: 151200 | Median Reward: 10.55 | Max Reward: 48.13
Iteration: 3728 | Episodes: 151300 | Median Reward: 4.11 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -97.3         |
| time/                   |               |
|    fps                  | 310           |
|    iterations           | 3730          |
|    time_elapsed         | 49190         |
|    total_timesteps      | 15278080      |
| train/                  |               |
|    approx_kl            | 0.00054331194 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -237          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 84.1          |
|    n_updates            | 37290         |
|    policy_gradient_loss | 0.00363       |
|    std                  | 614           |
|    value_loss           | 192           |
-------------------------------------------
Iteration: 3731 | Episodes: 151400 | Median Reward: 5.42 | Max Reward: 48.13
Iteration: 3733 | Episodes: 151500 | Median Reward: 13.86 | Max Reward: 48.13
Iteration: 3736 | Episodes: 151600 | Median Reward: 13.19 | Max Reward: 48.13
Iteration: 3738 | Episodes: 151700 | Median Reward: 0.66 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -104          |
| time/                   |               |
|    fps                  | 310           |
|    iterations           | 3740          |
|    time_elapsed         | 49344         |
|    total_timesteps      | 15319040      |
| train/                  |               |
|    approx_kl            | 2.2406952e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -237          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 68            |
|    n_updates            | 37390         |
|    policy_gradient_loss | -0.000161     |
|    std                  | 623           |
|    value_loss           | 160           |
-------------------------------------------
Iteration: 3740 | Episodes: 151800 | Median Reward: 3.37 | Max Reward: 48.13
Iteration: 3743 | Episodes: 151900 | Median Reward: 8.40 | Max Reward: 48.13
Iteration: 3745 | Episodes: 152000 | Median Reward: 10.36 | Max Reward: 48.13
Iteration: 3748 | Episodes: 152100 | Median Reward: 11.23 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -90.5         |
| time/                   |               |
|    fps                  | 310           |
|    iterations           | 3750          |
|    time_elapsed         | 49496         |
|    total_timesteps      | 15360000      |
| train/                  |               |
|    approx_kl            | 5.5032724e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -237          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 128           |
|    n_updates            | 37490         |
|    policy_gradient_loss | -8.72e-06     |
|    std                  | 626           |
|    value_loss           | 280           |
-------------------------------------------
Iteration: 3750 | Episodes: 152200 | Median Reward: 1.10 | Max Reward: 48.13
Iteration: 3753 | Episodes: 152300 | Median Reward: 7.44 | Max Reward: 48.13
Iteration: 3755 | Episodes: 152400 | Median Reward: 8.02 | Max Reward: 48.13
Iteration: 3758 | Episodes: 152500 | Median Reward: 6.13 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.4         |
| time/                   |               |
|    fps                  | 310           |
|    iterations           | 3760          |
|    time_elapsed         | 49648         |
|    total_timesteps      | 15400960      |
| train/                  |               |
|    approx_kl            | 2.5885383e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -237          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 100           |
|    n_updates            | 37590         |
|    policy_gradient_loss | -0.000105     |
|    std                  | 635           |
|    value_loss           | 224           |
-------------------------------------------
Iteration: 3760 | Episodes: 152600 | Median Reward: 5.94 | Max Reward: 48.13
Iteration: 3763 | Episodes: 152700 | Median Reward: 9.25 | Max Reward: 48.13
Iteration: 3765 | Episodes: 152800 | Median Reward: 9.78 | Max Reward: 48.13
Iteration: 3768 | Episodes: 152900 | Median Reward: 7.13 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.7         |
| time/                   |               |
|    fps                  | 310           |
|    iterations           | 3770          |
|    time_elapsed         | 49801         |
|    total_timesteps      | 15441920      |
| train/                  |               |
|    approx_kl            | 0.00028680742 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -238          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 43.5          |
|    n_updates            | 37690         |
|    policy_gradient_loss | -0.000611     |
|    std                  | 651           |
|    value_loss           | 111           |
-------------------------------------------
Iteration: 3770 | Episodes: 153000 | Median Reward: 3.39 | Max Reward: 48.13
Iteration: 3773 | Episodes: 153100 | Median Reward: 9.68 | Max Reward: 48.13
Iteration: 3775 | Episodes: 153200 | Median Reward: 16.79 | Max Reward: 48.13
Iteration: 3777 | Episodes: 153300 | Median Reward: 12.09 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -94.9         |
| time/                   |               |
|    fps                  | 309           |
|    iterations           | 3780          |
|    time_elapsed         | 49957         |
|    total_timesteps      | 15482880      |
| train/                  |               |
|    approx_kl            | 0.00011958415 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -239          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 56.6          |
|    n_updates            | 37790         |
|    policy_gradient_loss | 0.000273      |
|    std                  | 672           |
|    value_loss           | 137           |
-------------------------------------------
Iteration: 3780 | Episodes: 153400 | Median Reward: 10.04 | Max Reward: 48.13
Iteration: 3782 | Episodes: 153500 | Median Reward: 10.90 | Max Reward: 48.13
Iteration: 3785 | Episodes: 153600 | Median Reward: 16.24 | Max Reward: 48.13
Iteration: 3787 | Episodes: 153700 | Median Reward: 8.15 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89           |
| time/                   |               |
|    fps                  | 309           |
|    iterations           | 3790          |
|    time_elapsed         | 50120         |
|    total_timesteps      | 15523840      |
| train/                  |               |
|    approx_kl            | 9.1317925e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -239          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 124           |
|    n_updates            | 37890         |
|    policy_gradient_loss | 8.93e-05      |
|    std                  | 683           |
|    value_loss           | 272           |
-------------------------------------------
Iteration: 3790 | Episodes: 153800 | Median Reward: 15.09 | Max Reward: 48.13
Iteration: 3792 | Episodes: 153900 | Median Reward: 7.25 | Max Reward: 48.13
Iteration: 3795 | Episodes: 154000 | Median Reward: 6.41 | Max Reward: 48.13
Iteration: 3797 | Episodes: 154100 | Median Reward: 13.37 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.4         |
| time/                   |               |
|    fps                  | 309           |
|    iterations           | 3800          |
|    time_elapsed         | 50274         |
|    total_timesteps      | 15564800      |
| train/                  |               |
|    approx_kl            | 8.8386325e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -240          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 108           |
|    n_updates            | 37990         |
|    policy_gradient_loss | -0.000445     |
|    std                  | 695           |
|    value_loss           | 241           |
-------------------------------------------
Iteration: 3800 | Episodes: 154200 | Median Reward: 3.74 | Max Reward: 48.13
Iteration: 3802 | Episodes: 154300 | Median Reward: 10.24 | Max Reward: 48.13
Iteration: 3805 | Episodes: 154400 | Median Reward: 4.09 | Max Reward: 48.13
Iteration: 3807 | Episodes: 154500 | Median Reward: 6.88 | Max Reward: 48.13
Iteration: 3809 | Episodes: 154600 | Median Reward: 3.72 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -95.7        |
| time/                   |              |
|    fps                  | 309          |
|    iterations           | 3810         |
|    time_elapsed         | 50427        |
|    total_timesteps      | 15605760     |
| train/                  |              |
|    approx_kl            | 0.0002125675 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -240         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 88.1         |
|    n_updates            | 38090        |
|    policy_gradient_loss | -0.000737    |
|    std                  | 700          |
|    value_loss           | 200          |
------------------------------------------
Iteration: 3812 | Episodes: 154700 | Median Reward: 7.77 | Max Reward: 48.13
Iteration: 3814 | Episodes: 154800 | Median Reward: 16.08 | Max Reward: 48.13
Iteration: 3817 | Episodes: 154900 | Median Reward: 6.38 | Max Reward: 48.13
Iteration: 3819 | Episodes: 155000 | Median Reward: 4.32 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -96.1        |
| time/                   |              |
|    fps                  | 309          |
|    iterations           | 3820         |
|    time_elapsed         | 50583        |
|    total_timesteps      | 15646720     |
| train/                  |              |
|    approx_kl            | 9.933395e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -240         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 67           |
|    n_updates            | 38190        |
|    policy_gradient_loss | 0.00145      |
|    std                  | 712          |
|    value_loss           | 158          |
------------------------------------------
Iteration: 3822 | Episodes: 155100 | Median Reward: 11.05 | Max Reward: 48.13
Iteration: 3824 | Episodes: 155200 | Median Reward: 12.31 | Max Reward: 48.13
Iteration: 3827 | Episodes: 155300 | Median Reward: 14.75 | Max Reward: 48.13
Iteration: 3829 | Episodes: 155400 | Median Reward: 8.05 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.5         |
| time/                   |               |
|    fps                  | 309           |
|    iterations           | 3830          |
|    time_elapsed         | 50736         |
|    total_timesteps      | 15687680      |
| train/                  |               |
|    approx_kl            | 0.00027540818 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -240          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 74.4          |
|    n_updates            | 38290         |
|    policy_gradient_loss | 4.38e-05      |
|    std                  | 722           |
|    value_loss           | 173           |
-------------------------------------------
Iteration: 3832 | Episodes: 155500 | Median Reward: 7.91 | Max Reward: 48.13
Iteration: 3834 | Episodes: 155600 | Median Reward: 10.53 | Max Reward: 48.13
Iteration: 3837 | Episodes: 155700 | Median Reward: 12.92 | Max Reward: 48.13
Iteration: 3839 | Episodes: 155800 | Median Reward: 4.61 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -90.8         |
| time/                   |               |
|    fps                  | 309           |
|    iterations           | 3840          |
|    time_elapsed         | 50889         |
|    total_timesteps      | 15728640      |
| train/                  |               |
|    approx_kl            | 2.0190113e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -241          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 74.3          |
|    n_updates            | 38390         |
|    policy_gradient_loss | -0.000112     |
|    std                  | 732           |
|    value_loss           | 173           |
-------------------------------------------
Iteration: 3842 | Episodes: 155900 | Median Reward: 13.08 | Max Reward: 48.13
Iteration: 3844 | Episodes: 156000 | Median Reward: 13.12 | Max Reward: 48.13
Iteration: 3846 | Episodes: 156100 | Median Reward: 0.56 | Max Reward: 48.13
Iteration: 3849 | Episodes: 156200 | Median Reward: 2.14 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.4         |
| time/                   |               |
|    fps                  | 308           |
|    iterations           | 3850          |
|    time_elapsed         | 51040         |
|    total_timesteps      | 15769600      |
| train/                  |               |
|    approx_kl            | 0.00019249103 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -241          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 84.1          |
|    n_updates            | 38490         |
|    policy_gradient_loss | -0.000482     |
|    std                  | 739           |
|    value_loss           | 192           |
-------------------------------------------
Iteration: 3851 | Episodes: 156300 | Median Reward: 6.17 | Max Reward: 48.13
Iteration: 3854 | Episodes: 156400 | Median Reward: 4.83 | Max Reward: 48.13
Iteration: 3856 | Episodes: 156500 | Median Reward: 5.48 | Max Reward: 48.13
Iteration: 3859 | Episodes: 156600 | Median Reward: 13.79 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -90.7       |
| time/                   |             |
|    fps                  | 308         |
|    iterations           | 3860        |
|    time_elapsed         | 51207       |
|    total_timesteps      | 15810560    |
| train/                  |             |
|    approx_kl            | 0.003211802 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -242        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 115         |
|    n_updates            | 38590       |
|    policy_gradient_loss | 0.00123     |
|    std                  | 756         |
|    value_loss           | 254         |
-----------------------------------------
Iteration: 3861 | Episodes: 156700 | Median Reward: 8.49 | Max Reward: 48.13
Iteration: 3864 | Episodes: 156800 | Median Reward: 7.88 | Max Reward: 48.13
Iteration: 3866 | Episodes: 156900 | Median Reward: 7.19 | Max Reward: 48.13
Iteration: 3869 | Episodes: 157000 | Median Reward: 13.91 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -89          |
| time/                   |              |
|    fps                  | 308          |
|    iterations           | 3870         |
|    time_elapsed         | 51359        |
|    total_timesteps      | 15851520     |
| train/                  |              |
|    approx_kl            | 0.0008708781 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -242         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 114          |
|    n_updates            | 38690        |
|    policy_gradient_loss | -0.000444    |
|    std                  | 776          |
|    value_loss           | 252          |
------------------------------------------
Iteration: 3871 | Episodes: 157100 | Median Reward: 12.64 | Max Reward: 48.13
Iteration: 3874 | Episodes: 157200 | Median Reward: 15.94 | Max Reward: 48.13
Iteration: 3876 | Episodes: 157300 | Median Reward: 11.28 | Max Reward: 48.13
Iteration: 3879 | Episodes: 157400 | Median Reward: 12.23 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -89.3       |
| time/                   |             |
|    fps                  | 308         |
|    iterations           | 3880        |
|    time_elapsed         | 51512       |
|    total_timesteps      | 15892480    |
| train/                  |             |
|    approx_kl            | 1.78857e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -243        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 105         |
|    n_updates            | 38790       |
|    policy_gradient_loss | -0.000155   |
|    std                  | 789         |
|    value_loss           | 235         |
-----------------------------------------
Iteration: 3881 | Episodes: 157500 | Median Reward: 6.16 | Max Reward: 48.13
Iteration: 3883 | Episodes: 157600 | Median Reward: 8.74 | Max Reward: 48.13
Iteration: 3886 | Episodes: 157700 | Median Reward: 2.23 | Max Reward: 48.13
Iteration: 3888 | Episodes: 157800 | Median Reward: 8.23 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.4         |
| time/                   |               |
|    fps                  | 308           |
|    iterations           | 3890          |
|    time_elapsed         | 51664         |
|    total_timesteps      | 15933440      |
| train/                  |               |
|    approx_kl            | 6.2770705e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -243          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 81.9          |
|    n_updates            | 38890         |
|    policy_gradient_loss | -5.32e-05     |
|    std                  | 795           |
|    value_loss           | 188           |
-------------------------------------------
Iteration: 3891 | Episodes: 157900 | Median Reward: 4.33 | Max Reward: 48.13
Iteration: 3893 | Episodes: 158000 | Median Reward: 7.80 | Max Reward: 48.13
Iteration: 3896 | Episodes: 158100 | Median Reward: 7.13 | Max Reward: 48.13
Iteration: 3898 | Episodes: 158200 | Median Reward: 11.25 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.7         |
| time/                   |               |
|    fps                  | 308           |
|    iterations           | 3900          |
|    time_elapsed         | 51816         |
|    total_timesteps      | 15974400      |
| train/                  |               |
|    approx_kl            | 0.00015670447 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -243          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 97.2          |
|    n_updates            | 38990         |
|    policy_gradient_loss | -0.000338     |
|    std                  | 802           |
|    value_loss           | 219           |
-------------------------------------------
Iteration: 3901 | Episodes: 158300 | Median Reward: 4.89 | Max Reward: 48.13
Iteration: 3903 | Episodes: 158400 | Median Reward: 11.95 | Max Reward: 48.13
Iteration: 3906 | Episodes: 158500 | Median Reward: 8.34 | Max Reward: 48.13
Iteration: 3908 | Episodes: 158600 | Median Reward: 8.91 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.7         |
| time/                   |               |
|    fps                  | 308           |
|    iterations           | 3910          |
|    time_elapsed         | 51970         |
|    total_timesteps      | 16015360      |
| train/                  |               |
|    approx_kl            | 0.00026657124 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -243          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 83.4          |
|    n_updates            | 39090         |
|    policy_gradient_loss | -0.000424     |
|    std                  | 812           |
|    value_loss           | 191           |
-------------------------------------------
Iteration: 3911 | Episodes: 158700 | Median Reward: 0.11 | Max Reward: 48.13
Iteration: 3913 | Episodes: 158800 | Median Reward: 9.50 | Max Reward: 48.13
Iteration: 3916 | Episodes: 158900 | Median Reward: 10.63 | Max Reward: 48.13
Iteration: 3918 | Episodes: 159000 | Median Reward: 10.00 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -97.8         |
| time/                   |               |
|    fps                  | 307           |
|    iterations           | 3920          |
|    time_elapsed         | 52131         |
|    total_timesteps      | 16056320      |
| train/                  |               |
|    approx_kl            | 0.00029484258 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -244          |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0005        |
|    loss                 | 66            |
|    n_updates            | 39190         |
|    policy_gradient_loss | 0.000373      |
|    std                  | 821           |
|    value_loss           | 156           |
-------------------------------------------
Iteration: 3920 | Episodes: 159100 | Median Reward: 3.88 | Max Reward: 48.13
Iteration: 3923 | Episodes: 159200 | Median Reward: -3.12 | Max Reward: 48.13
Iteration: 3925 | Episodes: 159300 | Median Reward: 12.92 | Max Reward: 48.13
Iteration: 3928 | Episodes: 159400 | Median Reward: 7.45 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -94.1        |
| time/                   |              |
|    fps                  | 307          |
|    iterations           | 3930         |
|    time_elapsed         | 52285        |
|    total_timesteps      | 16097280     |
| train/                  |              |
|    approx_kl            | 0.0041862293 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -244         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 89.6         |
|    n_updates            | 39290        |
|    policy_gradient_loss | -0.00414     |
|    std                  | 836          |
|    value_loss           | 204          |
------------------------------------------
Iteration: 3930 | Episodes: 159500 | Median Reward: 14.19 | Max Reward: 48.13
Iteration: 3933 | Episodes: 159600 | Median Reward: 9.92 | Max Reward: 48.13
Iteration: 3935 | Episodes: 159700 | Median Reward: 15.43 | Max Reward: 48.13
Iteration: 3938 | Episodes: 159800 | Median Reward: 2.30 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -91.8        |
| time/                   |              |
|    fps                  | 307          |
|    iterations           | 3940         |
|    time_elapsed         | 52437        |
|    total_timesteps      | 16138240     |
| train/                  |              |
|    approx_kl            | 0.0022674005 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -244         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 100          |
|    n_updates            | 39390        |
|    policy_gradient_loss | -0.00417     |
|    std                  | 858          |
|    value_loss           | 225          |
------------------------------------------
Iteration: 3940 | Episodes: 159900 | Median Reward: 13.00 | Max Reward: 48.13
Iteration: 3943 | Episodes: 160000 | Median Reward: 6.25 | Max Reward: 48.13
Iteration: 3945 | Episodes: 160100 | Median Reward: 13.16 | Max Reward: 48.13
Iteration: 3948 | Episodes: 160200 | Median Reward: 12.10 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87.9        |
| time/                   |              |
|    fps                  | 307          |
|    iterations           | 3950         |
|    time_elapsed         | 52591        |
|    total_timesteps      | 16179200     |
| train/                  |              |
|    approx_kl            | 2.325834e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -245         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 94.8         |
|    n_updates            | 39490        |
|    policy_gradient_loss | -2.21e-06    |
|    std                  | 868          |
|    value_loss           | 214          |
------------------------------------------
Iteration: 3950 | Episodes: 160300 | Median Reward: 8.09 | Max Reward: 48.13
Iteration: 3953 | Episodes: 160400 | Median Reward: 8.46 | Max Reward: 48.13
Iteration: 3955 | Episodes: 160500 | Median Reward: 12.53 | Max Reward: 48.13
Iteration: 3957 | Episodes: 160600 | Median Reward: 4.47 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.5         |
| time/                   |               |
|    fps                  | 307           |
|    iterations           | 3960          |
|    time_elapsed         | 52743         |
|    total_timesteps      | 16220160      |
| train/                  |               |
|    approx_kl            | 0.00011824496 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -245          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 67.8          |
|    n_updates            | 39590         |
|    policy_gradient_loss | -0.000556     |
|    std                  | 884           |
|    value_loss           | 160           |
-------------------------------------------
Iteration: 3960 | Episodes: 160700 | Median Reward: 2.06 | Max Reward: 48.13
Iteration: 3962 | Episodes: 160800 | Median Reward: 12.75 | Max Reward: 48.13
Iteration: 3965 | Episodes: 160900 | Median Reward: 0.25 | Max Reward: 48.13
Iteration: 3967 | Episodes: 161000 | Median Reward: 6.17 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -94.3         |
| time/                   |               |
|    fps                  | 307           |
|    iterations           | 3970          |
|    time_elapsed         | 52897         |
|    total_timesteps      | 16261120      |
| train/                  |               |
|    approx_kl            | 2.3987974e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -245          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 73.9          |
|    n_updates            | 39690         |
|    policy_gradient_loss | -0.000176     |
|    std                  | 892           |
|    value_loss           | 172           |
-------------------------------------------
Iteration: 3970 | Episodes: 161100 | Median Reward: 3.85 | Max Reward: 48.13
Iteration: 3972 | Episodes: 161200 | Median Reward: 10.14 | Max Reward: 48.13
Iteration: 3975 | Episodes: 161300 | Median Reward: 9.78 | Max Reward: 48.13
Iteration: 3977 | Episodes: 161400 | Median Reward: 6.64 | Max Reward: 48.13
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -92        |
| time/                   |            |
|    fps                  | 307        |
|    iterations           | 3980       |
|    time_elapsed         | 53049      |
|    total_timesteps      | 16302080   |
| train/                  |            |
|    approx_kl            | 0.00293248 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -246       |
|    explained_variance   | 0          |
|    learning_rate        | 0.0005     |
|    loss                 | 89.1       |
|    n_updates            | 39790      |
|    policy_gradient_loss | -0.00373   |
|    std                  | 913        |
|    value_loss           | 203        |
----------------------------------------
Iteration: 3980 | Episodes: 161500 | Median Reward: 10.95 | Max Reward: 48.13
Iteration: 3982 | Episodes: 161600 | Median Reward: 4.38 | Max Reward: 48.13
Iteration: 3985 | Episodes: 161700 | Median Reward: 14.25 | Max Reward: 48.13
Iteration: 3987 | Episodes: 161800 | Median Reward: 1.32 | Max Reward: 48.13
Iteration: 3989 | Episodes: 161900 | Median Reward: -0.80 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.2         |
| time/                   |               |
|    fps                  | 307           |
|    iterations           | 3990          |
|    time_elapsed         | 53211         |
|    total_timesteps      | 16343040      |
| train/                  |               |
|    approx_kl            | 4.5272172e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -246          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 62.5          |
|    n_updates            | 39890         |
|    policy_gradient_loss | -0.000296     |
|    std                  | 924           |
|    value_loss           | 150           |
-------------------------------------------
Iteration: 3992 | Episodes: 162000 | Median Reward: 4.48 | Max Reward: 48.13
Iteration: 3994 | Episodes: 162100 | Median Reward: 4.82 | Max Reward: 48.13
Iteration: 3997 | Episodes: 162200 | Median Reward: 11.44 | Max Reward: 48.13
Iteration: 3999 | Episodes: 162300 | Median Reward: 5.70 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.3         |
| time/                   |               |
|    fps                  | 307           |
|    iterations           | 4000          |
|    time_elapsed         | 53366         |
|    total_timesteps      | 16384000      |
| train/                  |               |
|    approx_kl            | 4.4978457e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -247          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 83.6          |
|    n_updates            | 39990         |
|    policy_gradient_loss | 0.000258      |
|    std                  | 941           |
|    value_loss           | 192           |
-------------------------------------------
Iteration: 4002 | Episodes: 162400 | Median Reward: 2.58 | Max Reward: 48.13
Iteration: 4004 | Episodes: 162500 | Median Reward: 1.87 | Max Reward: 48.13
Iteration: 4007 | Episodes: 162600 | Median Reward: 10.16 | Max Reward: 48.13
Iteration: 4009 | Episodes: 162700 | Median Reward: 14.49 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.1        |
| time/                   |              |
|    fps                  | 306          |
|    iterations           | 4010         |
|    time_elapsed         | 53517        |
|    total_timesteps      | 16424960     |
| train/                  |              |
|    approx_kl            | 8.017756e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -247         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 122          |
|    n_updates            | 40090        |
|    policy_gradient_loss | 7.59e-05     |
|    std                  | 949          |
|    value_loss           | 269          |
------------------------------------------
Iteration: 4012 | Episodes: 162800 | Median Reward: 9.32 | Max Reward: 48.13
Iteration: 4014 | Episodes: 162900 | Median Reward: 7.37 | Max Reward: 48.13
Iteration: 4017 | Episodes: 163000 | Median Reward: 1.36 | Max Reward: 48.13
Iteration: 4019 | Episodes: 163100 | Median Reward: 11.58 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -91.5        |
| time/                   |              |
|    fps                  | 306          |
|    iterations           | 4020         |
|    time_elapsed         | 53672        |
|    total_timesteps      | 16465920     |
| train/                  |              |
|    approx_kl            | 0.0013486433 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -247         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 101          |
|    n_updates            | 40190        |
|    policy_gradient_loss | -0.00208     |
|    std                  | 961          |
|    value_loss           | 227          |
------------------------------------------
Iteration: 4022 | Episodes: 163200 | Median Reward: -1.37 | Max Reward: 48.13
Iteration: 4024 | Episodes: 163300 | Median Reward: 1.79 | Max Reward: 48.13
Iteration: 4026 | Episodes: 163400 | Median Reward: 4.00 | Max Reward: 48.13
Iteration: 4029 | Episodes: 163500 | Median Reward: 17.78 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -90.2         |
| time/                   |               |
|    fps                  | 306           |
|    iterations           | 4030          |
|    time_elapsed         | 53827         |
|    total_timesteps      | 16506880      |
| train/                  |               |
|    approx_kl            | 4.2446307e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -248          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 128           |
|    n_updates            | 40290         |
|    policy_gradient_loss | -0.000224     |
|    std                  | 987           |
|    value_loss           | 282           |
-------------------------------------------
Iteration: 4031 | Episodes: 163600 | Median Reward: 8.22 | Max Reward: 48.13
Iteration: 4034 | Episodes: 163700 | Median Reward: 10.37 | Max Reward: 48.13
Iteration: 4036 | Episodes: 163800 | Median Reward: 19.98 | Max Reward: 48.13
Iteration: 4039 | Episodes: 163900 | Median Reward: 7.33 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.6         |
| time/                   |               |
|    fps                  | 306           |
|    iterations           | 4040          |
|    time_elapsed         | 53984         |
|    total_timesteps      | 16547840      |
| train/                  |               |
|    approx_kl            | 1.9288622e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -248          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 74.9          |
|    n_updates            | 40390         |
|    policy_gradient_loss | -0.000169     |
|    std                  | 1.01e+03      |
|    value_loss           | 175           |
-------------------------------------------
Iteration: 4041 | Episodes: 164000 | Median Reward: 5.95 | Max Reward: 48.13
Iteration: 4044 | Episodes: 164100 | Median Reward: 5.95 | Max Reward: 48.13
Iteration: 4046 | Episodes: 164200 | Median Reward: 7.26 | Max Reward: 48.13
Iteration: 4049 | Episodes: 164300 | Median Reward: 5.12 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -97.6         |
| time/                   |               |
|    fps                  | 306           |
|    iterations           | 4050          |
|    time_elapsed         | 54139         |
|    total_timesteps      | 16588800      |
| train/                  |               |
|    approx_kl            | 0.00020715212 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -249          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 70.8          |
|    n_updates            | 40490         |
|    policy_gradient_loss | -0.000118     |
|    std                  | 1.02e+03      |
|    value_loss           | 167           |
-------------------------------------------
Iteration: 4051 | Episodes: 164400 | Median Reward: 12.28 | Max Reward: 48.13
Iteration: 4054 | Episodes: 164500 | Median Reward: 7.18 | Max Reward: 48.13
Iteration: 4056 | Episodes: 164600 | Median Reward: 17.32 | Max Reward: 48.13
Iteration: 4059 | Episodes: 164700 | Median Reward: 0.12 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -101          |
| time/                   |               |
|    fps                  | 306           |
|    iterations           | 4060          |
|    time_elapsed         | 54297         |
|    total_timesteps      | 16629760      |
| train/                  |               |
|    approx_kl            | 2.0757318e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -249          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 67            |
|    n_updates            | 40590         |
|    policy_gradient_loss | 9.76e-05      |
|    std                  | 1.05e+03      |
|    value_loss           | 159           |
-------------------------------------------
Iteration: 4061 | Episodes: 164800 | Median Reward: 8.25 | Max Reward: 48.13
Iteration: 4063 | Episodes: 164900 | Median Reward: 8.43 | Max Reward: 48.13
Iteration: 4066 | Episodes: 165000 | Median Reward: 8.18 | Max Reward: 48.13
Iteration: 4068 | Episodes: 165100 | Median Reward: 4.93 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.2         |
| time/                   |               |
|    fps                  | 306           |
|    iterations           | 4070          |
|    time_elapsed         | 54454         |
|    total_timesteps      | 16670720      |
| train/                  |               |
|    approx_kl            | 0.00010978858 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -249          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 81.9          |
|    n_updates            | 40690         |
|    policy_gradient_loss | -0.000324     |
|    std                  | 1.05e+03      |
|    value_loss           | 189           |
-------------------------------------------
Iteration: 4071 | Episodes: 165200 | Median Reward: 8.03 | Max Reward: 48.13
Iteration: 4073 | Episodes: 165300 | Median Reward: -0.92 | Max Reward: 48.13
Iteration: 4076 | Episodes: 165400 | Median Reward: 10.00 | Max Reward: 48.13
Iteration: 4078 | Episodes: 165500 | Median Reward: 8.13 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -84.6       |
| time/                   |             |
|    fps                  | 306         |
|    iterations           | 4080        |
|    time_elapsed         | 54607       |
|    total_timesteps      | 16711680    |
| train/                  |             |
|    approx_kl            | 6.85718e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -250        |
|    explained_variance   | 1.19e-07    |
|    learning_rate        | 0.0005      |
|    loss                 | 119         |
|    n_updates            | 40790       |
|    policy_gradient_loss | -0.000106   |
|    std                  | 1.07e+03    |
|    value_loss           | 264         |
-----------------------------------------
Iteration: 4081 | Episodes: 165600 | Median Reward: 10.95 | Max Reward: 48.13
Iteration: 4083 | Episodes: 165700 | Median Reward: 6.08 | Max Reward: 48.13
Iteration: 4086 | Episodes: 165800 | Median Reward: 4.51 | Max Reward: 48.13
Iteration: 4088 | Episodes: 165900 | Median Reward: 4.70 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -99.3         |
| time/                   |               |
|    fps                  | 305           |
|    iterations           | 4090          |
|    time_elapsed         | 54759         |
|    total_timesteps      | 16752640      |
| train/                  |               |
|    approx_kl            | 3.4677214e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -250          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 62.2          |
|    n_updates            | 40890         |
|    policy_gradient_loss | 7.59e-05      |
|    std                  | 1.08e+03      |
|    value_loss           | 150           |
-------------------------------------------
Iteration: 4091 | Episodes: 166000 | Median Reward: 4.58 | Max Reward: 48.13
Iteration: 4093 | Episodes: 166100 | Median Reward: 10.74 | Max Reward: 48.13
Iteration: 4096 | Episodes: 166200 | Median Reward: 3.20 | Max Reward: 48.13
Iteration: 4098 | Episodes: 166300 | Median Reward: 5.62 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.5         |
| time/                   |               |
|    fps                  | 305           |
|    iterations           | 4100          |
|    time_elapsed         | 54913         |
|    total_timesteps      | 16793600      |
| train/                  |               |
|    approx_kl            | 0.00018573759 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -250          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 76.8          |
|    n_updates            | 40990         |
|    policy_gradient_loss | -0.000423     |
|    std                  | 1.1e+03       |
|    value_loss           | 179           |
-------------------------------------------
Iteration: 4100 | Episodes: 166400 | Median Reward: 4.33 | Max Reward: 48.13
Iteration: 4103 | Episodes: 166500 | Median Reward: 12.09 | Max Reward: 48.13
Iteration: 4105 | Episodes: 166600 | Median Reward: 5.62 | Max Reward: 48.13
Iteration: 4108 | Episodes: 166700 | Median Reward: 3.68 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.4         |
| time/                   |               |
|    fps                  | 305           |
|    iterations           | 4110          |
|    time_elapsed         | 55064         |
|    total_timesteps      | 16834560      |
| train/                  |               |
|    approx_kl            | 0.00028426806 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -251          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 77.8          |
|    n_updates            | 41090         |
|    policy_gradient_loss | -0.00125      |
|    std                  | 1.12e+03      |
|    value_loss           | 181           |
-------------------------------------------
Iteration: 4110 | Episodes: 166800 | Median Reward: 9.53 | Max Reward: 48.13
Iteration: 4113 | Episodes: 166900 | Median Reward: 9.56 | Max Reward: 48.13
Iteration: 4115 | Episodes: 167000 | Median Reward: 10.33 | Max Reward: 48.13
Iteration: 4118 | Episodes: 167100 | Median Reward: 9.09 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.5         |
| time/                   |               |
|    fps                  | 305           |
|    iterations           | 4120          |
|    time_elapsed         | 55216         |
|    total_timesteps      | 16875520      |
| train/                  |               |
|    approx_kl            | 3.1439908e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -252          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 63.3          |
|    n_updates            | 41190         |
|    policy_gradient_loss | -0.000119     |
|    std                  | 1.15e+03      |
|    value_loss           | 152           |
-------------------------------------------
Iteration: 4120 | Episodes: 167200 | Median Reward: 3.17 | Max Reward: 48.13
Iteration: 4123 | Episodes: 167300 | Median Reward: 14.19 | Max Reward: 48.13
Iteration: 4125 | Episodes: 167400 | Median Reward: 14.55 | Max Reward: 48.13
Iteration: 4128 | Episodes: 167500 | Median Reward: 6.90 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -97.7        |
| time/                   |              |
|    fps                  | 305          |
|    iterations           | 4130         |
|    time_elapsed         | 55381        |
|    total_timesteps      | 16916480     |
| train/                  |              |
|    approx_kl            | 0.0010485585 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -252         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 81.8         |
|    n_updates            | 41290        |
|    policy_gradient_loss | -0.00181     |
|    std                  | 1.17e+03     |
|    value_loss           | 189          |
------------------------------------------
Iteration: 4130 | Episodes: 167600 | Median Reward: -1.00 | Max Reward: 48.13
Iteration: 4133 | Episodes: 167700 | Median Reward: 10.80 | Max Reward: 48.13
Iteration: 4135 | Episodes: 167800 | Median Reward: 11.67 | Max Reward: 48.13
Iteration: 4137 | Episodes: 167900 | Median Reward: 4.96 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -99           |
| time/                   |               |
|    fps                  | 305           |
|    iterations           | 4140          |
|    time_elapsed         | 55532         |
|    total_timesteps      | 16957440      |
| train/                  |               |
|    approx_kl            | 5.8403428e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -252          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 79.1          |
|    n_updates            | 41390         |
|    policy_gradient_loss | -0.000463     |
|    std                  | 1.18e+03      |
|    value_loss           | 184           |
-------------------------------------------
Iteration: 4140 | Episodes: 168000 | Median Reward: -3.25 | Max Reward: 48.13
Iteration: 4142 | Episodes: 168100 | Median Reward: 1.87 | Max Reward: 48.13
Iteration: 4145 | Episodes: 168200 | Median Reward: 7.99 | Max Reward: 48.13
Iteration: 4147 | Episodes: 168300 | Median Reward: -0.75 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.5        |
| time/                   |              |
|    fps                  | 305          |
|    iterations           | 4150         |
|    time_elapsed         | 55685        |
|    total_timesteps      | 16998400     |
| train/                  |              |
|    approx_kl            | 6.618793e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -252         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 114          |
|    n_updates            | 41490        |
|    policy_gradient_loss | -3.13e-06    |
|    std                  | 1.2e+03      |
|    value_loss           | 254          |
------------------------------------------
Iteration: 4150 | Episodes: 168400 | Median Reward: 8.64 | Max Reward: 48.13
Iteration: 4152 | Episodes: 168500 | Median Reward: 6.67 | Max Reward: 48.13
Iteration: 4155 | Episodes: 168600 | Median Reward: 5.51 | Max Reward: 48.13
Iteration: 4157 | Episodes: 168700 | Median Reward: 3.96 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -90.1       |
| time/                   |             |
|    fps                  | 305         |
|    iterations           | 4160        |
|    time_elapsed         | 55839       |
|    total_timesteps      | 17039360    |
| train/                  |             |
|    approx_kl            | 0.000156618 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -253        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 93.2        |
|    n_updates            | 41590       |
|    policy_gradient_loss | -0.000828   |
|    std                  | 1.23e+03    |
|    value_loss           | 212         |
-----------------------------------------
Iteration: 4160 | Episodes: 168800 | Median Reward: 13.46 | Max Reward: 48.13
Iteration: 4162 | Episodes: 168900 | Median Reward: 3.93 | Max Reward: 48.13
Iteration: 4165 | Episodes: 169000 | Median Reward: 3.99 | Max Reward: 48.13
Iteration: 4167 | Episodes: 169100 | Median Reward: 12.99 | Max Reward: 48.13
Iteration: 4169 | Episodes: 169200 | Median Reward: 9.17 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.1        |
| time/                   |              |
|    fps                  | 305          |
|    iterations           | 4170         |
|    time_elapsed         | 55988        |
|    total_timesteps      | 17080320     |
| train/                  |              |
|    approx_kl            | 0.0002127213 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -253         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 145          |
|    n_updates            | 41690        |
|    policy_gradient_loss | 1.36e-05     |
|    std                  | 1.25e+03     |
|    value_loss           | 316          |
------------------------------------------
Iteration: 4172 | Episodes: 169300 | Median Reward: 5.66 | Max Reward: 48.13
Iteration: 4174 | Episodes: 169400 | Median Reward: 9.20 | Max Reward: 48.13
Iteration: 4177 | Episodes: 169500 | Median Reward: 2.60 | Max Reward: 48.13
Iteration: 4179 | Episodes: 169600 | Median Reward: 4.55 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.2         |
| time/                   |               |
|    fps                  | 304           |
|    iterations           | 4180          |
|    time_elapsed         | 56140         |
|    total_timesteps      | 17121280      |
| train/                  |               |
|    approx_kl            | 0.00016787967 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -254          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 95.3          |
|    n_updates            | 41790         |
|    policy_gradient_loss | -0.000338     |
|    std                  | 1.27e+03      |
|    value_loss           | 216           |
-------------------------------------------
Iteration: 4182 | Episodes: 169700 | Median Reward: 10.08 | Max Reward: 48.13
Iteration: 4184 | Episodes: 169800 | Median Reward: 12.05 | Max Reward: 48.13
Iteration: 4187 | Episodes: 169900 | Median Reward: 9.98 | Max Reward: 48.13
Iteration: 4189 | Episodes: 170000 | Median Reward: 5.59 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -94          |
| time/                   |              |
|    fps                  | 304          |
|    iterations           | 4190         |
|    time_elapsed         | 56300        |
|    total_timesteps      | 17162240     |
| train/                  |              |
|    approx_kl            | 7.704526e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -254         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 58.7         |
|    n_updates            | 41890        |
|    policy_gradient_loss | -0.000375    |
|    std                  | 1.3e+03      |
|    value_loss           | 143          |
------------------------------------------
Iteration: 4192 | Episodes: 170100 | Median Reward: 10.37 | Max Reward: 48.13
Iteration: 4194 | Episodes: 170200 | Median Reward: 10.44 | Max Reward: 48.13
Iteration: 4197 | Episodes: 170300 | Median Reward: 8.13 | Max Reward: 48.13
Iteration: 4199 | Episodes: 170400 | Median Reward: 7.53 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92          |
| time/                   |              |
|    fps                  | 304          |
|    iterations           | 4200         |
|    time_elapsed         | 56460        |
|    total_timesteps      | 17203200     |
| train/                  |              |
|    approx_kl            | 0.0004091363 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -255         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 84.9         |
|    n_updates            | 41990        |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.32e+03     |
|    value_loss           | 195          |
------------------------------------------
Iteration: 4202 | Episodes: 170500 | Median Reward: 2.65 | Max Reward: 48.13
Iteration: 4204 | Episodes: 170600 | Median Reward: 6.53 | Max Reward: 48.13
Iteration: 4206 | Episodes: 170700 | Median Reward: 14.21 | Max Reward: 48.13
Iteration: 4209 | Episodes: 170800 | Median Reward: 9.01 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.5         |
| time/                   |               |
|    fps                  | 304           |
|    iterations           | 4210          |
|    time_elapsed         | 56613         |
|    total_timesteps      | 17244160      |
| train/                  |               |
|    approx_kl            | 3.8131839e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -255          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 117           |
|    n_updates            | 42090         |
|    policy_gradient_loss | 4.36e-05      |
|    std                  | 1.33e+03      |
|    value_loss           | 260           |
-------------------------------------------
Iteration: 4211 | Episodes: 170900 | Median Reward: 4.34 | Max Reward: 48.13
Iteration: 4214 | Episodes: 171000 | Median Reward: 2.43 | Max Reward: 48.13
Iteration: 4216 | Episodes: 171100 | Median Reward: 4.14 | Max Reward: 48.13
Iteration: 4219 | Episodes: 171200 | Median Reward: -3.79 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.2         |
| time/                   |               |
|    fps                  | 304           |
|    iterations           | 4220          |
|    time_elapsed         | 56767         |
|    total_timesteps      | 17285120      |
| train/                  |               |
|    approx_kl            | 0.00012948153 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -255          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 74.7          |
|    n_updates            | 42190         |
|    policy_gradient_loss | -0.000649     |
|    std                  | 1.34e+03      |
|    value_loss           | 175           |
-------------------------------------------
Iteration: 4221 | Episodes: 171300 | Median Reward: 7.25 | Max Reward: 48.13
Iteration: 4224 | Episodes: 171400 | Median Reward: 6.76 | Max Reward: 48.13
Iteration: 4226 | Episodes: 171500 | Median Reward: 5.63 | Max Reward: 48.13
Iteration: 4229 | Episodes: 171600 | Median Reward: 16.72 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88.6         |
| time/                   |               |
|    fps                  | 304           |
|    iterations           | 4230          |
|    time_elapsed         | 56921         |
|    total_timesteps      | 17326080      |
| train/                  |               |
|    approx_kl            | 1.3420446e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -256          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 104           |
|    n_updates            | 42290         |
|    policy_gradient_loss | -0.000163     |
|    std                  | 1.36e+03      |
|    value_loss           | 233           |
-------------------------------------------
Iteration: 4231 | Episodes: 171700 | Median Reward: -1.75 | Max Reward: 48.13
Iteration: 4234 | Episodes: 171800 | Median Reward: 4.05 | Max Reward: 48.13
Iteration: 4236 | Episodes: 171900 | Median Reward: 10.46 | Max Reward: 48.13
Iteration: 4239 | Episodes: 172000 | Median Reward: 2.09 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -97.1         |
| time/                   |               |
|    fps                  | 304           |
|    iterations           | 4240          |
|    time_elapsed         | 57075         |
|    total_timesteps      | 17367040      |
| train/                  |               |
|    approx_kl            | 3.7306454e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -256          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 72.1          |
|    n_updates            | 42390         |
|    policy_gradient_loss | -0.000562     |
|    std                  | 1.39e+03      |
|    value_loss           | 170           |
-------------------------------------------
Iteration: 4241 | Episodes: 172100 | Median Reward: 5.38 | Max Reward: 48.13
Iteration: 4243 | Episodes: 172200 | Median Reward: 4.24 | Max Reward: 48.13
Iteration: 4246 | Episodes: 172300 | Median Reward: 13.78 | Max Reward: 48.13
Iteration: 4248 | Episodes: 172400 | Median Reward: 6.03 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.9        |
| time/                   |              |
|    fps                  | 304          |
|    iterations           | 4250         |
|    time_elapsed         | 57227        |
|    total_timesteps      | 17408000     |
| train/                  |              |
|    approx_kl            | 5.977272e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -256         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 96.7         |
|    n_updates            | 42490        |
|    policy_gradient_loss | -6.2e-05     |
|    std                  | 1.4e+03      |
|    value_loss           | 219          |
------------------------------------------
Iteration: 4251 | Episodes: 172500 | Median Reward: 11.44 | Max Reward: 48.13
Iteration: 4253 | Episodes: 172600 | Median Reward: 14.00 | Max Reward: 48.13
Iteration: 4256 | Episodes: 172700 | Median Reward: 9.96 | Max Reward: 48.13
Iteration: 4258 | Episodes: 172800 | Median Reward: 4.40 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -91.5        |
| time/                   |              |
|    fps                  | 304          |
|    iterations           | 4260         |
|    time_elapsed         | 57391        |
|    total_timesteps      | 17448960     |
| train/                  |              |
|    approx_kl            | 0.0013102179 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -256         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 92           |
|    n_updates            | 42590        |
|    policy_gradient_loss | -0.00316     |
|    std                  | 1.41e+03     |
|    value_loss           | 210          |
------------------------------------------
Iteration: 4261 | Episodes: 172900 | Median Reward: 2.29 | Max Reward: 48.13
Iteration: 4263 | Episodes: 173000 | Median Reward: -9.06 | Max Reward: 48.13
Iteration: 4266 | Episodes: 173100 | Median Reward: 5.50 | Max Reward: 48.13
Iteration: 4268 | Episodes: 173200 | Median Reward: 6.07 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -99.8        |
| time/                   |              |
|    fps                  | 303          |
|    iterations           | 4270         |
|    time_elapsed         | 57544        |
|    total_timesteps      | 17489920     |
| train/                  |              |
|    approx_kl            | 0.0012579852 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -257         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 85.9         |
|    n_updates            | 42690        |
|    policy_gradient_loss | -0.00114     |
|    std                  | 1.43e+03     |
|    value_loss           | 198          |
------------------------------------------
Iteration: 4271 | Episodes: 173300 | Median Reward: 0.65 | Max Reward: 48.13
Iteration: 4273 | Episodes: 173400 | Median Reward: 5.06 | Max Reward: 48.13
Iteration: 4276 | Episodes: 173500 | Median Reward: 9.75 | Max Reward: 48.13
Iteration: 4278 | Episodes: 173600 | Median Reward: 11.10 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -97.2         |
| time/                   |               |
|    fps                  | 303           |
|    iterations           | 4280          |
|    time_elapsed         | 57691         |
|    total_timesteps      | 17530880      |
| train/                  |               |
|    approx_kl            | 1.4015852e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -257          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 76.5          |
|    n_updates            | 42790         |
|    policy_gradient_loss | 0.000164      |
|    std                  | 1.47e+03      |
|    value_loss           | 179           |
-------------------------------------------
Iteration: 4280 | Episodes: 173700 | Median Reward: 4.26 | Max Reward: 48.13
Iteration: 4283 | Episodes: 173800 | Median Reward: 13.65 | Max Reward: 48.13
Iteration: 4285 | Episodes: 173900 | Median Reward: 8.30 | Max Reward: 48.13
Iteration: 4288 | Episodes: 174000 | Median Reward: 7.54 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.9         |
| time/                   |               |
|    fps                  | 303           |
|    iterations           | 4290          |
|    time_elapsed         | 57841         |
|    total_timesteps      | 17571840      |
| train/                  |               |
|    approx_kl            | 0.00014401757 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -258          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 92.5          |
|    n_updates            | 42890         |
|    policy_gradient_loss | -0.000572     |
|    std                  | 1.5e+03       |
|    value_loss           | 211           |
-------------------------------------------
Iteration: 4290 | Episodes: 174100 | Median Reward: 12.77 | Max Reward: 48.13
Iteration: 4293 | Episodes: 174200 | Median Reward: 1.38 | Max Reward: 48.13
Iteration: 4295 | Episodes: 174300 | Median Reward: 1.88 | Max Reward: 48.13
Iteration: 4298 | Episodes: 174400 | Median Reward: 1.88 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -91.5        |
| time/                   |              |
|    fps                  | 303          |
|    iterations           | 4300         |
|    time_elapsed         | 57988        |
|    total_timesteps      | 17612800     |
| train/                  |              |
|    approx_kl            | 8.217896e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -258         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 77.1         |
|    n_updates            | 42990        |
|    policy_gradient_loss | -0.000363    |
|    std                  | 1.52e+03     |
|    value_loss           | 180          |
------------------------------------------
Iteration: 4300 | Episodes: 174500 | Median Reward: 3.77 | Max Reward: 48.13
Iteration: 4303 | Episodes: 174600 | Median Reward: 17.35 | Max Reward: 48.13
Iteration: 4305 | Episodes: 174700 | Median Reward: 8.44 | Max Reward: 48.13
Iteration: 4308 | Episodes: 174800 | Median Reward: 15.00 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -102          |
| time/                   |               |
|    fps                  | 303           |
|    iterations           | 4310          |
|    time_elapsed         | 58146         |
|    total_timesteps      | 17653760      |
| train/                  |               |
|    approx_kl            | 2.9760413e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -258          |
|    explained_variance   | 1.79e-07      |
|    learning_rate        | 0.0005        |
|    loss                 | 62.8          |
|    n_updates            | 43090         |
|    policy_gradient_loss | -7.19e-05     |
|    std                  | 1.53e+03      |
|    value_loss           | 152           |
-------------------------------------------
Iteration: 4310 | Episodes: 174900 | Median Reward: -1.04 | Max Reward: 48.13
Iteration: 4313 | Episodes: 175000 | Median Reward: 8.50 | Max Reward: 48.13
Iteration: 4315 | Episodes: 175100 | Median Reward: 12.65 | Max Reward: 48.13
Iteration: 4317 | Episodes: 175200 | Median Reward: 6.81 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.9         |
| time/                   |               |
|    fps                  | 303           |
|    iterations           | 4320          |
|    time_elapsed         | 58295         |
|    total_timesteps      | 17694720      |
| train/                  |               |
|    approx_kl            | 9.2390765e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -259          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 109           |
|    n_updates            | 43190         |
|    policy_gradient_loss | -0.000306     |
|    std                  | 1.54e+03      |
|    value_loss           | 244           |
-------------------------------------------
Iteration: 4320 | Episodes: 175300 | Median Reward: 8.90 | Max Reward: 48.13
Iteration: 4322 | Episodes: 175400 | Median Reward: 9.97 | Max Reward: 48.13
Iteration: 4325 | Episodes: 175500 | Median Reward: 6.36 | Max Reward: 48.13
Iteration: 4327 | Episodes: 175600 | Median Reward: 10.26 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -97.2         |
| time/                   |               |
|    fps                  | 303           |
|    iterations           | 4330          |
|    time_elapsed         | 58444         |
|    total_timesteps      | 17735680      |
| train/                  |               |
|    approx_kl            | 2.0385734e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -259          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 70.4          |
|    n_updates            | 43290         |
|    policy_gradient_loss | -0.00013      |
|    std                  | 1.56e+03      |
|    value_loss           | 167           |
-------------------------------------------
Iteration: 4330 | Episodes: 175700 | Median Reward: 2.85 | Max Reward: 48.13
Iteration: 4332 | Episodes: 175800 | Median Reward: 4.46 | Max Reward: 48.13
Iteration: 4335 | Episodes: 175900 | Median Reward: 6.29 | Max Reward: 48.13
Iteration: 4337 | Episodes: 176000 | Median Reward: 8.14 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -94.6       |
| time/                   |             |
|    fps                  | 303         |
|    iterations           | 4340        |
|    time_elapsed         | 58592       |
|    total_timesteps      | 17776640    |
| train/                  |             |
|    approx_kl            | 0.000996132 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -259        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 88.2        |
|    n_updates            | 43390       |
|    policy_gradient_loss | -0.00178    |
|    std                  | 1.58e+03    |
|    value_loss           | 202         |
-----------------------------------------
Iteration: 4340 | Episodes: 176100 | Median Reward: 4.22 | Max Reward: 48.13
Iteration: 4342 | Episodes: 176200 | Median Reward: 3.88 | Max Reward: 48.13
Iteration: 4345 | Episodes: 176300 | Median Reward: 3.18 | Max Reward: 48.13
Iteration: 4347 | Episodes: 176400 | Median Reward: 5.22 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.2         |
| time/                   |               |
|    fps                  | 303           |
|    iterations           | 4350          |
|    time_elapsed         | 58748         |
|    total_timesteps      | 17817600      |
| train/                  |               |
|    approx_kl            | 4.1451698e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -260          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 107           |
|    n_updates            | 43490         |
|    policy_gradient_loss | -0.000104     |
|    std                  | 1.61e+03      |
|    value_loss           | 241           |
-------------------------------------------
Iteration: 4350 | Episodes: 176500 | Median Reward: 16.27 | Max Reward: 48.13
Iteration: 4352 | Episodes: 176600 | Median Reward: 3.58 | Max Reward: 48.13
Iteration: 4354 | Episodes: 176700 | Median Reward: 5.97 | Max Reward: 48.13
Iteration: 4357 | Episodes: 176800 | Median Reward: 13.39 | Max Reward: 48.13
Iteration: 4359 | Episodes: 176900 | Median Reward: 8.22 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.1        |
| time/                   |              |
|    fps                  | 303          |
|    iterations           | 4360         |
|    time_elapsed         | 58894        |
|    total_timesteps      | 17858560     |
| train/                  |              |
|    approx_kl            | 0.0033145412 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -260         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 85.9         |
|    n_updates            | 43590        |
|    policy_gradient_loss | -0.00251     |
|    std                  | 1.64e+03     |
|    value_loss           | 198          |
------------------------------------------
Iteration: 4362 | Episodes: 177000 | Median Reward: 10.46 | Max Reward: 48.13
Iteration: 4364 | Episodes: 177100 | Median Reward: 4.36 | Max Reward: 48.13
Iteration: 4367 | Episodes: 177200 | Median Reward: 3.98 | Max Reward: 48.13
Iteration: 4369 | Episodes: 177300 | Median Reward: 3.82 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.4         |
| time/                   |               |
|    fps                  | 303           |
|    iterations           | 4370          |
|    time_elapsed         | 59039         |
|    total_timesteps      | 17899520      |
| train/                  |               |
|    approx_kl            | 0.00047492227 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -261          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 77.9          |
|    n_updates            | 43690         |
|    policy_gradient_loss | -0.000291     |
|    std                  | 1.67e+03      |
|    value_loss           | 182           |
-------------------------------------------
Iteration: 4372 | Episodes: 177400 | Median Reward: 8.88 | Max Reward: 48.13
Iteration: 4374 | Episodes: 177500 | Median Reward: 8.43 | Max Reward: 48.13
Iteration: 4377 | Episodes: 177600 | Median Reward: 9.39 | Max Reward: 48.13
Iteration: 4379 | Episodes: 177700 | Median Reward: -7.17 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -99.1         |
| time/                   |               |
|    fps                  | 303           |
|    iterations           | 4380          |
|    time_elapsed         | 59186         |
|    total_timesteps      | 17940480      |
| train/                  |               |
|    approx_kl            | 2.1761603e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -261          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 79.7          |
|    n_updates            | 43790         |
|    policy_gradient_loss | -0.000317     |
|    std                  | 1.68e+03      |
|    value_loss           | 186           |
-------------------------------------------
Iteration: 4382 | Episodes: 177800 | Median Reward: 4.47 | Max Reward: 48.13
Iteration: 4384 | Episodes: 177900 | Median Reward: 9.04 | Max Reward: 48.13
Iteration: 4386 | Episodes: 178000 | Median Reward: 6.77 | Max Reward: 48.13
Iteration: 4389 | Episodes: 178100 | Median Reward: 5.09 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -94.3       |
| time/                   |             |
|    fps                  | 303         |
|    iterations           | 4390        |
|    time_elapsed         | 59342       |
|    total_timesteps      | 17981440    |
| train/                  |             |
|    approx_kl            | 0.011458496 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -261        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 95.5        |
|    n_updates            | 43890       |
|    policy_gradient_loss | -0.0104     |
|    std                  | 1.72e+03    |
|    value_loss           | 217         |
-----------------------------------------
Iteration: 4391 | Episodes: 178200 | Median Reward: 11.58 | Max Reward: 48.13
Iteration: 4394 | Episodes: 178300 | Median Reward: 5.38 | Max Reward: 48.13
Iteration: 4396 | Episodes: 178400 | Median Reward: 8.79 | Max Reward: 48.13
Iteration: 4399 | Episodes: 178500 | Median Reward: 8.87 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -89          |
| time/                   |              |
|    fps                  | 302          |
|    iterations           | 4400         |
|    time_elapsed         | 59487        |
|    total_timesteps      | 18022400     |
| train/                  |              |
|    approx_kl            | 5.471855e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -262         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 113          |
|    n_updates            | 43990        |
|    policy_gradient_loss | 1.22e-05     |
|    std                  | 1.74e+03     |
|    value_loss           | 252          |
------------------------------------------
Iteration: 4401 | Episodes: 178600 | Median Reward: 12.84 | Max Reward: 48.13
Iteration: 4404 | Episodes: 178700 | Median Reward: 8.41 | Max Reward: 48.13
Iteration: 4406 | Episodes: 178800 | Median Reward: 6.23 | Max Reward: 48.13
Iteration: 4409 | Episodes: 178900 | Median Reward: 8.47 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.8         |
| time/                   |               |
|    fps                  | 302           |
|    iterations           | 4410          |
|    time_elapsed         | 59634         |
|    total_timesteps      | 18063360      |
| train/                  |               |
|    approx_kl            | 1.3502853e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -262          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 77.5          |
|    n_updates            | 44090         |
|    policy_gradient_loss | -0.00024      |
|    std                  | 1.77e+03      |
|    value_loss           | 181           |
-------------------------------------------
Iteration: 4411 | Episodes: 179000 | Median Reward: 6.74 | Max Reward: 48.13
Iteration: 4414 | Episodes: 179100 | Median Reward: 5.72 | Max Reward: 48.13
Iteration: 4416 | Episodes: 179200 | Median Reward: 7.58 | Max Reward: 48.13
Iteration: 4419 | Episodes: 179300 | Median Reward: 5.99 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -95.5         |
| time/                   |               |
|    fps                  | 302           |
|    iterations           | 4420          |
|    time_elapsed         | 59781         |
|    total_timesteps      | 18104320      |
| train/                  |               |
|    approx_kl            | 8.2792976e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -262          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 97.4          |
|    n_updates            | 44190         |
|    policy_gradient_loss | -0.000394     |
|    std                  | 1.8e+03       |
|    value_loss           | 221           |
-------------------------------------------
Iteration: 4421 | Episodes: 179400 | Median Reward: 1.35 | Max Reward: 48.13
Iteration: 4423 | Episodes: 179500 | Median Reward: 8.88 | Max Reward: 48.13
Iteration: 4426 | Episodes: 179600 | Median Reward: 0.66 | Max Reward: 48.13
Iteration: 4428 | Episodes: 179700 | Median Reward: 2.68 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88           |
| time/                   |               |
|    fps                  | 302           |
|    iterations           | 4430          |
|    time_elapsed         | 59930         |
|    total_timesteps      | 18145280      |
| train/                  |               |
|    approx_kl            | 5.9123355e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -263          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 70.4          |
|    n_updates            | 44290         |
|    policy_gradient_loss | 0.000132      |
|    std                  | 1.82e+03      |
|    value_loss           | 167           |
-------------------------------------------
Iteration: 4431 | Episodes: 179800 | Median Reward: 22.39 | Max Reward: 48.13
Iteration: 4433 | Episodes: 179900 | Median Reward: 5.22 | Max Reward: 48.13
Iteration: 4436 | Episodes: 180000 | Median Reward: 5.47 | Max Reward: 48.13
Iteration: 4438 | Episodes: 180100 | Median Reward: 7.38 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.3        |
| time/                   |              |
|    fps                  | 302          |
|    iterations           | 4440         |
|    time_elapsed         | 60085        |
|    total_timesteps      | 18186240     |
| train/                  |              |
|    approx_kl            | 4.937421e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -263         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 139          |
|    n_updates            | 44390        |
|    policy_gradient_loss | -4.34e-05    |
|    std                  | 1.84e+03     |
|    value_loss           | 305          |
------------------------------------------
Iteration: 4441 | Episodes: 180200 | Median Reward: 12.23 | Max Reward: 48.13
Iteration: 4443 | Episodes: 180300 | Median Reward: 12.29 | Max Reward: 48.13
Iteration: 4446 | Episodes: 180400 | Median Reward: 8.01 | Max Reward: 48.13
Iteration: 4448 | Episodes: 180500 | Median Reward: 0.86 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -97.6        |
| time/                   |              |
|    fps                  | 302          |
|    iterations           | 4450         |
|    time_elapsed         | 60234        |
|    total_timesteps      | 18227200     |
| train/                  |              |
|    approx_kl            | 6.611712e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -263         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 78.3         |
|    n_updates            | 44490        |
|    policy_gradient_loss | -0.000997    |
|    std                  | 1.86e+03     |
|    value_loss           | 183          |
------------------------------------------
Iteration: 4451 | Episodes: 180600 | Median Reward: 11.18 | Max Reward: 48.13
Iteration: 4453 | Episodes: 180700 | Median Reward: 9.52 | Max Reward: 48.13
Iteration: 4456 | Episodes: 180800 | Median Reward: 4.11 | Max Reward: 48.13
Iteration: 4458 | Episodes: 180900 | Median Reward: 4.27 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -95.1         |
| time/                   |               |
|    fps                  | 302           |
|    iterations           | 4460          |
|    time_elapsed         | 60381         |
|    total_timesteps      | 18268160      |
| train/                  |               |
|    approx_kl            | 0.00023971366 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -263          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 67.4          |
|    n_updates            | 44590         |
|    policy_gradient_loss | -0.00119      |
|    std                  | 1.88e+03      |
|    value_loss           | 162           |
-------------------------------------------
Iteration: 4460 | Episodes: 181000 | Median Reward: 6.56 | Max Reward: 48.13
Iteration: 4463 | Episodes: 181100 | Median Reward: 8.25 | Max Reward: 48.13
Iteration: 4465 | Episodes: 181200 | Median Reward: 3.10 | Max Reward: 48.13
Iteration: 4468 | Episodes: 181300 | Median Reward: 11.69 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.5         |
| time/                   |               |
|    fps                  | 302           |
|    iterations           | 4470          |
|    time_elapsed         | 60525         |
|    total_timesteps      | 18309120      |
| train/                  |               |
|    approx_kl            | 5.7989964e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -264          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 111           |
|    n_updates            | 44690         |
|    policy_gradient_loss | 3.42e-05      |
|    std                  | 1.91e+03      |
|    value_loss           | 249           |
-------------------------------------------
Iteration: 4470 | Episodes: 181400 | Median Reward: 14.41 | Max Reward: 48.13
Iteration: 4473 | Episodes: 181500 | Median Reward: 1.54 | Max Reward: 48.13
Iteration: 4475 | Episodes: 181600 | Median Reward: 1.87 | Max Reward: 48.13
Iteration: 4478 | Episodes: 181700 | Median Reward: 1.89 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -91.4        |
| time/                   |              |
|    fps                  | 302          |
|    iterations           | 4480         |
|    time_elapsed         | 60681        |
|    total_timesteps      | 18350080     |
| train/                  |              |
|    approx_kl            | 9.569019e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -264         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 123          |
|    n_updates            | 44790        |
|    policy_gradient_loss | 1.43e-05     |
|    std                  | 1.94e+03     |
|    value_loss           | 273          |
------------------------------------------
Iteration: 4480 | Episodes: 181800 | Median Reward: 8.00 | Max Reward: 48.13
Iteration: 4483 | Episodes: 181900 | Median Reward: 8.24 | Max Reward: 48.13
Iteration: 4485 | Episodes: 182000 | Median Reward: 2.86 | Max Reward: 48.13
Iteration: 4488 | Episodes: 182100 | Median Reward: 3.49 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.8        |
| time/                   |              |
|    fps                  | 302          |
|    iterations           | 4490         |
|    time_elapsed         | 60829        |
|    total_timesteps      | 18391040     |
| train/                  |              |
|    approx_kl            | 5.929367e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -265         |
|    explained_variance   | 1.19e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 145          |
|    n_updates            | 44890        |
|    policy_gradient_loss | -3.41e-05    |
|    std                  | 1.97e+03     |
|    value_loss           | 316          |
------------------------------------------
Iteration: 4490 | Episodes: 182200 | Median Reward: 5.84 | Max Reward: 48.13
Iteration: 4493 | Episodes: 182300 | Median Reward: 4.43 | Max Reward: 48.13
Iteration: 4495 | Episodes: 182400 | Median Reward: 2.64 | Max Reward: 48.13
Iteration: 4497 | Episodes: 182500 | Median Reward: 11.12 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.9         |
| time/                   |               |
|    fps                  | 302           |
|    iterations           | 4500          |
|    time_elapsed         | 60977         |
|    total_timesteps      | 18432000      |
| train/                  |               |
|    approx_kl            | 0.00031773408 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -265          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 95.8          |
|    n_updates            | 44990         |
|    policy_gradient_loss | -0.000767     |
|    std                  | 2.02e+03      |
|    value_loss           | 218           |
-------------------------------------------
Iteration: 4500 | Episodes: 182600 | Median Reward: 14.01 | Max Reward: 48.13
Iteration: 4502 | Episodes: 182700 | Median Reward: 0.64 | Max Reward: 48.13
Iteration: 4505 | Episodes: 182800 | Median Reward: 0.30 | Max Reward: 48.13
Iteration: 4507 | Episodes: 182900 | Median Reward: 4.54 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -94.8         |
| time/                   |               |
|    fps                  | 302           |
|    iterations           | 4510          |
|    time_elapsed         | 61125         |
|    total_timesteps      | 18472960      |
| train/                  |               |
|    approx_kl            | 0.00012326725 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -265          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 75            |
|    n_updates            | 45090         |
|    policy_gradient_loss | -0.000536     |
|    std                  | 2.03e+03      |
|    value_loss           | 177           |
-------------------------------------------
Iteration: 4510 | Episodes: 183000 | Median Reward: 6.04 | Max Reward: 48.13
Iteration: 4512 | Episodes: 183100 | Median Reward: 5.36 | Max Reward: 48.13
Iteration: 4515 | Episodes: 183200 | Median Reward: 9.48 | Max Reward: 48.13
Iteration: 4517 | Episodes: 183300 | Median Reward: 0.32 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.3         |
| time/                   |               |
|    fps                  | 302           |
|    iterations           | 4520          |
|    time_elapsed         | 61280         |
|    total_timesteps      | 18513920      |
| train/                  |               |
|    approx_kl            | 4.6746703e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -266          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 93.2          |
|    n_updates            | 45190         |
|    policy_gradient_loss | -0.000249     |
|    std                  | 2.07e+03      |
|    value_loss           | 213           |
-------------------------------------------
Iteration: 4520 | Episodes: 183400 | Median Reward: 11.51 | Max Reward: 48.13
Iteration: 4522 | Episodes: 183500 | Median Reward: 9.11 | Max Reward: 48.13
Iteration: 4525 | Episodes: 183600 | Median Reward: -3.36 | Max Reward: 48.13
Iteration: 4527 | Episodes: 183700 | Median Reward: 2.76 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -95.1         |
| time/                   |               |
|    fps                  | 302           |
|    iterations           | 4530          |
|    time_elapsed         | 61426         |
|    total_timesteps      | 18554880      |
| train/                  |               |
|    approx_kl            | 0.00082850724 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -266          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 85.4          |
|    n_updates            | 45290         |
|    policy_gradient_loss | -0.000544     |
|    std                  | 2.09e+03      |
|    value_loss           | 197           |
-------------------------------------------
Iteration: 4530 | Episodes: 183800 | Median Reward: 3.87 | Max Reward: 48.13
Iteration: 4532 | Episodes: 183900 | Median Reward: 0.21 | Max Reward: 48.13
Iteration: 4534 | Episodes: 184000 | Median Reward: 6.27 | Max Reward: 48.13
Iteration: 4537 | Episodes: 184100 | Median Reward: 4.48 | Max Reward: 48.13
Iteration: 4539 | Episodes: 184200 | Median Reward: 3.90 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.7         |
| time/                   |               |
|    fps                  | 302           |
|    iterations           | 4540          |
|    time_elapsed         | 61572         |
|    total_timesteps      | 18595840      |
| train/                  |               |
|    approx_kl            | 5.7896206e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -266          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 78.3          |
|    n_updates            | 45390         |
|    policy_gradient_loss | -0.000358     |
|    std                  | 2.12e+03      |
|    value_loss           | 183           |
-------------------------------------------
Iteration: 4542 | Episodes: 184300 | Median Reward: 8.47 | Max Reward: 48.13
Iteration: 4544 | Episodes: 184400 | Median Reward: 9.47 | Max Reward: 48.13
Iteration: 4547 | Episodes: 184500 | Median Reward: 9.98 | Max Reward: 48.13
Iteration: 4549 | Episodes: 184600 | Median Reward: 5.51 | Max Reward: 48.13
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -95        |
| time/                   |            |
|    fps                  | 301        |
|    iterations           | 4550       |
|    time_elapsed         | 61718      |
|    total_timesteps      | 18636800   |
| train/                  |            |
|    approx_kl            | 5.8359e-07 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -267       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0005     |
|    loss                 | 57.5       |
|    n_updates            | 45490      |
|    policy_gradient_loss | -1.73e-05  |
|    std                  | 2.15e+03   |
|    value_loss           | 142        |
----------------------------------------
Iteration: 4552 | Episodes: 184700 | Median Reward: 10.44 | Max Reward: 48.13
Iteration: 4554 | Episodes: 184800 | Median Reward: 9.48 | Max Reward: 48.13
Iteration: 4557 | Episodes: 184900 | Median Reward: 10.48 | Max Reward: 48.13
Iteration: 4559 | Episodes: 185000 | Median Reward: 8.11 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92          |
| time/                   |              |
|    fps                  | 301          |
|    iterations           | 4560         |
|    time_elapsed         | 61864        |
|    total_timesteps      | 18677760     |
| train/                  |              |
|    approx_kl            | 0.0002659717 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -267         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 142          |
|    n_updates            | 45590        |
|    policy_gradient_loss | 0.000759     |
|    std                  | 2.2e+03      |
|    value_loss           | 311          |
------------------------------------------
Iteration: 4562 | Episodes: 185100 | Median Reward: 2.85 | Max Reward: 48.13
Iteration: 4564 | Episodes: 185200 | Median Reward: 5.31 | Max Reward: 48.13
Iteration: 4566 | Episodes: 185300 | Median Reward: 12.13 | Max Reward: 48.13
Iteration: 4569 | Episodes: 185400 | Median Reward: 1.81 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -94.7        |
| time/                   |              |
|    fps                  | 301          |
|    iterations           | 4570         |
|    time_elapsed         | 62019        |
|    total_timesteps      | 18718720     |
| train/                  |              |
|    approx_kl            | 0.0033895662 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -268         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 86.8         |
|    n_updates            | 45690        |
|    policy_gradient_loss | -0.0031      |
|    std                  | 2.23e+03     |
|    value_loss           | 200          |
------------------------------------------
Iteration: 4571 | Episodes: 185500 | Median Reward: 2.62 | Max Reward: 48.13
Iteration: 4574 | Episodes: 185600 | Median Reward: 10.35 | Max Reward: 48.13
Iteration: 4576 | Episodes: 185700 | Median Reward: 1.60 | Max Reward: 48.13
Iteration: 4579 | Episodes: 185800 | Median Reward: 0.30 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -101          |
| time/                   |               |
|    fps                  | 301           |
|    iterations           | 4580          |
|    time_elapsed         | 62165         |
|    total_timesteps      | 18759680      |
| train/                  |               |
|    approx_kl            | 3.5725388e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -268          |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0005        |
|    loss                 | 57.9          |
|    n_updates            | 45790         |
|    policy_gradient_loss | -1.73e-05     |
|    std                  | 2.28e+03      |
|    value_loss           | 143           |
-------------------------------------------
Iteration: 4581 | Episodes: 185900 | Median Reward: 16.81 | Max Reward: 48.13
Iteration: 4584 | Episodes: 186000 | Median Reward: 13.74 | Max Reward: 48.13
Iteration: 4586 | Episodes: 186100 | Median Reward: 9.56 | Max Reward: 48.13
Iteration: 4589 | Episodes: 186200 | Median Reward: 12.17 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.8         |
| time/                   |               |
|    fps                  | 301           |
|    iterations           | 4590          |
|    time_elapsed         | 62314         |
|    total_timesteps      | 18800640      |
| train/                  |               |
|    approx_kl            | 5.0476054e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -268          |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.0005        |
|    loss                 | 114           |
|    n_updates            | 45890         |
|    policy_gradient_loss | -0.000345     |
|    std                  | 2.3e+03       |
|    value_loss           | 255           |
-------------------------------------------
Iteration: 4591 | Episodes: 186300 | Median Reward: 7.08 | Max Reward: 48.13
Iteration: 4594 | Episodes: 186400 | Median Reward: 12.74 | Max Reward: 48.13
Iteration: 4596 | Episodes: 186500 | Median Reward: 10.21 | Max Reward: 48.13
Iteration: 4599 | Episodes: 186600 | Median Reward: 7.24 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -94.5         |
| time/                   |               |
|    fps                  | 301           |
|    iterations           | 4600          |
|    time_elapsed         | 62463         |
|    total_timesteps      | 18841600      |
| train/                  |               |
|    approx_kl            | 0.00093963166 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -269          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 88.8          |
|    n_updates            | 45990         |
|    policy_gradient_loss | -0.000353     |
|    std                  | 2.33e+03      |
|    value_loss           | 204           |
-------------------------------------------
Iteration: 4601 | Episodes: 186700 | Median Reward: 8.71 | Max Reward: 48.13
Iteration: 4603 | Episodes: 186800 | Median Reward: -0.06 | Max Reward: 48.13
Iteration: 4606 | Episodes: 186900 | Median Reward: 11.50 | Max Reward: 48.13
Iteration: 4608 | Episodes: 187000 | Median Reward: 11.49 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -95          |
| time/                   |              |
|    fps                  | 301          |
|    iterations           | 4610         |
|    time_elapsed         | 62618        |
|    total_timesteps      | 18882560     |
| train/                  |              |
|    approx_kl            | 0.0037285965 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -269         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 92.9         |
|    n_updates            | 46090        |
|    policy_gradient_loss | -0.00241     |
|    std                  | 2.38e+03     |
|    value_loss           | 213          |
------------------------------------------
Iteration: 4611 | Episodes: 187100 | Median Reward: -5.04 | Max Reward: 48.13
Iteration: 4613 | Episodes: 187200 | Median Reward: -2.09 | Max Reward: 48.13
Iteration: 4616 | Episodes: 187300 | Median Reward: 11.18 | Max Reward: 48.13
Iteration: 4618 | Episodes: 187400 | Median Reward: 6.38 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -91.9        |
| time/                   |              |
|    fps                  | 301          |
|    iterations           | 4620         |
|    time_elapsed         | 62762        |
|    total_timesteps      | 18923520     |
| train/                  |              |
|    approx_kl            | 2.091522e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -270         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 98.9         |
|    n_updates            | 46190        |
|    policy_gradient_loss | -3.03e-05    |
|    std                  | 2.41e+03     |
|    value_loss           | 225          |
------------------------------------------
Iteration: 4621 | Episodes: 187500 | Median Reward: 6.38 | Max Reward: 48.13
Iteration: 4623 | Episodes: 187600 | Median Reward: -0.35 | Max Reward: 48.13
Iteration: 4626 | Episodes: 187700 | Median Reward: -5.71 | Max Reward: 48.13
Iteration: 4628 | Episodes: 187800 | Median Reward: 6.83 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -89.6        |
| time/                   |              |
|    fps                  | 301          |
|    iterations           | 4630         |
|    time_elapsed         | 62905        |
|    total_timesteps      | 18964480     |
| train/                  |              |
|    approx_kl            | 3.450355e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -270         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 120          |
|    n_updates            | 46290        |
|    policy_gradient_loss | -0.000301    |
|    std                  | 2.45e+03     |
|    value_loss           | 268          |
------------------------------------------
Iteration: 4631 | Episodes: 187900 | Median Reward: 8.04 | Max Reward: 48.13
Iteration: 4633 | Episodes: 188000 | Median Reward: 5.15 | Max Reward: 48.13
Iteration: 4636 | Episodes: 188100 | Median Reward: 6.34 | Max Reward: 48.13
Iteration: 4638 | Episodes: 188200 | Median Reward: 10.68 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.6         |
| time/                   |               |
|    fps                  | 301           |
|    iterations           | 4640          |
|    time_elapsed         | 63052         |
|    total_timesteps      | 19005440      |
| train/                  |               |
|    approx_kl            | 7.8618876e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -270          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 105           |
|    n_updates            | 46390         |
|    policy_gradient_loss | 8.1e-05       |
|    std                  | 2.5e+03       |
|    value_loss           | 237           |
-------------------------------------------
Iteration: 4640 | Episodes: 188300 | Median Reward: 15.16 | Max Reward: 48.13
Iteration: 4643 | Episodes: 188400 | Median Reward: 3.55 | Max Reward: 48.13
Iteration: 4645 | Episodes: 188500 | Median Reward: 6.46 | Max Reward: 48.13
Iteration: 4648 | Episodes: 188600 | Median Reward: 6.09 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -91.1        |
| time/                   |              |
|    fps                  | 301          |
|    iterations           | 4650         |
|    time_elapsed         | 63198        |
|    total_timesteps      | 19046400     |
| train/                  |              |
|    approx_kl            | 0.0002459343 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -271         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 104          |
|    n_updates            | 46490        |
|    policy_gradient_loss | -0.000975    |
|    std                  | 2.53e+03     |
|    value_loss           | 236          |
------------------------------------------
Iteration: 4650 | Episodes: 188700 | Median Reward: 3.01 | Max Reward: 48.13
Iteration: 4653 | Episodes: 188800 | Median Reward: 3.94 | Max Reward: 48.13
Iteration: 4655 | Episodes: 188900 | Median Reward: 4.24 | Max Reward: 48.13
Iteration: 4658 | Episodes: 189000 | Median Reward: 3.62 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -90.7         |
| time/                   |               |
|    fps                  | 301           |
|    iterations           | 4660          |
|    time_elapsed         | 63349         |
|    total_timesteps      | 19087360      |
| train/                  |               |
|    approx_kl            | 2.8822222e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -271          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 111           |
|    n_updates            | 46590         |
|    policy_gradient_loss | 7.75e-05      |
|    std                  | 2.56e+03      |
|    value_loss           | 249           |
-------------------------------------------
Iteration: 4660 | Episodes: 189100 | Median Reward: 9.71 | Max Reward: 48.13
Iteration: 4663 | Episodes: 189200 | Median Reward: 15.06 | Max Reward: 48.13
Iteration: 4665 | Episodes: 189300 | Median Reward: 13.28 | Max Reward: 48.13
Iteration: 4668 | Episodes: 189400 | Median Reward: 11.39 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.6         |
| time/                   |               |
|    fps                  | 301           |
|    iterations           | 4670          |
|    time_elapsed         | 63496         |
|    total_timesteps      | 19128320      |
| train/                  |               |
|    approx_kl            | 3.3272576e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -272          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 73.5          |
|    n_updates            | 46690         |
|    policy_gradient_loss | -0.000286     |
|    std                  | 2.63e+03      |
|    value_loss           | 174           |
-------------------------------------------
Iteration: 4670 | Episodes: 189500 | Median Reward: 5.00 | Max Reward: 48.13
Iteration: 4673 | Episodes: 189600 | Median Reward: -1.37 | Max Reward: 48.13
Iteration: 4675 | Episodes: 189700 | Median Reward: 10.20 | Max Reward: 48.13
Iteration: 4677 | Episodes: 189800 | Median Reward: 5.84 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.3         |
| time/                   |               |
|    fps                  | 301           |
|    iterations           | 4680          |
|    time_elapsed         | 63645         |
|    total_timesteps      | 19169280      |
| train/                  |               |
|    approx_kl            | 3.9500825e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -272          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 96.7          |
|    n_updates            | 46790         |
|    policy_gradient_loss | -0.000128     |
|    std                  | 2.66e+03      |
|    value_loss           | 221           |
-------------------------------------------
Iteration: 4680 | Episodes: 189900 | Median Reward: 9.35 | Max Reward: 48.13
Iteration: 4682 | Episodes: 190000 | Median Reward: 4.09 | Max Reward: 48.13
Iteration: 4685 | Episodes: 190100 | Median Reward: 6.17 | Max Reward: 48.13
Iteration: 4687 | Episodes: 190200 | Median Reward: 17.45 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -97.3       |
| time/                   |             |
|    fps                  | 301         |
|    iterations           | 4690        |
|    time_elapsed         | 63793       |
|    total_timesteps      | 19210240    |
| train/                  |             |
|    approx_kl            | 9.04896e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -273        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 82.5        |
|    n_updates            | 46890       |
|    policy_gradient_loss | -0.000336   |
|    std                  | 2.74e+03    |
|    value_loss           | 192         |
-----------------------------------------
Iteration: 4690 | Episodes: 190300 | Median Reward: 4.28 | Max Reward: 48.13
Iteration: 4692 | Episodes: 190400 | Median Reward: 1.45 | Max Reward: 48.13
Iteration: 4695 | Episodes: 190500 | Median Reward: 7.08 | Max Reward: 48.13
Iteration: 4697 | Episodes: 190600 | Median Reward: 12.50 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.9         |
| time/                   |               |
|    fps                  | 301           |
|    iterations           | 4700          |
|    time_elapsed         | 63948         |
|    total_timesteps      | 19251200      |
| train/                  |               |
|    approx_kl            | 0.00034018263 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -273          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 77.7          |
|    n_updates            | 46990         |
|    policy_gradient_loss | -0.00106      |
|    std                  | 2.8e+03       |
|    value_loss           | 183           |
-------------------------------------------
Iteration: 4700 | Episodes: 190700 | Median Reward: 4.66 | Max Reward: 48.13
Iteration: 4702 | Episodes: 190800 | Median Reward: 15.26 | Max Reward: 48.13
Iteration: 4705 | Episodes: 190900 | Median Reward: 6.34 | Max Reward: 48.13
Iteration: 4707 | Episodes: 191000 | Median Reward: 3.53 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -99.7         |
| time/                   |               |
|    fps                  | 301           |
|    iterations           | 4710          |
|    time_elapsed         | 64091         |
|    total_timesteps      | 19292160      |
| train/                  |               |
|    approx_kl            | 3.1673262e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -274          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 82.4          |
|    n_updates            | 47090         |
|    policy_gradient_loss | -0.000302     |
|    std                  | 2.85e+03      |
|    value_loss           | 192           |
-------------------------------------------
Iteration: 4710 | Episodes: 191100 | Median Reward: -2.74 | Max Reward: 48.13
Iteration: 4712 | Episodes: 191200 | Median Reward: 6.08 | Max Reward: 48.13
Iteration: 4714 | Episodes: 191300 | Median Reward: 3.26 | Max Reward: 48.13
Iteration: 4717 | Episodes: 191400 | Median Reward: 11.51 | Max Reward: 48.13
Iteration: 4719 | Episodes: 191500 | Median Reward: 14.18 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.6         |
| time/                   |               |
|    fps                  | 300           |
|    iterations           | 4720          |
|    time_elapsed         | 64237         |
|    total_timesteps      | 19333120      |
| train/                  |               |
|    approx_kl            | 9.2530245e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -274          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 151           |
|    n_updates            | 47190         |
|    policy_gradient_loss | 0.000115      |
|    std                  | 2.91e+03      |
|    value_loss           | 330           |
-------------------------------------------
Iteration: 4722 | Episodes: 191600 | Median Reward: 7.32 | Max Reward: 48.13
Iteration: 4724 | Episodes: 191700 | Median Reward: 4.07 | Max Reward: 48.13
Iteration: 4727 | Episodes: 191800 | Median Reward: 10.57 | Max Reward: 48.13
Iteration: 4729 | Episodes: 191900 | Median Reward: 7.84 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -97.7        |
| time/                   |              |
|    fps                  | 300          |
|    iterations           | 4730         |
|    time_elapsed         | 64386        |
|    total_timesteps      | 19374080     |
| train/                  |              |
|    approx_kl            | 4.422423e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -274         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 78.6         |
|    n_updates            | 47290        |
|    policy_gradient_loss | -0.00038     |
|    std                  | 2.96e+03     |
|    value_loss           | 185          |
------------------------------------------
Iteration: 4732 | Episodes: 192000 | Median Reward: 5.29 | Max Reward: 48.13
Iteration: 4734 | Episodes: 192100 | Median Reward: -1.64 | Max Reward: 48.13
Iteration: 4737 | Episodes: 192200 | Median Reward: 0.52 | Max Reward: 48.13
Iteration: 4739 | Episodes: 192300 | Median Reward: 3.43 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -94.9         |
| time/                   |               |
|    fps                  | 300           |
|    iterations           | 4740          |
|    time_elapsed         | 64533         |
|    total_timesteps      | 19415040      |
| train/                  |               |
|    approx_kl            | 5.6838326e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -275          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 92.4          |
|    n_updates            | 47390         |
|    policy_gradient_loss | -0.000113     |
|    std                  | 3.02e+03      |
|    value_loss           | 212           |
-------------------------------------------
Iteration: 4742 | Episodes: 192400 | Median Reward: 1.93 | Max Reward: 48.13
Iteration: 4744 | Episodes: 192500 | Median Reward: 3.84 | Max Reward: 48.13
Iteration: 4746 | Episodes: 192600 | Median Reward: 8.92 | Max Reward: 48.13
Iteration: 4749 | Episodes: 192700 | Median Reward: 7.48 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -97.3         |
| time/                   |               |
|    fps                  | 300           |
|    iterations           | 4750          |
|    time_elapsed         | 64688         |
|    total_timesteps      | 19456000      |
| train/                  |               |
|    approx_kl            | 9.2384886e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -275          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 104           |
|    n_updates            | 47490         |
|    policy_gradient_loss | -0.000674     |
|    std                  | 3.06e+03      |
|    value_loss           | 236           |
-------------------------------------------
Iteration: 4751 | Episodes: 192800 | Median Reward: -3.07 | Max Reward: 48.13
Iteration: 4754 | Episodes: 192900 | Median Reward: 3.54 | Max Reward: 48.13
Iteration: 4756 | Episodes: 193000 | Median Reward: 9.05 | Max Reward: 48.13
Iteration: 4759 | Episodes: 193100 | Median Reward: 4.88 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -94.6        |
| time/                   |              |
|    fps                  | 300          |
|    iterations           | 4760         |
|    time_elapsed         | 64836        |
|    total_timesteps      | 19496960     |
| train/                  |              |
|    approx_kl            | 7.881239e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -275         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 96.8         |
|    n_updates            | 47590        |
|    policy_gradient_loss | -0.000278    |
|    std                  | 3.09e+03     |
|    value_loss           | 221          |
------------------------------------------
Iteration: 4761 | Episodes: 193200 | Median Reward: 2.65 | Max Reward: 48.13
Iteration: 4764 | Episodes: 193300 | Median Reward: 8.58 | Max Reward: 48.13
Iteration: 4766 | Episodes: 193400 | Median Reward: 16.27 | Max Reward: 48.13
Iteration: 4769 | Episodes: 193500 | Median Reward: 0.52 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -99.1         |
| time/                   |               |
|    fps                  | 300           |
|    iterations           | 4770          |
|    time_elapsed         | 64981         |
|    total_timesteps      | 19537920      |
| train/                  |               |
|    approx_kl            | 1.2629069e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -276          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 65.4          |
|    n_updates            | 47690         |
|    policy_gradient_loss | -0.000197     |
|    std                  | 3.14e+03      |
|    value_loss           | 159           |
-------------------------------------------
Iteration: 4771 | Episodes: 193600 | Median Reward: 6.00 | Max Reward: 48.13
Iteration: 4774 | Episodes: 193700 | Median Reward: -0.31 | Max Reward: 48.13
Iteration: 4776 | Episodes: 193800 | Median Reward: 8.75 | Max Reward: 48.13
Iteration: 4779 | Episodes: 193900 | Median Reward: 3.94 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -93.7        |
| time/                   |              |
|    fps                  | 300          |
|    iterations           | 4780         |
|    time_elapsed         | 65127        |
|    total_timesteps      | 19578880     |
| train/                  |              |
|    approx_kl            | 0.0012034769 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -276         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 83.6         |
|    n_updates            | 47790        |
|    policy_gradient_loss | -0.00179     |
|    std                  | 3.18e+03     |
|    value_loss           | 195          |
------------------------------------------
Iteration: 4781 | Episodes: 194000 | Median Reward: 8.79 | Max Reward: 48.13
Iteration: 4783 | Episodes: 194100 | Median Reward: 10.25 | Max Reward: 48.13
Iteration: 4786 | Episodes: 194200 | Median Reward: 2.02 | Max Reward: 48.13
Iteration: 4788 | Episodes: 194300 | Median Reward: -5.62 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -99.9         |
| time/                   |               |
|    fps                  | 300           |
|    iterations           | 4790          |
|    time_elapsed         | 65279         |
|    total_timesteps      | 19619840      |
| train/                  |               |
|    approx_kl            | 2.3637316e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -277          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 69.2          |
|    n_updates            | 47890         |
|    policy_gradient_loss | -0.000443     |
|    std                  | 3.24e+03      |
|    value_loss           | 166           |
-------------------------------------------
Iteration: 4791 | Episodes: 194400 | Median Reward: 5.37 | Max Reward: 48.13
Iteration: 4793 | Episodes: 194500 | Median Reward: 14.48 | Max Reward: 48.13
Iteration: 4796 | Episodes: 194600 | Median Reward: 10.53 | Max Reward: 48.13
Iteration: 4798 | Episodes: 194700 | Median Reward: 2.49 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -97.5         |
| time/                   |               |
|    fps                  | 300           |
|    iterations           | 4800          |
|    time_elapsed         | 65424         |
|    total_timesteps      | 19660800      |
| train/                  |               |
|    approx_kl            | 0.00013770214 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -277          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 80.2          |
|    n_updates            | 47990         |
|    policy_gradient_loss | -3.08e-06     |
|    std                  | 3.29e+03      |
|    value_loss           | 188           |
-------------------------------------------
Iteration: 4801 | Episodes: 194800 | Median Reward: 4.10 | Max Reward: 48.13
Iteration: 4803 | Episodes: 194900 | Median Reward: 3.45 | Max Reward: 48.13
Iteration: 4806 | Episodes: 195000 | Median Reward: 7.97 | Max Reward: 48.13
Iteration: 4808 | Episodes: 195100 | Median Reward: -1.18 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.5        |
| time/                   |              |
|    fps                  | 300          |
|    iterations           | 4810         |
|    time_elapsed         | 65571        |
|    total_timesteps      | 19701760     |
| train/                  |              |
|    approx_kl            | 5.136759e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -278         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 116          |
|    n_updates            | 48090        |
|    policy_gradient_loss | 0.000391     |
|    std                  | 3.37e+03     |
|    value_loss           | 260          |
------------------------------------------
Iteration: 4811 | Episodes: 195200 | Median Reward: 6.10 | Max Reward: 48.13
Iteration: 4813 | Episodes: 195300 | Median Reward: 7.56 | Max Reward: 48.13
Iteration: 4816 | Episodes: 195400 | Median Reward: 7.84 | Max Reward: 48.13
Iteration: 4818 | Episodes: 195500 | Median Reward: 4.54 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -93.7        |
| time/                   |              |
|    fps                  | 300          |
|    iterations           | 4820         |
|    time_elapsed         | 65719        |
|    total_timesteps      | 19742720     |
| train/                  |              |
|    approx_kl            | 0.0042074653 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -278         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 98.8         |
|    n_updates            | 48190        |
|    policy_gradient_loss | -0.00526     |
|    std                  | 3.41e+03     |
|    value_loss           | 225          |
------------------------------------------
Iteration: 4820 | Episodes: 195600 | Median Reward: 1.17 | Max Reward: 48.13
Iteration: 4823 | Episodes: 195700 | Median Reward: 9.65 | Max Reward: 48.13
Iteration: 4825 | Episodes: 195800 | Median Reward: 4.30 | Max Reward: 48.13
Iteration: 4828 | Episodes: 195900 | Median Reward: -3.82 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -98.9        |
| time/                   |              |
|    fps                  | 300          |
|    iterations           | 4830         |
|    time_elapsed         | 65866        |
|    total_timesteps      | 19783680     |
| train/                  |              |
|    approx_kl            | 8.479954e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -278         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 62.5         |
|    n_updates            | 48290        |
|    policy_gradient_loss | -8.15e-05    |
|    std                  | 3.46e+03     |
|    value_loss           | 153          |
------------------------------------------
Iteration: 4830 | Episodes: 196000 | Median Reward: 2.01 | Max Reward: 48.13
Iteration: 4833 | Episodes: 196100 | Median Reward: 8.79 | Max Reward: 48.13
Iteration: 4835 | Episodes: 196200 | Median Reward: 15.04 | Max Reward: 48.13
Iteration: 4838 | Episodes: 196300 | Median Reward: 1.94 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.6         |
| time/                   |               |
|    fps                  | 300           |
|    iterations           | 4840          |
|    time_elapsed         | 66021         |
|    total_timesteps      | 19824640      |
| train/                  |               |
|    approx_kl            | 7.1720424e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -279          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 150           |
|    n_updates            | 48390         |
|    policy_gradient_loss | 0.000171      |
|    std                  | 3.51e+03      |
|    value_loss           | 328           |
-------------------------------------------
Iteration: 4840 | Episodes: 196400 | Median Reward: 11.27 | Max Reward: 48.13
Iteration: 4843 | Episodes: 196500 | Median Reward: 7.85 | Max Reward: 48.13
Iteration: 4845 | Episodes: 196600 | Median Reward: 5.66 | Max Reward: 48.13
Iteration: 4848 | Episodes: 196700 | Median Reward: 0.23 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -95.6         |
| time/                   |               |
|    fps                  | 300           |
|    iterations           | 4850          |
|    time_elapsed         | 66166         |
|    total_timesteps      | 19865600      |
| train/                  |               |
|    approx_kl            | 8.1311737e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -279          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 47.2          |
|    n_updates            | 48490         |
|    policy_gradient_loss | -1.73e-05     |
|    std                  | 3.54e+03      |
|    value_loss           | 122           |
-------------------------------------------
Iteration: 4850 | Episodes: 196800 | Median Reward: 7.15 | Max Reward: 48.13
Iteration: 4853 | Episodes: 196900 | Median Reward: 6.03 | Max Reward: 48.13
Iteration: 4855 | Episodes: 197000 | Median Reward: 11.92 | Max Reward: 48.13
Iteration: 4857 | Episodes: 197100 | Median Reward: 12.98 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.7         |
| time/                   |               |
|    fps                  | 300           |
|    iterations           | 4860          |
|    time_elapsed         | 66312         |
|    total_timesteps      | 19906560      |
| train/                  |               |
|    approx_kl            | 1.7706843e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -279          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 74.3          |
|    n_updates            | 48590         |
|    policy_gradient_loss | -0.000163     |
|    std                  | 3.56e+03      |
|    value_loss           | 177           |
-------------------------------------------
Iteration: 4860 | Episodes: 197200 | Median Reward: 7.24 | Max Reward: 48.13
Iteration: 4862 | Episodes: 197300 | Median Reward: 8.70 | Max Reward: 48.13
Iteration: 4865 | Episodes: 197400 | Median Reward: 9.23 | Max Reward: 48.13
Iteration: 4867 | Episodes: 197500 | Median Reward: 13.25 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -99.6        |
| time/                   |              |
|    fps                  | 300          |
|    iterations           | 4870         |
|    time_elapsed         | 66461        |
|    total_timesteps      | 19947520     |
| train/                  |              |
|    approx_kl            | 0.0010731351 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -279         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 73.3         |
|    n_updates            | 48690        |
|    policy_gradient_loss | -0.00138     |
|    std                  | 3.63e+03     |
|    value_loss           | 175          |
------------------------------------------
Iteration: 4870 | Episodes: 197600 | Median Reward: 1.63 | Max Reward: 48.13
Iteration: 4872 | Episodes: 197700 | Median Reward: 5.52 | Max Reward: 48.13
Iteration: 4875 | Episodes: 197800 | Median Reward: 6.39 | Max Reward: 48.13
Iteration: 4877 | Episodes: 197900 | Median Reward: 6.13 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.3         |
| time/                   |               |
|    fps                  | 300           |
|    iterations           | 4880          |
|    time_elapsed         | 66617         |
|    total_timesteps      | 19988480      |
| train/                  |               |
|    approx_kl            | 3.4178258e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -279          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 113           |
|    n_updates            | 48790         |
|    policy_gradient_loss | -0.000182     |
|    std                  | 3.65e+03      |
|    value_loss           | 253           |
-------------------------------------------
Iteration: 4880 | Episodes: 198000 | Median Reward: 8.14 | Max Reward: 48.13
Iteration: 4882 | Episodes: 198100 | Median Reward: 16.00 | Max Reward: 48.13
Iteration: 4885 | Episodes: 198200 | Median Reward: 13.29 | Max Reward: 48.13
Iteration: 4887 | Episodes: 198300 | Median Reward: 10.71 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.4         |
| time/                   |               |
|    fps                  | 299           |
|    iterations           | 4890          |
|    time_elapsed         | 66765         |
|    total_timesteps      | 20029440      |
| train/                  |               |
|    approx_kl            | 0.00075617357 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -280          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 130           |
|    n_updates            | 48890         |
|    policy_gradient_loss | 0.000995      |
|    std                  | 3.69e+03      |
|    value_loss           | 288           |
-------------------------------------------
Iteration: 4890 | Episodes: 198400 | Median Reward: 13.38 | Max Reward: 48.13
Iteration: 4892 | Episodes: 198500 | Median Reward: 3.34 | Max Reward: 48.13
Iteration: 4894 | Episodes: 198600 | Median Reward: 5.12 | Max Reward: 48.13
Iteration: 4897 | Episodes: 198700 | Median Reward: 5.29 | Max Reward: 48.13
Iteration: 4899 | Episodes: 198800 | Median Reward: 5.31 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -94.7         |
| time/                   |               |
|    fps                  | 299           |
|    iterations           | 4900          |
|    time_elapsed         | 66914         |
|    total_timesteps      | 20070400      |
| train/                  |               |
|    approx_kl            | 0.00073411595 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -280          |
|    explained_variance   | 5.96e-08      |
|    learning_rate        | 0.0005        |
|    loss                 | 92.2          |
|    n_updates            | 48990         |
|    policy_gradient_loss | -0.00238      |
|    std                  | 3.74e+03      |
|    value_loss           | 213           |
-------------------------------------------
Iteration: 4902 | Episodes: 198900 | Median Reward: -1.53 | Max Reward: 48.13
Iteration: 4904 | Episodes: 199000 | Median Reward: 3.34 | Max Reward: 48.13
Iteration: 4907 | Episodes: 199100 | Median Reward: 2.94 | Max Reward: 48.13
Iteration: 4909 | Episodes: 199200 | Median Reward: 9.91 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -90.8         |
| time/                   |               |
|    fps                  | 299           |
|    iterations           | 4910          |
|    time_elapsed         | 67059         |
|    total_timesteps      | 20111360      |
| train/                  |               |
|    approx_kl            | 5.5446755e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -280          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 101           |
|    n_updates            | 49090         |
|    policy_gradient_loss | 4.35e-05      |
|    std                  | 3.8e+03       |
|    value_loss           | 230           |
-------------------------------------------
Iteration: 4912 | Episodes: 199300 | Median Reward: 13.63 | Max Reward: 48.13
Iteration: 4914 | Episodes: 199400 | Median Reward: 9.36 | Max Reward: 48.13
Iteration: 4917 | Episodes: 199500 | Median Reward: 9.76 | Max Reward: 48.13
Iteration: 4919 | Episodes: 199600 | Median Reward: 2.03 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.9         |
| time/                   |               |
|    fps                  | 299           |
|    iterations           | 4920          |
|    time_elapsed         | 67202         |
|    total_timesteps      | 20152320      |
| train/                  |               |
|    approx_kl            | 0.00016302543 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -281          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 104           |
|    n_updates            | 49190         |
|    policy_gradient_loss | -0.000969     |
|    std                  | 3.84e+03      |
|    value_loss           | 237           |
-------------------------------------------
Iteration: 4922 | Episodes: 199700 | Median Reward: -1.78 | Max Reward: 48.13
Iteration: 4924 | Episodes: 199800 | Median Reward: 1.11 | Max Reward: 48.13
Iteration: 4927 | Episodes: 199900 | Median Reward: 6.29 | Max Reward: 48.13
Iteration: 4929 | Episodes: 200000 | Median Reward: -0.53 | Max Reward: 48.13
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -96.7      |
| time/                   |            |
|    fps                  | 299        |
|    iterations           | 4930       |
|    time_elapsed         | 67361      |
|    total_timesteps      | 20193280   |
| train/                  |            |
|    approx_kl            | 0.01240045 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -281       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.0005     |
|    loss                 | 89.1       |
|    n_updates            | 49290      |
|    policy_gradient_loss | -0.00587   |
|    std                  | 3.91e+03   |
|    value_loss           | 206        |
----------------------------------------
Iteration: 4931 | Episodes: 200100 | Median Reward: 1.97 | Max Reward: 48.13
Iteration: 4934 | Episodes: 200200 | Median Reward: 8.42 | Max Reward: 48.13
Iteration: 4936 | Episodes: 200300 | Median Reward: 4.59 | Max Reward: 48.13
Iteration: 4939 | Episodes: 200400 | Median Reward: 6.07 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.5        |
| time/                   |              |
|    fps                  | 299          |
|    iterations           | 4940         |
|    time_elapsed         | 67508        |
|    total_timesteps      | 20234240     |
| train/                  |              |
|    approx_kl            | 0.0006451146 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -281         |
|    explained_variance   | 5.96e-08     |
|    learning_rate        | 0.0005       |
|    loss                 | 81.9         |
|    n_updates            | 49390        |
|    policy_gradient_loss | -0.000876    |
|    std                  | 3.98e+03     |
|    value_loss           | 192          |
------------------------------------------
Iteration: 4941 | Episodes: 200500 | Median Reward: 3.22 | Max Reward: 48.13
Iteration: 4944 | Episodes: 200600 | Median Reward: 4.90 | Max Reward: 48.13
Iteration: 4946 | Episodes: 200700 | Median Reward: 8.55 | Max Reward: 48.13
Iteration: 4949 | Episodes: 200800 | Median Reward: 12.33 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -92.3        |
| time/                   |              |
|    fps                  | 299          |
|    iterations           | 4950         |
|    time_elapsed         | 67656        |
|    total_timesteps      | 20275200     |
| train/                  |              |
|    approx_kl            | 0.0019546726 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -282         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 101          |
|    n_updates            | 49490        |
|    policy_gradient_loss | -0.00394     |
|    std                  | 4.08e+03     |
|    value_loss           | 230          |
------------------------------------------
Iteration: 4951 | Episodes: 200900 | Median Reward: 2.11 | Max Reward: 48.13
Iteration: 4954 | Episodes: 201000 | Median Reward: -2.67 | Max Reward: 48.13
Iteration: 4956 | Episodes: 201100 | Median Reward: 5.21 | Max Reward: 48.13
Iteration: 4959 | Episodes: 201200 | Median Reward: 18.25 | Max Reward: 48.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -87.3       |
| time/                   |             |
|    fps                  | 299         |
|    iterations           | 4960        |
|    time_elapsed         | 67799       |
|    total_timesteps      | 20316160    |
| train/                  |             |
|    approx_kl            | 0.000419236 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -282        |
|    explained_variance   | 0           |
|    learning_rate        | 0.0005      |
|    loss                 | 93.4        |
|    n_updates            | 49590       |
|    policy_gradient_loss | -0.00174    |
|    std                  | 4.15e+03    |
|    value_loss           | 215         |
-----------------------------------------
Iteration: 4961 | Episodes: 201300 | Median Reward: 11.00 | Max Reward: 48.13
Iteration: 4963 | Episodes: 201400 | Median Reward: 5.71 | Max Reward: 48.13
Iteration: 4966 | Episodes: 201500 | Median Reward: 6.34 | Max Reward: 48.13
Iteration: 4968 | Episodes: 201600 | Median Reward: 0.23 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -97.3        |
| time/                   |              |
|    fps                  | 299          |
|    iterations           | 4970         |
|    time_elapsed         | 67954        |
|    total_timesteps      | 20357120     |
| train/                  |              |
|    approx_kl            | 0.0024655734 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -283         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 80.8         |
|    n_updates            | 49690        |
|    policy_gradient_loss | -0.00154     |
|    std                  | 4.18e+03     |
|    value_loss           | 190          |
------------------------------------------
Iteration: 4971 | Episodes: 201700 | Median Reward: 1.98 | Max Reward: 48.13
Iteration: 4973 | Episodes: 201800 | Median Reward: 8.29 | Max Reward: 48.13
Iteration: 4976 | Episodes: 201900 | Median Reward: 9.02 | Max Reward: 48.13
Iteration: 4978 | Episodes: 202000 | Median Reward: 4.95 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.5         |
| time/                   |               |
|    fps                  | 299           |
|    iterations           | 4980          |
|    time_elapsed         | 68098         |
|    total_timesteps      | 20398080      |
| train/                  |               |
|    approx_kl            | 0.00014015552 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -283          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 132           |
|    n_updates            | 49790         |
|    policy_gradient_loss | 0.000754      |
|    std                  | 4.33e+03      |
|    value_loss           | 293           |
-------------------------------------------
Iteration: 4981 | Episodes: 202100 | Median Reward: 10.63 | Max Reward: 48.13
Iteration: 4983 | Episodes: 202200 | Median Reward: 1.68 | Max Reward: 48.13
Iteration: 4986 | Episodes: 202300 | Median Reward: 13.12 | Max Reward: 48.13
Iteration: 4988 | Episodes: 202400 | Median Reward: 3.28 | Max Reward: 48.13
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -91.8        |
| time/                   |              |
|    fps                  | 299          |
|    iterations           | 4990         |
|    time_elapsed         | 68245        |
|    total_timesteps      | 20439040     |
| train/                  |              |
|    approx_kl            | 0.0002910617 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -284         |
|    explained_variance   | 0            |
|    learning_rate        | 0.0005       |
|    loss                 | 81.4         |
|    n_updates            | 49890        |
|    policy_gradient_loss | -0.000714    |
|    std                  | 4.4e+03      |
|    value_loss           | 191          |
------------------------------------------
Iteration: 4991 | Episodes: 202500 | Median Reward: 14.35 | Max Reward: 48.13
Iteration: 4993 | Episodes: 202600 | Median Reward: 7.30 | Max Reward: 48.13
Iteration: 4996 | Episodes: 202700 | Median Reward: 3.82 | Max Reward: 48.13
Iteration: 4998 | Episodes: 202800 | Median Reward: 15.18 | Max Reward: 48.13
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -96.1         |
| time/                   |               |
|    fps                  | 299           |
|    iterations           | 5000          |
|    time_elapsed         | 68391         |
|    total_timesteps      | 20480000      |
| train/                  |               |
|    approx_kl            | 0.00027616572 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -284          |
|    explained_variance   | 0             |
|    learning_rate        | 0.0005        |
|    loss                 | 88.6          |
|    n_updates            | 49990         |
|    policy_gradient_loss | -0.00061      |
|    std                  | 4.43e+03      |
|    value_loss           | 206           |
-------------------------------------------
Training End | Episodes: 202862 | Median Reward: -1.40 | Max Reward: 48.13
Plot saved as fig_code2_1.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> e[K(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> exit()
exit

Script done on 2024-10-18 18:03:23-04:00 [COMMAND_EXIT_CODE="0"]
