Script started on 2024-10-17 02:44:08-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/13" COLUMNS="211" LINES="55"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> conda activate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python code_1[K[K1_1.py
^CTraceback (most recent call last):
  File "code1_1.py", line 7, in <module>
    import torch
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/torch/__init__.py", line 1880, in <module>
    from . import masked
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/torch/masked/__init__.py", line 1, in <module>
    from torch.masked._ops import (
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/torch/masked/_ops.py", line 9, in <module>
    from torch.masked.maskedtensor.core import is_masked_tensor, MaskedTensor
  File "/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/torch/masked/maskedtensor/__init__.py", line 8, in <module>
    from .unary import _apply_native_unary, _is_native_unary
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 934, in get_code
  File "<frozen importlib._bootstrap_external>", line 1032, in get_data
KeyboardInterrupt

(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> nano code1_1.py
[?2004h[?1049h[22;0;0t[1;55r(B[m[4l[?7h[39;49m[?1h=[?1h=[?25l[39;49m(B[m[H[2J[53;99H(B[0;7m[ Reading... ](B[m[53;97H(B[0;7m[ Read 317 lines ](B[m[H(B[0;7m  GNU nano 4.8                                                                                        code1_1.py                                                                                                   [1;210H(B[m[54d(B[0;7m^G(B[m Get Help[18G(B[0;7m^O(B[m Write Out     (B[0;7m^W(B[m Where Is[52G(B[0;7m^K(B[m Cut Text[69G(B[0;7m^J(B[m Justify[86G(B[0;7m^C(B[m Cur Pos[103G(B[0;7mM-U(B[m Undo[54;120H(B[0;7mM-A(B[m Mark Text    (B[0;7mM-](B[m To Bracket   (B[0;7mM-Q(B[m Previous     (B[0;7m^B(B[m Back[54;188H(B[0;7m^â—€(B[m Prev Word[55d(B[0;7m^X(B[m Exit[55;18H(B[0;7m^R(B[m Read File     (B[0;7m^\(B[m Replace[52G(B[0;7m^U(B[m Paste Text    (B[0;7m^T(B[m To Spell[86G(B[0;7m^_(B[m Go To Line    (B[0;7mM-E(B[m Redo[55;120H(B[0;7mM-6(B[m Copy Text    (B[0;7m^Q(B[m Where Was     (B[0;7mM-W(B[m Next[55;171H(B[0;7m^F(B[m Forward[188G(B[0;7m^â–¶(B[m Next Word[53d[2d(B[0;1m[36mimport[39m(B[m gymnasium (B[0;1m[36mas[39m(B[m gym[3d(B[0;1m[36mfrom[39m(B[m gymnasium (B[0;1m[36mimport[39m(B[m spaces[4d(B[0;1m[36mfrom[39m(B[m statistics (B[0;1m[36mimport[39m(B[m median[5d(B[0;1m[36mimport[39m(B[m numpy (B[0;1m[36mas[39m(B[m np[6d(B[0;1m[36mimport[39m(B[m mujoco_py[7d(B[0;1m[36mimport[39m(B[m os[8d(B[0;1m[36mimport[39m(B[m torch[9d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.vec_env (B[0;1m[36mimport[39m(B[m DummyVecEnv, VecMonitor[10d(B[0;1m[36mfrom[39m(B[m sb3_contrib (B[0;1m[36mimport[39m(B[m RecurrentPPO[11d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.callbacks (B[0;1m[36mimport[39m(B[m BaseCallback[12d(B[0;1m[36mimport[39m(B[m logging[13d(B[0;1m[36mimport[39m(B[m matplotlib.pyplot (B[0;1m[36mas[39m(B[m plt[42m  [15d[49m(B[mlogging.basicConfig(level=logging.INFO)[17d(B[0;1m[36mclass[39m(B[m HandEnv(gym.Env):[18;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self):[19;9Hsuper(HandEnv, self).__init__()[20;9Hxml_path = os.path.expanduser((B[0;1m[32m'/home/easgrad/dgusain/Bot_hand/bot_hand.xml'[39m(B[m)[21;9Hlogging.info(f(B[0;1m[32m"Attempting to load Mujoco model from: {xml_path}"[39m(B[m)[23;9H(B[0;1m[36mif[39m(B[m (B[0;1m[36mnot[39m(B[m os.path.isfile(xml_path):[24;13Hlogging.error(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[25;13H(B[0;1m[36mraise[39m(B[m FileNotFoundError(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[27;9H(B[0;1m[36mtry[39m(B[m:[28dself.model = mujoco_py.load_model_from_path(xml_path)[29;13Hself.sim = mujoco_py.MjSim(self.model)[30;13Hlogging.info((B[0;1m[32m"Mujoco model loaded successfully."[39m(B[m)[31;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[32;13Hlogging.error(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[33;13H(B[0;1m[36mraise[39m(B[m RuntimeError(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[35;9Hjoint_weights = [[36;13H0.5, (B[0;1m[31m # ID 0: WristJoint1[37;13H[39m(B[m0.5, (B[0;1m[31m # ID 1: WristJoint0[38;13H[39m(B[m3.0, (B[0;1m[31m # ID 2: ForeFingerJoint3 (MCP)[39;13H[39m(B[m2.5, (B[0;1m[31m # ID 3: ForeFingerJoint2 (MCP proximal)[40;13H[39m(B[m2.0, (B[0;1m[31m # ID 4: ForeFingerJoint1 (PIP)[41;13H[39m(B[m1.0, (B[0;1m[31m # ID 5: ForeFingerJoint0 (DIP)[42;13H[39m(B[m3.0, (B[0;1m[31m # ID 6: MiddleFingerJoint3 (MCP)[43;13H[39m(B[m2.5, (B[0;1m[31m # ID 7: MiddleFingerJoint2 (MCP proximal)[44;13H[39m(B[m2.0, (B[0;1m[31m # ID 8: MiddleFingerJoint1 (PIP)[45;13H[39m(B[m1.0, (B[0;1m[31m # ID 9: MiddleFingerJoint0 (DIP)[46;13H[39m(B[m3.0, (B[0;1m[31m # ID 10: RingFingerJoint3 (MCP)[47;13H[39m(B[m2.5, (B[0;1m[31m # ID 11: RingFingerJoint2 (MCP proximal)[48;13H[39m(B[m2.0, (B[0;1m[31m # ID 12: RingFingerJoint1 (PIP)[49;13H[39m(B[m1.0, (B[0;1m[31m # ID 13: RingFingerJoint0 (DIP)[50;13H[39m(B[m1.5, (B[0;1m[31m # ID 14: LittleFingerJoint4 (CMC Joint rotation)[51;13H[39m(B[m3.0, (B[0;1m[31m # ID 15: LittleFingerJoint3 (MCP)[52;13H[39m(B[m2.5, (B[0;1m[31m # ID 16: LittleFingerJoint2 (MCP proximal)[2d[39m(B[m[?12l[?25h[?25l[8d[?12l[?25h[?25l[14d[?12l[?25h[?25l[20d[?12l[?25h[?25l[26d[?12l[?25h[?25l[53d[K[32d[?12l[?25h[?25l[38d[?12l[?25h[?25l[44d[?12l[?25h[?25l[50d[?12l[?25h[?25l[2;53r[53;1H[4S[1;55r[49;13H2.0, (B[0;1m[31m # ID 17: LittleFingerJoint1 (PIP)[50;13H[39m(B[m1.0, (B[0;1m[31m # ID 18: LittleFingerJoint0 (DIP)[51;13H[39m(B[m1.5, (B[0;1m[31m # ID 19: ThumbJoint4 (CMC Joint abduction/adduction)[52;13H[39m(B[m1.5, (B[0;1m[31m # ID 20: ThumbJoint3 (CMC Joint flexion/extension)[39m(B[m[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;13H1.5, (B[0;1m[31m # ID 21: ThumbJoint2 (CMC Joint rotation)[48;13H[39m(B[m3.0, (B[0;1m[31m # ID 22: ThumbJoint1 (MCP)[49;13H[39m(B[m0.5, (B[0;1m[31m # ID 23: ThumbJoint0 (IP)[50;13H[39m(B[m0.0  (B[0;1m[31m # ID 24: None[51;9H[39m(B[m][52d(B[0;1m[31m # Convertiing joint weights to a tensor[39m(B[m[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;9Hself.joint_weights = torch.tensor(joint_weights, dtype=torch.float32)[48;8H(B[0;1m[31m # Normalize the weights so that their sum equals 1[49;9H[39m(B[mself.joint_weights /= self.joint_weights.sum()[51;9Hself.initial_state = self.sim.get_state()[52;8H(B[0;1m[31m # Define observation and action spaces[39m(B[m[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;9Hself.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(24,), dtype=np.float32)[48;9Hself.target_threshold = 100[49;9Hself.max_steps = 100[50;9Hself.steps_taken = 0[52;8H(B[0;1m[31m # Initialize actuator ranges[39m(B[m[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;9Hactuator_ranges = self.sim.model.actuator_ctrlrange[48;9Hself.actuator_min = torch.tensor(actuator_ranges[:, 0], dtype=torch.float32)[49;9Hself.actuator_max = torch.tensor(actuator_ranges[:, 1], dtype=torch.float32)[51;9Hself.action_space = spaces.Box(low=-1, high=1, shape=(self.actuator_min.shape[0],), dtype=np.float32)[52;9Hself.ground_truth_quats = torch.tensor(self.get_ground_truth_quaternion(), dtype=torch.float32)[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[48;5H(B[0;1m[36mdef[34m reset[39m(B[m(self, seed=(B[0;1m[35mNone[39m(B[m, options=(B[0;1m[35mNone[39m(B[m):[49;9Hsuper().reset(seed=seed)[50;8H(B[0;1m[31m #self.sim.reset()[51;9H[39m(B[mself.sim.set_state(self.initial_state)[52;9Hself.sim.forward()[?12l[?25h[?25l7[2;53r8[53d[24S[1;55r[29;9Hself.steps_taken = 0[30;9Hobs = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[31;9H(B[0;1m[36mreturn[39m(B[m obs, {}[33;5H(B[0;1m[36mdef[34m step[39m(B[m(self, action: np.ndarray):[34;9Haction_tensor = torch.from_numpy(action).float()[35;9Hrescaled_action = self.actuator_min + (action_tensor + 1) * (self.actuator_max - self.actuator_min) / 2[36;9Hself.sim.data.ctrl[:] = rescaled_action.numpy()[37;9Hself.sim.step()[39;8H(B[0;1m[31m # Fetch state as a tensor[40;9H[39m(B[mstate = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[41d[42m        [42d(B[0;1m[31m # Calculate done condition[43;9H[39m(B[mdone = self.calculate_done(state, flag=(B[0;1m[35mFalse[39m(B[m) (B[0;1m[36mor[39m(B[m self.steps_taken >= self.max_steps[44;9H(B[0;1m[36mif[39m(B[m done:[45;13Hreward = self.calculate_reward(state, flag=(B[0;1m[35mTrue[39m(B[m).item()[46;9H(B[0;1m[36melse[39m(B[m:[47dreward = -1.0 (B[0;1m[31m # Penalize extra steps[49;9H[39m(B[mself.steps_taken += 1[50;9Htruncated = (B[0;1m[35mFalse[51;9H[39m(B[minfo = {}[52d[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;9H(B[0;1m[36mreturn[39m(B[m state.numpy(), reward, done, truncated, info[49;5H(B[0;1m[36mdef[34m calculate_done[39m(B[m(self, state: torch.Tensor, flag: bool) -> bool:[50;9H(B[0;1m[36mif[39m(B[m flag:[51;13H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mFalse[52;9H[39m(B[mconfidence = self.calculate_confidence(state)[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;9H(B[0;1m[36mreturn[39m(B[m confidence >= self.target_threshold[49;5H(B[0;1m[36mdef[34m calculate_reward[39m(B[m(self, state: torch.Tensor, flag: bool) -> torch.Tensor:[50;9H(B[0;1m[36mif[39m(B[m flag:[51;13Hconfidence = self.calculate_confidence(state)[52;13H(B[0;1m[36mreturn[39m(B[m confidence - 50 (B[0;1m[31m # Reward calculation[39m(B[m[?12l[?25h[?25l7[2;53r8[53d[18S[1;55r[35;9H(B[0;1m[36melse[39m(B[m:[36d(B[0;1m[36mif[39m(B[m confidence > 85:[37;17H(B[0;1m[36mreturn[39m(B[m (confidence-50)/2.0(B[0;1m[31m # encouraging the model in the right direction[38;13H[36melse[39m(B[m:[39d(B[0;1m[36mreturn[39m(B[m (confidence-50)/4.0(B[0;1m[31m # lesser reward[41;5H[36mdef[34m calculate_confidence[39m(B[m(self, state: torch.Tensor) -> torch.Tensor:[42;9Hrendered_quat = self.get_rendered_pose_quaternion()[43;9Hrendered_quat = self.normalize_quaternion(rendered_quat)[44;9Hgt_quat = self.normalize_quaternion(self.ground_truth_quats)[45;9Hsimilarity = torch.abs(torch.sum(rendered_quat * gt_quat, dim=1)) * 100[46;9Hweighted_similarity = similarity * self.joint_weights[47;9Havg_confidence = weighted_similarity.sum()[48;9H(B[0;1m[36mreturn[39m(B[m avg_confidence[50;5H(B[0;1m[36mdef[34m get_rendered_pose_quaternion[39m(B[m(self) -> torch.Tensor:[51;9Hquaternions = [][52;9H(B[0;1m[36mfor[39m(B[m joint (B[0;1m[36min[39m(B[m range(self.model.njnt):[?12l[?25h[?25l7[2;53r8[53d[12S[1;55r[41;13Hq = self.sim.data.qpos[self.model.jnt_qposadr[joint]:self.model.jnt_qposadr[joint] + 4][42;13Hquaternions.append(q)[43;8H(B[0;1m[31m # Convert list of arrays to a single NumPy array[44;9H[39m(B[mquaternions_np = np.array(quaternions, dtype=np.float32)[45;8H(B[0;1m[31m # Convert NumPy array to PyTorch tensor[46;9H[36mreturn[39m(B[m torch.from_numpy(quaternions_np)[48;5H(B[0;1m[36mdef[34m normalize_quaternion[39m(B[m(self, q: torch.Tensor) -> torch.Tensor:[49;9Hnorm = torch.norm(q, dim=1, keepdim=(B[0;1m[35mTrue[39m(B[m)[50;9H(B[0;1m[36mreturn[39m(B[m q / norm.clamp(min=1e-8) (B[0;1m[31m # Prevent division by zero[52;5H[36mdef[34m get_ground_truth_quaternion[39m(B[m(self) -> np.ndarray:[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;9Hground_truth_quats = [[48;13H[6.32658406e-04,  1.25473697e-03, -9.99718591e-03,  1.56702220e+00],[49;13H[1.25473697e-03, -9.99718591e-03,  1.56702220e+00,  1.56730258e+00],[50;13H[-0.00999719,  1.5670222,   1.56730258,  1.5674143],[51;13H[1.5670222,   1.56730258,  1.5674143,  -0.00999801],[52;13H[1.56730258,  1.5674143,  -0.00999801,  1.56701926],[?12l[?25h[?25l7[2;53r8[53d[18S[1;55r[35;13H[1.5674143,  -0.00999801,  1.56701926,  1.56730194],[36;13H[-0.00999801,  1.56701926,  1.56730194,  1.5674143],[37;13H[1.56701926,  1.56730194,  1.5674143,  -0.00999756],[38;13H[1.56730194,  1.5674143,  -0.00999756,  1.56701717],[39;13H[1.5674143,  -0.00999756,  1.56701717,  1.56730177],[40;13H[-0.00999756,  1.56701717,  1.56730177,  1.56741429],[41;13H[1.56701717, 1.56730177, 1.56741429, 0.00868252],[42;13H[1.56730177,  1.56741429,  0.00868252, -0.01000805],[43;13H[1.56741429,  0.00868252, -0.01000805,  1.56708349],[44;13H[0.00868252, -0.01000805,  1.56708349,  1.56730911],[45;13H[-0.01000805,  1.56708349,  1.56730911,  1.56741508],[46;13H[1.56708349, 1.56730911, 1.56741508, 0.49916711],[47;13H[1.56730911, 1.56741508, 0.49916711, 0.50022545],[48;13H[1.56741508, 0.49916711, 0.50022545, 0.25330455],[49;13H[4.99167109e-01, 5.00225452e-01, 2.53304550e-01, 4.08440608e-04],[50;13H[5.00225452e-01,  2.53304550e-01,  4.08440608e-04, -9.00000689e-01],[51;13H[2.53304550e-01,  4.08440608e-04, -9.00000689e-01,  1.00000000e-01],[52;13H[4.08440608e-04, -9.00000689e-01,  1.00000000e-01, -1.00000000e-01],[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;13H[-0.90000069,  0.1,[47;40H-0.1,[47;54H0.01463168],[48;13H[0.1,[48;26H-0.1,[48;40H0.01463168,  1.0][49;9H][50d(B[0;1m[36mreturn[39m(B[m np.array(ground_truth_quats, dtype=np.float32)[52d(B[0;1m[36mclass[39m(B[m RewardCallback(BaseCallback):[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;5H(B[0;1m[32m"""[48d    Custom callback for logging average rewards over every 100 episodes, episode numbers, and iteration numbers.[49d    Also stores average rewards for plotting.[50d    """[52;5H[36mdef[34m __init__[39m(B[m(self, avg_interval=100):[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;9Hsuper(RewardCallback, self).__init__()[48;9Hself.episode_num = 0[49;9Hself.max_reward = -np.inf[50;9Hself.avg_interval = avg_interval (B[0;1m[31m # Number of episodes per average - not hyperparameter[51;9H[39m(B[mself.sum_rewards = 0.0[52;9Hself.count_rewards = 0[?12l[?25h[?25l7[2;53r8[53d[18S[1;55r[35;9Hself.iteration_num = 0[42m     [36;9H[49m(B[mself.rewards_list = [][37;9Hself.median_rewards = {}[39;5H(B[0;1m[36mdef[34m _on_rollout_end[39m(B[m(self) -> bool:[40;9H(B[0;1m[32m"""[41d        Called at the end of a rollout.[42d        Used to track the number of iterations.[43d        """[44;9H[39m(B[mself.iteration_num += 1[45;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[47;5H[36mdef[34m _on_step[39m(B[m(self) -> bool:[48;9H(B[0;1m[32m"""[49d        Called at every step. Checks if any episode has finished and logs the reward.[50d        """[51;8H[31m # Retrieve 'dones' and 'rewards' from the current step[52;9H[39m(B[mdones = self.locals.get((B[0;1m[32m'dones'[39m(B[m, [])[?12l[?25h[?25l7[2;53r8[53d[12S[1;55r[41;9Hrewards = self.locals.get((B[0;1m[32m'rewards'[39m(B[m, [])[43;8H(B[0;1m[31m # Check if any of the environments are done[44;9H[36mif[39m(B[m np.any(dones):[45;13H(B[0;1m[36mfor[39m(B[m done, reward (B[0;1m[36min[39m(B[m zip(dones, rewards):[46;17H(B[0;1m[36mif[39m(B[m done:[47;21Hself.episode_num += 1[48;21H(B[0;1m[36mif[39m(B[m reward > self.max_reward:[49;25Hself.max_reward = reward[50;21Hself.rewards_list.append(reward)[51;21H(B[0;1m[36mif[39m(B[m len(self.rewards_list) == self.avg_interval:[52;25Hmedian_reward = median(self.rewards_list)[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;25Hblock_num = self.episode_num // self.avg_interval[48;25Hself.median_rewards[block_num] = median_reward[49;25Hprint(f(B[0;1m[32m"Iteration: {self.iteration_num} | Episodes: {block_num * self.avg_interval} | Median Reward: {median_reward:.2f} | Max Reward: {self.max_reward:.2f}"[39m(B[m)[50;24H(B[0;1m[31m # Reset sum and count[51;25H[39m(B[mself.rewards_list = [][52;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[39m(B[m[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[48;5H(B[0;1m[36mdef[34m _on_training_end[39m(B[m(self) -> (B[0;1m[35mNone[39m(B[m:[49;9H(B[0;1m[32m"""[50d        Called at the end of training. If there are remaining episodes, compute and store the average.[51d        """[52;9H[36mif[39m(B[m len(self.rewards_list) > 0:[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;13Hmedian_reward = median(self.rewards_list)[48;13Hblock_num = self.episode_num // self.avg_interval + 1[49;13Hself.median_rewards[block_num] = median_reward[50;13Hcurrent_max = max(self.rewards_list)[51;13H(B[0;1m[36mif[39m(B[m current_max > self.max_reward:[52;17Hself.max_reward = current_max[42m           [49m(B[m[?12l[?25h[?25l[46d[?12l[?25h[?25l[2;53r[53;1H[12S[1;55r[41;13Hprint(f(B[0;1m[32m"Training End | Episodes: {self.episode_num} | Median Reward: {median_reward:.2f} | Max Reward: {self.max_reward:.2f}"[39m(B[m)[42;13Hself.rewards_list = [][44d(B[0;1m[36mdef[34m make_env[39m(B[m():[45;5H(B[0;1m[32m"""Utility function to create a HandEnv without Monitor."""[46;5H[36mdef[34m _init[39m(B[m():[47;9H(B[0;1m[36mtry[39m(B[m:[48denv = HandEnv()[49;13H(B[0;1m[36mreturn[39m(B[m env[50;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[51;13Hlogging.error(f(B[0;1m[32m"Failed to initialize HandEnv: {e}"[39m(B[m)[52;13H(B[0;1m[36mraise[39m(B[m e[?12l[?25h[?25l7[2;53r8[53d[12S[1;55r[41;5H(B[0;1m[36mreturn[39m(B[m _init[43d(B[0;1m[36mdef[34m train_on_gpu[39m(B[m(gpu_id: int, num_envs: int, total_timesteps: int, num_steps: int, save_interval: int):[44;5Hos.environ[(B[0;1m[32m'CUDA_VISIBLE_DEVICES'[39m(B[m] = str(gpu_id)[45;5Hdevice = (B[0;1m[32m'cuda'[39m(B[m (B[0;1m[36mif[39m(B[m torch.cuda.is_available() (B[0;1m[36melse[39m(B[m (B[0;1m[32m'cpu'[46;5H[39m(B[mprint(f(B[0;1m[32m"GPU {gpu_id}: Using {device} device"[39m(B[m)[47;5Henv = DummyVecEnv([make_env() (B[0;1m[36mfor[39m(B[m _ (B[0;1m[36min[39m(B[m range(num_envs)])[42m  [48;5H[49m(B[menv = VecMonitor(env) (B[0;1m[31m # Use VecMonitor instead of individual Monitor wrappers[50;5H[39m(B[mmodel = RecurrentPPO([51;9H(B[0;1m[32m'MlpLstmPolicy'[39m(B[m,[52;9Henv,[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;9Hverbose=1,[48;9Hdevice=device,[49;9Hent_coef=0.05,[50;9Hlearning_rate=0.0005,[51;9Hclip_range=0.4,[52;9Hn_steps=num_steps,[33G(B[0;1m[31m # Steps per environment per update[39m(B[m[?12l[?25h[?25l7[2;53r8[53d[18S[1;55r[35;9Hbatch_size=4096,[35;34H(B[0;1m[31m # Increased batch size[36;9H[39m(B[mgamma=0.99,[37;9Hgae_lambda=0.95,[38;9Hmax_grad_norm=0.5,[39;9Hvf_coef=0.5,[40;9Huse_sde=(B[0;1m[35mTrue[39m(B[m,[40;33H(B[0;1m[31m # Use State Dependent Exploration for better exploration[41;5H[39m(B[m)[43dcallback = RewardCallback(avg_interval=100)[45;5H(B[0;1m[36mtry[39m(B[m:[46dmodel.learn(total_timesteps=total_timesteps, callback=callback, log_interval=10)[47;5H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[48;9Hlogging.error(f(B[0;1m[32m"Error during training: {e}"[39m(B[m)[49;9H(B[0;1m[36mraise[39m(B[m e[51;5Hmodel.save(f(B[0;1m[32m"/home/easgrad/dgusain/Bot_hand/agents/agent_code1_1_gpu{gpu_id}"[39m(B[m)[52d[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;5H(B[0;1m[36mreturn[39m(B[m callback (B[0;1m[31m # Return the callback to access episode_rewards[49d[36mdef[34m main[39m(B[m():[50;5Hgpu_id = 4[42m          [51;5H[49m(B[mnum_envs = 4[42m        [52;5H[49m(B[mn_iter = 8000[42m         [49m(B[m[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;5Hn_steps = 1024     (B[0;1m[31m # Steps per environment per update[48;5H[39m(B[mtotal_timesteps = n_iter * n_steps * num_envs (B[0;1m[31m # Total training timesteps[49;5H[39m(B[msave_interval = 25[42m  [51;5H[49m(B[mcallback = train_on_gpu(gpu_id, num_envs, total_timesteps, n_steps, save_interval)[52;5Hblock_numbers = list(callback.median_rewards.keys())[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;5Hblock_rewards = list(callback.median_rewards.values())[48;5Hepisode_numbers = [block_num * 100 (B[0;1m[36mfor[39m(B[m block_num (B[0;1m[36min[39m(B[m block_numbers][50;5Hplt.figure(figsize=(10, 6))[51;5Hplt.plot(episode_numbers, block_rewards, marker=(B[0;1m[32m'o'[39m(B[m, linestyle=(B[0;1m[32m'-'[39m(B[m, color=(B[0;1m[32m'b'[39m(B[m)[52;5Hplt.xlabel((B[0;1m[32m'Episode Number'[39m(B[m)[?12l[?25h[?25l7[2;53r8[53d[6S[1;55r[47;5Hplt.ylabel((B[0;1m[32m'Median Reward (per 100 episodes)'[39m(B[m)[48;5Hplt.title((B[0;1m[32m'Median Reward vs Episode Number'[39m(B[m)[49;5Hplt.grid((B[0;1m[35mTrue[39m(B[m)[50;5Hplt.savefig((B[0;1m[32m'/home/easgrad/dgusain/Bot_hand/figs/fig_code1_1.png'[39m(B[m)[42m  [51;5H[49m(B[mplt.close()[42m [52;5H[49m(B[mprint((B[0;1m[32m"Plot saved as fig_code1_1.png"[39m(B[m)[?12l[?25h[?25l7[2;53r8[53d[5S[1;55r[49;1H(B[0;1m[36mif[39m(B[m __name__ == (B[0;1m[32m"__main__"[39m(B[m:[50;5Hmain()[52d[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[46d[?12l[?25h[?25l[40d[?12l[?25h[?25l[34d[?12l[?25h[?25l[28d[?12l[?25h[?25l[A[?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25lg[?12l[?25h[?25lp[?12l[?25h[?25lu[?12l[?25h[?25l_[?12l[?25h[?25li[?12l[?25h[?25ld[?12l[?25h[?25l [?12l[?25h[?25l=[?12l[?25h[?25l [?12l[?25h[?25l4[?12l[?25h[?25l[1;202H(B[0;7mModified(B[m[27;13H[42m [49m(B[m[1P[?12l[?25h[?25l 0[27;24H[42m [27;15H[49m(B[m[?12l[?25h[?25l[53d(B[0;7mSave modified buffer?                                                                                                                                                                                              [54;1H Y(B[m Yes[K[55d(B[0;7m N(B[m No  [55;17H(B[0;7m^C(B[m Cancel[K[53;23H[?12l[?25h[?25l[54d(B[0;7m^G(B[m Get Help[54;53H(B[0;7mM-D(B[m DOS Format[54;105H(B[0;7mM-A(B[m Append[54;157H(B[0;7mM-B(B[m Backup File[55d(B[0;7m^C(B[m Cancel[17G         [55;53H(B[0;7mM-M(B[m Mac Format[55;105H(B[0;7mM-P(B[m Prepend[55;157H(B[0;7m^T(B[m To Files[53d(B[0;7mFile Name to Write: code1_1.py(B[m[53;31H[?12l[?25h[?25l[53;98H[1K (B[0;7m[ Writing... ](B[m[K[1;202H(B[0;7m        (B[m[53;97H(B[0;7m[ Wrote 317 lines ](B[m[J[55d[?12l[?25h[55;1H[?1049l[23;0;0t[?1l>[?2004l(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> clear
[H[2J(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> clear[43Gnano code1_1.py[43G[2@python[60G
GPU 0: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
Iteration: 2 | Episodes: 100 | Median Reward: 24.00 | Max Reward: 37.12
Iteration: 4 | Episodes: 200 | Median Reward: 21.70 | Max Reward: 37.12
Iteration: 7 | Episodes: 300 | Median Reward: 20.77 | Max Reward: 41.13
Iteration: 9 | Episodes: 400 | Median Reward: 20.90 | Max Reward: 41.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -81.2       |
| time/                   |             |
|    fps                  | 452         |
|    iterations           | 10          |
|    time_elapsed         | 90          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.017311813 |
|    clip_fraction        | 0.00317     |
|    clip_range           | 0.4         |
|    entropy_loss         | -64.9       |
|    explained_variance   | -0.0159     |
|    learning_rate        | 0.0005      |
|    loss                 | 94.1        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0052     |
|    std                  | 1.01        |
|    value_loss           | 197         |
-----------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 16.40 | Max Reward: 41.13
Iteration: 14 | Episodes: 600 | Median Reward: 21.74 | Max Reward: 41.13
Iteration: 17 | Episodes: 700 | Median Reward: 19.44 | Max Reward: 41.13
Iteration: 19 | Episodes: 800 | Median Reward: 21.57 | Max Reward: 41.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.6       |
| time/                   |             |
|    fps                  | 474         |
|    iterations           | 20          |
|    time_elapsed         | 172         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.035117947 |
|    clip_fraction        | 0.0198      |
|    clip_range           | 0.4         |
|    entropy_loss         | -73.6       |
|    explained_variance   | -0.00693    |
|    learning_rate        | 0.0005      |
|    loss                 | 92          |
|    n_updates            | 190         |
|    policy_gradient_loss | 0.0033      |
|    std                  | 1.02        |
|    value_loss           | 193         |
-----------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 20.59 | Max Reward: 41.69
Iteration: 24 | Episodes: 1000 | Median Reward: 20.23 | Max Reward: 41.69
Iteration: 27 | Episodes: 1100 | Median Reward: 19.05 | Max Reward: 41.69
Iteration: 29 | Episodes: 1200 | Median Reward: 12.62 | Max Reward: 41.69
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -86.7       |
| time/                   |             |
|    fps                  | 489         |
|    iterations           | 30          |
|    time_elapsed         | 250         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.042192426 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.4         |
|    entropy_loss         | -76.7       |
|    explained_variance   | -0.00277    |
|    learning_rate        | 0.0005      |
|    loss                 | 79          |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0202     |
|    std                  | 1.03        |
|    value_loss           | 168         |
-----------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 11.28 | Max Reward: 41.69
Iteration: 34 | Episodes: 1400 | Median Reward: 22.34 | Max Reward: 41.69
Iteration: 36 | Episodes: 1500 | Median Reward: 26.15 | Max Reward: 41.90
Iteration: 39 | Episodes: 1600 | Median Reward: 15.40 | Max Reward: 41.90
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -82.7       |
| time/                   |             |
|    fps                  | 476         |
|    iterations           | 40          |
|    time_elapsed         | 343         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.075221136 |
|    clip_fraction        | 0.0413      |
|    clip_range           | 0.4         |
|    entropy_loss         | -80.2       |
|    explained_variance   | -0.00102    |
|    learning_rate        | 0.0005      |
|    loss                 | 92.2        |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0144     |
|    std                  | 1.05        |
|    value_loss           | 194         |
-----------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 20.63 | Max Reward: 41.90
Iteration: 44 | Episodes: 1800 | Median Reward: 28.43 | Max Reward: 45.11
Iteration: 46 | Episodes: 1900 | Median Reward: 24.21 | Max Reward: 45.11
Iteration: 49 | Episodes: 2000 | Median Reward: 25.94 | Max Reward: 45.11
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -74.7      |
| time/                   |            |
|    fps                  | 483        |
|    iterations           | 50         |
|    time_elapsed         | 423        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.07806255 |
|    clip_fraction        | 0.0892     |
|    clip_range           | 0.4        |
|    entropy_loss         | -82        |
|    explained_variance   | -0.000399  |
|    learning_rate        | 0.0005     |
|    loss                 | 116        |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0151    |
|    std                  | 1.07       |
|    value_loss           | 241        |
----------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 29.57 | Max Reward: 45.11
Iteration: 54 | Episodes: 2200 | Median Reward: 28.62 | Max Reward: 45.11
Iteration: 56 | Episodes: 2300 | Median Reward: 25.27 | Max Reward: 45.11
Iteration: 59 | Episodes: 2400 | Median Reward: 26.80 | Max Reward: 46.04
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -70       |
| time/                   |           |
|    fps                  | 477       |
|    iterations           | 60        |
|    time_elapsed         | 515       |
|    total_timesteps      | 245760    |
| train/                  |           |
|    approx_kl            | 0.2502678 |
|    clip_fraction        | 0.14      |
|    clip_range           | 0.4       |
|    entropy_loss         | -83.6     |
|    explained_variance   | -0.000144 |
|    learning_rate        | 0.0005    |
|    loss                 | 154       |
|    n_updates            | 590       |
|    policy_gradient_loss | -0.0271   |
|    std                  | 1.11      |
|    value_loss           | 316       |
---------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 30.32 | Max Reward: 46.04
Iteration: 64 | Episodes: 2600 | Median Reward: 23.63 | Max Reward: 46.04
Iteration: 66 | Episodes: 2700 | Median Reward: 20.04 | Max Reward: 46.04
Iteration: 69 | Episodes: 2800 | Median Reward: 21.81 | Max Reward: 46.04
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -78.4     |
| time/                   |           |
|    fps                  | 479       |
|    iterations           | 70        |
|    time_elapsed         | 597       |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.1551466 |
|    clip_fraction        | 0.22      |
|    clip_range           | 0.4       |
|    entropy_loss         | -69.9     |
|    explained_variance   | -5.28e-05 |
|    learning_rate        | 0.0005    |
|    loss                 | 121       |
|    n_updates            | 690       |
|    policy_gradient_loss | 0.012     |
|    std                  | 1.14      |
|    value_loss           | 249       |
---------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 17.12 | Max Reward: 46.04
Iteration: 73 | Episodes: 3000 | Median Reward: 18.59 | Max Reward: 46.04
Iteration: 76 | Episodes: 3100 | Median Reward: 18.77 | Max Reward: 46.04
Iteration: 78 | Episodes: 3200 | Median Reward: 20.80 | Max Reward: 46.04
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -82.3     |
| time/                   |           |
|    fps                  | 484       |
|    iterations           | 80        |
|    time_elapsed         | 676       |
|    total_timesteps      | 327680    |
| train/                  |           |
|    approx_kl            | 2.7349696 |
|    clip_fraction        | 0.742     |
|    clip_range           | 0.4       |
|    entropy_loss         | -67       |
|    explained_variance   | -3.89e-05 |
|    learning_rate        | 0.0005    |
|    loss                 | 106       |
|    n_updates            | 790       |
|    policy_gradient_loss | 0.141     |
|    std                  | 1.16      |
|    value_loss           | 219       |
---------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 13.65 | Max Reward: 46.04
Iteration: 83 | Episodes: 3400 | Median Reward: 25.38 | Max Reward: 46.04
Iteration: 86 | Episodes: 3500 | Median Reward: 25.73 | Max Reward: 46.04
Iteration: 88 | Episodes: 3600 | Median Reward: 25.28 | Max Reward: 46.04
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -78        |
| time/                   |            |
|    fps                  | 479        |
|    iterations           | 90         |
|    time_elapsed         | 769        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.35621256 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.4        |
|    entropy_loss         | -74.8      |
|    explained_variance   | -1.85e-05  |
|    learning_rate        | 0.0005     |
|    loss                 | 112        |
|    n_updates            | 890        |
|    policy_gradient_loss | 0.026      |
|    std                  | 1.18       |
|    value_loss           | 231        |
----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 27.88 | Max Reward: 46.04
Iteration: 93 | Episodes: 3800 | Median Reward: 3.71 | Max Reward: 46.04
Iteration: 96 | Episodes: 3900 | Median Reward: -0.09 | Max Reward: 46.04
Iteration: 98 | Episodes: 4000 | Median Reward: -2.97 | Max Reward: 46.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -104        |
| time/                   |             |
|    fps                  | 483         |
|    iterations           | 100         |
|    time_elapsed         | 846         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.018427037 |
|    clip_fraction        | 0.039       |
|    clip_range           | 0.4         |
|    entropy_loss         | -77.8       |
|    explained_variance   | -2.3e-05    |
|    learning_rate        | 0.0005      |
|    loss                 | 60.6        |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00548    |
|    std                  | 1.19        |
|    value_loss           | 130         |
-----------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 2.50 | Max Reward: 46.04
Iteration: 103 | Episodes: 4200 | Median Reward: -0.70 | Max Reward: 46.04
Iteration: 106 | Episodes: 4300 | Median Reward: -0.70 | Max Reward: 46.04
Iteration: 108 | Episodes: 4400 | Median Reward: 4.43 | Max Reward: 46.04
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -95.7     |
| time/                   |           |
|    fps                  | 479       |
|    iterations           | 110       |
|    time_elapsed         | 939       |
|    total_timesteps      | 450560    |
| train/                  |           |
|    approx_kl            | 0.1567921 |
|    clip_fraction        | 0.11      |
|    clip_range           | 0.4       |
|    entropy_loss         | -78.9     |
|    explained_variance   | -1.78e-05 |
|    learning_rate        | 0.0005    |
|    loss                 | 91.9      |
|    n_updates            | 1090      |
|    policy_gradient_loss | -0.000501 |
|    std                  | 1.2       |
|    value_loss           | 192       |
---------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: -1.78 | Max Reward: 46.04
Iteration: 113 | Episodes: 4600 | Median Reward: 6.29 | Max Reward: 46.04
Iteration: 115 | Episodes: 4700 | Median Reward: 20.21 | Max Reward: 46.04
Iteration: 118 | Episodes: 4800 | Median Reward: 19.50 | Max Reward: 46.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.6        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 120          |
|    time_elapsed         | 1021         |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0033378426 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -80.3        |
|    explained_variance   | -1.24e-05    |
|    learning_rate        | 0.0005       |
|    loss                 | 129          |
|    n_updates            | 1190         |
|    policy_gradient_loss | 0.000639     |
|    std                  | 1.21         |
|    value_loss           | 266          |
------------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 18.49 | Max Reward: 46.04
Iteration: 123 | Episodes: 5000 | Median Reward: 16.22 | Max Reward: 46.04
Iteration: 125 | Episodes: 5100 | Median Reward: 22.69 | Max Reward: 46.04
Iteration: 128 | Episodes: 5200 | Median Reward: 12.85 | Max Reward: 46.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -89.6       |
| time/                   |             |
|    fps                  | 483         |
|    iterations           | 130         |
|    time_elapsed         | 1101        |
|    total_timesteps      | 532480      |
| train/                  |             |
|    approx_kl            | 0.019828044 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.4         |
|    entropy_loss         | -83.6       |
|    explained_variance   | -1.13e-05   |
|    learning_rate        | 0.0005      |
|    loss                 | 91.6        |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0111     |
|    std                  | 1.23        |
|    value_loss           | 192         |
-----------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 12.58 | Max Reward: 46.04
Iteration: 133 | Episodes: 5400 | Median Reward: 14.20 | Max Reward: 46.04
Iteration: 135 | Episodes: 5500 | Median Reward: 16.86 | Max Reward: 46.04
Iteration: 138 | Episodes: 5600 | Median Reward: 23.02 | Max Reward: 46.04
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -79.5      |
| time/                   |            |
|    fps                  | 479        |
|    iterations           | 140        |
|    time_elapsed         | 1197       |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.12375878 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.4        |
|    entropy_loss         | -86.3      |
|    explained_variance   | -5.84e-06  |
|    learning_rate        | 0.0005     |
|    loss                 | 133        |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.0124    |
|    std                  | 1.27       |
|    value_loss           | 275        |
----------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 24.02 | Max Reward: 46.04
Iteration: 143 | Episodes: 5800 | Median Reward: 27.50 | Max Reward: 46.04
Iteration: 145 | Episodes: 5900 | Median Reward: 26.49 | Max Reward: 46.04
Iteration: 147 | Episodes: 6000 | Median Reward: 28.01 | Max Reward: 46.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.6        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 150          |
|    time_elapsed         | 1274         |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0003227808 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -87.8        |
|    explained_variance   | -5.01e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 148          |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.000639    |
|    std                  | 1.28         |
|    value_loss           | 304          |
------------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 25.89 | Max Reward: 46.04
Iteration: 152 | Episodes: 6200 | Median Reward: 19.13 | Max Reward: 46.04
Iteration: 155 | Episodes: 6300 | Median Reward: 30.01 | Max Reward: 46.04
Iteration: 157 | Episodes: 6400 | Median Reward: 30.09 | Max Reward: 46.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.1        |
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 160          |
|    time_elapsed         | 1367         |
|    total_timesteps      | 655360       |
| train/                  |              |
|    approx_kl            | 0.0009271414 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -89.9        |
|    explained_variance   | -4.05e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 157          |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.000804    |
|    std                  | 1.3          |
|    value_loss           | 323          |
------------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 30.00 | Max Reward: 46.04
Iteration: 162 | Episodes: 6600 | Median Reward: 30.21 | Max Reward: 46.04
Iteration: 165 | Episodes: 6700 | Median Reward: 33.35 | Max Reward: 46.04
Iteration: 167 | Episodes: 6800 | Median Reward: 23.56 | Max Reward: 46.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.2        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 170          |
|    time_elapsed         | 1450         |
|    total_timesteps      | 696320       |
| train/                  |              |
|    approx_kl            | 0.0014484975 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -90.3        |
|    explained_variance   | -3.46e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 161          |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.000538    |
|    std                  | 1.32         |
|    value_loss           | 330          |
------------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 33.46 | Max Reward: 46.04
Iteration: 172 | Episodes: 7000 | Median Reward: 29.64 | Max Reward: 46.04
Iteration: 175 | Episodes: 7100 | Median Reward: 23.26 | Max Reward: 46.04
Iteration: 177 | Episodes: 7200 | Median Reward: 31.02 | Max Reward: 46.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.2        |
| time/                   |              |
|    fps                  | 482          |
|    iterations           | 180          |
|    time_elapsed         | 1529         |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.0052133994 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -90.9        |
|    explained_variance   | -2.86e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 149          |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 1.35         |
|    value_loss           | 306          |
------------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 31.16 | Max Reward: 46.04
Iteration: 182 | Episodes: 7400 | Median Reward: 31.38 | Max Reward: 46.04
Iteration: 184 | Episodes: 7500 | Median Reward: 33.16 | Max Reward: 46.04
Iteration: 187 | Episodes: 7600 | Median Reward: 29.81 | Max Reward: 46.04
Iteration: 189 | Episodes: 7700 | Median Reward: 33.60 | Max Reward: 46.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.9        |
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 190          |
|    time_elapsed         | 1623         |
|    total_timesteps      | 778240       |
| train/                  |              |
|    approx_kl            | 0.0047169523 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -91.7        |
|    explained_variance   | -2.03e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 156          |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.003       |
|    std                  | 1.4          |
|    value_loss           | 322          |
------------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 35.61 | Max Reward: 46.04
Iteration: 194 | Episodes: 7900 | Median Reward: 32.80 | Max Reward: 46.04
Iteration: 197 | Episodes: 8000 | Median Reward: 22.30 | Max Reward: 46.04
Iteration: 199 | Episodes: 8100 | Median Reward: 37.82 | Max Reward: 46.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64          |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 200          |
|    time_elapsed         | 1704         |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0016783352 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -92.2        |
|    explained_variance   | -1.79e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 178          |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 1.43         |
|    value_loss           | 365          |
------------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 30.89 | Max Reward: 46.04
Iteration: 204 | Episodes: 8300 | Median Reward: 32.55 | Max Reward: 46.04
Iteration: 207 | Episodes: 8400 | Median Reward: 33.58 | Max Reward: 46.04
Iteration: 209 | Episodes: 8500 | Median Reward: 35.75 | Max Reward: 46.04
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.6         |
| time/                   |               |
|    fps                  | 478           |
|    iterations           | 210           |
|    time_elapsed         | 1795          |
|    total_timesteps      | 860160        |
| train/                  |               |
|    approx_kl            | 0.00030936472 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -92.5         |
|    explained_variance   | -1.43e-06     |
|    learning_rate        | 0.0005        |
|    loss                 | 180           |
|    n_updates            | 2090          |
|    policy_gradient_loss | -0.00107      |
|    std                  | 1.45          |
|    value_loss           | 370           |
-------------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 29.77 | Max Reward: 46.04
Iteration: 214 | Episodes: 8700 | Median Reward: 27.68 | Max Reward: 46.04
Iteration: 216 | Episodes: 8800 | Median Reward: 33.78 | Max Reward: 46.04
Iteration: 219 | Episodes: 8900 | Median Reward: 28.14 | Max Reward: 46.04
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.1        |
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 220          |
|    time_elapsed         | 1879         |
|    total_timesteps      | 901120       |
| train/                  |              |
|    approx_kl            | 0.0004964692 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.3        |
|    explained_variance   | -1.67e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 107          |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.000756    |
|    std                  | 1.49         |
|    value_loss           | 225          |
------------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 27.50 | Max Reward: 46.04
Iteration: 224 | Episodes: 9100 | Median Reward: 29.67 | Max Reward: 46.44
Iteration: 226 | Episodes: 9200 | Median Reward: 30.97 | Max Reward: 46.44
Iteration: 229 | Episodes: 9300 | Median Reward: 30.73 | Max Reward: 46.44
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72.2        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 230          |
|    time_elapsed         | 1960         |
|    total_timesteps      | 942080       |
| train/                  |              |
|    approx_kl            | 0.0070462273 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -94.2        |
|    explained_variance   | -1.19e-06    |
|    learning_rate        | 0.0005       |
|    loss                 | 145          |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 1.55         |
|    value_loss           | 300          |
------------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 25.60 | Max Reward: 46.44
Iteration: 234 | Episodes: 9500 | Median Reward: 30.47 | Max Reward: 46.44
Iteration: 236 | Episodes: 9600 | Median Reward: 31.16 | Max Reward: 46.44
Iteration: 239 | Episodes: 9700 | Median Reward: 36.70 | Max Reward: 47.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.2       |
| time/                   |             |
|    fps                  | 478         |
|    iterations           | 240         |
|    time_elapsed         | 2055        |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.008073031 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -94.9       |
|    explained_variance   | -1.19e-06   |
|    learning_rate        | 0.0005      |
|    loss                 | 169         |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00233    |
|    std                  | 1.59        |
|    value_loss           | 348         |
-----------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 36.75 | Max Reward: 47.91
Iteration: 244 | Episodes: 9900 | Median Reward: 32.65 | Max Reward: 47.91
Iteration: 246 | Episodes: 10000 | Median Reward: 34.73 | Max Reward: 47.91
Iteration: 249 | Episodes: 10100 | Median Reward: 40.06 | Max Reward: 47.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.9       |
| time/                   |             |
|    fps                  | 479         |
|    iterations           | 250         |
|    time_elapsed         | 2136        |
|    total_timesteps      | 1024000     |
| train/                  |             |
|    approx_kl            | 0.001026767 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -95.4       |
|    explained_variance   | -9.54e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 164         |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.000819   |
|    std                  | 1.62        |
|    value_loss           | 337         |
-----------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 31.20 | Max Reward: 47.91
Iteration: 253 | Episodes: 10300 | Median Reward: 37.63 | Max Reward: 47.91
Iteration: 256 | Episodes: 10400 | Median Reward: 32.39 | Max Reward: 47.91
Iteration: 258 | Episodes: 10500 | Median Reward: 32.59 | Max Reward: 47.91
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.9       |
| time/                   |             |
|    fps                  | 478         |
|    iterations           | 260         |
|    time_elapsed         | 2224        |
|    total_timesteps      | 1064960     |
| train/                  |             |
|    approx_kl            | 0.002358719 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -95.9       |
|    explained_variance   | -8.34e-07   |
|    learning_rate        | 0.0005      |
|    loss                 | 152         |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.00102    |
|    std                  | 1.65        |
|    value_loss           | 314         |
-----------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 30.81 | Max Reward: 47.91
Iteration: 263 | Episodes: 10700 | Median Reward: 30.15 | Max Reward: 47.91
Iteration: 266 | Episodes: 10800 | Median Reward: 28.73 | Max Reward: 47.91
Iteration: 268 | Episodes: 10900 | Median Reward: 33.90 | Max Reward: 47.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65          |
| time/                   |              |
|    fps                  | 478          |
|    iterations           | 270          |
|    time_elapsed         | 2312         |
|    total_timesteps      | 1105920      |
| train/                  |              |
|    approx_kl            | 0.0005387494 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -96.5        |
|    explained_variance   | -7.15e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 162          |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.000453    |
|    std                  | 1.7          |
|    value_loss           | 335          |
------------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 37.56 | Max Reward: 47.91
Iteration: 273 | Episodes: 11100 | Median Reward: 31.90 | Max Reward: 47.91
Iteration: 276 | Episodes: 11200 | Median Reward: 29.96 | Max Reward: 47.91
Iteration: 278 | Episodes: 11300 | Median Reward: 30.63 | Max Reward: 47.91
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -68.2      |
| time/                   |            |
|    fps                  | 479        |
|    iterations           | 280        |
|    time_elapsed         | 2392       |
|    total_timesteps      | 1146880    |
| train/                  |            |
|    approx_kl            | 0.00165368 |
|    clip_fraction        | 0.000488   |
|    clip_range           | 0.4        |
|    entropy_loss         | -97.2      |
|    explained_variance   | -7.15e-07  |
|    learning_rate        | 0.0005     |
|    loss                 | 154        |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.00136   |
|    std                  | 1.75       |
|    value_loss           | 317        |
----------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 34.24 | Max Reward: 47.91
Iteration: 283 | Episodes: 11500 | Median Reward: 32.70 | Max Reward: 47.91
Iteration: 286 | Episodes: 11600 | Median Reward: 32.52 | Max Reward: 47.91
Iteration: 288 | Episodes: 11700 | Median Reward: 34.24 | Max Reward: 47.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.6        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 290          |
|    time_elapsed         | 2486         |
|    total_timesteps      | 1187840      |
| train/                  |              |
|    approx_kl            | 0.0026026126 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.7        |
|    explained_variance   | -5.96e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 172          |
|    n_updates            | 2890         |
|    policy_gradient_loss | 0.000502     |
|    std                  | 1.79         |
|    value_loss           | 354          |
------------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 33.61 | Max Reward: 47.91
Iteration: 293 | Episodes: 11900 | Median Reward: 30.23 | Max Reward: 47.91
Iteration: 295 | Episodes: 12000 | Median Reward: 33.42 | Max Reward: 47.91
Iteration: 298 | Episodes: 12100 | Median Reward: 30.75 | Max Reward: 47.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.2        |
| time/                   |              |
|    fps                  | 478          |
|    iterations           | 300          |
|    time_elapsed         | 2567         |
|    total_timesteps      | 1228800      |
| train/                  |              |
|    approx_kl            | 0.0025107306 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -98.6        |
|    explained_variance   | -5.96e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 146          |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.00103     |
|    std                  | 1.85         |
|    value_loss           | 301          |
------------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 27.34 | Max Reward: 47.91
Iteration: 303 | Episodes: 12300 | Median Reward: 29.99 | Max Reward: 47.91
Iteration: 305 | Episodes: 12400 | Median Reward: 37.67 | Max Reward: 47.91
Iteration: 308 | Episodes: 12500 | Median Reward: 33.11 | Max Reward: 47.91
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.4         |
| time/                   |               |
|    fps                  | 478           |
|    iterations           | 310           |
|    time_elapsed         | 2653          |
|    total_timesteps      | 1269760       |
| train/                  |               |
|    approx_kl            | 0.00028011002 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -99.2         |
|    explained_variance   | -4.77e-07     |
|    learning_rate        | 0.0005        |
|    loss                 | 166           |
|    n_updates            | 3090          |
|    policy_gradient_loss | -0.000418     |
|    std                  | 1.9           |
|    value_loss           | 343           |
-------------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: 32.66 | Max Reward: 47.91
Iteration: 313 | Episodes: 12700 | Median Reward: 38.50 | Max Reward: 47.91
Iteration: 315 | Episodes: 12800 | Median Reward: 30.33 | Max Reward: 47.91
Iteration: 318 | Episodes: 12900 | Median Reward: 33.02 | Max Reward: 47.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.5        |
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 320          |
|    time_elapsed         | 2743         |
|    total_timesteps      | 1310720      |
| train/                  |              |
|    approx_kl            | 8.021614e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -100         |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 114          |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.000152    |
|    std                  | 1.95         |
|    value_loss           | 238          |
------------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: 34.63 | Max Reward: 47.91
Iteration: 323 | Episodes: 13100 | Median Reward: 36.53 | Max Reward: 47.91
Iteration: 325 | Episodes: 13200 | Median Reward: 34.71 | Max Reward: 47.91
Iteration: 327 | Episodes: 13300 | Median Reward: 31.72 | Max Reward: 47.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67          |
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 330          |
|    time_elapsed         | 2819         |
|    total_timesteps      | 1351680      |
| train/                  |              |
|    approx_kl            | 0.0008932452 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -101         |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.0005       |
|    loss                 | 154          |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.000217    |
|    std                  | 2            |
|    value_loss           | 318          |
------------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: 34.47 | Max Reward: 47.91
Iteration: 332 | Episodes: 13500 | Median Reward: 33.88 | Max Reward: 47.91
Iteration: 335 | Episodes: 13600 | Median Reward: 30.52 | Max Reward: 47.91
Iteration: 337 | Episodes: 13700 | Median Reward: 33.51 | Max Reward: 47.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.1        |
| time/                   |              |
|    fps                  | 478          |
|    iterations           | 340          |
|    time_elapsed         | 2911         |
|    total_timesteps      | 1392640      |
| train/                  |              |
|    approx_kl            | 0.0011086538 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -101         |
|    explained_variance   | 3.58e-07     |
|    learning_rate        | 0.0005       |
|    loss                 | 171          |
|    n_updates            | 3390         |
|    policy_gradient_loss | -0.000336    |
|    std                  | 2.08         |
|    value_loss           | 351          |
------------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: 36.36 | Max Reward: 47.91
Iteration: 342 | Episodes: 13900 | Median Reward: 35.12 | Max Reward: 47.91
Iteration: 345 | Episodes: 14000 | Median Reward: 34.53 | Max Reward: 47.91
Iteration: 347 | Episodes: 14100 | Median Reward: 33.75 | Max Reward: 47.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.4        |
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 350          |
|    time_elapsed         | 2991         |
|    total_timesteps      | 1433600      |
| train/                  |              |
|    approx_kl            | 2.330725e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.0273       |
|    learning_rate        | 0.0005       |
|    loss                 | 139          |
|    n_updates            | 3490         |
|    policy_gradient_loss | -0.000188    |
|    std                  | 2.11         |
|    value_loss           | 289          |
------------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: 34.10 | Max Reward: 47.91
Iteration: 352 | Episodes: 14300 | Median Reward: 35.81 | Max Reward: 47.91
Iteration: 355 | Episodes: 14400 | Median Reward: 31.39 | Max Reward: 47.91
Iteration: 357 | Episodes: 14500 | Median Reward: 34.13 | Max Reward: 47.91
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.5         |
| time/                   |               |
|    fps                  | 478           |
|    iterations           | 360           |
|    time_elapsed         | 3078          |
|    total_timesteps      | 1474560       |
| train/                  |               |
|    approx_kl            | 1.9578481e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0.118         |
|    learning_rate        | 0.0005        |
|    loss                 | 121           |
|    n_updates            | 3590          |
|    policy_gradient_loss | -0.000293     |
|    std                  | 2.11          |
|    value_loss           | 253           |
-------------------------------------------
Iteration: 360 | Episodes: 14600 | Median Reward: 30.56 | Max Reward: 47.91
Iteration: 362 | Episodes: 14700 | Median Reward: 38.62 | Max Reward: 47.91
Iteration: 364 | Episodes: 14800 | Median Reward: 38.75 | Max Reward: 47.91
Iteration: 367 | Episodes: 14900 | Median Reward: 32.41 | Max Reward: 47.91
Iteration: 369 | Episodes: 15000 | Median Reward: 39.24 | Max Reward: 47.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.5        |
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 370          |
|    time_elapsed         | 3163         |
|    total_timesteps      | 1515520      |
| train/                  |              |
|    approx_kl            | 7.319526e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.164        |
|    learning_rate        | 0.0005       |
|    loss                 | 144          |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00011     |
|    std                  | 2.11         |
|    value_loss           | 298          |
------------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: 31.11 | Max Reward: 47.91
Iteration: 374 | Episodes: 15200 | Median Reward: 36.41 | Max Reward: 47.91
Iteration: 377 | Episodes: 15300 | Median Reward: 37.59 | Max Reward: 47.91
Iteration: 379 | Episodes: 15400 | Median Reward: 33.84 | Max Reward: 47.91
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67           |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 380           |
|    time_elapsed         | 3242          |
|    total_timesteps      | 1556480       |
| train/                  |               |
|    approx_kl            | 6.4142223e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0.2           |
|    learning_rate        | 0.0005        |
|    loss                 | 134           |
|    n_updates            | 3790          |
|    policy_gradient_loss | -3.33e-05     |
|    std                  | 2.12          |
|    value_loss           | 280           |
-------------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: 38.71 | Max Reward: 47.91
Iteration: 384 | Episodes: 15600 | Median Reward: 38.36 | Max Reward: 47.91
Iteration: 387 | Episodes: 15700 | Median Reward: 36.12 | Max Reward: 47.91
Iteration: 389 | Episodes: 15800 | Median Reward: 33.79 | Max Reward: 47.91
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.3        |
| time/                   |              |
|    fps                  | 478          |
|    iterations           | 390          |
|    time_elapsed         | 3335         |
|    total_timesteps      | 1597440      |
| train/                  |              |
|    approx_kl            | 5.169277e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.0005       |
|    loss                 | 116          |
|    n_updates            | 3890         |
|    policy_gradient_loss | -6.25e-07    |
|    std                  | 2.12         |
|    value_loss           | 243          |
------------------------------------------
Iteration: 392 | Episodes: 15900 | Median Reward: 37.64 | Max Reward: 47.91
Iteration: 394 | Episodes: 16000 | Median Reward: 40.20 | Max Reward: 47.98
Iteration: 396 | Episodes: 16100 | Median Reward: 36.00 | Max Reward: 47.98
Iteration: 399 | Episodes: 16200 | Median Reward: 38.57 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.5         |
| time/                   |               |
|    fps                  | 479           |
|    iterations           | 400           |
|    time_elapsed         | 3414          |
|    total_timesteps      | 1638400       |
| train/                  |               |
|    approx_kl            | 4.4102344e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0.314         |
|    learning_rate        | 0.0005        |
|    loss                 | 115           |
|    n_updates            | 3990          |
|    policy_gradient_loss | -6.46e-05     |
|    std                  | 2.13          |
|    value_loss           | 240           |
-------------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: 34.87 | Max Reward: 47.98
Iteration: 404 | Episodes: 16400 | Median Reward: 33.50 | Max Reward: 47.98
Iteration: 406 | Episodes: 16500 | Median Reward: 37.56 | Max Reward: 47.98
Iteration: 409 | Episodes: 16600 | Median Reward: 36.70 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.1         |
| time/                   |               |
|    fps                  | 479           |
|    iterations           | 410           |
|    time_elapsed         | 3502          |
|    total_timesteps      | 1679360       |
| train/                  |               |
|    approx_kl            | 5.5226265e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0.343         |
|    learning_rate        | 0.0005        |
|    loss                 | 111           |
|    n_updates            | 4090          |
|    policy_gradient_loss | -0.00047      |
|    std                  | 2.13          |
|    value_loss           | 235           |
-------------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: 39.22 | Max Reward: 47.98
Iteration: 414 | Episodes: 16800 | Median Reward: 36.68 | Max Reward: 47.98
Iteration: 416 | Episodes: 16900 | Median Reward: 38.04 | Max Reward: 47.98
Iteration: 419 | Episodes: 17000 | Median Reward: 33.53 | Max Reward: 47.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.7        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 420          |
|    time_elapsed         | 3583         |
|    total_timesteps      | 1720320      |
| train/                  |              |
|    approx_kl            | 7.230454e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.395        |
|    learning_rate        | 0.0005       |
|    loss                 | 110          |
|    n_updates            | 4190         |
|    policy_gradient_loss | -4.4e-05     |
|    std                  | 2.13         |
|    value_loss           | 230          |
------------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: 33.66 | Max Reward: 47.98
Iteration: 424 | Episodes: 17200 | Median Reward: 36.37 | Max Reward: 47.98
Iteration: 426 | Episodes: 17300 | Median Reward: 33.94 | Max Reward: 47.98
Iteration: 429 | Episodes: 17400 | Median Reward: 39.09 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.3         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 430           |
|    time_elapsed         | 3662          |
|    total_timesteps      | 1761280       |
| train/                  |               |
|    approx_kl            | 1.1676311e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0.403         |
|    learning_rate        | 0.0005        |
|    loss                 | 122           |
|    n_updates            | 4290          |
|    policy_gradient_loss | 3.21e-06      |
|    std                  | 2.14          |
|    value_loss           | 254           |
-------------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: 31.61 | Max Reward: 47.98
Iteration: 433 | Episodes: 17600 | Median Reward: 37.30 | Max Reward: 47.98
Iteration: 436 | Episodes: 17700 | Median Reward: 38.70 | Max Reward: 47.98
Iteration: 438 | Episodes: 17800 | Median Reward: 36.02 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.6         |
| time/                   |               |
|    fps                  | 479           |
|    iterations           | 440           |
|    time_elapsed         | 3754          |
|    total_timesteps      | 1802240       |
| train/                  |               |
|    approx_kl            | 3.4576515e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0.433         |
|    learning_rate        | 0.0005        |
|    loss                 | 112           |
|    n_updates            | 4390          |
|    policy_gradient_loss | -3.96e-05     |
|    std                  | 2.14          |
|    value_loss           | 234           |
-------------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 34.22 | Max Reward: 47.98
Iteration: 443 | Episodes: 18000 | Median Reward: 35.00 | Max Reward: 47.98
Iteration: 446 | Episodes: 18100 | Median Reward: 39.97 | Max Reward: 47.98
Iteration: 448 | Episodes: 18200 | Median Reward: 36.36 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.2         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 450           |
|    time_elapsed         | 3833          |
|    total_timesteps      | 1843200       |
| train/                  |               |
|    approx_kl            | 2.3155473e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | 0.478         |
|    learning_rate        | 0.0005        |
|    loss                 | 94.5          |
|    n_updates            | 4490          |
|    policy_gradient_loss | -0.00011      |
|    std                  | 2.15          |
|    value_loss           | 200           |
-------------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 34.97 | Max Reward: 47.98
Iteration: 453 | Episodes: 18400 | Median Reward: 37.76 | Max Reward: 47.98
Iteration: 456 | Episodes: 18500 | Median Reward: 34.39 | Max Reward: 47.98
Iteration: 458 | Episodes: 18600 | Median Reward: 28.92 | Max Reward: 47.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.7        |
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 460          |
|    time_elapsed         | 3925         |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 4.812026e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.53         |
|    learning_rate        | 0.0005       |
|    loss                 | 81.7         |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.000342    |
|    std                  | 2.15         |
|    value_loss           | 174          |
------------------------------------------
Iteration: 461 | Episodes: 18700 | Median Reward: 38.40 | Max Reward: 47.98
Iteration: 463 | Episodes: 18800 | Median Reward: 35.15 | Max Reward: 47.98
Iteration: 466 | Episodes: 18900 | Median Reward: 36.44 | Max Reward: 47.98
Iteration: 468 | Episodes: 19000 | Median Reward: 31.74 | Max Reward: 47.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.8        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 470          |
|    time_elapsed         | 4007         |
|    total_timesteps      | 1925120      |
| train/                  |              |
|    approx_kl            | 1.640353e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.55         |
|    learning_rate        | 0.0005       |
|    loss                 | 82.9         |
|    n_updates            | 4690         |
|    policy_gradient_loss | -0.000229    |
|    std                  | 2.16         |
|    value_loss           | 176          |
------------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 32.38 | Max Reward: 47.98
Iteration: 473 | Episodes: 19200 | Median Reward: 34.71 | Max Reward: 47.98
Iteration: 475 | Episodes: 19300 | Median Reward: 35.55 | Max Reward: 47.98
Iteration: 478 | Episodes: 19400 | Median Reward: 38.43 | Max Reward: 47.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.6        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 480          |
|    time_elapsed         | 4088         |
|    total_timesteps      | 1966080      |
| train/                  |              |
|    approx_kl            | 9.813839e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | 0.57         |
|    learning_rate        | 0.0005       |
|    loss                 | 77.1         |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.000559    |
|    std                  | 2.17         |
|    value_loss           | 165          |
------------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 38.01 | Max Reward: 47.98
Iteration: 483 | Episodes: 19600 | Median Reward: 31.63 | Max Reward: 47.98
Iteration: 485 | Episodes: 19700 | Median Reward: 34.07 | Max Reward: 47.98
Iteration: 488 | Episodes: 19800 | Median Reward: 35.62 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.1         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 490           |
|    time_elapsed         | 4180          |
|    total_timesteps      | 2007040       |
| train/                  |               |
|    approx_kl            | 3.0656607e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | 0.581         |
|    learning_rate        | 0.0005        |
|    loss                 | 79.6          |
|    n_updates            | 4890          |
|    policy_gradient_loss | -0.000207     |
|    std                  | 2.17          |
|    value_loss           | 170           |
-------------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 33.51 | Max Reward: 47.98
Iteration: 493 | Episodes: 20000 | Median Reward: 34.65 | Max Reward: 47.98
Iteration: 495 | Episodes: 20100 | Median Reward: 36.24 | Max Reward: 47.98
Iteration: 498 | Episodes: 20200 | Median Reward: 35.12 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64           |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 500           |
|    time_elapsed         | 4257          |
|    total_timesteps      | 2048000       |
| train/                  |               |
|    approx_kl            | 2.9046103e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | 0.601         |
|    learning_rate        | 0.0005        |
|    loss                 | 77.6          |
|    n_updates            | 4990          |
|    policy_gradient_loss | -0.00013      |
|    std                  | 2.18          |
|    value_loss           | 166           |
-------------------------------------------
Iteration: 500 | Episodes: 20300 | Median Reward: 37.62 | Max Reward: 47.98
Iteration: 503 | Episodes: 20400 | Median Reward: 35.16 | Max Reward: 47.98
Iteration: 505 | Episodes: 20500 | Median Reward: 32.56 | Max Reward: 47.98
Iteration: 507 | Episodes: 20600 | Median Reward: 36.96 | Max Reward: 47.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.6        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 510          |
|    time_elapsed         | 4350         |
|    total_timesteps      | 2088960      |
| train/                  |              |
|    approx_kl            | 7.508187e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -103         |
|    explained_variance   | 0.631        |
|    learning_rate        | 0.0005       |
|    loss                 | 66.4         |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.000512    |
|    std                  | 2.19         |
|    value_loss           | 143          |
------------------------------------------
Iteration: 510 | Episodes: 20700 | Median Reward: 35.33 | Max Reward: 47.98
Iteration: 512 | Episodes: 20800 | Median Reward: 35.39 | Max Reward: 47.98
Iteration: 515 | Episodes: 20900 | Median Reward: 39.34 | Max Reward: 47.98
Iteration: 517 | Episodes: 21000 | Median Reward: 33.57 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.6         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 520           |
|    time_elapsed         | 4430          |
|    total_timesteps      | 2129920       |
| train/                  |               |
|    approx_kl            | 3.2002514e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | 0.607         |
|    learning_rate        | 0.0005        |
|    loss                 | 84.9          |
|    n_updates            | 5190          |
|    policy_gradient_loss | -1.84e-05     |
|    std                  | 2.19          |
|    value_loss           | 181           |
-------------------------------------------
Iteration: 520 | Episodes: 21100 | Median Reward: 39.44 | Max Reward: 47.98
Iteration: 522 | Episodes: 21200 | Median Reward: 38.04 | Max Reward: 47.98
Iteration: 525 | Episodes: 21300 | Median Reward: 33.63 | Max Reward: 47.98
Iteration: 527 | Episodes: 21400 | Median Reward: 39.43 | Max Reward: 47.98
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.2        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 530          |
|    time_elapsed         | 4512         |
|    total_timesteps      | 2170880      |
| train/                  |              |
|    approx_kl            | 4.611448e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -103         |
|    explained_variance   | 0.628        |
|    learning_rate        | 0.0005       |
|    loss                 | 81.3         |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.000181    |
|    std                  | 2.2          |
|    value_loss           | 173          |
------------------------------------------
Iteration: 530 | Episodes: 21500 | Median Reward: 38.13 | Max Reward: 47.98
Iteration: 532 | Episodes: 21600 | Median Reward: 33.83 | Max Reward: 47.98
Iteration: 535 | Episodes: 21700 | Median Reward: 38.67 | Max Reward: 47.98
Iteration: 537 | Episodes: 21800 | Median Reward: 36.11 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.9         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 540           |
|    time_elapsed         | 4604          |
|    total_timesteps      | 2211840       |
| train/                  |               |
|    approx_kl            | 4.2148968e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | 0.66          |
|    learning_rate        | 0.0005        |
|    loss                 | 68.8          |
|    n_updates            | 5390          |
|    policy_gradient_loss | -0.000191     |
|    std                  | 2.21          |
|    value_loss           | 148           |
-------------------------------------------
Iteration: 540 | Episodes: 21900 | Median Reward: 38.25 | Max Reward: 47.98
Iteration: 542 | Episodes: 22000 | Median Reward: 36.94 | Max Reward: 47.98
Iteration: 544 | Episodes: 22100 | Median Reward: 41.54 | Max Reward: 47.98
Iteration: 547 | Episodes: 22200 | Median Reward: 34.16 | Max Reward: 47.98
Iteration: 549 | Episodes: 22300 | Median Reward: 35.18 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.7         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 550           |
|    time_elapsed         | 4683          |
|    total_timesteps      | 2252800       |
| train/                  |               |
|    approx_kl            | 9.5452924e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | 0.699         |
|    learning_rate        | 0.0005        |
|    loss                 | 54.1          |
|    n_updates            | 5490          |
|    policy_gradient_loss | -0.000688     |
|    std                  | 2.22          |
|    value_loss           | 119           |
-------------------------------------------
Iteration: 552 | Episodes: 22400 | Median Reward: 37.30 | Max Reward: 47.98
Iteration: 554 | Episodes: 22500 | Median Reward: 31.54 | Max Reward: 47.98
Iteration: 557 | Episodes: 22600 | Median Reward: 33.13 | Max Reward: 47.98
Iteration: 559 | Episodes: 22700 | Median Reward: 39.50 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.4         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 560           |
|    time_elapsed         | 4776          |
|    total_timesteps      | 2293760       |
| train/                  |               |
|    approx_kl            | 4.0101106e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | 0.681         |
|    learning_rate        | 0.0005        |
|    loss                 | 67.3          |
|    n_updates            | 5590          |
|    policy_gradient_loss | -0.000189     |
|    std                  | 2.23          |
|    value_loss           | 145           |
-------------------------------------------
Iteration: 562 | Episodes: 22800 | Median Reward: 35.93 | Max Reward: 47.98
Iteration: 564 | Episodes: 22900 | Median Reward: 32.91 | Max Reward: 47.98
Iteration: 567 | Episodes: 23000 | Median Reward: 37.28 | Max Reward: 47.98
Iteration: 569 | Episodes: 23100 | Median Reward: 37.48 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.3         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 570           |
|    time_elapsed         | 4856          |
|    total_timesteps      | 2334720       |
| train/                  |               |
|    approx_kl            | 4.7844005e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | 0.73          |
|    learning_rate        | 0.0005        |
|    loss                 | 49.1          |
|    n_updates            | 5690          |
|    policy_gradient_loss | -0.000244     |
|    std                  | 2.25          |
|    value_loss           | 109           |
-------------------------------------------
Iteration: 572 | Episodes: 23200 | Median Reward: 38.55 | Max Reward: 47.98
Iteration: 574 | Episodes: 23300 | Median Reward: 33.54 | Max Reward: 47.98
Iteration: 577 | Episodes: 23400 | Median Reward: 37.41 | Max Reward: 47.98
Iteration: 579 | Episodes: 23500 | Median Reward: 33.51 | Max Reward: 47.98
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.5         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 580           |
|    time_elapsed         | 4935          |
|    total_timesteps      | 2375680       |
| train/                  |               |
|    approx_kl            | 0.00011641451 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | 0.728         |
|    learning_rate        | 0.0005        |
|    loss                 | 52.4          |
|    n_updates            | 5790          |
|    policy_gradient_loss | -0.000618     |
|    std                  | 2.26          |
|    value_loss           | 115           |
-------------------------------------------
Iteration: 581 | Episodes: 23600 | Median Reward: 38.97 | Max Reward: 47.98
Iteration: 584 | Episodes: 23700 | Median Reward: 39.56 | Max Reward: 48.19
Iteration: 586 | Episodes: 23800 | Median Reward: 35.93 | Max Reward: 48.19
Iteration: 589 | Episodes: 23900 | Median Reward: 38.06 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.3         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 590           |
|    time_elapsed         | 5026          |
|    total_timesteps      | 2416640       |
| train/                  |               |
|    approx_kl            | 6.3580985e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -104          |
|    explained_variance   | 0.725         |
|    learning_rate        | 0.0005        |
|    loss                 | 57.2          |
|    n_updates            | 5890          |
|    policy_gradient_loss | -0.000314     |
|    std                  | 2.27          |
|    value_loss           | 125           |
-------------------------------------------
Iteration: 591 | Episodes: 24000 | Median Reward: 33.55 | Max Reward: 48.19
Iteration: 594 | Episodes: 24100 | Median Reward: 35.46 | Max Reward: 48.19
Iteration: 596 | Episodes: 24200 | Median Reward: 31.47 | Max Reward: 48.19
Iteration: 599 | Episodes: 24300 | Median Reward: 36.48 | Max Reward: 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 600          |
|    time_elapsed         | 5106         |
|    total_timesteps      | 2457600      |
| train/                  |              |
|    approx_kl            | 9.606153e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -104         |
|    explained_variance   | 0.742        |
|    learning_rate        | 0.0005       |
|    loss                 | 54.5         |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.000192    |
|    std                  | 2.29         |
|    value_loss           | 120          |
------------------------------------------
Iteration: 601 | Episodes: 24400 | Median Reward: 38.74 | Max Reward: 48.19
Iteration: 604 | Episodes: 24500 | Median Reward: 37.30 | Max Reward: 48.19
Iteration: 606 | Episodes: 24600 | Median Reward: 39.89 | Max Reward: 48.19
Iteration: 609 | Episodes: 24700 | Median Reward: 38.02 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64           |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 610           |
|    time_elapsed         | 5198          |
|    total_timesteps      | 2498560       |
| train/                  |               |
|    approx_kl            | 0.00033959368 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -104          |
|    explained_variance   | 0.776         |
|    learning_rate        | 0.0005        |
|    loss                 | 41.8          |
|    n_updates            | 6090          |
|    policy_gradient_loss | -0.000691     |
|    std                  | 2.3           |
|    value_loss           | 94.3          |
-------------------------------------------
Iteration: 611 | Episodes: 24800 | Median Reward: 36.41 | Max Reward: 48.19
Iteration: 613 | Episodes: 24900 | Median Reward: 34.16 | Max Reward: 48.19
Iteration: 616 | Episodes: 25000 | Median Reward: 34.85 | Max Reward: 48.19
Iteration: 618 | Episodes: 25100 | Median Reward: 37.82 | Max Reward: 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.5        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 620          |
|    time_elapsed         | 5277         |
|    total_timesteps      | 2539520      |
| train/                  |              |
|    approx_kl            | 9.211156e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -104         |
|    explained_variance   | 0.775        |
|    learning_rate        | 0.0005       |
|    loss                 | 44.1         |
|    n_updates            | 6190         |
|    policy_gradient_loss | -0.000201    |
|    std                  | 2.31         |
|    value_loss           | 98.8         |
------------------------------------------
Iteration: 621 | Episodes: 25200 | Median Reward: 35.81 | Max Reward: 48.19
Iteration: 623 | Episodes: 25300 | Median Reward: 38.11 | Max Reward: 48.19
Iteration: 626 | Episodes: 25400 | Median Reward: 35.42 | Max Reward: 48.19
Iteration: 628 | Episodes: 25500 | Median Reward: 38.11 | Max Reward: 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.6        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 630          |
|    time_elapsed         | 5359         |
|    total_timesteps      | 2580480      |
| train/                  |              |
|    approx_kl            | 0.0005352704 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -104         |
|    explained_variance   | 0.781        |
|    learning_rate        | 0.0005       |
|    loss                 | 43.6         |
|    n_updates            | 6290         |
|    policy_gradient_loss | -0.0014      |
|    std                  | 2.33         |
|    value_loss           | 98           |
------------------------------------------
Iteration: 631 | Episodes: 25600 | Median Reward: 38.48 | Max Reward: 48.19
Iteration: 633 | Episodes: 25700 | Median Reward: 37.29 | Max Reward: 48.19
Iteration: 636 | Episodes: 25800 | Median Reward: 36.58 | Max Reward: 48.19
Iteration: 638 | Episodes: 25900 | Median Reward: 37.85 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.3         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 640           |
|    time_elapsed         | 5447          |
|    total_timesteps      | 2621440       |
| train/                  |               |
|    approx_kl            | 0.00039730064 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -104          |
|    explained_variance   | 0.785         |
|    learning_rate        | 0.0005        |
|    loss                 | 44.3          |
|    n_updates            | 6390          |
|    policy_gradient_loss | -0.000569     |
|    std                  | 2.35          |
|    value_loss           | 99.3          |
-------------------------------------------
Iteration: 641 | Episodes: 26000 | Median Reward: 35.94 | Max Reward: 48.19
Iteration: 643 | Episodes: 26100 | Median Reward: 37.18 | Max Reward: 48.19
Iteration: 646 | Episodes: 26200 | Median Reward: 34.46 | Max Reward: 48.19
Iteration: 648 | Episodes: 26300 | Median Reward: 37.99 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.8         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 650           |
|    time_elapsed         | 5525          |
|    total_timesteps      | 2662400       |
| train/                  |               |
|    approx_kl            | 0.00013312581 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -105          |
|    explained_variance   | 0.796         |
|    learning_rate        | 0.0005        |
|    loss                 | 40.8          |
|    n_updates            | 6490          |
|    policy_gradient_loss | -0.000376     |
|    std                  | 2.37          |
|    value_loss           | 92.3          |
-------------------------------------------
Iteration: 650 | Episodes: 26400 | Median Reward: 35.77 | Max Reward: 48.19
Iteration: 653 | Episodes: 26500 | Median Reward: 38.98 | Max Reward: 48.19
Iteration: 655 | Episodes: 26600 | Median Reward: 36.44 | Max Reward: 48.19
Iteration: 658 | Episodes: 26700 | Median Reward: 37.62 | Max Reward: 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.6        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 660          |
|    time_elapsed         | 5616         |
|    total_timesteps      | 2703360      |
| train/                  |              |
|    approx_kl            | 0.0010468932 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -105         |
|    explained_variance   | 0.789        |
|    learning_rate        | 0.0005       |
|    loss                 | 45.7         |
|    n_updates            | 6590         |
|    policy_gradient_loss | -0.00275     |
|    std                  | 2.38         |
|    value_loss           | 102          |
------------------------------------------
Iteration: 660 | Episodes: 26800 | Median Reward: 35.89 | Max Reward: 48.19
Iteration: 663 | Episodes: 26900 | Median Reward: 35.98 | Max Reward: 48.19
Iteration: 665 | Episodes: 27000 | Median Reward: 36.97 | Max Reward: 48.19
Iteration: 668 | Episodes: 27100 | Median Reward: 37.92 | Max Reward: 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.2        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 670          |
|    time_elapsed         | 5697         |
|    total_timesteps      | 2744320      |
| train/                  |              |
|    approx_kl            | 0.0003551861 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -105         |
|    explained_variance   | 0.806        |
|    learning_rate        | 0.0005       |
|    loss                 | 41.6         |
|    n_updates            | 6690         |
|    policy_gradient_loss | -0.000797    |
|    std                  | 2.41         |
|    value_loss           | 94           |
------------------------------------------
Iteration: 670 | Episodes: 27200 | Median Reward: 41.51 | Max Reward: 48.19
Iteration: 673 | Episodes: 27300 | Median Reward: 34.80 | Max Reward: 48.19
Iteration: 675 | Episodes: 27400 | Median Reward: 41.68 | Max Reward: 48.19
Iteration: 678 | Episodes: 27500 | Median Reward: 36.00 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 680           |
|    time_elapsed         | 5783          |
|    total_timesteps      | 2785280       |
| train/                  |               |
|    approx_kl            | 0.00017851623 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -105          |
|    explained_variance   | 0.804         |
|    learning_rate        | 0.0005        |
|    loss                 | 45.3          |
|    n_updates            | 6790          |
|    policy_gradient_loss | -0.000384     |
|    std                  | 2.43          |
|    value_loss           | 101           |
-------------------------------------------
Iteration: 680 | Episodes: 27600 | Median Reward: 37.72 | Max Reward: 48.19
Iteration: 683 | Episodes: 27700 | Median Reward: 40.85 | Max Reward: 48.19
Iteration: 685 | Episodes: 27800 | Median Reward: 38.06 | Max Reward: 48.19
Iteration: 687 | Episodes: 27900 | Median Reward: 36.48 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.4         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 690           |
|    time_elapsed         | 5872          |
|    total_timesteps      | 2826240       |
| train/                  |               |
|    approx_kl            | 0.00021822016 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -105          |
|    explained_variance   | 0.831         |
|    learning_rate        | 0.0005        |
|    loss                 | 34.4          |
|    n_updates            | 6890          |
|    policy_gradient_loss | -0.000546     |
|    std                  | 2.45          |
|    value_loss           | 80            |
-------------------------------------------
Iteration: 690 | Episodes: 28000 | Median Reward: 33.49 | Max Reward: 48.19
Iteration: 692 | Episodes: 28100 | Median Reward: 38.75 | Max Reward: 48.19
Iteration: 695 | Episodes: 28200 | Median Reward: 33.97 | Max Reward: 48.19
Iteration: 697 | Episodes: 28300 | Median Reward: 35.23 | Max Reward: 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 700          |
|    time_elapsed         | 5952         |
|    total_timesteps      | 2867200      |
| train/                  |              |
|    approx_kl            | 0.0010733155 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -106         |
|    explained_variance   | 0.862        |
|    learning_rate        | 0.0005       |
|    loss                 | 26.1         |
|    n_updates            | 6990         |
|    policy_gradient_loss | -0.00292     |
|    std                  | 2.47         |
|    value_loss           | 62.9         |
------------------------------------------
Iteration: 700 | Episodes: 28400 | Median Reward: 35.34 | Max Reward: 48.19
Iteration: 702 | Episodes: 28500 | Median Reward: 37.86 | Max Reward: 48.19
Iteration: 705 | Episodes: 28600 | Median Reward: 35.94 | Max Reward: 48.19
Iteration: 707 | Episodes: 28700 | Median Reward: 37.47 | Max Reward: 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.9        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 710          |
|    time_elapsed         | 6044         |
|    total_timesteps      | 2908160      |
| train/                  |              |
|    approx_kl            | 0.0001553503 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -106         |
|    explained_variance   | 0.864        |
|    learning_rate        | 0.0005       |
|    loss                 | 27.6         |
|    n_updates            | 7090         |
|    policy_gradient_loss | -0.000431    |
|    std                  | 2.49         |
|    value_loss           | 66           |
------------------------------------------
Iteration: 710 | Episodes: 28800 | Median Reward: 37.40 | Max Reward: 48.19
Iteration: 712 | Episodes: 28900 | Median Reward: 38.15 | Max Reward: 48.19
Iteration: 715 | Episodes: 29000 | Median Reward: 37.82 | Max Reward: 48.19
Iteration: 717 | Episodes: 29100 | Median Reward: 32.97 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 720           |
|    time_elapsed         | 6125          |
|    total_timesteps      | 2949120       |
| train/                  |               |
|    approx_kl            | 0.00022231495 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -106          |
|    explained_variance   | 0.844         |
|    learning_rate        | 0.0005        |
|    loss                 | 35.8          |
|    n_updates            | 7190          |
|    policy_gradient_loss | -0.000835     |
|    std                  | 2.51          |
|    value_loss           | 82.5          |
-------------------------------------------
Iteration: 720 | Episodes: 29200 | Median Reward: 41.41 | Max Reward: 48.19
Iteration: 722 | Episodes: 29300 | Median Reward: 38.45 | Max Reward: 48.19
Iteration: 724 | Episodes: 29400 | Median Reward: 34.39 | Max Reward: 48.19
Iteration: 727 | Episodes: 29500 | Median Reward: 34.86 | Max Reward: 48.19
Iteration: 729 | Episodes: 29600 | Median Reward: 37.76 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.4         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 730           |
|    time_elapsed         | 6209          |
|    total_timesteps      | 2990080       |
| train/                  |               |
|    approx_kl            | 0.00018754552 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -106          |
|    explained_variance   | 0.85          |
|    learning_rate        | 0.0005        |
|    loss                 | 36.7          |
|    n_updates            | 7290          |
|    policy_gradient_loss | -0.000522     |
|    std                  | 2.54          |
|    value_loss           | 84.2          |
-------------------------------------------
Iteration: 732 | Episodes: 29700 | Median Reward: 39.93 | Max Reward: 48.19
Iteration: 734 | Episodes: 29800 | Median Reward: 35.67 | Max Reward: 48.19
Iteration: 737 | Episodes: 29900 | Median Reward: 38.04 | Max Reward: 48.19
Iteration: 739 | Episodes: 30000 | Median Reward: 42.08 | Max Reward: 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.9        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 740          |
|    time_elapsed         | 6296         |
|    total_timesteps      | 3031040      |
| train/                  |              |
|    approx_kl            | 0.0022507284 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -107         |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.0005       |
|    loss                 | 26.6         |
|    n_updates            | 7390         |
|    policy_gradient_loss | -0.00292     |
|    std                  | 2.57         |
|    value_loss           | 64.2         |
------------------------------------------
Iteration: 742 | Episodes: 30100 | Median Reward: 37.28 | Max Reward: 48.19
Iteration: 744 | Episodes: 30200 | Median Reward: 37.74 | Max Reward: 48.19
Iteration: 747 | Episodes: 30300 | Median Reward: 32.48 | Max Reward: 48.19
Iteration: 749 | Episodes: 30400 | Median Reward: 38.41 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.8         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 750           |
|    time_elapsed         | 6376          |
|    total_timesteps      | 3072000       |
| train/                  |               |
|    approx_kl            | 0.00021999999 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -107          |
|    explained_variance   | 0.879         |
|    learning_rate        | 0.0005        |
|    loss                 | 27.2          |
|    n_updates            | 7490          |
|    policy_gradient_loss | -0.000504     |
|    std                  | 2.6           |
|    value_loss           | 65.5          |
-------------------------------------------
Iteration: 752 | Episodes: 30500 | Median Reward: 39.29 | Max Reward: 48.19
Iteration: 754 | Episodes: 30600 | Median Reward: 38.86 | Max Reward: 48.19
Iteration: 757 | Episodes: 30700 | Median Reward: 37.03 | Max Reward: 48.19
Iteration: 759 | Episodes: 30800 | Median Reward: 39.64 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.9         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 760           |
|    time_elapsed         | 6469          |
|    total_timesteps      | 3112960       |
| train/                  |               |
|    approx_kl            | 0.00022301075 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -107          |
|    explained_variance   | 0.894         |
|    learning_rate        | 0.0005        |
|    loss                 | 23.2          |
|    n_updates            | 7590          |
|    policy_gradient_loss | -0.000563     |
|    std                  | 2.64          |
|    value_loss           | 57.1          |
-------------------------------------------
Iteration: 761 | Episodes: 30900 | Median Reward: 37.51 | Max Reward: 48.19
Iteration: 764 | Episodes: 31000 | Median Reward: 38.39 | Max Reward: 48.19
Iteration: 766 | Episodes: 31100 | Median Reward: 39.69 | Max Reward: 48.19
Iteration: 769 | Episodes: 31200 | Median Reward: 39.71 | Max Reward: 48.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.5       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 770         |
|    time_elapsed         | 6548        |
|    total_timesteps      | 3153920     |
| train/                  |             |
|    approx_kl            | 0.003083522 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -107        |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0005      |
|    loss                 | 20.7        |
|    n_updates            | 7690        |
|    policy_gradient_loss | -0.00317    |
|    std                  | 2.67        |
|    value_loss           | 52.4        |
-----------------------------------------
Iteration: 771 | Episodes: 31300 | Median Reward: 32.83 | Max Reward: 48.19
Iteration: 774 | Episodes: 31400 | Median Reward: 35.97 | Max Reward: 48.19
Iteration: 776 | Episodes: 31500 | Median Reward: 36.65 | Max Reward: 48.19
Iteration: 779 | Episodes: 31600 | Median Reward: 37.92 | Max Reward: 48.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.8       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 780         |
|    time_elapsed         | 6633        |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.002966273 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -108        |
|    explained_variance   | 0.911       |
|    learning_rate        | 0.0005      |
|    loss                 | 17.3        |
|    n_updates            | 7790        |
|    policy_gradient_loss | -0.00322    |
|    std                  | 2.71        |
|    value_loss           | 45.6        |
-----------------------------------------
Iteration: 781 | Episodes: 31700 | Median Reward: 33.60 | Max Reward: 48.19
Iteration: 784 | Episodes: 31800 | Median Reward: 39.84 | Max Reward: 48.19
Iteration: 786 | Episodes: 31900 | Median Reward: 40.72 | Max Reward: 48.19
Iteration: 789 | Episodes: 32000 | Median Reward: 41.13 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.8         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 790           |
|    time_elapsed         | 6718          |
|    total_timesteps      | 3235840       |
| train/                  |               |
|    approx_kl            | 0.00023794713 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -108          |
|    explained_variance   | 0.908         |
|    learning_rate        | 0.0005        |
|    loss                 | 19.5          |
|    n_updates            | 7890          |
|    policy_gradient_loss | -0.00048      |
|    std                  | 2.74          |
|    value_loss           | 50.1          |
-------------------------------------------
Iteration: 791 | Episodes: 32100 | Median Reward: 35.93 | Max Reward: 48.19
Iteration: 793 | Episodes: 32200 | Median Reward: 35.01 | Max Reward: 48.19
Iteration: 796 | Episodes: 32300 | Median Reward: 37.76 | Max Reward: 48.19
Iteration: 798 | Episodes: 32400 | Median Reward: 37.88 | Max Reward: 48.19
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.9         |
| time/                   |               |
|    fps                  | 482           |
|    iterations           | 800           |
|    time_elapsed         | 6797          |
|    total_timesteps      | 3276800       |
| train/                  |               |
|    approx_kl            | 0.00040618342 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -108          |
|    explained_variance   | 0.91          |
|    learning_rate        | 0.0005        |
|    loss                 | 19.4          |
|    n_updates            | 7990          |
|    policy_gradient_loss | -0.000669     |
|    std                  | 2.77          |
|    value_loss           | 49.8          |
-------------------------------------------
Iteration: 801 | Episodes: 32500 | Median Reward: 41.00 | Max Reward: 48.19
Iteration: 803 | Episodes: 32600 | Median Reward: 38.63 | Max Reward: 48.19
Iteration: 806 | Episodes: 32700 | Median Reward: 37.25 | Max Reward: 48.19
Iteration: 808 | Episodes: 32800 | Median Reward: 40.96 | Max Reward: 48.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.6       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 810         |
|    time_elapsed         | 6890        |
|    total_timesteps      | 3317760     |
| train/                  |             |
|    approx_kl            | 0.001093075 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -109        |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0005      |
|    loss                 | 19.3        |
|    n_updates            | 8090        |
|    policy_gradient_loss | -0.00147    |
|    std                  | 2.82        |
|    value_loss           | 49.7        |
-----------------------------------------
Iteration: 811 | Episodes: 32900 | Median Reward: 37.26 | Max Reward: 48.19
Iteration: 813 | Episodes: 33000 | Median Reward: 38.36 | Max Reward: 48.19
Iteration: 816 | Episodes: 33100 | Median Reward: 38.36 | Max Reward: 48.19
Iteration: 818 | Episodes: 33200 | Median Reward: 40.84 | Max Reward: 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.8        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 820          |
|    time_elapsed         | 6970         |
|    total_timesteps      | 3358720      |
| train/                  |              |
|    approx_kl            | 0.0016568209 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -109         |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.0005       |
|    loss                 | 10.6         |
|    n_updates            | 8190         |
|    policy_gradient_loss | -0.00369     |
|    std                  | 2.86         |
|    value_loss           | 32.5         |
------------------------------------------
Iteration: 821 | Episodes: 33300 | Median Reward: 40.36 | Max Reward: 48.19
Iteration: 823 | Episodes: 33400 | Median Reward: 42.38 | Max Reward: 48.19
Iteration: 826 | Episodes: 33500 | Median Reward: 38.03 | Max Reward: 48.19
Iteration: 828 | Episodes: 33600 | Median Reward: 40.79 | Max Reward: 48.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.4       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 830         |
|    time_elapsed         | 7058        |
|    total_timesteps      | 3399680     |
| train/                  |             |
|    approx_kl            | 0.002261878 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -109        |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.0005      |
|    loss                 | 18.6        |
|    n_updates            | 8290        |
|    policy_gradient_loss | -0.0025     |
|    std                  | 2.9         |
|    value_loss           | 48.2        |
-----------------------------------------
Iteration: 830 | Episodes: 33700 | Median Reward: 41.26 | Max Reward: 48.19
Iteration: 833 | Episodes: 33800 | Median Reward: 37.81 | Max Reward: 48.19
Iteration: 835 | Episodes: 33900 | Median Reward: 36.97 | Max Reward: 48.19
Iteration: 838 | Episodes: 34000 | Median Reward: 38.24 | Max Reward: 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.4        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 840          |
|    time_elapsed         | 7145         |
|    total_timesteps      | 3440640      |
| train/                  |              |
|    approx_kl            | 0.0012859713 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -110         |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.0005       |
|    loss                 | 13.4         |
|    n_updates            | 8390         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 2.95         |
|    value_loss           | 38.1         |
------------------------------------------
Iteration: 840 | Episodes: 34100 | Median Reward: 40.06 | Max Reward: 48.19
Iteration: 843 | Episodes: 34200 | Median Reward: 37.59 | Max Reward: 48.19
Iteration: 845 | Episodes: 34300 | Median Reward: 34.50 | Max Reward: 48.19
Iteration: 848 | Episodes: 34400 | Median Reward: 39.38 | Max Reward: 48.19
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.5       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 850         |
|    time_elapsed         | 7224        |
|    total_timesteps      | 3481600     |
| train/                  |             |
|    approx_kl            | 0.001950545 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -110        |
|    explained_variance   | 0.943       |
|    learning_rate        | 0.0005      |
|    loss                 | 10.5        |
|    n_updates            | 8490        |
|    policy_gradient_loss | -0.0028     |
|    std                  | 3.01        |
|    value_loss           | 32.1        |
-----------------------------------------
Iteration: 850 | Episodes: 34500 | Median Reward: 40.13 | Max Reward: 48.19
Iteration: 853 | Episodes: 34600 | Median Reward: 42.99 | Max Reward: 48.19
Iteration: 855 | Episodes: 34700 | Median Reward: 36.01 | Max Reward: 48.19
Iteration: 858 | Episodes: 34800 | Median Reward: 39.43 | Max Reward: 48.19
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 860          |
|    time_elapsed         | 7317         |
|    total_timesteps      | 3522560      |
| train/                  |              |
|    approx_kl            | 0.0032834867 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0005       |
|    loss                 | 9.89         |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.00296     |
|    std                  | 3.05         |
|    value_loss           | 31           |
------------------------------------------
Iteration: 860 | Episodes: 34900 | Median Reward: 35.24 | Max Reward: 48.19
Iteration: 863 | Episodes: 35000 | Median Reward: 38.68 | Max Reward: 48.19
Iteration: 865 | Episodes: 35100 | Median Reward: 41.57 | Max Reward: 48.19
Iteration: 867 | Episodes: 35200 | Median Reward: 42.73 | Max Reward: 48.22
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.5         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 870           |
|    time_elapsed         | 7397          |
|    total_timesteps      | 3563520       |
| train/                  |               |
|    approx_kl            | 0.00086757273 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -111          |
|    explained_variance   | 0.942         |
|    learning_rate        | 0.0005        |
|    loss                 | 11.5          |
|    n_updates            | 8690          |
|    policy_gradient_loss | -0.00191      |
|    std                  | 3.09          |
|    value_loss           | 34.6          |
-------------------------------------------
Iteration: 870 | Episodes: 35300 | Median Reward: 33.59 | Max Reward: 48.22
Iteration: 872 | Episodes: 35400 | Median Reward: 39.78 | Max Reward: 48.22
Iteration: 875 | Episodes: 35500 | Median Reward: 38.70 | Max Reward: 48.22
Iteration: 877 | Episodes: 35600 | Median Reward: 36.86 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.1        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 880          |
|    time_elapsed         | 7484         |
|    total_timesteps      | 3604480      |
| train/                  |              |
|    approx_kl            | 7.989026e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.937        |
|    learning_rate        | 0.0005       |
|    loss                 | 14.4         |
|    n_updates            | 8790         |
|    policy_gradient_loss | -0.000212    |
|    std                  | 3.13         |
|    value_loss           | 40.3         |
------------------------------------------
Iteration: 880 | Episodes: 35700 | Median Reward: 42.15 | Max Reward: 48.22
Iteration: 882 | Episodes: 35800 | Median Reward: 42.23 | Max Reward: 48.22
Iteration: 885 | Episodes: 35900 | Median Reward: 36.93 | Max Reward: 48.22
Iteration: 887 | Episodes: 36000 | Median Reward: 39.92 | Max Reward: 48.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.1       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 890         |
|    time_elapsed         | 7571        |
|    total_timesteps      | 3645440     |
| train/                  |             |
|    approx_kl            | 0.009145368 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -112        |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0005      |
|    loss                 | 8.36        |
|    n_updates            | 8890        |
|    policy_gradient_loss | -0.016      |
|    std                  | 3.2         |
|    value_loss           | 28.2        |
-----------------------------------------
Iteration: 890 | Episodes: 36100 | Median Reward: 40.82 | Max Reward: 48.22
Iteration: 892 | Episodes: 36200 | Median Reward: 35.33 | Max Reward: 48.22
Iteration: 895 | Episodes: 36300 | Median Reward: 37.51 | Max Reward: 48.22
Iteration: 897 | Episodes: 36400 | Median Reward: 39.75 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.8        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 900          |
|    time_elapsed         | 7651         |
|    total_timesteps      | 3686400      |
| train/                  |              |
|    approx_kl            | 0.0037900736 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.0005       |
|    loss                 | 4.75         |
|    n_updates            | 8990         |
|    policy_gradient_loss | -0.00571     |
|    std                  | 3.24         |
|    value_loss           | 20.6         |
------------------------------------------
Iteration: 900 | Episodes: 36500 | Median Reward: 42.08 | Max Reward: 48.22
Iteration: 902 | Episodes: 36600 | Median Reward: 39.39 | Max Reward: 48.22
Iteration: 904 | Episodes: 36700 | Median Reward: 39.92 | Max Reward: 48.22
Iteration: 907 | Episodes: 36800 | Median Reward: 40.05 | Max Reward: 48.22
Iteration: 909 | Episodes: 36900 | Median Reward: 39.55 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.6        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 910          |
|    time_elapsed         | 7743         |
|    total_timesteps      | 3727360      |
| train/                  |              |
|    approx_kl            | 0.0011687063 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.0005       |
|    loss                 | 6.99         |
|    n_updates            | 9090         |
|    policy_gradient_loss | -0.00331     |
|    std                  | 3.3          |
|    value_loss           | 25.3         |
------------------------------------------
Iteration: 912 | Episodes: 37000 | Median Reward: 39.57 | Max Reward: 48.22
Iteration: 914 | Episodes: 37100 | Median Reward: 41.35 | Max Reward: 48.22
Iteration: 917 | Episodes: 37200 | Median Reward: 40.91 | Max Reward: 48.22
Iteration: 919 | Episodes: 37300 | Median Reward: 39.20 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 920          |
|    time_elapsed         | 7824         |
|    total_timesteps      | 3768320      |
| train/                  |              |
|    approx_kl            | 0.0009622235 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -113         |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0005       |
|    loss                 | 2.61         |
|    n_updates            | 9190         |
|    policy_gradient_loss | -0.00342     |
|    std                  | 3.34         |
|    value_loss           | 16.6         |
------------------------------------------
Iteration: 922 | Episodes: 37400 | Median Reward: 39.54 | Max Reward: 48.22
Iteration: 924 | Episodes: 37500 | Median Reward: 36.52 | Max Reward: 48.22
Iteration: 927 | Episodes: 37600 | Median Reward: 36.29 | Max Reward: 48.22
Iteration: 929 | Episodes: 37700 | Median Reward: 36.82 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.9        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 930          |
|    time_elapsed         | 7910         |
|    total_timesteps      | 3809280      |
| train/                  |              |
|    approx_kl            | 9.104003e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -113         |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0005       |
|    loss                 | 4.08         |
|    n_updates            | 9290         |
|    policy_gradient_loss | 6.56e-05     |
|    std                  | 3.39         |
|    value_loss           | 19.6         |
------------------------------------------
Iteration: 932 | Episodes: 37800 | Median Reward: 39.52 | Max Reward: 48.22
Iteration: 934 | Episodes: 37900 | Median Reward: 38.85 | Max Reward: 48.22
Iteration: 937 | Episodes: 38000 | Median Reward: 35.36 | Max Reward: 48.22
Iteration: 939 | Episodes: 38100 | Median Reward: 39.99 | Max Reward: 48.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.8       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 940         |
|    time_elapsed         | 7996        |
|    total_timesteps      | 3850240     |
| train/                  |             |
|    approx_kl            | 0.004200345 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -113        |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.0005      |
|    loss                 | 5.05        |
|    n_updates            | 9390        |
|    policy_gradient_loss | -0.00228    |
|    std                  | 3.44        |
|    value_loss           | 21.6        |
-----------------------------------------
Iteration: 941 | Episodes: 38200 | Median Reward: 38.79 | Max Reward: 48.22
Iteration: 944 | Episodes: 38300 | Median Reward: 35.25 | Max Reward: 48.22
Iteration: 946 | Episodes: 38400 | Median Reward: 39.31 | Max Reward: 48.22
Iteration: 949 | Episodes: 38500 | Median Reward: 40.69 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.9        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 950          |
|    time_elapsed         | 8075         |
|    total_timesteps      | 3891200      |
| train/                  |              |
|    approx_kl            | 0.0035023573 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -114         |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0005       |
|    loss                 | 4.06         |
|    n_updates            | 9490         |
|    policy_gradient_loss | -0.00518     |
|    std                  | 3.48         |
|    value_loss           | 20.1         |
------------------------------------------
Iteration: 951 | Episodes: 38600 | Median Reward: 36.87 | Max Reward: 48.22
Iteration: 954 | Episodes: 38700 | Median Reward: 40.17 | Max Reward: 48.22
Iteration: 956 | Episodes: 38800 | Median Reward: 39.25 | Max Reward: 48.22
Iteration: 959 | Episodes: 38900 | Median Reward: 40.55 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.9        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 960          |
|    time_elapsed         | 8170         |
|    total_timesteps      | 3932160      |
| train/                  |              |
|    approx_kl            | 0.0022042538 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -114         |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.0005       |
|    loss                 | 3.55         |
|    n_updates            | 9590         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 3.53         |
|    value_loss           | 18.7         |
------------------------------------------
Iteration: 961 | Episodes: 39000 | Median Reward: 39.35 | Max Reward: 48.22
Iteration: 964 | Episodes: 39100 | Median Reward: 42.58 | Max Reward: 48.22
Iteration: 966 | Episodes: 39200 | Median Reward: 42.74 | Max Reward: 48.22
Iteration: 969 | Episodes: 39300 | Median Reward: 41.03 | Max Reward: 48.22
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.4         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 970           |
|    time_elapsed         | 8250          |
|    total_timesteps      | 3973120       |
| train/                  |               |
|    approx_kl            | 0.00072335545 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -114          |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.0005        |
|    loss                 | 2.27          |
|    n_updates            | 9690          |
|    policy_gradient_loss | -0.00204      |
|    std                  | 3.6           |
|    value_loss           | 16.2          |
-------------------------------------------
Iteration: 971 | Episodes: 39400 | Median Reward: 42.41 | Max Reward: 48.22
Iteration: 973 | Episodes: 39500 | Median Reward: 41.75 | Max Reward: 48.22
Iteration: 976 | Episodes: 39600 | Median Reward: 38.93 | Max Reward: 48.22
Iteration: 978 | Episodes: 39700 | Median Reward: 39.65 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.7        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 980          |
|    time_elapsed         | 8336         |
|    total_timesteps      | 4014080      |
| train/                  |              |
|    approx_kl            | 0.0011833734 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0005       |
|    loss                 | 0.672        |
|    n_updates            | 9790         |
|    policy_gradient_loss | -0.00221     |
|    std                  | 3.65         |
|    value_loss           | 13.1         |
------------------------------------------
Iteration: 981 | Episodes: 39800 | Median Reward: 38.87 | Max Reward: 48.22
Iteration: 983 | Episodes: 39900 | Median Reward: 41.62 | Max Reward: 48.22
Iteration: 986 | Episodes: 40000 | Median Reward: 36.50 | Max Reward: 48.22
Iteration: 988 | Episodes: 40100 | Median Reward: 42.10 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.1        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 990          |
|    time_elapsed         | 8423         |
|    total_timesteps      | 4055040      |
| train/                  |              |
|    approx_kl            | 0.0011969772 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0005       |
|    loss                 | 3.81         |
|    n_updates            | 9890         |
|    policy_gradient_loss | -0.00173     |
|    std                  | 3.7          |
|    value_loss           | 19.3         |
------------------------------------------
Iteration: 991 | Episodes: 40200 | Median Reward: 40.55 | Max Reward: 48.22
Iteration: 993 | Episodes: 40300 | Median Reward: 39.43 | Max Reward: 48.22
Iteration: 996 | Episodes: 40400 | Median Reward: 41.84 | Max Reward: 48.22
Iteration: 998 | Episodes: 40500 | Median Reward: 39.62 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1000         |
|    time_elapsed         | 8501         |
|    total_timesteps      | 4096000      |
| train/                  |              |
|    approx_kl            | 0.0012750537 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.22         |
|    n_updates            | 9990         |
|    policy_gradient_loss | -0.00229     |
|    std                  | 3.78         |
|    value_loss           | 14.4         |
------------------------------------------
Iteration: 1001 | Episodes: 40600 | Median Reward: 43.79 | Max Reward: 48.22
Iteration: 1003 | Episodes: 40700 | Median Reward: 41.55 | Max Reward: 48.22
Iteration: 1006 | Episodes: 40800 | Median Reward: 40.06 | Max Reward: 48.22
Iteration: 1008 | Episodes: 40900 | Median Reward: 38.34 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62          |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1010         |
|    time_elapsed         | 8593         |
|    total_timesteps      | 4136960      |
| train/                  |              |
|    approx_kl            | 0.0003075357 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0005       |
|    loss                 | -0.479       |
|    n_updates            | 10090        |
|    policy_gradient_loss | -0.00114     |
|    std                  | 3.84         |
|    value_loss           | 10.3         |
------------------------------------------
Iteration: 1010 | Episodes: 41000 | Median Reward: 40.86 | Max Reward: 48.22
Iteration: 1013 | Episodes: 41100 | Median Reward: 39.15 | Max Reward: 48.22
Iteration: 1015 | Episodes: 41200 | Median Reward: 41.24 | Max Reward: 48.22
Iteration: 1018 | Episodes: 41300 | Median Reward: 39.41 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.7        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1020         |
|    time_elapsed         | 8673         |
|    total_timesteps      | 4177920      |
| train/                  |              |
|    approx_kl            | 0.0045290906 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.07         |
|    n_updates            | 10190        |
|    policy_gradient_loss | -0.00668     |
|    std                  | 3.89         |
|    value_loss           | 13.8         |
------------------------------------------
Iteration: 1020 | Episodes: 41400 | Median Reward: 42.64 | Max Reward: 48.22
Iteration: 1023 | Episodes: 41500 | Median Reward: 41.66 | Max Reward: 48.22
Iteration: 1025 | Episodes: 41600 | Median Reward: 42.88 | Max Reward: 48.22
Iteration: 1028 | Episodes: 41700 | Median Reward: 43.47 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.2        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1030         |
|    time_elapsed         | 8761         |
|    total_timesteps      | 4218880      |
| train/                  |              |
|    approx_kl            | 7.582527e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -117         |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.87         |
|    n_updates            | 10290        |
|    policy_gradient_loss | 6e-05        |
|    std                  | 3.95         |
|    value_loss           | 15.6         |
------------------------------------------
Iteration: 1030 | Episodes: 41800 | Median Reward: 42.98 | Max Reward: 48.22
Iteration: 1033 | Episodes: 41900 | Median Reward: 38.58 | Max Reward: 48.22
Iteration: 1035 | Episodes: 42000 | Median Reward: 37.57 | Max Reward: 48.22
Iteration: 1038 | Episodes: 42100 | Median Reward: 38.75 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.2        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1040         |
|    time_elapsed         | 8847         |
|    total_timesteps      | 4259840      |
| train/                  |              |
|    approx_kl            | 0.0027322872 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -117         |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0005       |
|    loss                 | 0.0586       |
|    n_updates            | 10390        |
|    policy_gradient_loss | -0.00349     |
|    std                  | 4            |
|    value_loss           | 12           |
------------------------------------------
Iteration: 1040 | Episodes: 42200 | Median Reward: 41.77 | Max Reward: 48.22
Iteration: 1043 | Episodes: 42300 | Median Reward: 42.13 | Max Reward: 48.22
Iteration: 1045 | Episodes: 42400 | Median Reward: 36.69 | Max Reward: 48.22
Iteration: 1047 | Episodes: 42500 | Median Reward: 39.05 | Max Reward: 48.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.5       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 1050        |
|    time_elapsed         | 8928        |
|    total_timesteps      | 4300800     |
| train/                  |             |
|    approx_kl            | 0.001265648 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -117        |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.46       |
|    n_updates            | 10490       |
|    policy_gradient_loss | -0.0033     |
|    std                  | 4.05        |
|    value_loss           | 8.63        |
-----------------------------------------
Iteration: 1050 | Episodes: 42600 | Median Reward: 42.48 | Max Reward: 48.22
Iteration: 1052 | Episodes: 42700 | Median Reward: 39.74 | Max Reward: 48.22
Iteration: 1055 | Episodes: 42800 | Median Reward: 41.23 | Max Reward: 48.22
Iteration: 1057 | Episodes: 42900 | Median Reward: 39.39 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.9        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1060         |
|    time_elapsed         | 9021         |
|    total_timesteps      | 4341760      |
| train/                  |              |
|    approx_kl            | 0.0027394332 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -118         |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0005       |
|    loss                 | -1.71        |
|    n_updates            | 10590        |
|    policy_gradient_loss | -0.00464     |
|    std                  | 4.1          |
|    value_loss           | 8.81         |
------------------------------------------
Iteration: 1060 | Episodes: 43000 | Median Reward: 40.05 | Max Reward: 48.22
Iteration: 1062 | Episodes: 43100 | Median Reward: 42.21 | Max Reward: 48.22
Iteration: 1065 | Episodes: 43200 | Median Reward: 42.89 | Max Reward: 48.22
Iteration: 1067 | Episodes: 43300 | Median Reward: 41.66 | Max Reward: 48.22
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.7         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1070          |
|    time_elapsed         | 9102          |
|    total_timesteps      | 4382720       |
| train/                  |               |
|    approx_kl            | 0.00067091896 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -118          |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.0005        |
|    loss                 | -0.59         |
|    n_updates            | 10690         |
|    policy_gradient_loss | -0.00223      |
|    std                  | 4.16          |
|    value_loss           | 10            |
-------------------------------------------
Iteration: 1070 | Episodes: 43400 | Median Reward: 42.15 | Max Reward: 48.22
Iteration: 1072 | Episodes: 43500 | Median Reward: 41.88 | Max Reward: 48.22
Iteration: 1075 | Episodes: 43600 | Median Reward: 39.35 | Max Reward: 48.22
Iteration: 1077 | Episodes: 43700 | Median Reward: 39.54 | Max Reward: 48.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.2       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 1080        |
|    time_elapsed         | 9187        |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.001051793 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -118        |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.36       |
|    n_updates            | 10790       |
|    policy_gradient_loss | -0.00225    |
|    std                  | 4.2         |
|    value_loss           | 11.3        |
-----------------------------------------
Iteration: 1080 | Episodes: 43800 | Median Reward: 43.19 | Max Reward: 48.22
Iteration: 1082 | Episodes: 43900 | Median Reward: 38.07 | Max Reward: 48.22
Iteration: 1084 | Episodes: 44000 | Median Reward: 38.56 | Max Reward: 48.22
Iteration: 1087 | Episodes: 44100 | Median Reward: 38.82 | Max Reward: 48.22
Iteration: 1089 | Episodes: 44200 | Median Reward: 38.82 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.6        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1090         |
|    time_elapsed         | 9274         |
|    total_timesteps      | 4464640      |
| train/                  |              |
|    approx_kl            | 0.0008023493 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -118         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0005       |
|    loss                 | -2.82        |
|    n_updates            | 10890        |
|    policy_gradient_loss | -0.00313     |
|    std                  | 4.27         |
|    value_loss           | 6.74         |
------------------------------------------
Iteration: 1092 | Episodes: 44300 | Median Reward: 40.44 | Max Reward: 48.22
Iteration: 1094 | Episodes: 44400 | Median Reward: 42.94 | Max Reward: 48.22
Iteration: 1097 | Episodes: 44500 | Median Reward: 42.55 | Max Reward: 48.22
Iteration: 1099 | Episodes: 44600 | Median Reward: 40.07 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.4        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1100         |
|    time_elapsed         | 9355         |
|    total_timesteps      | 4505600      |
| train/                  |              |
|    approx_kl            | 0.0015670699 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -119         |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0005       |
|    loss                 | -2.2         |
|    n_updates            | 10990        |
|    policy_gradient_loss | -0.00107     |
|    std                  | 4.33         |
|    value_loss           | 7.54         |
------------------------------------------
Iteration: 1102 | Episodes: 44700 | Median Reward: 42.89 | Max Reward: 48.22
Iteration: 1104 | Episodes: 44800 | Median Reward: 38.75 | Max Reward: 48.22
Iteration: 1107 | Episodes: 44900 | Median Reward: 38.99 | Max Reward: 48.22
Iteration: 1109 | Episodes: 45000 | Median Reward: 43.52 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1110         |
|    time_elapsed         | 9447         |
|    total_timesteps      | 4546560      |
| train/                  |              |
|    approx_kl            | 0.0013600874 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -119         |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0005       |
|    loss                 | -1.14        |
|    n_updates            | 11090        |
|    policy_gradient_loss | -0.00262     |
|    std                  | 4.38         |
|    value_loss           | 9.03         |
------------------------------------------
Iteration: 1112 | Episodes: 45100 | Median Reward: 39.29 | Max Reward: 48.22
Iteration: 1114 | Episodes: 45200 | Median Reward: 39.57 | Max Reward: 48.22
Iteration: 1117 | Episodes: 45300 | Median Reward: 43.10 | Max Reward: 48.22
Iteration: 1119 | Episodes: 45400 | Median Reward: 41.94 | Max Reward: 48.22
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -58.6      |
| time/                   |            |
|    fps                  | 481        |
|    iterations           | 1120       |
|    time_elapsed         | 9527       |
|    total_timesteps      | 4587520    |
| train/                  |            |
|    approx_kl            | 0.00501723 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -119       |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0005     |
|    loss                 | -0.491     |
|    n_updates            | 11190      |
|    policy_gradient_loss | -0.00608   |
|    std                  | 4.44       |
|    value_loss           | 10.5       |
----------------------------------------
Iteration: 1121 | Episodes: 45500 | Median Reward: 39.80 | Max Reward: 48.22
Iteration: 1124 | Episodes: 45600 | Median Reward: 42.17 | Max Reward: 48.22
Iteration: 1126 | Episodes: 45700 | Median Reward: 40.05 | Max Reward: 48.22
Iteration: 1129 | Episodes: 45800 | Median Reward: 41.70 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.1        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1130         |
|    time_elapsed         | 9612         |
|    total_timesteps      | 4628480      |
| train/                  |              |
|    approx_kl            | 0.0021684219 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -119         |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0005       |
|    loss                 | -3.38        |
|    n_updates            | 11290        |
|    policy_gradient_loss | -0.00536     |
|    std                  | 4.46         |
|    value_loss           | 5.5          |
------------------------------------------
Iteration: 1131 | Episodes: 45900 | Median Reward: 41.70 | Max Reward: 48.22
Iteration: 1134 | Episodes: 46000 | Median Reward: 43.73 | Max Reward: 48.22
Iteration: 1136 | Episodes: 46100 | Median Reward: 41.54 | Max Reward: 48.22
Iteration: 1139 | Episodes: 46200 | Median Reward: 41.74 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.6        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1140         |
|    time_elapsed         | 9699         |
|    total_timesteps      | 4669440      |
| train/                  |              |
|    approx_kl            | 0.0021066885 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0005       |
|    loss                 | -3.06        |
|    n_updates            | 11390        |
|    policy_gradient_loss | -0.00563     |
|    std                  | 4.5          |
|    value_loss           | 5.53         |
------------------------------------------
Iteration: 1141 | Episodes: 46300 | Median Reward: 43.22 | Max Reward: 48.22
Iteration: 1144 | Episodes: 46400 | Median Reward: 39.91 | Max Reward: 48.22
Iteration: 1146 | Episodes: 46500 | Median Reward: 37.40 | Max Reward: 48.22
Iteration: 1149 | Episodes: 46600 | Median Reward: 39.42 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1150         |
|    time_elapsed         | 9780         |
|    total_timesteps      | 4710400      |
| train/                  |              |
|    approx_kl            | 0.0004101544 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.1         |
|    n_updates            | 11490        |
|    policy_gradient_loss | -0.000106    |
|    std                  | 4.56         |
|    value_loss           | 4.07         |
------------------------------------------
Iteration: 1151 | Episodes: 46700 | Median Reward: 41.00 | Max Reward: 48.22
Iteration: 1154 | Episodes: 46800 | Median Reward: 41.67 | Max Reward: 48.22
Iteration: 1156 | Episodes: 46900 | Median Reward: 41.34 | Max Reward: 48.22
Iteration: 1158 | Episodes: 47000 | Median Reward: 38.60 | Max Reward: 48.22
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1160          |
|    time_elapsed         | 9873          |
|    total_timesteps      | 4751360       |
| train/                  |               |
|    approx_kl            | 0.00018799177 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -120          |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.14         |
|    n_updates            | 11590         |
|    policy_gradient_loss | -0.0015       |
|    std                  | 4.62          |
|    value_loss           | 3.92          |
-------------------------------------------
Iteration: 1161 | Episodes: 47100 | Median Reward: 43.45 | Max Reward: 48.22
Iteration: 1163 | Episodes: 47200 | Median Reward: 40.56 | Max Reward: 48.22
Iteration: 1166 | Episodes: 47300 | Median Reward: 40.94 | Max Reward: 48.22
Iteration: 1168 | Episodes: 47400 | Median Reward: 40.68 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1170         |
|    time_elapsed         | 9953         |
|    total_timesteps      | 4792320      |
| train/                  |              |
|    approx_kl            | 0.0009464839 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.85        |
|    n_updates            | 11690        |
|    policy_gradient_loss | -0.00239     |
|    std                  | 4.64         |
|    value_loss           | 2.51         |
------------------------------------------
Iteration: 1171 | Episodes: 47500 | Median Reward: 40.22 | Max Reward: 48.22
Iteration: 1173 | Episodes: 47600 | Median Reward: 40.67 | Max Reward: 48.22
Iteration: 1176 | Episodes: 47700 | Median Reward: 43.38 | Max Reward: 48.22
Iteration: 1178 | Episodes: 47800 | Median Reward: 41.26 | Max Reward: 48.22
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.5         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1180          |
|    time_elapsed         | 10039         |
|    total_timesteps      | 4833280       |
| train/                  |               |
|    approx_kl            | 0.00039537714 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -121          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -3.94         |
|    n_updates            | 11790         |
|    policy_gradient_loss | -0.000609     |
|    std                  | 4.71          |
|    value_loss           | 3.87          |
-------------------------------------------
Iteration: 1181 | Episodes: 47900 | Median Reward: 42.27 | Max Reward: 48.22
Iteration: 1183 | Episodes: 48000 | Median Reward: 39.87 | Max Reward: 48.22
Iteration: 1186 | Episodes: 48100 | Median Reward: 37.91 | Max Reward: 48.22
Iteration: 1188 | Episodes: 48200 | Median Reward: 42.13 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.9        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1190         |
|    time_elapsed         | 10125        |
|    total_timesteps      | 4874240      |
| train/                  |              |
|    approx_kl            | 0.0012040851 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -121         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.16        |
|    n_updates            | 11890        |
|    policy_gradient_loss | -0.00245     |
|    std                  | 4.76         |
|    value_loss           | 3.88         |
------------------------------------------
Iteration: 1190 | Episodes: 48300 | Median Reward: 41.82 | Max Reward: 48.22
Iteration: 1193 | Episodes: 48400 | Median Reward: 42.05 | Max Reward: 48.22
Iteration: 1195 | Episodes: 48500 | Median Reward: 43.86 | Max Reward: 48.22
Iteration: 1198 | Episodes: 48600 | Median Reward: 42.39 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.2        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1200         |
|    time_elapsed         | 10206        |
|    total_timesteps      | 4915200      |
| train/                  |              |
|    approx_kl            | 0.0014017716 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -121         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -3.39        |
|    n_updates            | 11990        |
|    policy_gradient_loss | -0.00252     |
|    std                  | 4.8          |
|    value_loss           | 4.2          |
------------------------------------------
Iteration: 1200 | Episodes: 48700 | Median Reward: 39.50 | Max Reward: 48.22
Iteration: 1203 | Episodes: 48800 | Median Reward: 40.18 | Max Reward: 48.22
Iteration: 1205 | Episodes: 48900 | Median Reward: 39.83 | Max Reward: 48.22
Iteration: 1208 | Episodes: 49000 | Median Reward: 42.61 | Max Reward: 48.22
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.7        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1210         |
|    time_elapsed         | 10298        |
|    total_timesteps      | 4956160      |
| train/                  |              |
|    approx_kl            | 0.0030609292 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -121         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.09        |
|    n_updates            | 12090        |
|    policy_gradient_loss | -0.00239     |
|    std                  | 4.85         |
|    value_loss           | 4.16         |
------------------------------------------
Iteration: 1210 | Episodes: 49100 | Median Reward: 38.14 | Max Reward: 48.22
Iteration: 1213 | Episodes: 49200 | Median Reward: 39.55 | Max Reward: 48.22
Iteration: 1215 | Episodes: 49300 | Median Reward: 40.15 | Max Reward: 48.22
Iteration: 1218 | Episodes: 49400 | Median Reward: 44.10 | Max Reward: 48.22
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -58.1      |
| time/                   |            |
|    fps                  | 481        |
|    iterations           | 1220       |
|    time_elapsed         | 10378      |
|    total_timesteps      | 4997120    |
| train/                  |            |
|    approx_kl            | 0.00101318 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -122       |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0005     |
|    loss                 | -4.3       |
|    n_updates            | 12190      |
|    policy_gradient_loss | -0.00317   |
|    std                  | 4.92       |
|    value_loss           | 3.39       |
----------------------------------------
Iteration: 1220 | Episodes: 49500 | Median Reward: 42.26 | Max Reward: 48.22
Iteration: 1223 | Episodes: 49600 | Median Reward: 41.56 | Max Reward: 48.22
Iteration: 1225 | Episodes: 49700 | Median Reward: 39.12 | Max Reward: 48.22
Iteration: 1227 | Episodes: 49800 | Median Reward: 43.62 | Max Reward: 48.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.4       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 1230        |
|    time_elapsed         | 10462       |
|    total_timesteps      | 5038080     |
| train/                  |             |
|    approx_kl            | 0.004466637 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -122        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.09       |
|    n_updates            | 12290       |
|    policy_gradient_loss | -0.0109     |
|    std                  | 4.95        |
|    value_loss           | 2.26        |
-----------------------------------------
Iteration: 1230 | Episodes: 49900 | Median Reward: 42.01 | Max Reward: 48.58
Iteration: 1232 | Episodes: 50000 | Median Reward: 44.77 | Max Reward: 48.58
Iteration: 1235 | Episodes: 50100 | Median Reward: 39.72 | Max Reward: 48.58
Iteration: 1237 | Episodes: 50200 | Median Reward: 42.32 | Max Reward: 48.58
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -61.3     |
| time/                   |           |
|    fps                  | 481       |
|    iterations           | 1240      |
|    time_elapsed         | 10547     |
|    total_timesteps      | 5079040   |
| train/                  |           |
|    approx_kl            | 0.0004738 |
|    clip_fraction        | 0         |
|    clip_range           | 0.4       |
|    entropy_loss         | -122      |
|    explained_variance   | 0.997     |
|    learning_rate        | 0.0005    |
|    loss                 | -4.49     |
|    n_updates            | 12390     |
|    policy_gradient_loss | -0.00158  |
|    std                  | 5.03      |
|    value_loss           | 2.25      |
---------------------------------------
Iteration: 1240 | Episodes: 50300 | Median Reward: 38.99 | Max Reward: 48.58
Iteration: 1242 | Episodes: 50400 | Median Reward: 42.64 | Max Reward: 48.58
Iteration: 1245 | Episodes: 50500 | Median Reward: 41.16 | Max Reward: 48.58
Iteration: 1247 | Episodes: 50600 | Median Reward: 42.55 | Max Reward: 48.58
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -58.2      |
| time/                   |            |
|    fps                  | 481        |
|    iterations           | 1250       |
|    time_elapsed         | 10626      |
|    total_timesteps      | 5120000    |
| train/                  |            |
|    approx_kl            | 0.01232281 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -122       |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0005     |
|    loss                 | -4.98      |
|    n_updates            | 12490      |
|    policy_gradient_loss | -0.0152    |
|    std                  | 5.08       |
|    value_loss           | 2.31       |
----------------------------------------
Iteration: 1250 | Episodes: 50700 | Median Reward: 43.29 | Max Reward: 48.58
Iteration: 1252 | Episodes: 50800 | Median Reward: 42.15 | Max Reward: 48.58
Iteration: 1255 | Episodes: 50900 | Median Reward: 42.94 | Max Reward: 48.58
Iteration: 1257 | Episodes: 51000 | Median Reward: 42.75 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1260          |
|    time_elapsed         | 10718         |
|    total_timesteps      | 5160960       |
| train/                  |               |
|    approx_kl            | 0.00037707118 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.02         |
|    n_updates            | 12590         |
|    policy_gradient_loss | -0.00138      |
|    std                  | 5.13          |
|    value_loss           | 2.68          |
-------------------------------------------
Iteration: 1260 | Episodes: 51100 | Median Reward: 41.71 | Max Reward: 48.58
Iteration: 1262 | Episodes: 51200 | Median Reward: 38.84 | Max Reward: 48.58
Iteration: 1264 | Episodes: 51300 | Median Reward: 39.36 | Max Reward: 48.58
Iteration: 1267 | Episodes: 51400 | Median Reward: 42.86 | Max Reward: 48.58
Iteration: 1269 | Episodes: 51500 | Median Reward: 43.32 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.5         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1270          |
|    time_elapsed         | 10798         |
|    total_timesteps      | 5201920       |
| train/                  |               |
|    approx_kl            | 0.00058719364 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.7          |
|    n_updates            | 12690         |
|    policy_gradient_loss | -0.00116      |
|    std                  | 5.17          |
|    value_loss           | 2.98          |
-------------------------------------------
Iteration: 1272 | Episodes: 51600 | Median Reward: 42.99 | Max Reward: 48.58
Iteration: 1274 | Episodes: 51700 | Median Reward: 42.81 | Max Reward: 48.58
Iteration: 1277 | Episodes: 51800 | Median Reward: 39.94 | Max Reward: 48.58
Iteration: 1279 | Episodes: 51900 | Median Reward: 42.10 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.8         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1280          |
|    time_elapsed         | 10887         |
|    total_timesteps      | 5242880       |
| train/                  |               |
|    approx_kl            | 0.00032533644 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.12         |
|    n_updates            | 12790         |
|    policy_gradient_loss | -0.00152      |
|    std                  | 5.2           |
|    value_loss           | 2.34          |
-------------------------------------------
Iteration: 1282 | Episodes: 52000 | Median Reward: 42.55 | Max Reward: 48.58
Iteration: 1284 | Episodes: 52100 | Median Reward: 41.17 | Max Reward: 48.58
Iteration: 1287 | Episodes: 52200 | Median Reward: 43.04 | Max Reward: 48.58
Iteration: 1289 | Episodes: 52300 | Median Reward: 42.32 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.1         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1290          |
|    time_elapsed         | 10973         |
|    total_timesteps      | 5283840       |
| train/                  |               |
|    approx_kl            | 0.00085187494 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.01         |
|    n_updates            | 12890         |
|    policy_gradient_loss | -0.00285      |
|    std                  | 5.26          |
|    value_loss           | 2.13          |
-------------------------------------------
Iteration: 1292 | Episodes: 52400 | Median Reward: 40.41 | Max Reward: 48.58
Iteration: 1294 | Episodes: 52500 | Median Reward: 42.55 | Max Reward: 48.58
Iteration: 1297 | Episodes: 52600 | Median Reward: 42.83 | Max Reward: 48.58
Iteration: 1299 | Episodes: 52700 | Median Reward: 44.53 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1300         |
|    time_elapsed         | 11056        |
|    total_timesteps      | 5324800      |
| train/                  |              |
|    approx_kl            | 0.0016186447 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -123         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.83        |
|    n_updates            | 12990        |
|    policy_gradient_loss | -0.00566     |
|    std                  | 5.31         |
|    value_loss           | 2.39         |
------------------------------------------
Iteration: 1301 | Episodes: 52800 | Median Reward: 41.15 | Max Reward: 48.58
Iteration: 1304 | Episodes: 52900 | Median Reward: 42.86 | Max Reward: 48.58
Iteration: 1306 | Episodes: 53000 | Median Reward: 42.77 | Max Reward: 48.58
Iteration: 1309 | Episodes: 53100 | Median Reward: 44.03 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.5         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1310          |
|    time_elapsed         | 11148         |
|    total_timesteps      | 5365760       |
| train/                  |               |
|    approx_kl            | 5.0230577e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -124          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.86         |
|    n_updates            | 13090         |
|    policy_gradient_loss | -0.000303     |
|    std                  | 5.35          |
|    value_loss           | 2.82          |
-------------------------------------------
Iteration: 1311 | Episodes: 53200 | Median Reward: 39.96 | Max Reward: 48.58
Iteration: 1314 | Episodes: 53300 | Median Reward: 38.85 | Max Reward: 48.58
Iteration: 1316 | Episodes: 53400 | Median Reward: 42.45 | Max Reward: 48.58
Iteration: 1319 | Episodes: 53500 | Median Reward: 41.56 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.1        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1320         |
|    time_elapsed         | 11227        |
|    total_timesteps      | 5406720      |
| train/                  |              |
|    approx_kl            | 0.0005785113 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -124         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.71        |
|    n_updates            | 13190        |
|    policy_gradient_loss | -0.00104     |
|    std                  | 5.36         |
|    value_loss           | 1.18         |
------------------------------------------
Iteration: 1321 | Episodes: 53600 | Median Reward: 43.36 | Max Reward: 48.58
Iteration: 1324 | Episodes: 53700 | Median Reward: 40.34 | Max Reward: 48.58
Iteration: 1326 | Episodes: 53800 | Median Reward: 40.30 | Max Reward: 48.58
Iteration: 1329 | Episodes: 53900 | Median Reward: 41.86 | Max Reward: 48.58
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -60.4      |
| time/                   |            |
|    fps                  | 481        |
|    iterations           | 1330       |
|    time_elapsed         | 11312      |
|    total_timesteps      | 5447680    |
| train/                  |            |
|    approx_kl            | 0.02054149 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -124       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0005     |
|    loss                 | -5.67      |
|    n_updates            | 13290      |
|    policy_gradient_loss | -0.0334    |
|    std                  | 5.4        |
|    value_loss           | 1.26       |
----------------------------------------
Iteration: 1331 | Episodes: 54000 | Median Reward: 42.31 | Max Reward: 48.58
Iteration: 1334 | Episodes: 54100 | Median Reward: 42.70 | Max Reward: 48.58
Iteration: 1336 | Episodes: 54200 | Median Reward: 41.19 | Max Reward: 48.58
Iteration: 1338 | Episodes: 54300 | Median Reward: 40.75 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.7         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1340          |
|    time_elapsed         | 11399         |
|    total_timesteps      | 5488640       |
| train/                  |               |
|    approx_kl            | 0.00029481555 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -124          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.09         |
|    n_updates            | 13390         |
|    policy_gradient_loss | -0.00182      |
|    std                  | 5.43          |
|    value_loss           | 2.18          |
-------------------------------------------
Iteration: 1341 | Episodes: 54400 | Median Reward: 43.78 | Max Reward: 48.58
Iteration: 1343 | Episodes: 54500 | Median Reward: 38.76 | Max Reward: 48.58
Iteration: 1346 | Episodes: 54600 | Median Reward: 40.43 | Max Reward: 48.58
Iteration: 1348 | Episodes: 54700 | Median Reward: 44.22 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.6         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1350          |
|    time_elapsed         | 11478         |
|    total_timesteps      | 5529600       |
| train/                  |               |
|    approx_kl            | 0.00023718816 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -124          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -4.97         |
|    n_updates            | 13490         |
|    policy_gradient_loss | -0.00112      |
|    std                  | 5.46          |
|    value_loss           | 2.5           |
-------------------------------------------
Iteration: 1351 | Episodes: 54800 | Median Reward: 42.18 | Max Reward: 48.58
Iteration: 1353 | Episodes: 54900 | Median Reward: 42.25 | Max Reward: 48.58
Iteration: 1356 | Episodes: 55000 | Median Reward: 42.81 | Max Reward: 48.58
Iteration: 1358 | Episodes: 55100 | Median Reward: 42.67 | Max Reward: 48.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.7       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 1360        |
|    time_elapsed         | 11570       |
|    total_timesteps      | 5570560     |
| train/                  |             |
|    approx_kl            | 0.009680026 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -124        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.56       |
|    n_updates            | 13590       |
|    policy_gradient_loss | -0.0145     |
|    std                  | 5.48        |
|    value_loss           | 1.23        |
-----------------------------------------
Iteration: 1361 | Episodes: 55200 | Median Reward: 43.18 | Max Reward: 48.58
Iteration: 1363 | Episodes: 55300 | Median Reward: 43.14 | Max Reward: 48.58
Iteration: 1366 | Episodes: 55400 | Median Reward: 38.59 | Max Reward: 48.58
Iteration: 1368 | Episodes: 55500 | Median Reward: 42.66 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.7        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1370         |
|    time_elapsed         | 11649        |
|    total_timesteps      | 5611520      |
| train/                  |              |
|    approx_kl            | 0.0015392792 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -124         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.79        |
|    n_updates            | 13690        |
|    policy_gradient_loss | -0.00546     |
|    std                  | 5.53         |
|    value_loss           | 1.04         |
------------------------------------------
Iteration: 1370 | Episodes: 55600 | Median Reward: 39.79 | Max Reward: 48.58
Iteration: 1373 | Episodes: 55700 | Median Reward: 39.54 | Max Reward: 48.58
Iteration: 1375 | Episodes: 55800 | Median Reward: 39.87 | Max Reward: 48.58
Iteration: 1378 | Episodes: 55900 | Median Reward: 40.16 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1380          |
|    time_elapsed         | 11737         |
|    total_timesteps      | 5652480       |
| train/                  |               |
|    approx_kl            | 0.00040968446 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -125          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.47         |
|    n_updates            | 13790         |
|    policy_gradient_loss | -0.0032       |
|    std                  | 5.57          |
|    value_loss           | 1.69          |
-------------------------------------------
Iteration: 1380 | Episodes: 56000 | Median Reward: 42.83 | Max Reward: 48.58
Iteration: 1383 | Episodes: 56100 | Median Reward: 43.76 | Max Reward: 48.58
Iteration: 1385 | Episodes: 56200 | Median Reward: 41.07 | Max Reward: 48.58
Iteration: 1388 | Episodes: 56300 | Median Reward: 42.70 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.1        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1390         |
|    time_elapsed         | 11825        |
|    total_timesteps      | 5693440      |
| train/                  |              |
|    approx_kl            | 0.0018620489 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -125         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.79        |
|    n_updates            | 13890        |
|    policy_gradient_loss | -0.011       |
|    std                  | 5.6          |
|    value_loss           | 1.55         |
------------------------------------------
Iteration: 1390 | Episodes: 56400 | Median Reward: 43.33 | Max Reward: 48.58
Iteration: 1393 | Episodes: 56500 | Median Reward: 40.78 | Max Reward: 48.58
Iteration: 1395 | Episodes: 56600 | Median Reward: 41.02 | Max Reward: 48.58
Iteration: 1398 | Episodes: 56700 | Median Reward: 39.68 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.8        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1400         |
|    time_elapsed         | 11905        |
|    total_timesteps      | 5734400      |
| train/                  |              |
|    approx_kl            | 0.0026418653 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -125         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.85        |
|    n_updates            | 13990        |
|    policy_gradient_loss | -0.00756     |
|    std                  | 5.64         |
|    value_loss           | 1            |
------------------------------------------
Iteration: 1400 | Episodes: 56800 | Median Reward: 43.47 | Max Reward: 48.58
Iteration: 1403 | Episodes: 56900 | Median Reward: 41.73 | Max Reward: 48.58
Iteration: 1405 | Episodes: 57000 | Median Reward: 41.52 | Max Reward: 48.58
Iteration: 1407 | Episodes: 57100 | Median Reward: 43.18 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.6        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1410         |
|    time_elapsed         | 11999        |
|    total_timesteps      | 5775360      |
| train/                  |              |
|    approx_kl            | 0.0016952585 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -125         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.86        |
|    n_updates            | 14090        |
|    policy_gradient_loss | -0.00429     |
|    std                  | 5.69         |
|    value_loss           | 0.853        |
------------------------------------------
Iteration: 1410 | Episodes: 57200 | Median Reward: 43.52 | Max Reward: 48.58
Iteration: 1412 | Episodes: 57300 | Median Reward: 43.93 | Max Reward: 48.58
Iteration: 1415 | Episodes: 57400 | Median Reward: 43.36 | Max Reward: 48.58
Iteration: 1417 | Episodes: 57500 | Median Reward: 42.40 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.9         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1420          |
|    time_elapsed         | 12080         |
|    total_timesteps      | 5816320       |
| train/                  |               |
|    approx_kl            | 0.00080029434 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -125          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.82         |
|    n_updates            | 14190         |
|    policy_gradient_loss | -0.00339      |
|    std                  | 5.74          |
|    value_loss           | 0.811         |
-------------------------------------------
Iteration: 1420 | Episodes: 57600 | Median Reward: 42.97 | Max Reward: 48.58
Iteration: 1422 | Episodes: 57700 | Median Reward: 44.68 | Max Reward: 48.58
Iteration: 1425 | Episodes: 57800 | Median Reward: 44.13 | Max Reward: 48.58
Iteration: 1427 | Episodes: 57900 | Median Reward: 44.41 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.8         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1430          |
|    time_elapsed         | 12166         |
|    total_timesteps      | 5857280       |
| train/                  |               |
|    approx_kl            | 0.00030998883 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -125          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.81         |
|    n_updates            | 14290         |
|    policy_gradient_loss | -0.000871     |
|    std                  | 5.8           |
|    value_loss           | 1             |
-------------------------------------------
Iteration: 1430 | Episodes: 58000 | Median Reward: 42.15 | Max Reward: 48.58
Iteration: 1432 | Episodes: 58100 | Median Reward: 41.79 | Max Reward: 48.58
Iteration: 1435 | Episodes: 58200 | Median Reward: 43.29 | Max Reward: 48.58
Iteration: 1437 | Episodes: 58300 | Median Reward: 43.89 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1440          |
|    time_elapsed         | 12254         |
|    total_timesteps      | 5898240       |
| train/                  |               |
|    approx_kl            | 8.9178415e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -126          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.44         |
|    n_updates            | 14390         |
|    policy_gradient_loss | -0.000362     |
|    std                  | 5.87          |
|    value_loss           | 1.25          |
-------------------------------------------
Iteration: 1440 | Episodes: 58400 | Median Reward: 45.65 | Max Reward: 48.58
Iteration: 1442 | Episodes: 58500 | Median Reward: 45.69 | Max Reward: 48.58
Iteration: 1444 | Episodes: 58600 | Median Reward: 43.54 | Max Reward: 48.58
Iteration: 1447 | Episodes: 58700 | Median Reward: 43.00 | Max Reward: 48.58
Iteration: 1449 | Episodes: 58800 | Median Reward: 43.87 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1450         |
|    time_elapsed         | 12334        |
|    total_timesteps      | 5939200      |
| train/                  |              |
|    approx_kl            | 0.0030560931 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -126         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.93        |
|    n_updates            | 14490        |
|    policy_gradient_loss | -0.00299     |
|    std                  | 5.9          |
|    value_loss           | 0.694        |
------------------------------------------
Iteration: 1452 | Episodes: 58900 | Median Reward: 43.16 | Max Reward: 48.58
Iteration: 1454 | Episodes: 59000 | Median Reward: 42.14 | Max Reward: 48.58
Iteration: 1457 | Episodes: 59100 | Median Reward: 42.12 | Max Reward: 48.58
Iteration: 1459 | Episodes: 59200 | Median Reward: 42.14 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.8        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1460         |
|    time_elapsed         | 12426        |
|    total_timesteps      | 5980160      |
| train/                  |              |
|    approx_kl            | 0.0011581493 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -126         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.01        |
|    n_updates            | 14590        |
|    policy_gradient_loss | -0.00437     |
|    std                  | 5.94         |
|    value_loss           | 0.697        |
------------------------------------------
Iteration: 1462 | Episodes: 59300 | Median Reward: 41.85 | Max Reward: 48.58
Iteration: 1464 | Episodes: 59400 | Median Reward: 43.00 | Max Reward: 48.58
Iteration: 1467 | Episodes: 59500 | Median Reward: 42.87 | Max Reward: 48.58
Iteration: 1469 | Episodes: 59600 | Median Reward: 44.08 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.1        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1470         |
|    time_elapsed         | 12506        |
|    total_timesteps      | 6021120      |
| train/                  |              |
|    approx_kl            | 0.0006452089 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -126         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.12        |
|    n_updates            | 14690        |
|    policy_gradient_loss | -0.00172     |
|    std                  | 6            |
|    value_loss           | 0.411        |
------------------------------------------
Iteration: 1472 | Episodes: 59700 | Median Reward: 45.28 | Max Reward: 48.58
Iteration: 1474 | Episodes: 59800 | Median Reward: 41.76 | Max Reward: 48.58
Iteration: 1477 | Episodes: 59900 | Median Reward: 42.85 | Max Reward: 48.58
Iteration: 1479 | Episodes: 60000 | Median Reward: 43.94 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1480         |
|    time_elapsed         | 12592        |
|    total_timesteps      | 6062080      |
| train/                  |              |
|    approx_kl            | 0.0008871846 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -126         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.28        |
|    n_updates            | 14790        |
|    policy_gradient_loss | -0.00254     |
|    std                  | 6.05         |
|    value_loss           | 1.11         |
------------------------------------------
Iteration: 1481 | Episodes: 60100 | Median Reward: 42.64 | Max Reward: 48.58
Iteration: 1484 | Episodes: 60200 | Median Reward: 41.42 | Max Reward: 48.58
Iteration: 1486 | Episodes: 60300 | Median Reward: 42.28 | Max Reward: 48.58
Iteration: 1489 | Episodes: 60400 | Median Reward: 43.83 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.2        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1490         |
|    time_elapsed         | 12680        |
|    total_timesteps      | 6103040      |
| train/                  |              |
|    approx_kl            | 0.0003787111 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -127         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.02        |
|    n_updates            | 14890        |
|    policy_gradient_loss | -0.000945    |
|    std                  | 6.09         |
|    value_loss           | 0.689        |
------------------------------------------
Iteration: 1491 | Episodes: 60500 | Median Reward: 43.52 | Max Reward: 48.58
Iteration: 1494 | Episodes: 60600 | Median Reward: 43.20 | Max Reward: 48.58
Iteration: 1496 | Episodes: 60700 | Median Reward: 44.33 | Max Reward: 48.58
Iteration: 1499 | Episodes: 60800 | Median Reward: 44.05 | Max Reward: 48.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1500         |
|    time_elapsed         | 12761        |
|    total_timesteps      | 6144000      |
| train/                  |              |
|    approx_kl            | 0.0019251755 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -127         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.05        |
|    n_updates            | 14990        |
|    policy_gradient_loss | -0.00441     |
|    std                  | 6.16         |
|    value_loss           | 0.661        |
------------------------------------------
Iteration: 1501 | Episodes: 60900 | Median Reward: 45.41 | Max Reward: 48.58
Iteration: 1504 | Episodes: 61000 | Median Reward: 44.49 | Max Reward: 48.58
Iteration: 1506 | Episodes: 61100 | Median Reward: 42.50 | Max Reward: 48.58
Iteration: 1509 | Episodes: 61200 | Median Reward: 40.59 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1510          |
|    time_elapsed         | 12853         |
|    total_timesteps      | 6184960       |
| train/                  |               |
|    approx_kl            | 0.00013540465 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -127          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.16         |
|    n_updates            | 15090         |
|    policy_gradient_loss | -0.000933     |
|    std                  | 6.21          |
|    value_loss           | 0.583         |
-------------------------------------------
Iteration: 1511 | Episodes: 61300 | Median Reward: 42.58 | Max Reward: 48.58
Iteration: 1514 | Episodes: 61400 | Median Reward: 45.74 | Max Reward: 48.58
Iteration: 1516 | Episodes: 61500 | Median Reward: 43.44 | Max Reward: 48.58
Iteration: 1518 | Episodes: 61600 | Median Reward: 42.68 | Max Reward: 48.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1520          |
|    time_elapsed         | 12933         |
|    total_timesteps      | 6225920       |
| train/                  |               |
|    approx_kl            | 0.00026792538 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -127          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.21         |
|    n_updates            | 15190         |
|    policy_gradient_loss | -6.85e-05     |
|    std                  | 6.25          |
|    value_loss           | 0.333         |
-------------------------------------------
Iteration: 1521 | Episodes: 61700 | Median Reward: 42.30 | Max Reward: 48.58
Iteration: 1523 | Episodes: 61800 | Median Reward: 43.72 | Max Reward: 48.63
Iteration: 1526 | Episodes: 61900 | Median Reward: 42.47 | Max Reward: 48.63
Iteration: 1528 | Episodes: 62000 | Median Reward: 42.66 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1530         |
|    time_elapsed         | 13018        |
|    total_timesteps      | 6266880      |
| train/                  |              |
|    approx_kl            | 5.359121e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -127         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.11        |
|    n_updates            | 15290        |
|    policy_gradient_loss | -0.000188    |
|    std                  | 6.31         |
|    value_loss           | 0.846        |
------------------------------------------
Iteration: 1531 | Episodes: 62100 | Median Reward: 43.68 | Max Reward: 48.63
Iteration: 1533 | Episodes: 62200 | Median Reward: 40.62 | Max Reward: 48.63
Iteration: 1536 | Episodes: 62300 | Median Reward: 43.04 | Max Reward: 48.63
Iteration: 1538 | Episodes: 62400 | Median Reward: 42.15 | Max Reward: 48.63
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.9         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1540          |
|    time_elapsed         | 13106         |
|    total_timesteps      | 6307840       |
| train/                  |               |
|    approx_kl            | 0.00041414303 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -127          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.21         |
|    n_updates            | 15390         |
|    policy_gradient_loss | -0.00281      |
|    std                  | 6.35          |
|    value_loss           | 0.453         |
-------------------------------------------
Iteration: 1541 | Episodes: 62500 | Median Reward: 42.43 | Max Reward: 48.63
Iteration: 1543 | Episodes: 62600 | Median Reward: 40.73 | Max Reward: 48.63
Iteration: 1546 | Episodes: 62700 | Median Reward: 42.39 | Max Reward: 48.63
Iteration: 1548 | Episodes: 62800 | Median Reward: 42.78 | Max Reward: 48.63
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.6       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 1550        |
|    time_elapsed         | 13185       |
|    total_timesteps      | 6348800     |
| train/                  |             |
|    approx_kl            | 0.003076688 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -128        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -6.18       |
|    n_updates            | 15490       |
|    policy_gradient_loss | -0.00696    |
|    std                  | 6.4         |
|    value_loss           | 0.517       |
-----------------------------------------
Iteration: 1551 | Episodes: 62900 | Median Reward: 43.57 | Max Reward: 48.63
Iteration: 1553 | Episodes: 63000 | Median Reward: 43.91 | Max Reward: 48.63
Iteration: 1555 | Episodes: 63100 | Median Reward: 42.83 | Max Reward: 48.63
Iteration: 1558 | Episodes: 63200 | Median Reward: 44.87 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1560         |
|    time_elapsed         | 13277        |
|    total_timesteps      | 6389760      |
| train/                  |              |
|    approx_kl            | 0.0010736103 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.17        |
|    n_updates            | 15590        |
|    policy_gradient_loss | -0.00313     |
|    std                  | 6.48         |
|    value_loss           | 0.497        |
------------------------------------------
Iteration: 1560 | Episodes: 63300 | Median Reward: 44.73 | Max Reward: 48.63
Iteration: 1563 | Episodes: 63400 | Median Reward: 43.09 | Max Reward: 48.63
Iteration: 1565 | Episodes: 63500 | Median Reward: 43.87 | Max Reward: 48.63
Iteration: 1568 | Episodes: 63600 | Median Reward: 46.29 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1570         |
|    time_elapsed         | 13355        |
|    total_timesteps      | 6430720      |
| train/                  |              |
|    approx_kl            | 0.0050357995 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.34        |
|    n_updates            | 15690        |
|    policy_gradient_loss | -0.00484     |
|    std                  | 6.52         |
|    value_loss           | 0.216        |
------------------------------------------
Iteration: 1570 | Episodes: 63700 | Median Reward: 43.20 | Max Reward: 48.63
Iteration: 1573 | Episodes: 63800 | Median Reward: 42.76 | Max Reward: 48.63
Iteration: 1575 | Episodes: 63900 | Median Reward: 44.40 | Max Reward: 48.63
Iteration: 1578 | Episodes: 64000 | Median Reward: 44.19 | Max Reward: 48.63
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.9         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1580          |
|    time_elapsed         | 13440         |
|    total_timesteps      | 6471680       |
| train/                  |               |
|    approx_kl            | 0.00020896361 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -128          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.93         |
|    n_updates            | 15790         |
|    policy_gradient_loss | -0.00176      |
|    std                  | 6.59          |
|    value_loss           | 0.931         |
-------------------------------------------
Iteration: 1580 | Episodes: 64100 | Median Reward: 43.69 | Max Reward: 48.63
Iteration: 1583 | Episodes: 64200 | Median Reward: 42.97 | Max Reward: 48.63
Iteration: 1585 | Episodes: 64300 | Median Reward: 44.29 | Max Reward: 48.63
Iteration: 1587 | Episodes: 64400 | Median Reward: 42.86 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.9        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1590         |
|    time_elapsed         | 13527        |
|    total_timesteps      | 6512640      |
| train/                  |              |
|    approx_kl            | 0.0006118694 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.11        |
|    n_updates            | 15890        |
|    policy_gradient_loss | -0.00343     |
|    std                  | 6.61         |
|    value_loss           | 0.337        |
------------------------------------------
Iteration: 1590 | Episodes: 64500 | Median Reward: 42.37 | Max Reward: 48.63
Iteration: 1592 | Episodes: 64600 | Median Reward: 38.92 | Max Reward: 48.63
Iteration: 1595 | Episodes: 64700 | Median Reward: 42.62 | Max Reward: 48.63
Iteration: 1597 | Episodes: 64800 | Median Reward: 42.72 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1600         |
|    time_elapsed         | 13606        |
|    total_timesteps      | 6553600      |
| train/                  |              |
|    approx_kl            | 0.0015440886 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -129         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.38        |
|    n_updates            | 15990        |
|    policy_gradient_loss | -0.00851     |
|    std                  | 6.68         |
|    value_loss           | 0.421        |
------------------------------------------
Iteration: 1600 | Episodes: 64900 | Median Reward: 41.93 | Max Reward: 48.63
Iteration: 1602 | Episodes: 65000 | Median Reward: 42.71 | Max Reward: 48.63
Iteration: 1605 | Episodes: 65100 | Median Reward: 43.17 | Max Reward: 48.63
Iteration: 1607 | Episodes: 65200 | Median Reward: 42.38 | Max Reward: 48.63
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1610          |
|    time_elapsed         | 13701         |
|    total_timesteps      | 6594560       |
| train/                  |               |
|    approx_kl            | 0.00015066047 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -129          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.27         |
|    n_updates            | 16090         |
|    policy_gradient_loss | -0.000787     |
|    std                  | 6.74          |
|    value_loss           | 0.332         |
-------------------------------------------
Iteration: 1610 | Episodes: 65300 | Median Reward: 42.27 | Max Reward: 48.63
Iteration: 1612 | Episodes: 65400 | Median Reward: 42.29 | Max Reward: 48.63
Iteration: 1615 | Episodes: 65500 | Median Reward: 43.18 | Max Reward: 48.63
Iteration: 1617 | Episodes: 65600 | Median Reward: 43.37 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58          |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1620         |
|    time_elapsed         | 13780        |
|    total_timesteps      | 6635520      |
| train/                  |              |
|    approx_kl            | 0.0011535378 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -129         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.34        |
|    n_updates            | 16190        |
|    policy_gradient_loss | -0.00648     |
|    std                  | 6.8          |
|    value_loss           | 0.294        |
------------------------------------------
Iteration: 1620 | Episodes: 65700 | Median Reward: 42.84 | Max Reward: 48.63
Iteration: 1622 | Episodes: 65800 | Median Reward: 42.92 | Max Reward: 48.63
Iteration: 1624 | Episodes: 65900 | Median Reward: 43.51 | Max Reward: 48.63
Iteration: 1627 | Episodes: 66000 | Median Reward: 43.31 | Max Reward: 48.63
Iteration: 1629 | Episodes: 66100 | Median Reward: 42.83 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1630         |
|    time_elapsed         | 13866        |
|    total_timesteps      | 6676480      |
| train/                  |              |
|    approx_kl            | 0.0004918246 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -129         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.29        |
|    n_updates            | 16290        |
|    policy_gradient_loss | -0.00148     |
|    std                  | 6.86         |
|    value_loss           | 0.488        |
------------------------------------------
Iteration: 1632 | Episodes: 66200 | Median Reward: 41.76 | Max Reward: 48.63
Iteration: 1634 | Episodes: 66300 | Median Reward: 44.10 | Max Reward: 48.63
Iteration: 1637 | Episodes: 66400 | Median Reward: 42.17 | Max Reward: 48.63
Iteration: 1639 | Episodes: 66500 | Median Reward: 43.23 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1640         |
|    time_elapsed         | 13954        |
|    total_timesteps      | 6717440      |
| train/                  |              |
|    approx_kl            | 0.0023479052 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -129         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.35        |
|    n_updates            | 16390        |
|    policy_gradient_loss | -0.00983     |
|    std                  | 6.91         |
|    value_loss           | 0.611        |
------------------------------------------
Iteration: 1642 | Episodes: 66600 | Median Reward: 43.34 | Max Reward: 48.63
Iteration: 1644 | Episodes: 66700 | Median Reward: 43.60 | Max Reward: 48.63
Iteration: 1647 | Episodes: 66800 | Median Reward: 42.83 | Max Reward: 48.63
Iteration: 1649 | Episodes: 66900 | Median Reward: 42.09 | Max Reward: 48.63
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.7         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1650          |
|    time_elapsed         | 14034         |
|    total_timesteps      | 6758400       |
| train/                  |               |
|    approx_kl            | 0.00015519722 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -129          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -5.72         |
|    n_updates            | 16490         |
|    policy_gradient_loss | -0.00113      |
|    std                  | 6.96          |
|    value_loss           | 0.765         |
-------------------------------------------
Iteration: 1652 | Episodes: 67000 | Median Reward: 42.57 | Max Reward: 48.63
Iteration: 1654 | Episodes: 67100 | Median Reward: 42.07 | Max Reward: 48.63
Iteration: 1657 | Episodes: 67200 | Median Reward: 43.33 | Max Reward: 48.63
Iteration: 1659 | Episodes: 67300 | Median Reward: 46.13 | Max Reward: 48.63
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1660          |
|    time_elapsed         | 14128         |
|    total_timesteps      | 6799360       |
| train/                  |               |
|    approx_kl            | 3.7693768e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -130          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.26         |
|    n_updates            | 16590         |
|    policy_gradient_loss | -0.000424     |
|    std                  | 7.03          |
|    value_loss           | 0.705         |
-------------------------------------------
Iteration: 1661 | Episodes: 67400 | Median Reward: 45.02 | Max Reward: 48.63
Iteration: 1664 | Episodes: 67500 | Median Reward: 44.05 | Max Reward: 48.63
Iteration: 1666 | Episodes: 67600 | Median Reward: 39.05 | Max Reward: 48.63
Iteration: 1669 | Episodes: 67700 | Median Reward: 42.91 | Max Reward: 48.63
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.2         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1670          |
|    time_elapsed         | 14209         |
|    total_timesteps      | 6840320       |
| train/                  |               |
|    approx_kl            | 0.00014799437 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -130          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.43         |
|    n_updates            | 16690         |
|    policy_gradient_loss | -0.000242     |
|    std                  | 7.08          |
|    value_loss           | 0.16          |
-------------------------------------------
Iteration: 1671 | Episodes: 67800 | Median Reward: 43.02 | Max Reward: 48.63
Iteration: 1674 | Episodes: 67900 | Median Reward: 45.90 | Max Reward: 48.63
Iteration: 1676 | Episodes: 68000 | Median Reward: 45.70 | Max Reward: 48.63
Iteration: 1679 | Episodes: 68100 | Median Reward: 43.48 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1680         |
|    time_elapsed         | 14294        |
|    total_timesteps      | 6881280      |
| train/                  |              |
|    approx_kl            | 0.0011146362 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -130         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.38        |
|    n_updates            | 16790        |
|    policy_gradient_loss | -0.00486     |
|    std                  | 7.11         |
|    value_loss           | 0.397        |
------------------------------------------
Iteration: 1681 | Episodes: 68200 | Median Reward: 43.13 | Max Reward: 48.63
Iteration: 1684 | Episodes: 68300 | Median Reward: 43.72 | Max Reward: 48.63
Iteration: 1686 | Episodes: 68400 | Median Reward: 43.47 | Max Reward: 48.63
Iteration: 1689 | Episodes: 68500 | Median Reward: 44.78 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.7        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1690         |
|    time_elapsed         | 14384        |
|    total_timesteps      | 6922240      |
| train/                  |              |
|    approx_kl            | 0.0001926076 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -130         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.24        |
|    n_updates            | 16890        |
|    policy_gradient_loss | -0.00185     |
|    std                  | 7.18         |
|    value_loss           | 0.63         |
------------------------------------------
Iteration: 1691 | Episodes: 68600 | Median Reward: 45.17 | Max Reward: 48.63
Iteration: 1694 | Episodes: 68700 | Median Reward: 44.55 | Max Reward: 48.63
Iteration: 1696 | Episodes: 68800 | Median Reward: 43.65 | Max Reward: 48.63
Iteration: 1698 | Episodes: 68900 | Median Reward: 43.86 | Max Reward: 48.63
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.7       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 1700        |
|    time_elapsed         | 14463       |
|    total_timesteps      | 6963200     |
| train/                  |             |
|    approx_kl            | 0.004453796 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -130        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -6.4        |
|    n_updates            | 16990       |
|    policy_gradient_loss | -0.0151     |
|    std                  | 7.22        |
|    value_loss           | 0.555       |
-----------------------------------------
Iteration: 1701 | Episodes: 69000 | Median Reward: 42.42 | Max Reward: 48.63
Iteration: 1703 | Episodes: 69100 | Median Reward: 42.88 | Max Reward: 48.63
Iteration: 1706 | Episodes: 69200 | Median Reward: 42.86 | Max Reward: 48.63
Iteration: 1708 | Episodes: 69300 | Median Reward: 41.84 | Max Reward: 48.63
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.4         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1710          |
|    time_elapsed         | 14559         |
|    total_timesteps      | 7004160       |
| train/                  |               |
|    approx_kl            | 0.00014314591 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -130          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.34         |
|    n_updates            | 17090         |
|    policy_gradient_loss | -0.000818     |
|    std                  | 7.27          |
|    value_loss           | 0.563         |
-------------------------------------------
Iteration: 1711 | Episodes: 69400 | Median Reward: 46.15 | Max Reward: 48.63
Iteration: 1713 | Episodes: 69500 | Median Reward: 44.98 | Max Reward: 48.63
Iteration: 1716 | Episodes: 69600 | Median Reward: 42.05 | Max Reward: 48.63
Iteration: 1718 | Episodes: 69700 | Median Reward: 44.04 | Max Reward: 48.63
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.3         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1720          |
|    time_elapsed         | 14640         |
|    total_timesteps      | 7045120       |
| train/                  |               |
|    approx_kl            | 0.00014733622 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -130          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.44         |
|    n_updates            | 17190         |
|    policy_gradient_loss | -0.00142      |
|    std                  | 7.31          |
|    value_loss           | 0.908         |
-------------------------------------------
Iteration: 1721 | Episodes: 69800 | Median Reward: 41.03 | Max Reward: 48.63
Iteration: 1723 | Episodes: 69900 | Median Reward: 43.86 | Max Reward: 48.63
Iteration: 1726 | Episodes: 70000 | Median Reward: 43.55 | Max Reward: 48.63
Iteration: 1728 | Episodes: 70100 | Median Reward: 43.39 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.5        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1730         |
|    time_elapsed         | 14722        |
|    total_timesteps      | 7086080      |
| train/                  |              |
|    approx_kl            | 0.0012827571 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.32        |
|    n_updates            | 17290        |
|    policy_gradient_loss | -0.00636     |
|    std                  | 7.38         |
|    value_loss           | 0.57         |
------------------------------------------
Iteration: 1731 | Episodes: 70200 | Median Reward: 42.08 | Max Reward: 48.63
Iteration: 1733 | Episodes: 70300 | Median Reward: 42.72 | Max Reward: 48.63
Iteration: 1735 | Episodes: 70400 | Median Reward: 43.01 | Max Reward: 48.63
Iteration: 1738 | Episodes: 70500 | Median Reward: 40.41 | Max Reward: 48.63
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.2       |
| time/                   |             |
|    fps                  | 480         |
|    iterations           | 1740        |
|    time_elapsed         | 14817       |
|    total_timesteps      | 7127040     |
| train/                  |             |
|    approx_kl            | 0.000997341 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -131        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -6.2        |
|    n_updates            | 17390       |
|    policy_gradient_loss | -0.00446    |
|    std                  | 7.46        |
|    value_loss           | 0.512       |
-----------------------------------------
Iteration: 1740 | Episodes: 70600 | Median Reward: 40.31 | Max Reward: 48.63
Iteration: 1743 | Episodes: 70700 | Median Reward: 43.31 | Max Reward: 48.63
Iteration: 1745 | Episodes: 70800 | Median Reward: 44.77 | Max Reward: 48.63
Iteration: 1748 | Episodes: 70900 | Median Reward: 39.38 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1750         |
|    time_elapsed         | 14897        |
|    total_timesteps      | 7168000      |
| train/                  |              |
|    approx_kl            | 4.852118e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.12        |
|    n_updates            | 17490        |
|    policy_gradient_loss | -0.00044     |
|    std                  | 7.52         |
|    value_loss           | 0.621        |
------------------------------------------
Iteration: 1750 | Episodes: 71000 | Median Reward: 44.20 | Max Reward: 48.63
Iteration: 1753 | Episodes: 71100 | Median Reward: 43.47 | Max Reward: 48.63
Iteration: 1755 | Episodes: 71200 | Median Reward: 43.62 | Max Reward: 48.63
Iteration: 1758 | Episodes: 71300 | Median Reward: 45.15 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.1        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 1760         |
|    time_elapsed         | 14987        |
|    total_timesteps      | 7208960      |
| train/                  |              |
|    approx_kl            | 0.0009841622 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.38        |
|    n_updates            | 17590        |
|    policy_gradient_loss | -0.00328     |
|    std                  | 7.56         |
|    value_loss           | 0.392        |
------------------------------------------
Iteration: 1760 | Episodes: 71400 | Median Reward: 43.43 | Max Reward: 48.63
Iteration: 1763 | Episodes: 71500 | Median Reward: 43.98 | Max Reward: 48.63
Iteration: 1765 | Episodes: 71600 | Median Reward: 44.01 | Max Reward: 48.63
Iteration: 1767 | Episodes: 71700 | Median Reward: 42.72 | Max Reward: 48.63
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.1         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1770          |
|    time_elapsed         | 15069         |
|    total_timesteps      | 7249920       |
| train/                  |               |
|    approx_kl            | 0.00019901585 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.36         |
|    n_updates            | 17690         |
|    policy_gradient_loss | -0.000675     |
|    std                  | 7.62          |
|    value_loss           | 0.441         |
-------------------------------------------
Iteration: 1770 | Episodes: 71800 | Median Reward: 44.41 | Max Reward: 48.63
Iteration: 1772 | Episodes: 71900 | Median Reward: 43.14 | Max Reward: 48.63
Iteration: 1775 | Episodes: 72000 | Median Reward: 43.40 | Max Reward: 48.63
Iteration: 1777 | Episodes: 72100 | Median Reward: 44.26 | Max Reward: 48.63
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.3         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1780          |
|    time_elapsed         | 15151         |
|    total_timesteps      | 7290880       |
| train/                  |               |
|    approx_kl            | 0.00022767235 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.52         |
|    n_updates            | 17790         |
|    policy_gradient_loss | -0.00119      |
|    std                  | 7.69          |
|    value_loss           | 0.248         |
-------------------------------------------
Iteration: 1780 | Episodes: 72200 | Median Reward: 46.61 | Max Reward: 48.63
Iteration: 1782 | Episodes: 72300 | Median Reward: 45.46 | Max Reward: 48.63
Iteration: 1785 | Episodes: 72400 | Median Reward: 39.96 | Max Reward: 48.63
Iteration: 1787 | Episodes: 72500 | Median Reward: 41.59 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 1790         |
|    time_elapsed         | 15245        |
|    total_timesteps      | 7331840      |
| train/                  |              |
|    approx_kl            | 7.937664e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -132         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.06        |
|    n_updates            | 17890        |
|    policy_gradient_loss | -0.000626    |
|    std                  | 7.71         |
|    value_loss           | 0.996        |
------------------------------------------
Iteration: 1790 | Episodes: 72600 | Median Reward: 44.53 | Max Reward: 48.63
Iteration: 1792 | Episodes: 72700 | Median Reward: 43.12 | Max Reward: 48.63
Iteration: 1795 | Episodes: 72800 | Median Reward: 42.76 | Max Reward: 48.63
Iteration: 1797 | Episodes: 72900 | Median Reward: 41.17 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.3        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1800         |
|    time_elapsed         | 15325        |
|    total_timesteps      | 7372800      |
| train/                  |              |
|    approx_kl            | 0.0003187666 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -132         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.43        |
|    n_updates            | 17990        |
|    policy_gradient_loss | -0.001       |
|    std                  | 7.75         |
|    value_loss           | 0.355        |
------------------------------------------
Iteration: 1800 | Episodes: 73000 | Median Reward: 44.24 | Max Reward: 48.63
Iteration: 1802 | Episodes: 73100 | Median Reward: 44.18 | Max Reward: 48.63
Iteration: 1804 | Episodes: 73200 | Median Reward: 43.02 | Max Reward: 48.63
Iteration: 1807 | Episodes: 73300 | Median Reward: 42.79 | Max Reward: 48.63
Iteration: 1809 | Episodes: 73400 | Median Reward: 42.24 | Max Reward: 48.63
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 1810         |
|    time_elapsed         | 15414        |
|    total_timesteps      | 7413760      |
| train/                  |              |
|    approx_kl            | 0.0008422598 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -132         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.45        |
|    n_updates            | 18090        |
|    policy_gradient_loss | -0.00312     |
|    std                  | 7.79         |
|    value_loss           | 0.272        |
------------------------------------------
Iteration: 1812 | Episodes: 73500 | Median Reward: 44.30 | Max Reward: 48.63
Iteration: 1814 | Episodes: 73600 | Median Reward: 40.43 | Max Reward: 48.63
Iteration: 1817 | Episodes: 73700 | Median Reward: 45.26 | Max Reward: 48.63
Iteration: 1819 | Episodes: 73800 | Median Reward: 43.72 | Max Reward: 48.63
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1820          |
|    time_elapsed         | 15498         |
|    total_timesteps      | 7454720       |
| train/                  |               |
|    approx_kl            | 0.00016893473 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -132          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.42         |
|    n_updates            | 18190         |
|    policy_gradient_loss | -0.000173     |
|    std                  | 7.85          |
|    value_loss           | 0.224         |
-------------------------------------------
Iteration: 1822 | Episodes: 73900 | Median Reward: 46.76 | Max Reward: 48.63
Iteration: 1824 | Episodes: 74000 | Median Reward: 44.29 | Max Reward: 48.63
Iteration: 1827 | Episodes: 74100 | Median Reward: 41.86 | Max Reward: 48.63
Iteration: 1829 | Episodes: 74200 | Median Reward: 46.89 | Max Reward: 48.63
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.1       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 1830        |
|    time_elapsed         | 15578       |
|    total_timesteps      | 7495680     |
| train/                  |             |
|    approx_kl            | 0.000225162 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -132        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -6.48       |
|    n_updates            | 18290       |
|    policy_gradient_loss | -0.00137    |
|    std                  | 7.95        |
|    value_loss           | 0.444       |
-----------------------------------------
Iteration: 1832 | Episodes: 74300 | Median Reward: 42.91 | Max Reward: 48.88
Iteration: 1834 | Episodes: 74400 | Median Reward: 43.92 | Max Reward: 48.88
Iteration: 1837 | Episodes: 74500 | Median Reward: 43.76 | Max Reward: 48.88
Iteration: 1839 | Episodes: 74600 | Median Reward: 43.35 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.8         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 1840          |
|    time_elapsed         | 15672         |
|    total_timesteps      | 7536640       |
| train/                  |               |
|    approx_kl            | 0.00012486475 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -132          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.09         |
|    n_updates            | 18390         |
|    policy_gradient_loss | -0.000261     |
|    std                  | 8.04          |
|    value_loss           | 0.701         |
-------------------------------------------
Iteration: 1841 | Episodes: 74700 | Median Reward: 44.12 | Max Reward: 48.88
Iteration: 1844 | Episodes: 74800 | Median Reward: 42.45 | Max Reward: 48.88
Iteration: 1846 | Episodes: 74900 | Median Reward: 42.50 | Max Reward: 48.88
Iteration: 1849 | Episodes: 75000 | Median Reward: 43.33 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1850         |
|    time_elapsed         | 15750        |
|    total_timesteps      | 7577600      |
| train/                  |              |
|    approx_kl            | 6.142567e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -133         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.57        |
|    n_updates            | 18490        |
|    policy_gradient_loss | -0.000305    |
|    std                  | 8.1          |
|    value_loss           | 0.184        |
------------------------------------------
Iteration: 1851 | Episodes: 75100 | Median Reward: 43.10 | Max Reward: 48.88
Iteration: 1854 | Episodes: 75200 | Median Reward: 39.39 | Max Reward: 48.88
Iteration: 1856 | Episodes: 75300 | Median Reward: 42.22 | Max Reward: 48.88
Iteration: 1859 | Episodes: 75400 | Median Reward: 42.57 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 1860         |
|    time_elapsed         | 15840        |
|    total_timesteps      | 7618560      |
| train/                  |              |
|    approx_kl            | 7.982284e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -133         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.53        |
|    n_updates            | 18590        |
|    policy_gradient_loss | -0.000905    |
|    std                  | 8.16         |
|    value_loss           | 0.34         |
------------------------------------------
Iteration: 1861 | Episodes: 75500 | Median Reward: 45.54 | Max Reward: 48.88
Iteration: 1864 | Episodes: 75600 | Median Reward: 43.49 | Max Reward: 48.88
Iteration: 1866 | Episodes: 75700 | Median Reward: 42.07 | Max Reward: 48.88
Iteration: 1869 | Episodes: 75800 | Median Reward: 43.39 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1870          |
|    time_elapsed         | 15921         |
|    total_timesteps      | 7659520       |
| train/                  |               |
|    approx_kl            | 0.00073154544 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.51         |
|    n_updates            | 18690         |
|    policy_gradient_loss | -0.00341      |
|    std                  | 8.2           |
|    value_loss           | 0.305         |
-------------------------------------------
Iteration: 1871 | Episodes: 75900 | Median Reward: 43.19 | Max Reward: 48.88
Iteration: 1874 | Episodes: 76000 | Median Reward: 41.16 | Max Reward: 48.88
Iteration: 1876 | Episodes: 76100 | Median Reward: 43.28 | Max Reward: 48.88
Iteration: 1878 | Episodes: 76200 | Median Reward: 44.27 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.5         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1880          |
|    time_elapsed         | 15999         |
|    total_timesteps      | 7700480       |
| train/                  |               |
|    approx_kl            | 6.4434484e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.52         |
|    n_updates            | 18790         |
|    policy_gradient_loss | 0.000542      |
|    std                  | 8.25          |
|    value_loss           | 0.491         |
-------------------------------------------
Iteration: 1881 | Episodes: 76300 | Median Reward: 46.87 | Max Reward: 48.88
Iteration: 1883 | Episodes: 76400 | Median Reward: 44.42 | Max Reward: 48.88
Iteration: 1886 | Episodes: 76500 | Median Reward: 44.34 | Max Reward: 48.88
Iteration: 1888 | Episodes: 76600 | Median Reward: 44.62 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.2         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1890          |
|    time_elapsed         | 16093         |
|    total_timesteps      | 7741440       |
| train/                  |               |
|    approx_kl            | 0.00018841907 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.53         |
|    n_updates            | 18890         |
|    policy_gradient_loss | -0.00161      |
|    std                  | 8.29          |
|    value_loss           | 0.463         |
-------------------------------------------
Iteration: 1891 | Episodes: 76700 | Median Reward: 44.61 | Max Reward: 48.88
Iteration: 1893 | Episodes: 76800 | Median Reward: 44.55 | Max Reward: 48.88
Iteration: 1896 | Episodes: 76900 | Median Reward: 43.42 | Max Reward: 48.88
Iteration: 1898 | Episodes: 77000 | Median Reward: 43.42 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.1         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1900          |
|    time_elapsed         | 16173         |
|    total_timesteps      | 7782400       |
| train/                  |               |
|    approx_kl            | 0.00024759804 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.47         |
|    n_updates            | 18990         |
|    policy_gradient_loss | -0.00118      |
|    std                  | 8.37          |
|    value_loss           | 0.299         |
-------------------------------------------
Iteration: 1901 | Episodes: 77100 | Median Reward: 43.81 | Max Reward: 48.88
Iteration: 1903 | Episodes: 77200 | Median Reward: 45.25 | Max Reward: 48.88
Iteration: 1906 | Episodes: 77300 | Median Reward: 44.31 | Max Reward: 48.88
Iteration: 1908 | Episodes: 77400 | Median Reward: 44.51 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1910          |
|    time_elapsed         | 16264         |
|    total_timesteps      | 7823360       |
| train/                  |               |
|    approx_kl            | 0.00020532346 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.37         |
|    n_updates            | 19090         |
|    policy_gradient_loss | -0.000655     |
|    std                  | 8.44          |
|    value_loss           | 0.37          |
-------------------------------------------
Iteration: 1911 | Episodes: 77500 | Median Reward: 41.07 | Max Reward: 48.88
Iteration: 1913 | Episodes: 77600 | Median Reward: 42.43 | Max Reward: 48.88
Iteration: 1915 | Episodes: 77700 | Median Reward: 43.44 | Max Reward: 48.88
Iteration: 1918 | Episodes: 77800 | Median Reward: 43.72 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.2        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1920         |
|    time_elapsed         | 16347        |
|    total_timesteps      | 7864320      |
| train/                  |              |
|    approx_kl            | 0.0019924045 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -134         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.47        |
|    n_updates            | 19190        |
|    policy_gradient_loss | -0.00661     |
|    std                  | 8.49         |
|    value_loss           | 0.503        |
------------------------------------------
Iteration: 1920 | Episodes: 77900 | Median Reward: 44.41 | Max Reward: 48.88
Iteration: 1923 | Episodes: 78000 | Median Reward: 42.78 | Max Reward: 48.88
Iteration: 1925 | Episodes: 78100 | Median Reward: 41.54 | Max Reward: 48.88
Iteration: 1928 | Episodes: 78200 | Median Reward: 44.02 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.8        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 1930         |
|    time_elapsed         | 16427        |
|    total_timesteps      | 7905280      |
| train/                  |              |
|    approx_kl            | 0.0010911884 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -134         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.62        |
|    n_updates            | 19290        |
|    policy_gradient_loss | -0.0059      |
|    std                  | 8.58         |
|    value_loss           | 0.353        |
------------------------------------------
Iteration: 1930 | Episodes: 78300 | Median Reward: 42.70 | Max Reward: 48.88
Iteration: 1933 | Episodes: 78400 | Median Reward: 39.51 | Max Reward: 48.88
Iteration: 1935 | Episodes: 78500 | Median Reward: 43.20 | Max Reward: 48.88
Iteration: 1938 | Episodes: 78600 | Median Reward: 42.69 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.3         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 1940          |
|    time_elapsed         | 16521         |
|    total_timesteps      | 7946240       |
| train/                  |               |
|    approx_kl            | 3.7196616e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.36         |
|    n_updates            | 19390         |
|    policy_gradient_loss | -0.000438     |
|    std                  | 8.66          |
|    value_loss           | 0.639         |
-------------------------------------------
Iteration: 1940 | Episodes: 78700 | Median Reward: 43.24 | Max Reward: 48.88
Iteration: 1943 | Episodes: 78800 | Median Reward: 43.45 | Max Reward: 48.88
Iteration: 1945 | Episodes: 78900 | Median Reward: 43.36 | Max Reward: 48.88
Iteration: 1947 | Episodes: 79000 | Median Reward: 44.53 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.2         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 1950          |
|    time_elapsed         | 16602         |
|    total_timesteps      | 7987200       |
| train/                  |               |
|    approx_kl            | 0.00044001138 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.69         |
|    n_updates            | 19490         |
|    policy_gradient_loss | 2.05e-05      |
|    std                  | 8.73          |
|    value_loss           | 0.0802        |
-------------------------------------------
Iteration: 1950 | Episodes: 79100 | Median Reward: 44.33 | Max Reward: 48.88
Iteration: 1952 | Episodes: 79200 | Median Reward: 44.26 | Max Reward: 48.88
Iteration: 1955 | Episodes: 79300 | Median Reward: 43.45 | Max Reward: 48.88
Iteration: 1957 | Episodes: 79400 | Median Reward: 43.37 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.7         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 1960          |
|    time_elapsed         | 16691         |
|    total_timesteps      | 8028160       |
| train/                  |               |
|    approx_kl            | 0.00042337258 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.62         |
|    n_updates            | 19590         |
|    policy_gradient_loss | -0.00139      |
|    std                  | 8.83          |
|    value_loss           | 0.183         |
-------------------------------------------
Iteration: 1960 | Episodes: 79500 | Median Reward: 43.51 | Max Reward: 48.88
Iteration: 1962 | Episodes: 79600 | Median Reward: 42.69 | Max Reward: 48.88
Iteration: 1965 | Episodes: 79700 | Median Reward: 43.64 | Max Reward: 48.88
Iteration: 1967 | Episodes: 79800 | Median Reward: 43.45 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.4        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 1970         |
|    time_elapsed         | 16777        |
|    total_timesteps      | 8069120      |
| train/                  |              |
|    approx_kl            | 0.0023512226 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -135         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.64        |
|    n_updates            | 19690        |
|    policy_gradient_loss | -0.00557     |
|    std                  | 8.92         |
|    value_loss           | 0.233        |
------------------------------------------
Iteration: 1970 | Episodes: 79900 | Median Reward: 42.17 | Max Reward: 48.88
Iteration: 1972 | Episodes: 80000 | Median Reward: 43.84 | Max Reward: 48.88
Iteration: 1975 | Episodes: 80100 | Median Reward: 44.14 | Max Reward: 48.88
Iteration: 1977 | Episodes: 80200 | Median Reward: 44.05 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.5       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 1980        |
|    time_elapsed         | 16857       |
|    total_timesteps      | 8110080     |
| train/                  |             |
|    approx_kl            | 0.001458318 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -135        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.68       |
|    n_updates            | 19790       |
|    policy_gradient_loss | -0.00928    |
|    std                  | 8.99        |
|    value_loss           | 0.236       |
-----------------------------------------
Iteration: 1980 | Episodes: 80300 | Median Reward: 42.75 | Max Reward: 48.88
Iteration: 1982 | Episodes: 80400 | Median Reward: 43.19 | Max Reward: 48.88
Iteration: 1984 | Episodes: 80500 | Median Reward: 44.80 | Max Reward: 48.88
Iteration: 1987 | Episodes: 80600 | Median Reward: 43.38 | Max Reward: 48.88
Iteration: 1989 | Episodes: 80700 | Median Reward: 43.75 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.3        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 1990         |
|    time_elapsed         | 16951        |
|    total_timesteps      | 8151040      |
| train/                  |              |
|    approx_kl            | 0.0033972105 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -135         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.7         |
|    n_updates            | 19890        |
|    policy_gradient_loss | -0.00482     |
|    std                  | 9.11         |
|    value_loss           | 0.187        |
------------------------------------------
Iteration: 1992 | Episodes: 80800 | Median Reward: 43.13 | Max Reward: 48.88
Iteration: 1994 | Episodes: 80900 | Median Reward: 42.96 | Max Reward: 48.88
Iteration: 1997 | Episodes: 81000 | Median Reward: 43.08 | Max Reward: 48.88
Iteration: 1999 | Episodes: 81100 | Median Reward: 42.39 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.9         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2000          |
|    time_elapsed         | 17030         |
|    total_timesteps      | 8192000       |
| train/                  |               |
|    approx_kl            | 1.4470512e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -135          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.62         |
|    n_updates            | 19990         |
|    policy_gradient_loss | -2.11e-05     |
|    std                  | 9.19          |
|    value_loss           | 0.516         |
-------------------------------------------
Iteration: 2002 | Episodes: 81200 | Median Reward: 44.52 | Max Reward: 48.88
Iteration: 2004 | Episodes: 81300 | Median Reward: 43.37 | Max Reward: 48.88
Iteration: 2007 | Episodes: 81400 | Median Reward: 43.21 | Max Reward: 48.88
Iteration: 2009 | Episodes: 81500 | Median Reward: 42.08 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.9         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 2010          |
|    time_elapsed         | 17117         |
|    total_timesteps      | 8232960       |
| train/                  |               |
|    approx_kl            | 0.00037872946 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.56         |
|    n_updates            | 20090         |
|    policy_gradient_loss | -0.00122      |
|    std                  | 9.28          |
|    value_loss           | 0.436         |
-------------------------------------------
Iteration: 2012 | Episodes: 81600 | Median Reward: 43.31 | Max Reward: 48.88
Iteration: 2014 | Episodes: 81700 | Median Reward: 40.32 | Max Reward: 48.88
Iteration: 2017 | Episodes: 81800 | Median Reward: 42.06 | Max Reward: 48.88
Iteration: 2019 | Episodes: 81900 | Median Reward: 43.62 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.8        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 2020         |
|    time_elapsed         | 17202        |
|    total_timesteps      | 8273920      |
| train/                  |              |
|    approx_kl            | 0.0009668105 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -136         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.55        |
|    n_updates            | 20190        |
|    policy_gradient_loss | -0.00469     |
|    std                  | 9.33         |
|    value_loss           | 0.557        |
------------------------------------------
Iteration: 2021 | Episodes: 82000 | Median Reward: 43.93 | Max Reward: 48.88
Iteration: 2024 | Episodes: 82100 | Median Reward: 45.85 | Max Reward: 48.88
Iteration: 2026 | Episodes: 82200 | Median Reward: 39.51 | Max Reward: 48.88
Iteration: 2029 | Episodes: 82300 | Median Reward: 42.23 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.5         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2030          |
|    time_elapsed         | 17282         |
|    total_timesteps      | 8314880       |
| train/                  |               |
|    approx_kl            | 0.00022375409 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.46         |
|    n_updates            | 20290         |
|    policy_gradient_loss | -0.00202      |
|    std                  | 9.39          |
|    value_loss           | 0.532         |
-------------------------------------------
Iteration: 2031 | Episodes: 82400 | Median Reward: 41.80 | Max Reward: 48.88
Iteration: 2034 | Episodes: 82500 | Median Reward: 43.10 | Max Reward: 48.88
Iteration: 2036 | Episodes: 82600 | Median Reward: 43.16 | Max Reward: 48.88
Iteration: 2039 | Episodes: 82700 | Median Reward: 44.13 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.8        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 2040         |
|    time_elapsed         | 17375        |
|    total_timesteps      | 8355840      |
| train/                  |              |
|    approx_kl            | 0.0023324338 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -136         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.71        |
|    n_updates            | 20390        |
|    policy_gradient_loss | -0.00726     |
|    std                  | 9.45         |
|    value_loss           | 0.267        |
------------------------------------------
Iteration: 2041 | Episodes: 82800 | Median Reward: 46.55 | Max Reward: 48.88
Iteration: 2044 | Episodes: 82900 | Median Reward: 42.39 | Max Reward: 48.88
Iteration: 2046 | Episodes: 83000 | Median Reward: 42.96 | Max Reward: 48.88
Iteration: 2049 | Episodes: 83100 | Median Reward: 40.07 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2050          |
|    time_elapsed         | 17454         |
|    total_timesteps      | 8396800       |
| train/                  |               |
|    approx_kl            | 3.1593343e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.53         |
|    n_updates            | 20490         |
|    policy_gradient_loss | -0.000177     |
|    std                  | 9.46          |
|    value_loss           | 0.813         |
-------------------------------------------
Iteration: 2051 | Episodes: 83200 | Median Reward: 42.02 | Max Reward: 48.88
Iteration: 2054 | Episodes: 83300 | Median Reward: 43.85 | Max Reward: 48.88
Iteration: 2056 | Episodes: 83400 | Median Reward: 42.36 | Max Reward: 48.88
Iteration: 2058 | Episodes: 83500 | Median Reward: 42.70 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.2         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 2060          |
|    time_elapsed         | 17542         |
|    total_timesteps      | 8437760       |
| train/                  |               |
|    approx_kl            | 0.00092686387 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.69         |
|    n_updates            | 20590         |
|    policy_gradient_loss | -0.00485      |
|    std                  | 9.54          |
|    value_loss           | 0.287         |
-------------------------------------------
Iteration: 2061 | Episodes: 83600 | Median Reward: 43.82 | Max Reward: 48.88
Iteration: 2063 | Episodes: 83700 | Median Reward: 43.42 | Max Reward: 48.88
Iteration: 2066 | Episodes: 83800 | Median Reward: 40.17 | Max Reward: 48.88
Iteration: 2068 | Episodes: 83900 | Median Reward: 44.08 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 2070         |
|    time_elapsed         | 17630        |
|    total_timesteps      | 8478720      |
| train/                  |              |
|    approx_kl            | 0.0001293594 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -136         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.66        |
|    n_updates            | 20690        |
|    policy_gradient_loss | -0.000351    |
|    std                  | 9.59         |
|    value_loss           | 0.498        |
------------------------------------------
Iteration: 2071 | Episodes: 84000 | Median Reward: 42.16 | Max Reward: 48.88
Iteration: 2073 | Episodes: 84100 | Median Reward: 42.76 | Max Reward: 48.88
Iteration: 2076 | Episodes: 84200 | Median Reward: 42.22 | Max Reward: 48.88
Iteration: 2078 | Episodes: 84300 | Median Reward: 42.82 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.4       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 2080        |
|    time_elapsed         | 17710       |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.004119278 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -136        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -6.64       |
|    n_updates            | 20790       |
|    policy_gradient_loss | -0.0117     |
|    std                  | 9.69        |
|    value_loss           | 0.334       |
-----------------------------------------
Iteration: 2081 | Episodes: 84400 | Median Reward: 42.87 | Max Reward: 48.88
Iteration: 2083 | Episodes: 84500 | Median Reward: 43.64 | Max Reward: 48.88
Iteration: 2086 | Episodes: 84600 | Median Reward: 40.14 | Max Reward: 48.88
Iteration: 2088 | Episodes: 84700 | Median Reward: 44.18 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.3         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 2090          |
|    time_elapsed         | 17803         |
|    total_timesteps      | 8560640       |
| train/                  |               |
|    approx_kl            | 0.00026516165 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.72         |
|    n_updates            | 20890         |
|    policy_gradient_loss | -0.00192      |
|    std                  | 9.76          |
|    value_loss           | 0.494         |
-------------------------------------------
Iteration: 2091 | Episodes: 84800 | Median Reward: 43.01 | Max Reward: 48.88
Iteration: 2093 | Episodes: 84900 | Median Reward: 42.22 | Max Reward: 48.88
Iteration: 2095 | Episodes: 85000 | Median Reward: 44.38 | Max Reward: 48.88
Iteration: 2098 | Episodes: 85100 | Median Reward: 43.45 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.8        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 2100         |
|    time_elapsed         | 17883        |
|    total_timesteps      | 8601600      |
| train/                  |              |
|    approx_kl            | 0.0010801548 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -137         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.76        |
|    n_updates            | 20990        |
|    policy_gradient_loss | -0.0039      |
|    std                  | 9.81         |
|    value_loss           | 0.339        |
------------------------------------------
Iteration: 2100 | Episodes: 85200 | Median Reward: 44.51 | Max Reward: 48.88
Iteration: 2103 | Episodes: 85300 | Median Reward: 44.33 | Max Reward: 48.88
Iteration: 2105 | Episodes: 85400 | Median Reward: 42.20 | Max Reward: 48.88
Iteration: 2108 | Episodes: 85500 | Median Reward: 43.43 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.3        |
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 2110         |
|    time_elapsed         | 17967        |
|    total_timesteps      | 8642560      |
| train/                  |              |
|    approx_kl            | 0.0022693048 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -137         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.7         |
|    n_updates            | 21090        |
|    policy_gradient_loss | -0.0113      |
|    std                  | 9.92         |
|    value_loss           | 0.283        |
------------------------------------------
Iteration: 2110 | Episodes: 85600 | Median Reward: 42.68 | Max Reward: 48.88
Iteration: 2113 | Episodes: 85700 | Median Reward: 41.73 | Max Reward: 48.88
Iteration: 2115 | Episodes: 85800 | Median Reward: 45.55 | Max Reward: 48.88
Iteration: 2118 | Episodes: 85900 | Median Reward: 41.07 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.9       |
| time/                   |             |
|    fps                  | 480         |
|    iterations           | 2120        |
|    time_elapsed         | 18054       |
|    total_timesteps      | 8683520     |
| train/                  |             |
|    approx_kl            | 0.001004837 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -137        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.76       |
|    n_updates            | 21190       |
|    policy_gradient_loss | -0.00673    |
|    std                  | 9.96        |
|    value_loss           | 0.329       |
-----------------------------------------
Iteration: 2120 | Episodes: 86000 | Median Reward: 43.34 | Max Reward: 48.88
Iteration: 2123 | Episodes: 86100 | Median Reward: 43.63 | Max Reward: 48.88
Iteration: 2125 | Episodes: 86200 | Median Reward: 43.55 | Max Reward: 48.88
Iteration: 2128 | Episodes: 86300 | Median Reward: 44.64 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.7         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2130          |
|    time_elapsed         | 18134         |
|    total_timesteps      | 8724480       |
| train/                  |               |
|    approx_kl            | 0.00031538118 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.69         |
|    n_updates            | 21290         |
|    policy_gradient_loss | -0.000881     |
|    std                  | 10            |
|    value_loss           | 0.395         |
-------------------------------------------
Iteration: 2130 | Episodes: 86400 | Median Reward: 42.59 | Max Reward: 48.88
Iteration: 2132 | Episodes: 86500 | Median Reward: 42.82 | Max Reward: 48.88
Iteration: 2135 | Episodes: 86600 | Median Reward: 44.25 | Max Reward: 48.88
Iteration: 2137 | Episodes: 86700 | Median Reward: 43.00 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.9         |
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 2140          |
|    time_elapsed         | 18227         |
|    total_timesteps      | 8765440       |
| train/                  |               |
|    approx_kl            | 2.2490844e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.77         |
|    n_updates            | 21390         |
|    policy_gradient_loss | -8.42e-05     |
|    std                  | 10.1          |
|    value_loss           | 0.203         |
-------------------------------------------
Iteration: 2140 | Episodes: 86800 | Median Reward: 42.84 | Max Reward: 48.88
Iteration: 2142 | Episodes: 86900 | Median Reward: 43.28 | Max Reward: 48.88
Iteration: 2145 | Episodes: 87000 | Median Reward: 43.72 | Max Reward: 48.88
Iteration: 2147 | Episodes: 87100 | Median Reward: 46.04 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.7         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2150          |
|    time_elapsed         | 18304         |
|    total_timesteps      | 8806400       |
| train/                  |               |
|    approx_kl            | 0.00020464213 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.67         |
|    n_updates            | 21490         |
|    policy_gradient_loss | -0.00214      |
|    std                  | 10.2          |
|    value_loss           | 0.74          |
-------------------------------------------
Iteration: 2150 | Episodes: 87200 | Median Reward: 42.93 | Max Reward: 48.88
Iteration: 2152 | Episodes: 87300 | Median Reward: 43.57 | Max Reward: 48.88
Iteration: 2155 | Episodes: 87400 | Median Reward: 43.42 | Max Reward: 48.88
Iteration: 2157 | Episodes: 87500 | Median Reward: 41.51 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.2         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2160          |
|    time_elapsed         | 18389         |
|    total_timesteps      | 8847360       |
| train/                  |               |
|    approx_kl            | 0.00040483635 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -138          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.84         |
|    n_updates            | 21590         |
|    policy_gradient_loss | -0.00117      |
|    std                  | 10.3          |
|    value_loss           | 0.132         |
-------------------------------------------
Iteration: 2160 | Episodes: 87600 | Median Reward: 43.70 | Max Reward: 48.88
Iteration: 2162 | Episodes: 87700 | Median Reward: 43.91 | Max Reward: 48.88
Iteration: 2164 | Episodes: 87800 | Median Reward: 42.70 | Max Reward: 48.88
Iteration: 2167 | Episodes: 87900 | Median Reward: 44.84 | Max Reward: 48.88
Iteration: 2169 | Episodes: 88000 | Median Reward: 43.68 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.1         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2170          |
|    time_elapsed         | 18471         |
|    total_timesteps      | 8888320       |
| train/                  |               |
|    approx_kl            | 5.1536204e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -138          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.75         |
|    n_updates            | 21690         |
|    policy_gradient_loss | -0.000177     |
|    std                  | 10.4          |
|    value_loss           | 0.348         |
-------------------------------------------
Iteration: 2172 | Episodes: 88100 | Median Reward: 46.05 | Max Reward: 48.88
Iteration: 2174 | Episodes: 88200 | Median Reward: 42.88 | Max Reward: 48.88
Iteration: 2177 | Episodes: 88300 | Median Reward: 42.75 | Max Reward: 48.88
Iteration: 2179 | Episodes: 88400 | Median Reward: 43.33 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 2180         |
|    time_elapsed         | 18548        |
|    total_timesteps      | 8929280      |
| train/                  |              |
|    approx_kl            | 0.0006118881 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -138         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.76        |
|    n_updates            | 21790        |
|    policy_gradient_loss | -0.00121     |
|    std                  | 10.5         |
|    value_loss           | 0.183        |
------------------------------------------
Iteration: 2182 | Episodes: 88500 | Median Reward: 43.40 | Max Reward: 48.88
Iteration: 2184 | Episodes: 88600 | Median Reward: 43.56 | Max Reward: 48.88
Iteration: 2187 | Episodes: 88700 | Median Reward: 43.69 | Max Reward: 48.88
Iteration: 2189 | Episodes: 88800 | Median Reward: 43.17 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.4       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 2190        |
|    time_elapsed         | 18638       |
|    total_timesteps      | 8970240     |
| train/                  |             |
|    approx_kl            | 0.000246972 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -138        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -5.74       |
|    n_updates            | 21890       |
|    policy_gradient_loss | -0.00239    |
|    std                  | 10.6        |
|    value_loss           | 0.874       |
-----------------------------------------
Iteration: 2192 | Episodes: 88900 | Median Reward: 43.29 | Max Reward: 48.88
Iteration: 2194 | Episodes: 89000 | Median Reward: 42.15 | Max Reward: 48.88
Iteration: 2197 | Episodes: 89100 | Median Reward: 40.32 | Max Reward: 48.88
Iteration: 2199 | Episodes: 89200 | Median Reward: 43.27 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.6         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2200          |
|    time_elapsed         | 18713         |
|    total_timesteps      | 9011200       |
| train/                  |               |
|    approx_kl            | 0.00018353079 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -139          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.86         |
|    n_updates            | 21990         |
|    policy_gradient_loss | -8.66e-05     |
|    std                  | 10.7          |
|    value_loss           | 0.407         |
-------------------------------------------
Iteration: 2201 | Episodes: 89300 | Median Reward: 42.80 | Max Reward: 48.88
Iteration: 2204 | Episodes: 89400 | Median Reward: 44.00 | Max Reward: 48.88
Iteration: 2206 | Episodes: 89500 | Median Reward: 43.29 | Max Reward: 48.88
Iteration: 2209 | Episodes: 89600 | Median Reward: 46.23 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.5        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 2210         |
|    time_elapsed         | 18799        |
|    total_timesteps      | 9052160      |
| train/                  |              |
|    approx_kl            | 0.0010326754 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -139         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.79        |
|    n_updates            | 22090        |
|    policy_gradient_loss | -0.00487     |
|    std                  | 10.7         |
|    value_loss           | 0.293        |
------------------------------------------
Iteration: 2211 | Episodes: 89700 | Median Reward: 44.09 | Max Reward: 48.88
Iteration: 2214 | Episodes: 89800 | Median Reward: 45.72 | Max Reward: 48.88
Iteration: 2216 | Episodes: 89900 | Median Reward: 39.50 | Max Reward: 48.88
Iteration: 2219 | Episodes: 90000 | Median Reward: 42.82 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.3         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2220          |
|    time_elapsed         | 18880         |
|    total_timesteps      | 9093120       |
| train/                  |               |
|    approx_kl            | 0.00039802698 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -139          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.88         |
|    n_updates            | 22190         |
|    policy_gradient_loss | -0.00399      |
|    std                  | 10.8          |
|    value_loss           | 0.892         |
-------------------------------------------
Iteration: 2221 | Episodes: 90100 | Median Reward: 42.12 | Max Reward: 48.88
Iteration: 2224 | Episodes: 90200 | Median Reward: 43.29 | Max Reward: 48.88
Iteration: 2226 | Episodes: 90300 | Median Reward: 46.07 | Max Reward: 48.88
Iteration: 2229 | Episodes: 90400 | Median Reward: 43.85 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 2230         |
|    time_elapsed         | 18958        |
|    total_timesteps      | 9134080      |
| train/                  |              |
|    approx_kl            | 0.0002494355 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -139         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.66        |
|    n_updates            | 22290        |
|    policy_gradient_loss | -0.000915    |
|    std                  | 10.9         |
|    value_loss           | 0.617        |
------------------------------------------
Iteration: 2231 | Episodes: 90500 | Median Reward: 42.98 | Max Reward: 48.88
Iteration: 2234 | Episodes: 90600 | Median Reward: 40.33 | Max Reward: 48.88
Iteration: 2236 | Episodes: 90700 | Median Reward: 43.35 | Max Reward: 48.88
Iteration: 2238 | Episodes: 90800 | Median Reward: 40.91 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58           |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2240          |
|    time_elapsed         | 19049         |
|    total_timesteps      | 9175040       |
| train/                  |               |
|    approx_kl            | 0.00013724313 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -139          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.7          |
|    n_updates            | 22390         |
|    policy_gradient_loss | -0.000316     |
|    std                  | 11            |
|    value_loss           | 0.447         |
-------------------------------------------
Iteration: 2241 | Episodes: 90900 | Median Reward: 43.38 | Max Reward: 48.88
Iteration: 2243 | Episodes: 91000 | Median Reward: 44.05 | Max Reward: 48.88
Iteration: 2246 | Episodes: 91100 | Median Reward: 43.18 | Max Reward: 48.88
Iteration: 2248 | Episodes: 91200 | Median Reward: 42.77 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2250          |
|    time_elapsed         | 19127         |
|    total_timesteps      | 9216000       |
| train/                  |               |
|    approx_kl            | 0.00056228845 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -139          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.77         |
|    n_updates            | 22490         |
|    policy_gradient_loss | -0.00322      |
|    std                  | 11.1          |
|    value_loss           | 0.379         |
-------------------------------------------
Iteration: 2251 | Episodes: 91300 | Median Reward: 42.75 | Max Reward: 48.88
Iteration: 2253 | Episodes: 91400 | Median Reward: 42.78 | Max Reward: 48.88
Iteration: 2256 | Episodes: 91500 | Median Reward: 42.48 | Max Reward: 48.88
Iteration: 2258 | Episodes: 91600 | Median Reward: 41.64 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.1       |
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 2260        |
|    time_elapsed         | 19214       |
|    total_timesteps      | 9256960     |
| train/                  |             |
|    approx_kl            | 0.004282833 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -139        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -6.83       |
|    n_updates            | 22590       |
|    policy_gradient_loss | -0.0156     |
|    std                  | 11.1        |
|    value_loss           | 0.446       |
-----------------------------------------
Iteration: 2261 | Episodes: 91700 | Median Reward: 45.69 | Max Reward: 48.88
Iteration: 2263 | Episodes: 91800 | Median Reward: 43.87 | Max Reward: 48.88
Iteration: 2266 | Episodes: 91900 | Median Reward: 44.25 | Max Reward: 48.88
Iteration: 2268 | Episodes: 92000 | Median Reward: 43.91 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.6        |
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 2270         |
|    time_elapsed         | 19294        |
|    total_timesteps      | 9297920      |
| train/                  |              |
|    approx_kl            | 0.0012927111 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -139         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.9         |
|    n_updates            | 22690        |
|    policy_gradient_loss | -0.00779     |
|    std                  | 11.2         |
|    value_loss           | 0.437        |
------------------------------------------
Iteration: 2271 | Episodes: 92100 | Median Reward: 45.75 | Max Reward: 48.88
Iteration: 2273 | Episodes: 92200 | Median Reward: 42.28 | Max Reward: 48.88
Iteration: 2275 | Episodes: 92300 | Median Reward: 42.80 | Max Reward: 48.88
Iteration: 2278 | Episodes: 92400 | Median Reward: 42.81 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60          |
| time/                   |              |
|    fps                  | 482          |
|    iterations           | 2280         |
|    time_elapsed         | 19370        |
|    total_timesteps      | 9338880      |
| train/                  |              |
|    approx_kl            | 0.0024508801 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -140         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.63        |
|    n_updates            | 22790        |
|    policy_gradient_loss | -0.00128     |
|    std                  | 11.3         |
|    value_loss           | 0.59         |
------------------------------------------
Iteration: 2280 | Episodes: 92500 | Median Reward: 38.98 | Max Reward: 48.88
Iteration: 2283 | Episodes: 92600 | Median Reward: 42.48 | Max Reward: 48.88
Iteration: 2285 | Episodes: 92700 | Median Reward: 43.09 | Max Reward: 48.88
Iteration: 2288 | Episodes: 92800 | Median Reward: 43.67 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.4         |
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 2290          |
|    time_elapsed         | 19461         |
|    total_timesteps      | 9379840       |
| train/                  |               |
|    approx_kl            | 0.00016042042 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -140          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.93         |
|    n_updates            | 22890         |
|    policy_gradient_loss | -0.000935     |
|    std                  | 11.4          |
|    value_loss           | 0.202         |
-------------------------------------------
Iteration: 2290 | Episodes: 92900 | Median Reward: 43.77 | Max Reward: 48.88
Iteration: 2293 | Episodes: 93000 | Median Reward: 43.49 | Max Reward: 48.88
Iteration: 2295 | Episodes: 93100 | Median Reward: 37.70 | Max Reward: 48.88
Iteration: 2298 | Episodes: 93200 | Median Reward: 42.54 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.3        |
| time/                   |              |
|    fps                  | 482          |
|    iterations           | 2300         |
|    time_elapsed         | 19540        |
|    total_timesteps      | 9420800      |
| train/                  |              |
|    approx_kl            | 9.667367e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -140         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.84        |
|    n_updates            | 22990        |
|    policy_gradient_loss | -0.000666    |
|    std                  | 11.5         |
|    value_loss           | 0.561        |
------------------------------------------
Iteration: 2300 | Episodes: 93300 | Median Reward: 42.02 | Max Reward: 48.88
Iteration: 2303 | Episodes: 93400 | Median Reward: 46.75 | Max Reward: 48.88
Iteration: 2305 | Episodes: 93500 | Median Reward: 43.11 | Max Reward: 48.88
Iteration: 2308 | Episodes: 93600 | Median Reward: 44.12 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.6         |
| time/                   |               |
|    fps                  | 482           |
|    iterations           | 2310          |
|    time_elapsed         | 19627         |
|    total_timesteps      | 9461760       |
| train/                  |               |
|    approx_kl            | 1.4885809e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -140          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.72         |
|    n_updates            | 23090         |
|    policy_gradient_loss | 8.89e-06      |
|    std                  | 11.6          |
|    value_loss           | 0.429         |
-------------------------------------------
Iteration: 2310 | Episodes: 93700 | Median Reward: 42.33 | Max Reward: 48.88
Iteration: 2312 | Episodes: 93800 | Median Reward: 43.45 | Max Reward: 48.88
Iteration: 2315 | Episodes: 93900 | Median Reward: 43.37 | Max Reward: 48.88
Iteration: 2317 | Episodes: 94000 | Median Reward: 43.61 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.1         |
| time/                   |               |
|    fps                  | 483           |
|    iterations           | 2320          |
|    time_elapsed         | 19671         |
|    total_timesteps      | 9502720       |
| train/                  |               |
|    approx_kl            | 0.00035791533 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -140          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.83         |
|    n_updates            | 23190         |
|    policy_gradient_loss | -0.00159      |
|    std                  | 11.7          |
|    value_loss           | 0.167         |
-------------------------------------------
Iteration: 2320 | Episodes: 94100 | Median Reward: 41.56 | Max Reward: 48.88
Iteration: 2322 | Episodes: 94200 | Median Reward: 43.38 | Max Reward: 48.88
Iteration: 2325 | Episodes: 94300 | Median Reward: 43.55 | Max Reward: 48.88
Iteration: 2327 | Episodes: 94400 | Median Reward: 42.25 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.2         |
| time/                   |               |
|    fps                  | 484           |
|    iterations           | 2330          |
|    time_elapsed         | 19708         |
|    total_timesteps      | 9543680       |
| train/                  |               |
|    approx_kl            | 0.00039015664 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.88         |
|    n_updates            | 23290         |
|    policy_gradient_loss | -3.64e-05     |
|    std                  | 11.8          |
|    value_loss           | 0.436         |
-------------------------------------------
Iteration: 2330 | Episodes: 94500 | Median Reward: 42.69 | Max Reward: 48.88
Iteration: 2332 | Episodes: 94600 | Median Reward: 42.83 | Max Reward: 48.88
Iteration: 2335 | Episodes: 94700 | Median Reward: 40.04 | Max Reward: 48.88
Iteration: 2337 | Episodes: 94800 | Median Reward: 42.90 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58           |
| time/                   |               |
|    fps                  | 485           |
|    iterations           | 2340          |
|    time_elapsed         | 19746         |
|    total_timesteps      | 9584640       |
| train/                  |               |
|    approx_kl            | 0.00033130433 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.99         |
|    n_updates            | 23390         |
|    policy_gradient_loss | -0.00136      |
|    std                  | 11.9          |
|    value_loss           | 0.149         |
-------------------------------------------
Iteration: 2340 | Episodes: 94900 | Median Reward: 43.08 | Max Reward: 48.88
Iteration: 2342 | Episodes: 95000 | Median Reward: 42.62 | Max Reward: 48.88
Iteration: 2344 | Episodes: 95100 | Median Reward: 42.33 | Max Reward: 48.88
Iteration: 2347 | Episodes: 95200 | Median Reward: 42.91 | Max Reward: 48.88
Iteration: 2349 | Episodes: 95300 | Median Reward: 43.74 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.3         |
| time/                   |               |
|    fps                  | 486           |
|    iterations           | 2350          |
|    time_elapsed         | 19784         |
|    total_timesteps      | 9625600       |
| train/                  |               |
|    approx_kl            | 0.00015199023 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.87         |
|    n_updates            | 23490         |
|    policy_gradient_loss | -0.00109      |
|    std                  | 12            |
|    value_loss           | 0.512         |
-------------------------------------------
Iteration: 2352 | Episodes: 95400 | Median Reward: 41.02 | Max Reward: 48.88
Iteration: 2354 | Episodes: 95500 | Median Reward: 42.71 | Max Reward: 48.88
Iteration: 2357 | Episodes: 95600 | Median Reward: 46.13 | Max Reward: 48.88
Iteration: 2359 | Episodes: 95700 | Median Reward: 38.13 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.9         |
| time/                   |               |
|    fps                  | 487           |
|    iterations           | 2360          |
|    time_elapsed         | 19821         |
|    total_timesteps      | 9666560       |
| train/                  |               |
|    approx_kl            | 0.00019413314 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.6          |
|    n_updates            | 23590         |
|    policy_gradient_loss | -0.000935     |
|    std                  | 12.1          |
|    value_loss           | 1.5           |
-------------------------------------------
Iteration: 2362 | Episodes: 95800 | Median Reward: 42.40 | Max Reward: 48.88
Iteration: 2364 | Episodes: 95900 | Median Reward: 42.46 | Max Reward: 48.88
Iteration: 2367 | Episodes: 96000 | Median Reward: 43.46 | Max Reward: 48.88
Iteration: 2369 | Episodes: 96100 | Median Reward: 43.37 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 488           |
|    iterations           | 2370          |
|    time_elapsed         | 19859         |
|    total_timesteps      | 9707520       |
| train/                  |               |
|    approx_kl            | 0.00027397912 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.01         |
|    n_updates            | 23690         |
|    policy_gradient_loss | -0.00173      |
|    std                  | 12.2          |
|    value_loss           | 0.152         |
-------------------------------------------
Iteration: 2372 | Episodes: 96200 | Median Reward: 42.24 | Max Reward: 48.88
Iteration: 2374 | Episodes: 96300 | Median Reward: 43.01 | Max Reward: 48.88
Iteration: 2377 | Episodes: 96400 | Median Reward: 43.09 | Max Reward: 48.88
Iteration: 2379 | Episodes: 96500 | Median Reward: 42.28 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.2        |
| time/                   |              |
|    fps                  | 489          |
|    iterations           | 2380         |
|    time_elapsed         | 19897        |
|    total_timesteps      | 9748480      |
| train/                  |              |
|    approx_kl            | 0.0013347301 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -142         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.94        |
|    n_updates            | 23790        |
|    policy_gradient_loss | -0.00341     |
|    std                  | 12.3         |
|    value_loss           | 0.17         |
------------------------------------------
Iteration: 2381 | Episodes: 96600 | Median Reward: 43.02 | Max Reward: 48.88
Iteration: 2384 | Episodes: 96700 | Median Reward: 43.12 | Max Reward: 48.88
Iteration: 2386 | Episodes: 96800 | Median Reward: 43.66 | Max Reward: 48.88
Iteration: 2389 | Episodes: 96900 | Median Reward: 42.18 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.5       |
| time/                   |             |
|    fps                  | 491         |
|    iterations           | 2390        |
|    time_elapsed         | 19934       |
|    total_timesteps      | 9789440     |
| train/                  |             |
|    approx_kl            | 0.002200909 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -142        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.03       |
|    n_updates            | 23890       |
|    policy_gradient_loss | -0.00687    |
|    std                  | 12.4        |
|    value_loss           | 0.262       |
-----------------------------------------
Iteration: 2391 | Episodes: 97000 | Median Reward: 43.27 | Max Reward: 48.88
Iteration: 2394 | Episodes: 97100 | Median Reward: 42.24 | Max Reward: 48.88
Iteration: 2396 | Episodes: 97200 | Median Reward: 40.82 | Max Reward: 48.88
Iteration: 2399 | Episodes: 97300 | Median Reward: 43.10 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.9        |
| time/                   |              |
|    fps                  | 492          |
|    iterations           | 2400         |
|    time_elapsed         | 19972        |
|    total_timesteps      | 9830400      |
| train/                  |              |
|    approx_kl            | 0.0010040498 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -142         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.99        |
|    n_updates            | 23990        |
|    policy_gradient_loss | -0.0039      |
|    std                  | 12.6         |
|    value_loss           | 0.346        |
------------------------------------------
Iteration: 2401 | Episodes: 97400 | Median Reward: 42.15 | Max Reward: 48.88
Iteration: 2404 | Episodes: 97500 | Median Reward: 42.89 | Max Reward: 48.88
Iteration: 2406 | Episodes: 97600 | Median Reward: 42.37 | Max Reward: 48.88
Iteration: 2409 | Episodes: 97700 | Median Reward: 42.82 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.4        |
| time/                   |              |
|    fps                  | 493          |
|    iterations           | 2410         |
|    time_elapsed         | 20009        |
|    total_timesteps      | 9871360      |
| train/                  |              |
|    approx_kl            | 0.0027638942 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -142         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.88        |
|    n_updates            | 24090        |
|    policy_gradient_loss | -0.00619     |
|    std                  | 12.7         |
|    value_loss           | 0.646        |
------------------------------------------
Iteration: 2411 | Episodes: 97800 | Median Reward: 42.98 | Max Reward: 48.88
Iteration: 2414 | Episodes: 97900 | Median Reward: 43.02 | Max Reward: 48.88
Iteration: 2416 | Episodes: 98000 | Median Reward: 42.42 | Max Reward: 48.88
Iteration: 2418 | Episodes: 98100 | Median Reward: 43.69 | Max Reward: 48.88
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -59.2      |
| time/                   |            |
|    fps                  | 494        |
|    iterations           | 2420       |
|    time_elapsed         | 20047      |
|    total_timesteps      | 9912320    |
| train/                  |            |
|    approx_kl            | 0.00150202 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -142       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -6.93      |
|    n_updates            | 24190      |
|    policy_gradient_loss | -0.0048    |
|    std                  | 12.7       |
|    value_loss           | 0.479      |
----------------------------------------
Iteration: 2421 | Episodes: 98200 | Median Reward: 42.78 | Max Reward: 48.88
Iteration: 2423 | Episodes: 98300 | Median Reward: 42.18 | Max Reward: 48.88
Iteration: 2426 | Episodes: 98400 | Median Reward: 43.47 | Max Reward: 48.88
Iteration: 2428 | Episodes: 98500 | Median Reward: 42.79 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.1        |
| time/                   |              |
|    fps                  | 495          |
|    iterations           | 2430         |
|    time_elapsed         | 20084        |
|    total_timesteps      | 9953280      |
| train/                  |              |
|    approx_kl            | 0.0023889844 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -143         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.93        |
|    n_updates            | 24290        |
|    policy_gradient_loss | -0.00931     |
|    std                  | 12.9         |
|    value_loss           | 0.386        |
------------------------------------------
Iteration: 2431 | Episodes: 98600 | Median Reward: 44.03 | Max Reward: 48.88
Iteration: 2433 | Episodes: 98700 | Median Reward: 42.91 | Max Reward: 48.88
Iteration: 2436 | Episodes: 98800 | Median Reward: 45.71 | Max Reward: 48.88
Iteration: 2438 | Episodes: 98900 | Median Reward: 43.84 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.4         |
| time/                   |               |
|    fps                  | 496           |
|    iterations           | 2440          |
|    time_elapsed         | 20122         |
|    total_timesteps      | 9994240       |
| train/                  |               |
|    approx_kl            | 0.00060918485 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -143          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.98         |
|    n_updates            | 24390         |
|    policy_gradient_loss | -0.00195      |
|    std                  | 13.1          |
|    value_loss           | 0.284         |
-------------------------------------------
Iteration: 2441 | Episodes: 99000 | Median Reward: 42.75 | Max Reward: 48.88
Iteration: 2443 | Episodes: 99100 | Median Reward: 43.31 | Max Reward: 48.88
Iteration: 2446 | Episodes: 99200 | Median Reward: 42.74 | Max Reward: 48.88
Iteration: 2448 | Episodes: 99300 | Median Reward: 41.77 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.5       |
| time/                   |             |
|    fps                  | 497         |
|    iterations           | 2450        |
|    time_elapsed         | 20159       |
|    total_timesteps      | 10035200    |
| train/                  |             |
|    approx_kl            | 0.000216694 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -143        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.07       |
|    n_updates            | 24490       |
|    policy_gradient_loss | -0.00145    |
|    std                  | 13.1        |
|    value_loss           | 0.353       |
-----------------------------------------
Iteration: 2451 | Episodes: 99400 | Median Reward: 41.78 | Max Reward: 48.88
Iteration: 2453 | Episodes: 99500 | Median Reward: 42.57 | Max Reward: 48.88
Iteration: 2455 | Episodes: 99600 | Median Reward: 43.59 | Max Reward: 48.88
Iteration: 2458 | Episodes: 99700 | Median Reward: 41.89 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 498          |
|    iterations           | 2460         |
|    time_elapsed         | 20197        |
|    total_timesteps      | 10076160     |
| train/                  |              |
|    approx_kl            | 0.0069830907 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -143         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.03        |
|    n_updates            | 24590        |
|    policy_gradient_loss | -0.0185      |
|    std                  | 13.2         |
|    value_loss           | 0.309        |
------------------------------------------
Iteration: 2460 | Episodes: 99800 | Median Reward: 40.27 | Max Reward: 48.88
Iteration: 2463 | Episodes: 99900 | Median Reward: 42.25 | Max Reward: 48.88
Iteration: 2465 | Episodes: 100000 | Median Reward: 43.00 | Max Reward: 48.88
Iteration: 2468 | Episodes: 100100 | Median Reward: 43.15 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 499           |
|    iterations           | 2470          |
|    time_elapsed         | 20234         |
|    total_timesteps      | 10117120      |
| train/                  |               |
|    approx_kl            | 1.8387072e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -143          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.01         |
|    n_updates            | 24690         |
|    policy_gradient_loss | 1.53e-05      |
|    std                  | 13.4          |
|    value_loss           | 0.398         |
-------------------------------------------
Iteration: 2470 | Episodes: 100200 | Median Reward: 43.22 | Max Reward: 48.88
Iteration: 2473 | Episodes: 100300 | Median Reward: 43.65 | Max Reward: 48.88
Iteration: 2475 | Episodes: 100400 | Median Reward: 43.46 | Max Reward: 48.88
Iteration: 2478 | Episodes: 100500 | Median Reward: 42.39 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.5       |
| time/                   |             |
|    fps                  | 501         |
|    iterations           | 2480        |
|    time_elapsed         | 20272       |
|    total_timesteps      | 10158080    |
| train/                  |             |
|    approx_kl            | 0.008091409 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -143        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.07       |
|    n_updates            | 24790       |
|    policy_gradient_loss | -0.0255     |
|    std                  | 13.4        |
|    value_loss           | 0.357       |
-----------------------------------------
Iteration: 2480 | Episodes: 100600 | Median Reward: 41.73 | Max Reward: 48.88
Iteration: 2483 | Episodes: 100700 | Median Reward: 42.29 | Max Reward: 48.88
Iteration: 2485 | Episodes: 100800 | Median Reward: 42.41 | Max Reward: 48.88
Iteration: 2488 | Episodes: 100900 | Median Reward: 40.44 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.7         |
| time/                   |               |
|    fps                  | 502           |
|    iterations           | 2490          |
|    time_elapsed         | 20310         |
|    total_timesteps      | 10199040      |
| train/                  |               |
|    approx_kl            | 0.00011131063 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -143          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.07         |
|    n_updates            | 24890         |
|    policy_gradient_loss | -0.00059      |
|    std                  | 13.5          |
|    value_loss           | 0.411         |
-------------------------------------------
Iteration: 2490 | Episodes: 101000 | Median Reward: 43.37 | Max Reward: 48.88
Iteration: 2492 | Episodes: 101100 | Median Reward: 42.53 | Max Reward: 48.88
Iteration: 2495 | Episodes: 101200 | Median Reward: 43.25 | Max Reward: 48.88
Iteration: 2497 | Episodes: 101300 | Median Reward: 43.01 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.1         |
| time/                   |               |
|    fps                  | 503           |
|    iterations           | 2500          |
|    time_elapsed         | 20348         |
|    total_timesteps      | 10240000      |
| train/                  |               |
|    approx_kl            | 0.00023599582 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -144          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.11         |
|    n_updates            | 24990         |
|    policy_gradient_loss | -0.000785     |
|    std                  | 13.6          |
|    value_loss           | 0.279         |
-------------------------------------------
Iteration: 2500 | Episodes: 101400 | Median Reward: 42.48 | Max Reward: 48.88
Iteration: 2502 | Episodes: 101500 | Median Reward: 42.95 | Max Reward: 48.88
Iteration: 2505 | Episodes: 101600 | Median Reward: 43.57 | Max Reward: 48.88
Iteration: 2507 | Episodes: 101700 | Median Reward: 44.47 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 504          |
|    iterations           | 2510         |
|    time_elapsed         | 20385        |
|    total_timesteps      | 10280960     |
| train/                  |              |
|    approx_kl            | 0.0020237835 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.7         |
|    n_updates            | 25090        |
|    policy_gradient_loss | -0.00784     |
|    std                  | 13.8         |
|    value_loss           | 0.425        |
------------------------------------------
Iteration: 2510 | Episodes: 101800 | Median Reward: 44.24 | Max Reward: 48.88
Iteration: 2512 | Episodes: 101900 | Median Reward: 39.69 | Max Reward: 48.88
Iteration: 2515 | Episodes: 102000 | Median Reward: 38.90 | Max Reward: 48.88
Iteration: 2517 | Episodes: 102100 | Median Reward: 39.78 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.9         |
| time/                   |               |
|    fps                  | 505           |
|    iterations           | 2520          |
|    time_elapsed         | 20423         |
|    total_timesteps      | 10321920      |
| train/                  |               |
|    approx_kl            | 0.00017671868 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -144          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.1          |
|    n_updates            | 25190         |
|    policy_gradient_loss | -0.00257      |
|    std                  | 13.9          |
|    value_loss           | 0.61          |
-------------------------------------------
Iteration: 2520 | Episodes: 102200 | Median Reward: 40.77 | Max Reward: 48.88
Iteration: 2522 | Episodes: 102300 | Median Reward: 38.89 | Max Reward: 48.88
Iteration: 2524 | Episodes: 102400 | Median Reward: 42.54 | Max Reward: 48.88
Iteration: 2527 | Episodes: 102500 | Median Reward: 42.14 | Max Reward: 48.88
Iteration: 2529 | Episodes: 102600 | Median Reward: 43.67 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 506          |
|    iterations           | 2530         |
|    time_elapsed         | 20460        |
|    total_timesteps      | 10362880     |
| train/                  |              |
|    approx_kl            | 0.0005607378 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.09        |
|    n_updates            | 25290        |
|    policy_gradient_loss | -0.00193     |
|    std                  | 14           |
|    value_loss           | 0.241        |
------------------------------------------
Iteration: 2532 | Episodes: 102700 | Median Reward: 42.52 | Max Reward: 48.88
Iteration: 2534 | Episodes: 102800 | Median Reward: 38.92 | Max Reward: 48.88
Iteration: 2537 | Episodes: 102900 | Median Reward: 43.45 | Max Reward: 48.88
Iteration: 2539 | Episodes: 103000 | Median Reward: 42.43 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.5        |
| time/                   |              |
|    fps                  | 507          |
|    iterations           | 2540         |
|    time_elapsed         | 20498        |
|    total_timesteps      | 10403840     |
| train/                  |              |
|    approx_kl            | 3.297339e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.07        |
|    n_updates            | 25390        |
|    policy_gradient_loss | -0.000308    |
|    std                  | 14           |
|    value_loss           | 0.505        |
------------------------------------------
Iteration: 2542 | Episodes: 103100 | Median Reward: 43.52 | Max Reward: 48.88
Iteration: 2544 | Episodes: 103200 | Median Reward: 44.28 | Max Reward: 48.88
Iteration: 2547 | Episodes: 103300 | Median Reward: 40.57 | Max Reward: 48.88
Iteration: 2549 | Episodes: 103400 | Median Reward: 41.66 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.7        |
| time/                   |              |
|    fps                  | 508          |
|    iterations           | 2550         |
|    time_elapsed         | 20535        |
|    total_timesteps      | 10444800     |
| train/                  |              |
|    approx_kl            | 0.0018841317 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.08        |
|    n_updates            | 25490        |
|    policy_gradient_loss | -0.00533     |
|    std                  | 14.2         |
|    value_loss           | 0.209        |
------------------------------------------
Iteration: 2552 | Episodes: 103500 | Median Reward: 42.82 | Max Reward: 48.88
Iteration: 2554 | Episodes: 103600 | Median Reward: 43.29 | Max Reward: 48.88
Iteration: 2557 | Episodes: 103700 | Median Reward: 41.82 | Max Reward: 48.88
Iteration: 2559 | Episodes: 103800 | Median Reward: 42.43 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.3        |
| time/                   |              |
|    fps                  | 509          |
|    iterations           | 2560         |
|    time_elapsed         | 20573        |
|    total_timesteps      | 10485760     |
| train/                  |              |
|    approx_kl            | 0.0010185335 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -145         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.02        |
|    n_updates            | 25590        |
|    policy_gradient_loss | -0.00456     |
|    std                  | 14.3         |
|    value_loss           | 0.391        |
------------------------------------------
Iteration: 2561 | Episodes: 103900 | Median Reward: 42.96 | Max Reward: 48.88
Iteration: 2564 | Episodes: 104000 | Median Reward: 39.76 | Max Reward: 48.88
Iteration: 2566 | Episodes: 104100 | Median Reward: 42.40 | Max Reward: 48.88
Iteration: 2569 | Episodes: 104200 | Median Reward: 40.26 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.2         |
| time/                   |               |
|    fps                  | 510           |
|    iterations           | 2570          |
|    time_elapsed         | 20610         |
|    total_timesteps      | 10526720      |
| train/                  |               |
|    approx_kl            | 0.00042957132 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -145          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.93         |
|    n_updates            | 25690         |
|    policy_gradient_loss | -0.00239      |
|    std                  | 14.4          |
|    value_loss           | 0.611         |
-------------------------------------------
Iteration: 2571 | Episodes: 104300 | Median Reward: 42.18 | Max Reward: 48.88
Iteration: 2574 | Episodes: 104400 | Median Reward: 43.43 | Max Reward: 48.88
Iteration: 2576 | Episodes: 104500 | Median Reward: 42.96 | Max Reward: 48.88
Iteration: 2579 | Episodes: 104600 | Median Reward: 43.45 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.6         |
| time/                   |               |
|    fps                  | 511           |
|    iterations           | 2580          |
|    time_elapsed         | 20648         |
|    total_timesteps      | 10567680      |
| train/                  |               |
|    approx_kl            | 5.5338576e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -145          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.16         |
|    n_updates            | 25790         |
|    policy_gradient_loss | -0.000475     |
|    std                  | 14.5          |
|    value_loss           | 0.274         |
-------------------------------------------
Iteration: 2581 | Episodes: 104700 | Median Reward: 42.99 | Max Reward: 48.88
Iteration: 2584 | Episodes: 104800 | Median Reward: 42.69 | Max Reward: 48.88
Iteration: 2586 | Episodes: 104900 | Median Reward: 42.99 | Max Reward: 48.88
Iteration: 2589 | Episodes: 105000 | Median Reward: 42.84 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.5         |
| time/                   |               |
|    fps                  | 512           |
|    iterations           | 2590          |
|    time_elapsed         | 20686         |
|    total_timesteps      | 10608640      |
| train/                  |               |
|    approx_kl            | 0.00034915938 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -145          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -6.61         |
|    n_updates            | 25890         |
|    policy_gradient_loss | -0.00312      |
|    std                  | 14.6          |
|    value_loss           | 0.573         |
-------------------------------------------
Iteration: 2591 | Episodes: 105100 | Median Reward: 43.51 | Max Reward: 48.88
Iteration: 2594 | Episodes: 105200 | Median Reward: 42.20 | Max Reward: 48.88
Iteration: 2596 | Episodes: 105300 | Median Reward: 44.16 | Max Reward: 48.88
Iteration: 2598 | Episodes: 105400 | Median Reward: 41.43 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.2        |
| time/                   |              |
|    fps                  | 513          |
|    iterations           | 2600         |
|    time_elapsed         | 20724        |
|    total_timesteps      | 10649600     |
| train/                  |              |
|    approx_kl            | 0.0006508672 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -145         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.89        |
|    n_updates            | 25990        |
|    policy_gradient_loss | -0.004       |
|    std                  | 14.8         |
|    value_loss           | 0.381        |
------------------------------------------
Iteration: 2601 | Episodes: 105500 | Median Reward: 38.88 | Max Reward: 48.88
Iteration: 2603 | Episodes: 105600 | Median Reward: 43.11 | Max Reward: 48.88
Iteration: 2606 | Episodes: 105700 | Median Reward: 39.49 | Max Reward: 48.88
Iteration: 2608 | Episodes: 105800 | Median Reward: 42.79 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.3        |
| time/                   |              |
|    fps                  | 514          |
|    iterations           | 2610         |
|    time_elapsed         | 20762        |
|    total_timesteps      | 10690560     |
| train/                  |              |
|    approx_kl            | 0.0009190303 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -146         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.09        |
|    n_updates            | 26090        |
|    policy_gradient_loss | -0.00346     |
|    std                  | 14.9         |
|    value_loss           | 0.527        |
------------------------------------------
Iteration: 2611 | Episodes: 105900 | Median Reward: 41.67 | Max Reward: 48.88
Iteration: 2613 | Episodes: 106000 | Median Reward: 42.17 | Max Reward: 48.88
Iteration: 2616 | Episodes: 106100 | Median Reward: 42.17 | Max Reward: 48.88
Iteration: 2618 | Episodes: 106200 | Median Reward: 41.15 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 515          |
|    iterations           | 2620         |
|    time_elapsed         | 20799        |
|    total_timesteps      | 10731520     |
| train/                  |              |
|    approx_kl            | 0.0005038263 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -146         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.01        |
|    n_updates            | 26190        |
|    policy_gradient_loss | -0.000952    |
|    std                  | 15.1         |
|    value_loss           | 0.489        |
------------------------------------------
Iteration: 2621 | Episodes: 106300 | Median Reward: 42.87 | Max Reward: 48.88
Iteration: 2623 | Episodes: 106400 | Median Reward: 42.26 | Max Reward: 48.88
Iteration: 2626 | Episodes: 106500 | Median Reward: 39.05 | Max Reward: 48.88
Iteration: 2628 | Episodes: 106600 | Median Reward: 39.00 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.7        |
| time/                   |              |
|    fps                  | 516          |
|    iterations           | 2630         |
|    time_elapsed         | 20837        |
|    total_timesteps      | 10772480     |
| train/                  |              |
|    approx_kl            | 5.486209e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -146         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.2         |
|    n_updates            | 26290        |
|    policy_gradient_loss | 2.09e-06     |
|    std                  | 15.1         |
|    value_loss           | 0.339        |
------------------------------------------
Iteration: 2631 | Episodes: 106700 | Median Reward: 42.55 | Max Reward: 48.88
Iteration: 2633 | Episodes: 106800 | Median Reward: 42.21 | Max Reward: 48.88
Iteration: 2635 | Episodes: 106900 | Median Reward: 39.93 | Max Reward: 48.88
Iteration: 2638 | Episodes: 107000 | Median Reward: 38.92 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.6         |
| time/                   |               |
|    fps                  | 517           |
|    iterations           | 2640          |
|    time_elapsed         | 20875         |
|    total_timesteps      | 10813440      |
| train/                  |               |
|    approx_kl            | 0.00064186123 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -146          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.19         |
|    n_updates            | 26390         |
|    policy_gradient_loss | -0.00218      |
|    std                  | 15.2          |
|    value_loss           | 0.325         |
-------------------------------------------
Iteration: 2640 | Episodes: 107100 | Median Reward: 39.42 | Max Reward: 48.88
Iteration: 2643 | Episodes: 107200 | Median Reward: 40.17 | Max Reward: 48.88
Iteration: 2645 | Episodes: 107300 | Median Reward: 42.62 | Max Reward: 48.88
Iteration: 2648 | Episodes: 107400 | Median Reward: 41.48 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.2       |
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 2650        |
|    time_elapsed         | 20913       |
|    total_timesteps      | 10854400    |
| train/                  |             |
|    approx_kl            | 0.003889854 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -146        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.28       |
|    n_updates            | 26490       |
|    policy_gradient_loss | -0.0226     |
|    std                  | 15.3        |
|    value_loss           | 0.382       |
-----------------------------------------
Iteration: 2650 | Episodes: 107500 | Median Reward: 43.36 | Max Reward: 48.88
Iteration: 2653 | Episodes: 107600 | Median Reward: 42.79 | Max Reward: 48.88
Iteration: 2655 | Episodes: 107700 | Median Reward: 43.00 | Max Reward: 48.88
Iteration: 2658 | Episodes: 107800 | Median Reward: 42.75 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 2660         |
|    time_elapsed         | 20950        |
|    total_timesteps      | 10895360     |
| train/                  |              |
|    approx_kl            | 0.0007856035 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -146         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.23        |
|    n_updates            | 26590        |
|    policy_gradient_loss | -0.0052      |
|    std                  | 15.5         |
|    value_loss           | 0.236        |
------------------------------------------
Iteration: 2660 | Episodes: 107900 | Median Reward: 42.75 | Max Reward: 48.88
Iteration: 2663 | Episodes: 108000 | Median Reward: 42.86 | Max Reward: 48.88
Iteration: 2665 | Episodes: 108100 | Median Reward: 43.27 | Max Reward: 48.88
Iteration: 2668 | Episodes: 108200 | Median Reward: 42.64 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.9       |
| time/                   |             |
|    fps                  | 521         |
|    iterations           | 2670        |
|    time_elapsed         | 20988       |
|    total_timesteps      | 10936320    |
| train/                  |             |
|    approx_kl            | 0.000509971 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -147        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.09       |
|    n_updates            | 26690       |
|    policy_gradient_loss | -0.00304    |
|    std                  | 15.7        |
|    value_loss           | 0.641       |
-----------------------------------------
Iteration: 2670 | Episodes: 108300 | Median Reward: 44.37 | Max Reward: 48.88
Iteration: 2672 | Episodes: 108400 | Median Reward: 43.96 | Max Reward: 48.88
Iteration: 2675 | Episodes: 108500 | Median Reward: 43.02 | Max Reward: 48.88
Iteration: 2677 | Episodes: 108600 | Median Reward: 43.33 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.6         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 2680          |
|    time_elapsed         | 21026         |
|    total_timesteps      | 10977280      |
| train/                  |               |
|    approx_kl            | 3.7613674e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -147          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.16         |
|    n_updates            | 26790         |
|    policy_gradient_loss | -0.000273     |
|    std                  | 15.8          |
|    value_loss           | 0.299         |
-------------------------------------------
Iteration: 2680 | Episodes: 108700 | Median Reward: 40.61 | Max Reward: 48.88
Iteration: 2682 | Episodes: 108800 | Median Reward: 43.45 | Max Reward: 48.88
Iteration: 2685 | Episodes: 108900 | Median Reward: 40.90 | Max Reward: 48.88
Iteration: 2687 | Episodes: 109000 | Median Reward: 41.09 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.6        |
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 2690         |
|    time_elapsed         | 21063        |
|    total_timesteps      | 11018240     |
| train/                  |              |
|    approx_kl            | 0.0035605028 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -147         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.22        |
|    n_updates            | 26890        |
|    policy_gradient_loss | -0.0165      |
|    std                  | 15.9         |
|    value_loss           | 0.564        |
------------------------------------------
Iteration: 2690 | Episodes: 109100 | Median Reward: 39.94 | Max Reward: 48.88
Iteration: 2692 | Episodes: 109200 | Median Reward: 41.25 | Max Reward: 48.88
Iteration: 2695 | Episodes: 109300 | Median Reward: 42.00 | Max Reward: 48.88
Iteration: 2697 | Episodes: 109400 | Median Reward: 39.54 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.2         |
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 2700          |
|    time_elapsed         | 21101         |
|    total_timesteps      | 11059200      |
| train/                  |               |
|    approx_kl            | 2.2124295e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -147          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.12         |
|    n_updates            | 26990         |
|    policy_gradient_loss | -0.00014      |
|    std                  | 16.1          |
|    value_loss           | 0.487         |
-------------------------------------------
Iteration: 2700 | Episodes: 109500 | Median Reward: 42.23 | Max Reward: 48.88
Iteration: 2702 | Episodes: 109600 | Median Reward: 43.02 | Max Reward: 48.88
Iteration: 2705 | Episodes: 109700 | Median Reward: 41.16 | Max Reward: 48.88
Iteration: 2707 | Episodes: 109800 | Median Reward: 42.69 | Max Reward: 48.88
Iteration: 2709 | Episodes: 109900 | Median Reward: 42.63 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.8        |
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 2710         |
|    time_elapsed         | 21139        |
|    total_timesteps      | 11100160     |
| train/                  |              |
|    approx_kl            | 0.0019338595 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -147         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.26        |
|    n_updates            | 27090        |
|    policy_gradient_loss | -0.00796     |
|    std                  | 16.2         |
|    value_loss           | 0.309        |
------------------------------------------
Iteration: 2712 | Episodes: 110000 | Median Reward: 44.23 | Max Reward: 48.88
Iteration: 2714 | Episodes: 110100 | Median Reward: 40.52 | Max Reward: 48.88
Iteration: 2717 | Episodes: 110200 | Median Reward: 42.58 | Max Reward: 48.88
Iteration: 2719 | Episodes: 110300 | Median Reward: 39.76 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.3         |
| time/                   |               |
|    fps                  | 526           |
|    iterations           | 2720          |
|    time_elapsed         | 21177         |
|    total_timesteps      | 11141120      |
| train/                  |               |
|    approx_kl            | 0.00036457548 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -147          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.28         |
|    n_updates            | 27190         |
|    policy_gradient_loss | -0.0028       |
|    std                  | 16.4          |
|    value_loss           | 0.257         |
-------------------------------------------
Iteration: 2722 | Episodes: 110400 | Median Reward: 42.20 | Max Reward: 48.88
Iteration: 2724 | Episodes: 110500 | Median Reward: 40.21 | Max Reward: 48.88
Iteration: 2727 | Episodes: 110600 | Median Reward: 39.88 | Max Reward: 48.88
Iteration: 2729 | Episodes: 110700 | Median Reward: 42.98 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.7         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 2730          |
|    time_elapsed         | 21215         |
|    total_timesteps      | 11182080      |
| train/                  |               |
|    approx_kl            | 0.00030201612 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -148          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.33         |
|    n_updates            | 27290         |
|    policy_gradient_loss | -0.00199      |
|    std                  | 16.5          |
|    value_loss           | 0.399         |
-------------------------------------------
Iteration: 2732 | Episodes: 110800 | Median Reward: 43.51 | Max Reward: 48.88
Iteration: 2734 | Episodes: 110900 | Median Reward: 43.94 | Max Reward: 48.88
Iteration: 2737 | Episodes: 111000 | Median Reward: 39.90 | Max Reward: 48.88
Iteration: 2739 | Episodes: 111100 | Median Reward: 43.38 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.6        |
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 2740         |
|    time_elapsed         | 21252        |
|    total_timesteps      | 11223040     |
| train/                  |              |
|    approx_kl            | 0.0005104594 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -148         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.25        |
|    n_updates            | 27390        |
|    policy_gradient_loss | -0.00117     |
|    std                  | 16.7         |
|    value_loss           | 0.304        |
------------------------------------------
Iteration: 2741 | Episodes: 111200 | Median Reward: 44.28 | Max Reward: 48.88
Iteration: 2744 | Episodes: 111300 | Median Reward: 43.60 | Max Reward: 48.88
Iteration: 2746 | Episodes: 111400 | Median Reward: 43.01 | Max Reward: 48.88
Iteration: 2749 | Episodes: 111500 | Median Reward: 41.56 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.1         |
| time/                   |               |
|    fps                  | 529           |
|    iterations           | 2750          |
|    time_elapsed         | 21290         |
|    total_timesteps      | 11264000      |
| train/                  |               |
|    approx_kl            | 0.00025095587 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -148          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.36         |
|    n_updates            | 27490         |
|    policy_gradient_loss | -0.00165      |
|    std                  | 16.9          |
|    value_loss           | 0.251         |
-------------------------------------------
Iteration: 2751 | Episodes: 111600 | Median Reward: 41.56 | Max Reward: 48.88
Iteration: 2754 | Episodes: 111700 | Median Reward: 41.59 | Max Reward: 48.88
Iteration: 2756 | Episodes: 111800 | Median Reward: 39.87 | Max Reward: 48.88
Iteration: 2759 | Episodes: 111900 | Median Reward: 39.89 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.7        |
| time/                   |              |
|    fps                  | 530          |
|    iterations           | 2760         |
|    time_elapsed         | 21328        |
|    total_timesteps      | 11304960     |
| train/                  |              |
|    approx_kl            | 0.0013843575 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -148         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.36        |
|    n_updates            | 27590        |
|    policy_gradient_loss | -0.00451     |
|    std                  | 17.1         |
|    value_loss           | 0.203        |
------------------------------------------
Iteration: 2761 | Episodes: 112000 | Median Reward: 41.00 | Max Reward: 48.88
Iteration: 2764 | Episodes: 112100 | Median Reward: 41.52 | Max Reward: 48.88
Iteration: 2766 | Episodes: 112200 | Median Reward: 42.77 | Max Reward: 48.88
Iteration: 2769 | Episodes: 112300 | Median Reward: 41.51 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.1         |
| time/                   |               |
|    fps                  | 531           |
|    iterations           | 2770          |
|    time_elapsed         | 21365         |
|    total_timesteps      | 11345920      |
| train/                  |               |
|    approx_kl            | 0.00043191103 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -149          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.1          |
|    n_updates            | 27690         |
|    policy_gradient_loss | -0.00274      |
|    std                  | 17.3          |
|    value_loss           | 0.925         |
-------------------------------------------
Iteration: 2771 | Episodes: 112400 | Median Reward: 42.82 | Max Reward: 48.88
Iteration: 2774 | Episodes: 112500 | Median Reward: 43.41 | Max Reward: 48.88
Iteration: 2776 | Episodes: 112600 | Median Reward: 40.78 | Max Reward: 48.88
Iteration: 2778 | Episodes: 112700 | Median Reward: 40.97 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.2         |
| time/                   |               |
|    fps                  | 532           |
|    iterations           | 2780          |
|    time_elapsed         | 21403         |
|    total_timesteps      | 11386880      |
| train/                  |               |
|    approx_kl            | 0.00013716788 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -149          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.37         |
|    n_updates            | 27790         |
|    policy_gradient_loss | -0.00118      |
|    std                  | 17.4          |
|    value_loss           | 0.267         |
-------------------------------------------
Iteration: 2781 | Episodes: 112800 | Median Reward: 42.84 | Max Reward: 48.88
Iteration: 2783 | Episodes: 112900 | Median Reward: 40.85 | Max Reward: 48.88
Iteration: 2786 | Episodes: 113000 | Median Reward: 41.56 | Max Reward: 48.88
Iteration: 2788 | Episodes: 113100 | Median Reward: 43.24 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.7        |
| time/                   |              |
|    fps                  | 532          |
|    iterations           | 2790         |
|    time_elapsed         | 21440        |
|    total_timesteps      | 11427840     |
| train/                  |              |
|    approx_kl            | 4.382126e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -149         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.29        |
|    n_updates            | 27890        |
|    policy_gradient_loss | -0.00015     |
|    std                  | 17.6         |
|    value_loss           | 0.304        |
------------------------------------------
Iteration: 2791 | Episodes: 113200 | Median Reward: 42.83 | Max Reward: 48.88
Iteration: 2793 | Episodes: 113300 | Median Reward: 42.34 | Max Reward: 48.88
Iteration: 2796 | Episodes: 113400 | Median Reward: 39.65 | Max Reward: 48.88
Iteration: 2798 | Episodes: 113500 | Median Reward: 42.15 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.8         |
| time/                   |               |
|    fps                  | 533           |
|    iterations           | 2800          |
|    time_elapsed         | 21478         |
|    total_timesteps      | 11468800      |
| train/                  |               |
|    approx_kl            | 0.00011582306 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -149          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.94         |
|    n_updates            | 27990         |
|    policy_gradient_loss | -0.000794     |
|    std                  | 17.8          |
|    value_loss           | 0.989         |
-------------------------------------------
Iteration: 2801 | Episodes: 113600 | Median Reward: 37.46 | Max Reward: 48.88
Iteration: 2803 | Episodes: 113700 | Median Reward: 42.26 | Max Reward: 48.88
Iteration: 2806 | Episodes: 113800 | Median Reward: 41.64 | Max Reward: 48.88
Iteration: 2808 | Episodes: 113900 | Median Reward: 42.77 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62           |
| time/                   |               |
|    fps                  | 534           |
|    iterations           | 2810          |
|    time_elapsed         | 21516         |
|    total_timesteps      | 11509760      |
| train/                  |               |
|    approx_kl            | 1.2199947e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -149          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.29         |
|    n_updates            | 28090         |
|    policy_gradient_loss | -0.000241     |
|    std                  | 17.9          |
|    value_loss           | 0.516         |
-------------------------------------------
Iteration: 2811 | Episodes: 114000 | Median Reward: 40.01 | Max Reward: 48.88
Iteration: 2813 | Episodes: 114100 | Median Reward: 43.14 | Max Reward: 48.88
Iteration: 2815 | Episodes: 114200 | Median Reward: 41.55 | Max Reward: 48.88
Iteration: 2818 | Episodes: 114300 | Median Reward: 43.59 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.8        |
| time/                   |              |
|    fps                  | 535          |
|    iterations           | 2820         |
|    time_elapsed         | 21553        |
|    total_timesteps      | 11550720     |
| train/                  |              |
|    approx_kl            | 0.0020855889 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -150         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.25        |
|    n_updates            | 28190        |
|    policy_gradient_loss | -0.00838     |
|    std                  | 18           |
|    value_loss           | 0.397        |
------------------------------------------
Iteration: 2820 | Episodes: 114400 | Median Reward: 43.35 | Max Reward: 48.88
Iteration: 2823 | Episodes: 114500 | Median Reward: 37.33 | Max Reward: 48.88
Iteration: 2825 | Episodes: 114600 | Median Reward: 39.20 | Max Reward: 48.88
Iteration: 2828 | Episodes: 114700 | Median Reward: 38.86 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.9        |
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 2830         |
|    time_elapsed         | 21591        |
|    total_timesteps      | 11591680     |
| train/                  |              |
|    approx_kl            | 0.0008349861 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -150         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.42        |
|    n_updates            | 28290        |
|    policy_gradient_loss | -0.00591     |
|    std                  | 18.2         |
|    value_loss           | 0.256        |
------------------------------------------
Iteration: 2830 | Episodes: 114800 | Median Reward: 42.80 | Max Reward: 48.88
Iteration: 2833 | Episodes: 114900 | Median Reward: 39.60 | Max Reward: 48.88
Iteration: 2835 | Episodes: 115000 | Median Reward: 42.76 | Max Reward: 48.88
Iteration: 2838 | Episodes: 115100 | Median Reward: 41.18 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.6         |
| time/                   |               |
|    fps                  | 537           |
|    iterations           | 2840          |
|    time_elapsed         | 21629         |
|    total_timesteps      | 11632640      |
| train/                  |               |
|    approx_kl            | 0.00022573711 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -150          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.3          |
|    n_updates            | 28390         |
|    policy_gradient_loss | -0.00258      |
|    std                  | 18.4          |
|    value_loss           | 0.648         |
-------------------------------------------
Iteration: 2840 | Episodes: 115200 | Median Reward: 42.72 | Max Reward: 48.88
Iteration: 2843 | Episodes: 115300 | Median Reward: 42.99 | Max Reward: 48.88
Iteration: 2845 | Episodes: 115400 | Median Reward: 40.62 | Max Reward: 48.88
Iteration: 2848 | Episodes: 115500 | Median Reward: 42.58 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.5         |
| time/                   |               |
|    fps                  | 538           |
|    iterations           | 2850          |
|    time_elapsed         | 21666         |
|    total_timesteps      | 11673600      |
| train/                  |               |
|    approx_kl            | 1.2109391e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -150          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.09         |
|    n_updates            | 28490         |
|    policy_gradient_loss | 3.45e-05      |
|    std                  | 18.6          |
|    value_loss           | 1.43          |
-------------------------------------------
Iteration: 2850 | Episodes: 115600 | Median Reward: 39.97 | Max Reward: 48.88
Iteration: 2852 | Episodes: 115700 | Median Reward: 45.58 | Max Reward: 48.88
Iteration: 2855 | Episodes: 115800 | Median Reward: 41.52 | Max Reward: 48.88
Iteration: 2857 | Episodes: 115900 | Median Reward: 41.39 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.7        |
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 2860         |
|    time_elapsed         | 21704        |
|    total_timesteps      | 11714560     |
| train/                  |              |
|    approx_kl            | 0.0003340479 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -150         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.4         |
|    n_updates            | 28590        |
|    policy_gradient_loss | -0.00222     |
|    std                  | 18.8         |
|    value_loss           | 0.199        |
------------------------------------------
Iteration: 2860 | Episodes: 116000 | Median Reward: 42.84 | Max Reward: 48.88
Iteration: 2862 | Episodes: 116100 | Median Reward: 43.62 | Max Reward: 48.88
Iteration: 2865 | Episodes: 116200 | Median Reward: 43.52 | Max Reward: 48.88
Iteration: 2867 | Episodes: 116300 | Median Reward: 39.04 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.6        |
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 2870         |
|    time_elapsed         | 21741        |
|    total_timesteps      | 11755520     |
| train/                  |              |
|    approx_kl            | 0.0012878017 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -150         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.27        |
|    n_updates            | 28690        |
|    policy_gradient_loss | -0.00295     |
|    std                  | 19           |
|    value_loss           | 0.454        |
------------------------------------------
Iteration: 2870 | Episodes: 116400 | Median Reward: 38.49 | Max Reward: 48.88
Iteration: 2872 | Episodes: 116500 | Median Reward: 39.38 | Max Reward: 48.88
Iteration: 2875 | Episodes: 116600 | Median Reward: 40.70 | Max Reward: 48.88
Iteration: 2877 | Episodes: 116700 | Median Reward: 41.02 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.2       |
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 2880        |
|    time_elapsed         | 21779       |
|    total_timesteps      | 11796480    |
| train/                  |             |
|    approx_kl            | 0.003608313 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -151        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.38       |
|    n_updates            | 28790       |
|    policy_gradient_loss | -0.0103     |
|    std                  | 19          |
|    value_loss           | 0.544       |
-----------------------------------------
Iteration: 2880 | Episodes: 116800 | Median Reward: 39.30 | Max Reward: 48.88
Iteration: 2882 | Episodes: 116900 | Median Reward: 41.56 | Max Reward: 48.88
Iteration: 2885 | Episodes: 117000 | Median Reward: 41.88 | Max Reward: 48.88
Iteration: 2887 | Episodes: 117100 | Median Reward: 42.95 | Max Reward: 48.88
Iteration: 2889 | Episodes: 117200 | Median Reward: 39.22 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.7        |
| time/                   |              |
|    fps                  | 542          |
|    iterations           | 2890         |
|    time_elapsed         | 21816        |
|    total_timesteps      | 11837440     |
| train/                  |              |
|    approx_kl            | 0.0017080908 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -151         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.42        |
|    n_updates            | 28890        |
|    policy_gradient_loss | -0.00923     |
|    std                  | 19.2         |
|    value_loss           | 0.835        |
------------------------------------------
Iteration: 2892 | Episodes: 117300 | Median Reward: 39.42 | Max Reward: 48.88
Iteration: 2894 | Episodes: 117400 | Median Reward: 41.55 | Max Reward: 48.88
Iteration: 2897 | Episodes: 117500 | Median Reward: 42.78 | Max Reward: 48.88
Iteration: 2899 | Episodes: 117600 | Median Reward: 42.87 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.9         |
| time/                   |               |
|    fps                  | 543           |
|    iterations           | 2900          |
|    time_elapsed         | 21854         |
|    total_timesteps      | 11878400      |
| train/                  |               |
|    approx_kl            | 0.00013899774 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -151          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.39         |
|    n_updates            | 28990         |
|    policy_gradient_loss | -0.00109      |
|    std                  | 19.3          |
|    value_loss           | 0.238         |
-------------------------------------------
Iteration: 2902 | Episodes: 117700 | Median Reward: 43.09 | Max Reward: 48.88
Iteration: 2904 | Episodes: 117800 | Median Reward: 39.70 | Max Reward: 48.88
Iteration: 2907 | Episodes: 117900 | Median Reward: 42.50 | Max Reward: 48.88
Iteration: 2909 | Episodes: 118000 | Median Reward: 36.89 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.3         |
| time/                   |               |
|    fps                  | 544           |
|    iterations           | 2910          |
|    time_elapsed         | 21891         |
|    total_timesteps      | 11919360      |
| train/                  |               |
|    approx_kl            | 0.00095387944 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -151          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.43         |
|    n_updates            | 29090         |
|    policy_gradient_loss | -0.00386      |
|    std                  | 19.4          |
|    value_loss           | 0.353         |
-------------------------------------------
Iteration: 2912 | Episodes: 118100 | Median Reward: 42.24 | Max Reward: 48.88
Iteration: 2914 | Episodes: 118200 | Median Reward: 39.48 | Max Reward: 48.88
Iteration: 2917 | Episodes: 118300 | Median Reward: 42.01 | Max Reward: 48.88
Iteration: 2919 | Episodes: 118400 | Median Reward: 40.25 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -58.1          |
| time/                   |                |
|    fps                  | 545            |
|    iterations           | 2920           |
|    time_elapsed         | 21928          |
|    total_timesteps      | 11960320       |
| train/                  |                |
|    approx_kl            | 0.000107109896 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -151           |
|    explained_variance   | 0.999          |
|    learning_rate        | 0.0005         |
|    loss                 | -7.21          |
|    n_updates            | 29190          |
|    policy_gradient_loss | -0.00143       |
|    std                  | 19.6           |
|    value_loss           | 0.551          |
--------------------------------------------
Iteration: 2921 | Episodes: 118500 | Median Reward: 43.66 | Max Reward: 48.88
Iteration: 2924 | Episodes: 118600 | Median Reward: 39.15 | Max Reward: 48.88
Iteration: 2926 | Episodes: 118700 | Median Reward: 38.84 | Max Reward: 48.88
Iteration: 2929 | Episodes: 118800 | Median Reward: 38.90 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.4        |
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 2930         |
|    time_elapsed         | 21966        |
|    total_timesteps      | 12001280     |
| train/                  |              |
|    approx_kl            | 8.713048e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -151         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.45        |
|    n_updates            | 29290        |
|    policy_gradient_loss | -0.000299    |
|    std                  | 19.7         |
|    value_loss           | 1.58         |
------------------------------------------
Iteration: 2931 | Episodes: 118900 | Median Reward: 42.39 | Max Reward: 48.88
Iteration: 2934 | Episodes: 119000 | Median Reward: 39.86 | Max Reward: 48.88
Iteration: 2936 | Episodes: 119100 | Median Reward: 40.19 | Max Reward: 48.88
Iteration: 2939 | Episodes: 119200 | Median Reward: 42.71 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.6         |
| time/                   |               |
|    fps                  | 547           |
|    iterations           | 2940          |
|    time_elapsed         | 22003         |
|    total_timesteps      | 12042240      |
| train/                  |               |
|    approx_kl            | 0.00048195192 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.4          |
|    n_updates            | 29390         |
|    policy_gradient_loss | -0.00262      |
|    std                  | 20            |
|    value_loss           | 0.609         |
-------------------------------------------
Iteration: 2941 | Episodes: 119300 | Median Reward: 41.70 | Max Reward: 48.88
Iteration: 2944 | Episodes: 119400 | Median Reward: 42.14 | Max Reward: 48.88
Iteration: 2946 | Episodes: 119500 | Median Reward: 38.77 | Max Reward: 48.88
Iteration: 2949 | Episodes: 119600 | Median Reward: 41.15 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.1        |
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 2950         |
|    time_elapsed         | 22041        |
|    total_timesteps      | 12083200     |
| train/                  |              |
|    approx_kl            | 0.0017747853 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -152         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.48        |
|    n_updates            | 29490        |
|    policy_gradient_loss | -0.00336     |
|    std                  | 20.2         |
|    value_loss           | 0.271        |
------------------------------------------
Iteration: 2951 | Episodes: 119700 | Median Reward: 39.68 | Max Reward: 48.88
Iteration: 2954 | Episodes: 119800 | Median Reward: 39.50 | Max Reward: 48.88
Iteration: 2956 | Episodes: 119900 | Median Reward: 42.38 | Max Reward: 48.88
Iteration: 2958 | Episodes: 120000 | Median Reward: 40.42 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.9        |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 2960         |
|    time_elapsed         | 22079        |
|    total_timesteps      | 12124160     |
| train/                  |              |
|    approx_kl            | 0.0006118858 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -152         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -6.7         |
|    n_updates            | 29590        |
|    policy_gradient_loss | -0.00288     |
|    std                  | 20.5         |
|    value_loss           | 0.572        |
------------------------------------------
Iteration: 2961 | Episodes: 120100 | Median Reward: 40.38 | Max Reward: 48.88
Iteration: 2963 | Episodes: 120200 | Median Reward: 39.46 | Max Reward: 48.88
Iteration: 2966 | Episodes: 120300 | Median Reward: 42.88 | Max Reward: 48.88
Iteration: 2968 | Episodes: 120400 | Median Reward: 42.88 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.8         |
| time/                   |               |
|    fps                  | 550           |
|    iterations           | 2970          |
|    time_elapsed         | 22117         |
|    total_timesteps      | 12165120      |
| train/                  |               |
|    approx_kl            | 0.00069722254 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.54         |
|    n_updates            | 29690         |
|    policy_gradient_loss | -0.00227      |
|    std                  | 20.7          |
|    value_loss           | 0.251         |
-------------------------------------------
Iteration: 2971 | Episodes: 120500 | Median Reward: 40.21 | Max Reward: 48.88
Iteration: 2973 | Episodes: 120600 | Median Reward: 40.44 | Max Reward: 48.88
Iteration: 2976 | Episodes: 120700 | Median Reward: 38.48 | Max Reward: 48.88
Iteration: 2978 | Episodes: 120800 | Median Reward: 35.86 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.6         |
| time/                   |               |
|    fps                  | 550           |
|    iterations           | 2980          |
|    time_elapsed         | 22154         |
|    total_timesteps      | 12206080      |
| train/                  |               |
|    approx_kl            | 0.00039594836 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.45         |
|    n_updates            | 29790         |
|    policy_gradient_loss | -0.00254      |
|    std                  | 20.9          |
|    value_loss           | 0.479         |
-------------------------------------------
Iteration: 2981 | Episodes: 120900 | Median Reward: 40.62 | Max Reward: 48.88
Iteration: 2983 | Episodes: 121000 | Median Reward: 42.18 | Max Reward: 48.88
Iteration: 2986 | Episodes: 121100 | Median Reward: 37.13 | Max Reward: 48.88
Iteration: 2988 | Episodes: 121200 | Median Reward: 41.34 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.7         |
| time/                   |               |
|    fps                  | 551           |
|    iterations           | 2990          |
|    time_elapsed         | 22191         |
|    total_timesteps      | 12247040      |
| train/                  |               |
|    approx_kl            | 0.00077360775 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -153          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.33         |
|    n_updates            | 29890         |
|    policy_gradient_loss | -0.00277      |
|    std                  | 21.1          |
|    value_loss           | 0.335         |
-------------------------------------------
Iteration: 2991 | Episodes: 121300 | Median Reward: 43.63 | Max Reward: 48.88
Iteration: 2993 | Episodes: 121400 | Median Reward: 41.10 | Max Reward: 48.88
Iteration: 2995 | Episodes: 121500 | Median Reward: 40.10 | Max Reward: 48.88
Iteration: 2998 | Episodes: 121600 | Median Reward: 37.30 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.9         |
| time/                   |               |
|    fps                  | 552           |
|    iterations           | 3000          |
|    time_elapsed         | 22229         |
|    total_timesteps      | 12288000      |
| train/                  |               |
|    approx_kl            | 0.00033124757 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -153          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.55         |
|    n_updates            | 29990         |
|    policy_gradient_loss | -0.00165      |
|    std                  | 21.3          |
|    value_loss           | 0.304         |
-------------------------------------------
Iteration: 3000 | Episodes: 121700 | Median Reward: 41.86 | Max Reward: 48.88
Iteration: 3003 | Episodes: 121800 | Median Reward: 39.91 | Max Reward: 48.88
Iteration: 3005 | Episodes: 121900 | Median Reward: 42.78 | Max Reward: 48.88
Iteration: 3008 | Episodes: 122000 | Median Reward: 39.90 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.6        |
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 3010         |
|    time_elapsed         | 22266        |
|    total_timesteps      | 12328960     |
| train/                  |              |
|    approx_kl            | 6.773205e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -153         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.52        |
|    n_updates            | 30090        |
|    policy_gradient_loss | -0.000384    |
|    std                  | 21.5         |
|    value_loss           | 0.253        |
------------------------------------------
Iteration: 3010 | Episodes: 122100 | Median Reward: 40.14 | Max Reward: 48.88
Iteration: 3013 | Episodes: 122200 | Median Reward: 40.79 | Max Reward: 48.88
Iteration: 3015 | Episodes: 122300 | Median Reward: 39.88 | Max Reward: 48.88
Iteration: 3018 | Episodes: 122400 | Median Reward: 38.48 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.2        |
| time/                   |              |
|    fps                  | 554          |
|    iterations           | 3020         |
|    time_elapsed         | 22303        |
|    total_timesteps      | 12369920     |
| train/                  |              |
|    approx_kl            | 0.0005605799 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -153         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.4         |
|    n_updates            | 30190        |
|    policy_gradient_loss | -0.00206     |
|    std                  | 21.8         |
|    value_loss           | 0.774        |
------------------------------------------
Iteration: 3020 | Episodes: 122500 | Median Reward: 35.13 | Max Reward: 48.88
Iteration: 3023 | Episodes: 122600 | Median Reward: 38.00 | Max Reward: 48.88
Iteration: 3025 | Episodes: 122700 | Median Reward: 40.98 | Max Reward: 48.88
Iteration: 3028 | Episodes: 122800 | Median Reward: 40.88 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.8         |
| time/                   |               |
|    fps                  | 555           |
|    iterations           | 3030          |
|    time_elapsed         | 22341         |
|    total_timesteps      | 12410880      |
| train/                  |               |
|    approx_kl            | 2.1688873e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -153          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.38         |
|    n_updates            | 30290         |
|    policy_gradient_loss | -0.000113     |
|    std                  | 21.9          |
|    value_loss           | 0.682         |
-------------------------------------------
Iteration: 3030 | Episodes: 122900 | Median Reward: 39.88 | Max Reward: 48.88
Iteration: 3032 | Episodes: 123000 | Median Reward: 40.39 | Max Reward: 48.88
Iteration: 3035 | Episodes: 123100 | Median Reward: 40.40 | Max Reward: 48.88
Iteration: 3037 | Episodes: 123200 | Median Reward: 38.09 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.4       |
| time/                   |             |
|    fps                  | 556         |
|    iterations           | 3040        |
|    time_elapsed         | 22378       |
|    total_timesteps      | 12451840    |
| train/                  |             |
|    approx_kl            | 0.010300519 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -154        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.44       |
|    n_updates            | 30390       |
|    policy_gradient_loss | -0.0274     |
|    std                  | 22.2        |
|    value_loss           | 0.369       |
-----------------------------------------
Iteration: 3040 | Episodes: 123300 | Median Reward: 38.51 | Max Reward: 48.88
Iteration: 3042 | Episodes: 123400 | Median Reward: 38.31 | Max Reward: 48.88
Iteration: 3045 | Episodes: 123500 | Median Reward: 40.74 | Max Reward: 48.88
Iteration: 3047 | Episodes: 123600 | Median Reward: 37.99 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -60.3          |
| time/                   |                |
|    fps                  | 557            |
|    iterations           | 3050           |
|    time_elapsed         | 22415          |
|    total_timesteps      | 12492800       |
| train/                  |                |
|    approx_kl            | 0.000114504335 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -154           |
|    explained_variance   | 0.999          |
|    learning_rate        | 0.0005         |
|    loss                 | -7.47          |
|    n_updates            | 30490          |
|    policy_gradient_loss | -0.000495      |
|    std                  | 22.5           |
|    value_loss           | 0.646          |
--------------------------------------------
Iteration: 3050 | Episodes: 123700 | Median Reward: 40.93 | Max Reward: 48.88
Iteration: 3052 | Episodes: 123800 | Median Reward: 42.71 | Max Reward: 48.88
Iteration: 3055 | Episodes: 123900 | Median Reward: 42.93 | Max Reward: 48.88
Iteration: 3057 | Episodes: 124000 | Median Reward: 40.40 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.6         |
| time/                   |               |
|    fps                  | 558           |
|    iterations           | 3060          |
|    time_elapsed         | 22453         |
|    total_timesteps      | 12533760      |
| train/                  |               |
|    approx_kl            | 0.00070514623 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -154          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.18         |
|    n_updates            | 30590         |
|    policy_gradient_loss | -0.00437      |
|    std                  | 22.8          |
|    value_loss           | 0.542         |
-------------------------------------------
Iteration: 3060 | Episodes: 124100 | Median Reward: 40.05 | Max Reward: 48.88
Iteration: 3062 | Episodes: 124200 | Median Reward: 39.52 | Max Reward: 48.88
Iteration: 3065 | Episodes: 124300 | Median Reward: 42.28 | Max Reward: 48.88
Iteration: 3067 | Episodes: 124400 | Median Reward: 37.71 | Max Reward: 48.88
Iteration: 3069 | Episodes: 124500 | Median Reward: 38.04 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.4       |
| time/                   |             |
|    fps                  | 559         |
|    iterations           | 3070        |
|    time_elapsed         | 22490       |
|    total_timesteps      | 12574720    |
| train/                  |             |
|    approx_kl            | 0.007455681 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -154        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.59       |
|    n_updates            | 30690       |
|    policy_gradient_loss | -0.0115     |
|    std                  | 23          |
|    value_loss           | 0.338       |
-----------------------------------------
Iteration: 3072 | Episodes: 124600 | Median Reward: 41.56 | Max Reward: 48.88
Iteration: 3074 | Episodes: 124700 | Median Reward: 36.45 | Max Reward: 48.88
Iteration: 3077 | Episodes: 124800 | Median Reward: 39.23 | Max Reward: 48.88
Iteration: 3079 | Episodes: 124900 | Median Reward: 39.09 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.1         |
| time/                   |               |
|    fps                  | 559           |
|    iterations           | 3080          |
|    time_elapsed         | 22548         |
|    total_timesteps      | 12615680      |
| train/                  |               |
|    approx_kl            | 0.00018550736 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.52         |
|    n_updates            | 30790         |
|    policy_gradient_loss | -0.000793     |
|    std                  | 23.2          |
|    value_loss           | 0.485         |
-------------------------------------------
Iteration: 3082 | Episodes: 125000 | Median Reward: 40.21 | Max Reward: 48.88
Iteration: 3084 | Episodes: 125100 | Median Reward: 36.56 | Max Reward: 48.88
Iteration: 3087 | Episodes: 125200 | Median Reward: 38.73 | Max Reward: 48.88
Iteration: 3089 | Episodes: 125300 | Median Reward: 39.97 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.5        |
| time/                   |              |
|    fps                  | 559          |
|    iterations           | 3090         |
|    time_elapsed         | 22640        |
|    total_timesteps      | 12656640     |
| train/                  |              |
|    approx_kl            | 4.232221e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -155         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.57        |
|    n_updates            | 30890        |
|    policy_gradient_loss | -2.07e-05    |
|    std                  | 23.3         |
|    value_loss           | 0.528        |
------------------------------------------
Iteration: 3092 | Episodes: 125400 | Median Reward: 37.83 | Max Reward: 48.88
Iteration: 3094 | Episodes: 125500 | Median Reward: 42.01 | Max Reward: 48.88
Iteration: 3097 | Episodes: 125600 | Median Reward: 38.73 | Max Reward: 48.88
Iteration: 3099 | Episodes: 125700 | Median Reward: 36.88 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.5         |
| time/                   |               |
|    fps                  | 558           |
|    iterations           | 3100          |
|    time_elapsed         | 22717         |
|    total_timesteps      | 12697600      |
| train/                  |               |
|    approx_kl            | 0.00010501381 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.54         |
|    n_updates            | 30990         |
|    policy_gradient_loss | -0.00105      |
|    std                  | 23.5          |
|    value_loss           | 0.493         |
-------------------------------------------
Iteration: 3102 | Episodes: 125800 | Median Reward: 39.80 | Max Reward: 48.88
Iteration: 3104 | Episodes: 125900 | Median Reward: 40.93 | Max Reward: 48.88
Iteration: 3106 | Episodes: 126000 | Median Reward: 38.64 | Max Reward: 48.88
Iteration: 3109 | Episodes: 126100 | Median Reward: 39.99 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.9         |
| time/                   |               |
|    fps                  | 558           |
|    iterations           | 3110          |
|    time_elapsed         | 22799         |
|    total_timesteps      | 12738560      |
| train/                  |               |
|    approx_kl            | 0.00011949026 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.63         |
|    n_updates            | 31090         |
|    policy_gradient_loss | -0.000322     |
|    std                  | 23.6          |
|    value_loss           | 0.476         |
-------------------------------------------
Iteration: 3111 | Episodes: 126200 | Median Reward: 38.26 | Max Reward: 48.88
Iteration: 3114 | Episodes: 126300 | Median Reward: 37.81 | Max Reward: 48.88
Iteration: 3116 | Episodes: 126400 | Median Reward: 41.13 | Max Reward: 48.88
Iteration: 3119 | Episodes: 126500 | Median Reward: 41.21 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.7         |
| time/                   |               |
|    fps                  | 558           |
|    iterations           | 3120          |
|    time_elapsed         | 22887         |
|    total_timesteps      | 12779520      |
| train/                  |               |
|    approx_kl            | 0.00022537648 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.45         |
|    n_updates            | 31190         |
|    policy_gradient_loss | -0.00308      |
|    std                  | 23.9          |
|    value_loss           | 0.992         |
-------------------------------------------
Iteration: 3121 | Episodes: 126600 | Median Reward: 40.05 | Max Reward: 48.88
Iteration: 3124 | Episodes: 126700 | Median Reward: 38.99 | Max Reward: 48.88
Iteration: 3126 | Episodes: 126800 | Median Reward: 40.66 | Max Reward: 48.88
Iteration: 3129 | Episodes: 126900 | Median Reward: 41.88 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.8       |
| time/                   |             |
|    fps                  | 558         |
|    iterations           | 3130        |
|    time_elapsed         | 22962       |
|    total_timesteps      | 12820480    |
| train/                  |             |
|    approx_kl            | 0.000894958 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -155        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.39       |
|    n_updates            | 31290       |
|    policy_gradient_loss | -0.00231    |
|    std                  | 24.1        |
|    value_loss           | 0.787       |
-----------------------------------------
Iteration: 3131 | Episodes: 127000 | Median Reward: 38.83 | Max Reward: 48.88
Iteration: 3134 | Episodes: 127100 | Median Reward: 39.35 | Max Reward: 48.88
Iteration: 3136 | Episodes: 127200 | Median Reward: 41.84 | Max Reward: 48.88
Iteration: 3138 | Episodes: 127300 | Median Reward: 39.17 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.6        |
| time/                   |              |
|    fps                  | 557          |
|    iterations           | 3140         |
|    time_elapsed         | 23055        |
|    total_timesteps      | 12861440     |
| train/                  |              |
|    approx_kl            | 0.0012589835 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -156         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.66        |
|    n_updates            | 31390        |
|    policy_gradient_loss | -0.00925     |
|    std                  | 24.3         |
|    value_loss           | 0.383        |
------------------------------------------
Iteration: 3141 | Episodes: 127400 | Median Reward: 41.17 | Max Reward: 48.88
Iteration: 3143 | Episodes: 127500 | Median Reward: 42.48 | Max Reward: 48.88
Iteration: 3146 | Episodes: 127600 | Median Reward: 38.72 | Max Reward: 48.88
Iteration: 3148 | Episodes: 127700 | Median Reward: 33.55 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.3         |
| time/                   |               |
|    fps                  | 557           |
|    iterations           | 3150          |
|    time_elapsed         | 23131         |
|    total_timesteps      | 12902400      |
| train/                  |               |
|    approx_kl            | 0.00018978462 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -156          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.6          |
|    n_updates            | 31490         |
|    policy_gradient_loss | -0.000534     |
|    std                  | 24.4          |
|    value_loss           | 0.439         |
-------------------------------------------
Iteration: 3151 | Episodes: 127800 | Median Reward: 40.68 | Max Reward: 48.88
Iteration: 3153 | Episodes: 127900 | Median Reward: 38.67 | Max Reward: 48.88
Iteration: 3156 | Episodes: 128000 | Median Reward: 39.90 | Max Reward: 48.88
Iteration: 3158 | Episodes: 128100 | Median Reward: 40.51 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.6        |
| time/                   |              |
|    fps                  | 557          |
|    iterations           | 3160         |
|    time_elapsed         | 23216        |
|    total_timesteps      | 12943360     |
| train/                  |              |
|    approx_kl            | 6.849457e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -156         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.72        |
|    n_updates            | 31590        |
|    policy_gradient_loss | 0.000324     |
|    std                  | 24.6         |
|    value_loss           | 0.242        |
------------------------------------------
Iteration: 3161 | Episodes: 128200 | Median Reward: 40.85 | Max Reward: 48.88
Iteration: 3163 | Episodes: 128300 | Median Reward: 37.23 | Max Reward: 48.88
Iteration: 3166 | Episodes: 128400 | Median Reward: 40.36 | Max Reward: 48.88
Iteration: 3168 | Episodes: 128500 | Median Reward: 39.01 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.3         |
| time/                   |               |
|    fps                  | 557           |
|    iterations           | 3170          |
|    time_elapsed         | 23298         |
|    total_timesteps      | 12984320      |
| train/                  |               |
|    approx_kl            | 0.00022745377 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -156          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.62         |
|    n_updates            | 31690         |
|    policy_gradient_loss | -0.000393     |
|    std                  | 24.7          |
|    value_loss           | 0.444         |
-------------------------------------------
Iteration: 3171 | Episodes: 128600 | Median Reward: 38.67 | Max Reward: 48.88
Iteration: 3173 | Episodes: 128700 | Median Reward: 40.85 | Max Reward: 48.88
Iteration: 3175 | Episodes: 128800 | Median Reward: 43.38 | Max Reward: 48.88
Iteration: 3178 | Episodes: 128900 | Median Reward: 37.07 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.5         |
| time/                   |               |
|    fps                  | 557           |
|    iterations           | 3180          |
|    time_elapsed         | 23376         |
|    total_timesteps      | 13025280      |
| train/                  |               |
|    approx_kl            | 0.00029914468 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -156          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.75         |
|    n_updates            | 31790         |
|    policy_gradient_loss | -0.00101      |
|    std                  | 25.2          |
|    value_loss           | 0.334         |
-------------------------------------------
Iteration: 3180 | Episodes: 129000 | Median Reward: 39.18 | Max Reward: 48.88
Iteration: 3183 | Episodes: 129100 | Median Reward: 39.02 | Max Reward: 48.88
Iteration: 3185 | Episodes: 129200 | Median Reward: 38.51 | Max Reward: 48.88
Iteration: 3188 | Episodes: 129300 | Median Reward: 35.36 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.3         |
| time/                   |               |
|    fps                  | 556           |
|    iterations           | 3190          |
|    time_elapsed         | 23464         |
|    total_timesteps      | 13066240      |
| train/                  |               |
|    approx_kl            | 0.00044319208 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -157          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.56         |
|    n_updates            | 31890         |
|    policy_gradient_loss | -0.00308      |
|    std                  | 25.5          |
|    value_loss           | 0.871         |
-------------------------------------------
Iteration: 3190 | Episodes: 129400 | Median Reward: 38.28 | Max Reward: 48.88
Iteration: 3193 | Episodes: 129500 | Median Reward: 36.36 | Max Reward: 48.88
Iteration: 3195 | Episodes: 129600 | Median Reward: 36.70 | Max Reward: 48.88
Iteration: 3198 | Episodes: 129700 | Median Reward: 39.58 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60           |
| time/                   |               |
|    fps                  | 556           |
|    iterations           | 3200          |
|    time_elapsed         | 23540         |
|    total_timesteps      | 13107200      |
| train/                  |               |
|    approx_kl            | 2.5967835e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -157          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.72         |
|    n_updates            | 31990         |
|    policy_gradient_loss | -5.55e-06     |
|    std                  | 25.8          |
|    value_loss           | 0.329         |
-------------------------------------------
Iteration: 3200 | Episodes: 129800 | Median Reward: 41.01 | Max Reward: 48.88
Iteration: 3203 | Episodes: 129900 | Median Reward: 37.61 | Max Reward: 48.88
Iteration: 3205 | Episodes: 130000 | Median Reward: 40.97 | Max Reward: 48.88
Iteration: 3208 | Episodes: 130100 | Median Reward: 38.94 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.6       |
| time/                   |             |
|    fps                  | 556         |
|    iterations           | 3210        |
|    time_elapsed         | 23630       |
|    total_timesteps      | 13148160    |
| train/                  |             |
|    approx_kl            | 0.000452194 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -157        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.71       |
|    n_updates            | 32090       |
|    policy_gradient_loss | -0.00147    |
|    std                  | 26.1        |
|    value_loss           | 0.421       |
-----------------------------------------
Iteration: 3210 | Episodes: 130200 | Median Reward: 34.12 | Max Reward: 48.88
Iteration: 3212 | Episodes: 130300 | Median Reward: 35.50 | Max Reward: 48.88
Iteration: 3215 | Episodes: 130400 | Median Reward: 41.55 | Max Reward: 48.88
Iteration: 3217 | Episodes: 130500 | Median Reward: 40.93 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60           |
| time/                   |               |
|    fps                  | 556           |
|    iterations           | 3220          |
|    time_elapsed         | 23708         |
|    total_timesteps      | 13189120      |
| train/                  |               |
|    approx_kl            | 0.00013401282 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -157          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.8          |
|    n_updates            | 32190         |
|    policy_gradient_loss | -0.000921     |
|    std                  | 26.4          |
|    value_loss           | 0.337         |
-------------------------------------------
Iteration: 3220 | Episodes: 130600 | Median Reward: 38.83 | Max Reward: 48.88
Iteration: 3222 | Episodes: 130700 | Median Reward: 37.98 | Max Reward: 48.88
Iteration: 3225 | Episodes: 130800 | Median Reward: 39.50 | Max Reward: 48.88
Iteration: 3227 | Episodes: 130900 | Median Reward: 41.17 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.3         |
| time/                   |               |
|    fps                  | 556           |
|    iterations           | 3230          |
|    time_elapsed         | 23793         |
|    total_timesteps      | 13230080      |
| train/                  |               |
|    approx_kl            | 1.1311713e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -157          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.58         |
|    n_updates            | 32290         |
|    policy_gradient_loss | 4.35e-05      |
|    std                  | 26.5          |
|    value_loss           | 0.531         |
-------------------------------------------
Iteration: 3230 | Episodes: 131000 | Median Reward: 39.99 | Max Reward: 48.88
Iteration: 3232 | Episodes: 131100 | Median Reward: 38.23 | Max Reward: 48.88
Iteration: 3235 | Episodes: 131200 | Median Reward: 38.06 | Max Reward: 48.88
Iteration: 3237 | Episodes: 131300 | Median Reward: 39.20 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.1         |
| time/                   |               |
|    fps                  | 555           |
|    iterations           | 3240          |
|    time_elapsed         | 23874         |
|    total_timesteps      | 13271040      |
| train/                  |               |
|    approx_kl            | 2.4413515e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -158          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.12         |
|    n_updates            | 32390         |
|    policy_gradient_loss | -9.7e-05      |
|    std                  | 26.8          |
|    value_loss           | 1.54          |
-------------------------------------------
Iteration: 3240 | Episodes: 131400 | Median Reward: 39.32 | Max Reward: 48.88
Iteration: 3242 | Episodes: 131500 | Median Reward: 41.46 | Max Reward: 48.88
Iteration: 3245 | Episodes: 131600 | Median Reward: 35.75 | Max Reward: 48.88
Iteration: 3247 | Episodes: 131700 | Median Reward: 39.67 | Max Reward: 48.88
Iteration: 3249 | Episodes: 131800 | Median Reward: 36.07 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.5        |
| time/                   |              |
|    fps                  | 555          |
|    iterations           | 3250         |
|    time_elapsed         | 23950        |
|    total_timesteps      | 13312000     |
| train/                  |              |
|    approx_kl            | 0.0012327594 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -158         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.73        |
|    n_updates            | 32490        |
|    policy_gradient_loss | -0.00825     |
|    std                  | 27           |
|    value_loss           | 0.497        |
------------------------------------------
Iteration: 3252 | Episodes: 131900 | Median Reward: 38.42 | Max Reward: 48.88
Iteration: 3254 | Episodes: 132000 | Median Reward: 37.79 | Max Reward: 48.88
Iteration: 3257 | Episodes: 132100 | Median Reward: 38.77 | Max Reward: 48.88
Iteration: 3259 | Episodes: 132200 | Median Reward: 38.49 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.9         |
| time/                   |               |
|    fps                  | 555           |
|    iterations           | 3260          |
|    time_elapsed         | 24042         |
|    total_timesteps      | 13352960      |
| train/                  |               |
|    approx_kl            | 0.00033493145 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -158          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.75         |
|    n_updates            | 32590         |
|    policy_gradient_loss | -0.00152      |
|    std                  | 27.2          |
|    value_loss           | 0.369         |
-------------------------------------------
Iteration: 3262 | Episodes: 132300 | Median Reward: 38.09 | Max Reward: 48.88
Iteration: 3264 | Episodes: 132400 | Median Reward: 34.96 | Max Reward: 48.88
Iteration: 3267 | Episodes: 132500 | Median Reward: 38.74 | Max Reward: 48.88
Iteration: 3269 | Episodes: 132600 | Median Reward: 39.06 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.5        |
| time/                   |              |
|    fps                  | 555          |
|    iterations           | 3270         |
|    time_elapsed         | 24119        |
|    total_timesteps      | 13393920     |
| train/                  |              |
|    approx_kl            | 4.027663e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -158         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.69        |
|    n_updates            | 32690        |
|    policy_gradient_loss | -0.000401    |
|    std                  | 27.4         |
|    value_loss           | 0.339        |
------------------------------------------
Iteration: 3272 | Episodes: 132700 | Median Reward: 39.06 | Max Reward: 48.88
Iteration: 3274 | Episodes: 132800 | Median Reward: 39.24 | Max Reward: 48.88
Iteration: 3277 | Episodes: 132900 | Median Reward: 39.01 | Max Reward: 48.88
Iteration: 3279 | Episodes: 133000 | Median Reward: 39.13 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.6         |
| time/                   |               |
|    fps                  | 554           |
|    iterations           | 3280          |
|    time_elapsed         | 24209         |
|    total_timesteps      | 13434880      |
| train/                  |               |
|    approx_kl            | 0.00097526785 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -158          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.8          |
|    n_updates            | 32790         |
|    policy_gradient_loss | -0.0074       |
|    std                  | 27.6          |
|    value_loss           | 0.328         |
-------------------------------------------
Iteration: 3282 | Episodes: 133100 | Median Reward: 36.02 | Max Reward: 48.88
Iteration: 3284 | Episodes: 133200 | Median Reward: 38.85 | Max Reward: 48.88
Iteration: 3286 | Episodes: 133300 | Median Reward: 38.91 | Max Reward: 48.88
Iteration: 3289 | Episodes: 133400 | Median Reward: 38.43 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.5         |
| time/                   |               |
|    fps                  | 554           |
|    iterations           | 3290          |
|    time_elapsed         | 24290         |
|    total_timesteps      | 13475840      |
| train/                  |               |
|    approx_kl            | 0.00067109335 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -158          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.58         |
|    n_updates            | 32890         |
|    policy_gradient_loss | -0.00345      |
|    std                  | 27.9          |
|    value_loss           | 0.674         |
-------------------------------------------
Iteration: 3291 | Episodes: 133500 | Median Reward: 39.57 | Max Reward: 48.88
Iteration: 3294 | Episodes: 133600 | Median Reward: 38.53 | Max Reward: 48.88
Iteration: 3296 | Episodes: 133700 | Median Reward: 35.91 | Max Reward: 48.88
Iteration: 3299 | Episodes: 133800 | Median Reward: 41.97 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61           |
| time/                   |               |
|    fps                  | 554           |
|    iterations           | 3300          |
|    time_elapsed         | 24371         |
|    total_timesteps      | 13516800      |
| train/                  |               |
|    approx_kl            | 0.00012420962 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -158          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.46         |
|    n_updates            | 32990         |
|    policy_gradient_loss | -0.000805     |
|    std                  | 28            |
|    value_loss           | 0.671         |
-------------------------------------------
Iteration: 3301 | Episodes: 133900 | Median Reward: 36.02 | Max Reward: 48.88
Iteration: 3304 | Episodes: 134000 | Median Reward: 37.77 | Max Reward: 48.88
Iteration: 3306 | Episodes: 134100 | Median Reward: 40.03 | Max Reward: 48.88
Iteration: 3309 | Episodes: 134200 | Median Reward: 38.62 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -60.6          |
| time/                   |                |
|    fps                  | 554            |
|    iterations           | 3310           |
|    time_elapsed         | 24458          |
|    total_timesteps      | 13557760       |
| train/                  |                |
|    approx_kl            | 1.22860365e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -159           |
|    explained_variance   | 0.998          |
|    learning_rate        | 0.0005         |
|    loss                 | -7.51          |
|    n_updates            | 33090          |
|    policy_gradient_loss | -0.000231      |
|    std                  | 28.3           |
|    value_loss           | 1.07           |
--------------------------------------------
Iteration: 3311 | Episodes: 134300 | Median Reward: 41.72 | Max Reward: 48.88
Iteration: 3314 | Episodes: 134400 | Median Reward: 38.11 | Max Reward: 48.88
Iteration: 3316 | Episodes: 134500 | Median Reward: 38.98 | Max Reward: 48.88
Iteration: 3318 | Episodes: 134600 | Median Reward: 39.43 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.2         |
| time/                   |               |
|    fps                  | 554           |
|    iterations           | 3320          |
|    time_elapsed         | 24537         |
|    total_timesteps      | 13598720      |
| train/                  |               |
|    approx_kl            | 0.00014712926 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -159          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.73         |
|    n_updates            | 33190         |
|    policy_gradient_loss | -6.63e-05     |
|    std                  | 28.6          |
|    value_loss           | 0.524         |
-------------------------------------------
Iteration: 3321 | Episodes: 134700 | Median Reward: 38.84 | Max Reward: 48.88
Iteration: 3323 | Episodes: 134800 | Median Reward: 41.00 | Max Reward: 48.88
Iteration: 3326 | Episodes: 134900 | Median Reward: 37.88 | Max Reward: 48.88
Iteration: 3328 | Episodes: 135000 | Median Reward: 38.80 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.6         |
| time/                   |               |
|    fps                  | 553           |
|    iterations           | 3330          |
|    time_elapsed         | 24630         |
|    total_timesteps      | 13639680      |
| train/                  |               |
|    approx_kl            | 0.00021015466 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -159          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.75         |
|    n_updates            | 33290         |
|    policy_gradient_loss | -0.00232      |
|    std                  | 28.7          |
|    value_loss           | 0.439         |
-------------------------------------------
Iteration: 3331 | Episodes: 135100 | Median Reward: 40.89 | Max Reward: 48.88
Iteration: 3333 | Episodes: 135200 | Median Reward: 39.83 | Max Reward: 48.88
Iteration: 3336 | Episodes: 135300 | Median Reward: 39.43 | Max Reward: 48.88
Iteration: 3338 | Episodes: 135400 | Median Reward: 38.65 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.1         |
| time/                   |               |
|    fps                  | 553           |
|    iterations           | 3340          |
|    time_elapsed         | 24706         |
|    total_timesteps      | 13680640      |
| train/                  |               |
|    approx_kl            | 0.00047660587 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -159          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.71         |
|    n_updates            | 33390         |
|    policy_gradient_loss | -0.00214      |
|    std                  | 29            |
|    value_loss           | 0.432         |
-------------------------------------------
Iteration: 3341 | Episodes: 135500 | Median Reward: 39.03 | Max Reward: 48.88
Iteration: 3343 | Episodes: 135600 | Median Reward: 41.55 | Max Reward: 48.88
Iteration: 3346 | Episodes: 135700 | Median Reward: 35.61 | Max Reward: 48.88
Iteration: 3348 | Episodes: 135800 | Median Reward: 38.79 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.4         |
| time/                   |               |
|    fps                  | 553           |
|    iterations           | 3350          |
|    time_elapsed         | 24792         |
|    total_timesteps      | 13721600      |
| train/                  |               |
|    approx_kl            | 0.00027617998 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -159          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.9          |
|    n_updates            | 33490         |
|    policy_gradient_loss | -0.000201     |
|    std                  | 29.3          |
|    value_loss           | 0.22          |
-------------------------------------------
Iteration: 3351 | Episodes: 135900 | Median Reward: 42.73 | Max Reward: 48.88
Iteration: 3353 | Episodes: 136000 | Median Reward: 39.58 | Max Reward: 48.88
Iteration: 3355 | Episodes: 136100 | Median Reward: 38.28 | Max Reward: 48.88
Iteration: 3358 | Episodes: 136200 | Median Reward: 38.06 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.2        |
| time/                   |              |
|    fps                  | 553          |
|    iterations           | 3360         |
|    time_elapsed         | 24877        |
|    total_timesteps      | 13762560     |
| train/                  |              |
|    approx_kl            | 8.695503e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -160         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.87        |
|    n_updates            | 33590        |
|    policy_gradient_loss | -0.000563    |
|    std                  | 29.5         |
|    value_loss           | 0.296        |
------------------------------------------
Iteration: 3360 | Episodes: 136300 | Median Reward: 36.94 | Max Reward: 48.88
Iteration: 3363 | Episodes: 136400 | Median Reward: 42.29 | Max Reward: 48.88
Iteration: 3365 | Episodes: 136500 | Median Reward: 39.78 | Max Reward: 48.88
Iteration: 3368 | Episodes: 136600 | Median Reward: 37.60 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.6         |
| time/                   |               |
|    fps                  | 553           |
|    iterations           | 3370          |
|    time_elapsed         | 24955         |
|    total_timesteps      | 13803520      |
| train/                  |               |
|    approx_kl            | 2.2166932e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -160          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.68         |
|    n_updates            | 33690         |
|    policy_gradient_loss | 1.23e-05      |
|    std                  | 29.7          |
|    value_loss           | 1.19          |
-------------------------------------------
Iteration: 3370 | Episodes: 136700 | Median Reward: 37.10 | Max Reward: 48.88
Iteration: 3373 | Episodes: 136800 | Median Reward: 38.25 | Max Reward: 48.88
Iteration: 3375 | Episodes: 136900 | Median Reward: 40.05 | Max Reward: 48.88
Iteration: 3378 | Episodes: 137000 | Median Reward: 38.52 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.7         |
| time/                   |               |
|    fps                  | 552           |
|    iterations           | 3380          |
|    time_elapsed         | 25049         |
|    total_timesteps      | 13844480      |
| train/                  |               |
|    approx_kl            | 0.00019943158 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -160          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.85         |
|    n_updates            | 33790         |
|    policy_gradient_loss | -0.00165      |
|    std                  | 29.9          |
|    value_loss           | 1.05          |
-------------------------------------------
Iteration: 3380 | Episodes: 137100 | Median Reward: 38.59 | Max Reward: 48.88
Iteration: 3383 | Episodes: 137200 | Median Reward: 36.88 | Max Reward: 48.88
Iteration: 3385 | Episodes: 137300 | Median Reward: 38.56 | Max Reward: 48.88
Iteration: 3388 | Episodes: 137400 | Median Reward: 39.54 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.5         |
| time/                   |               |
|    fps                  | 552           |
|    iterations           | 3390          |
|    time_elapsed         | 25127         |
|    total_timesteps      | 13885440      |
| train/                  |               |
|    approx_kl            | 2.1216547e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -160          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.9          |
|    n_updates            | 33890         |
|    policy_gradient_loss | -0.000233     |
|    std                  | 30.1          |
|    value_loss           | 0.391         |
-------------------------------------------
Iteration: 3390 | Episodes: 137500 | Median Reward: 42.43 | Max Reward: 48.88
Iteration: 3392 | Episodes: 137600 | Median Reward: 40.38 | Max Reward: 48.88
Iteration: 3395 | Episodes: 137700 | Median Reward: 38.11 | Max Reward: 48.88
Iteration: 3397 | Episodes: 137800 | Median Reward: 39.87 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.9         |
| time/                   |               |
|    fps                  | 552           |
|    iterations           | 3400          |
|    time_elapsed         | 25216         |
|    total_timesteps      | 13926400      |
| train/                  |               |
|    approx_kl            | 0.00016188239 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -160          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -7.87         |
|    n_updates            | 33990         |
|    policy_gradient_loss | -0.00161      |
|    std                  | 30.4          |
|    value_loss           | 0.302         |
-------------------------------------------
Iteration: 3400 | Episodes: 137900 | Median Reward: 38.56 | Max Reward: 48.88
Iteration: 3402 | Episodes: 138000 | Median Reward: 37.70 | Max Reward: 48.88
Iteration: 3405 | Episodes: 138100 | Median Reward: 40.83 | Max Reward: 48.88
Iteration: 3407 | Episodes: 138200 | Median Reward: 38.67 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.9         |
| time/                   |               |
|    fps                  | 552           |
|    iterations           | 3410          |
|    time_elapsed         | 25300         |
|    total_timesteps      | 13967360      |
| train/                  |               |
|    approx_kl            | 4.3589433e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -160          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.67         |
|    n_updates            | 34090         |
|    policy_gradient_loss | 0.000305      |
|    std                  | 30.8          |
|    value_loss           | 0.782         |
-------------------------------------------
Iteration: 3410 | Episodes: 138300 | Median Reward: 39.89 | Max Reward: 48.88
Iteration: 3412 | Episodes: 138400 | Median Reward: 39.89 | Max Reward: 48.88
Iteration: 3415 | Episodes: 138500 | Median Reward: 38.35 | Max Reward: 48.88
Iteration: 3417 | Episodes: 138600 | Median Reward: 38.33 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63           |
| time/                   |               |
|    fps                  | 551           |
|    iterations           | 3420          |
|    time_elapsed         | 25379         |
|    total_timesteps      | 14008320      |
| train/                  |               |
|    approx_kl            | 0.00014955121 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -161          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.57         |
|    n_updates            | 34190         |
|    policy_gradient_loss | -0.00122      |
|    std                  | 31            |
|    value_loss           | 1.24          |
-------------------------------------------
Iteration: 3420 | Episodes: 138700 | Median Reward: 38.93 | Max Reward: 48.88
Iteration: 3422 | Episodes: 138800 | Median Reward: 34.93 | Max Reward: 48.88
Iteration: 3425 | Episodes: 138900 | Median Reward: 35.69 | Max Reward: 48.88
Iteration: 3427 | Episodes: 139000 | Median Reward: 42.64 | Max Reward: 48.88
Iteration: 3429 | Episodes: 139100 | Median Reward: 36.50 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.5         |
| time/                   |               |
|    fps                  | 551           |
|    iterations           | 3430          |
|    time_elapsed         | 25471         |
|    total_timesteps      | 14049280      |
| train/                  |               |
|    approx_kl            | 0.00016695686 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -161          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.88         |
|    n_updates            | 34290         |
|    policy_gradient_loss | -0.000791     |
|    std                  | 31.2          |
|    value_loss           | 0.464         |
-------------------------------------------
Iteration: 3432 | Episodes: 139200 | Median Reward: 34.84 | Max Reward: 48.88
Iteration: 3434 | Episodes: 139300 | Median Reward: 41.08 | Max Reward: 48.88
Iteration: 3437 | Episodes: 139400 | Median Reward: 38.68 | Max Reward: 48.88
Iteration: 3439 | Episodes: 139500 | Median Reward: 40.03 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.5         |
| time/                   |               |
|    fps                  | 551           |
|    iterations           | 3440          |
|    time_elapsed         | 25547         |
|    total_timesteps      | 14090240      |
| train/                  |               |
|    approx_kl            | 7.9540216e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -161          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.82         |
|    n_updates            | 34390         |
|    policy_gradient_loss | -0.000919     |
|    std                  | 31.6          |
|    value_loss           | 0.831         |
-------------------------------------------
Iteration: 3442 | Episodes: 139600 | Median Reward: 35.92 | Max Reward: 48.88
Iteration: 3444 | Episodes: 139700 | Median Reward: 39.16 | Max Reward: 48.88
Iteration: 3447 | Episodes: 139800 | Median Reward: 38.79 | Max Reward: 48.88
Iteration: 3449 | Episodes: 139900 | Median Reward: 40.57 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.6         |
| time/                   |               |
|    fps                  | 551           |
|    iterations           | 3450          |
|    time_elapsed         | 25636         |
|    total_timesteps      | 14131200      |
| train/                  |               |
|    approx_kl            | 4.4886285e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -161          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.62         |
|    n_updates            | 34490         |
|    policy_gradient_loss | 2.76e-05      |
|    std                  | 31.8          |
|    value_loss           | 0.836         |
-------------------------------------------
Iteration: 3452 | Episodes: 140000 | Median Reward: 39.80 | Max Reward: 48.88
Iteration: 3454 | Episodes: 140100 | Median Reward: 35.96 | Max Reward: 48.88
Iteration: 3457 | Episodes: 140200 | Median Reward: 38.33 | Max Reward: 48.88
Iteration: 3459 | Episodes: 140300 | Median Reward: 38.35 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63          |
| time/                   |              |
|    fps                  | 551          |
|    iterations           | 3460         |
|    time_elapsed         | 25718        |
|    total_timesteps      | 14172160     |
| train/                  |              |
|    approx_kl            | 0.0015221073 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -161         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -7.9         |
|    n_updates            | 34590        |
|    policy_gradient_loss | -0.00563     |
|    std                  | 32.2         |
|    value_loss           | 0.305        |
------------------------------------------
Iteration: 3462 | Episodes: 140400 | Median Reward: 36.00 | Max Reward: 48.88
Iteration: 3464 | Episodes: 140500 | Median Reward: 38.31 | Max Reward: 48.88
Iteration: 3466 | Episodes: 140600 | Median Reward: 35.52 | Max Reward: 48.88
Iteration: 3469 | Episodes: 140700 | Median Reward: 38.60 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.5        |
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 3470         |
|    time_elapsed         | 25796        |
|    total_timesteps      | 14213120     |
| train/                  |              |
|    approx_kl            | 8.866537e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -161         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.79        |
|    n_updates            | 34690        |
|    policy_gradient_loss | -0.000956    |
|    std                  | 32.4         |
|    value_loss           | 0.499        |
------------------------------------------
Iteration: 3471 | Episodes: 140800 | Median Reward: 38.26 | Max Reward: 48.88
Iteration: 3474 | Episodes: 140900 | Median Reward: 36.58 | Max Reward: 48.88
Iteration: 3476 | Episodes: 141000 | Median Reward: 36.78 | Max Reward: 48.88
Iteration: 3479 | Episodes: 141100 | Median Reward: 37.10 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.4        |
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 3480         |
|    time_elapsed         | 25887        |
|    total_timesteps      | 14254080     |
| train/                  |              |
|    approx_kl            | 3.373754e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -162         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.77        |
|    n_updates            | 34790        |
|    policy_gradient_loss | -0.000141    |
|    std                  | 32.6         |
|    value_loss           | 0.359        |
------------------------------------------
Iteration: 3481 | Episodes: 141200 | Median Reward: 38.90 | Max Reward: 48.88
Iteration: 3484 | Episodes: 141300 | Median Reward: 42.33 | Max Reward: 48.88
Iteration: 3486 | Episodes: 141400 | Median Reward: 38.61 | Max Reward: 48.88
Iteration: 3489 | Episodes: 141500 | Median Reward: 35.49 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.7         |
| time/                   |               |
|    fps                  | 550           |
|    iterations           | 3490          |
|    time_elapsed         | 25966         |
|    total_timesteps      | 14295040      |
| train/                  |               |
|    approx_kl            | 0.00015382336 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -162          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.96         |
|    n_updates            | 34890         |
|    policy_gradient_loss | -0.000414     |
|    std                  | 32.9          |
|    value_loss           | 0.513         |
-------------------------------------------
Iteration: 3491 | Episodes: 141600 | Median Reward: 40.84 | Max Reward: 48.88
Iteration: 3494 | Episodes: 141700 | Median Reward: 40.37 | Max Reward: 48.88
Iteration: 3496 | Episodes: 141800 | Median Reward: 40.74 | Max Reward: 48.88
Iteration: 3498 | Episodes: 141900 | Median Reward: 40.74 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.7        |
| time/                   |              |
|    fps                  | 550          |
|    iterations           | 3500         |
|    time_elapsed         | 26055        |
|    total_timesteps      | 14336000     |
| train/                  |              |
|    approx_kl            | 0.0006921225 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -162         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.89        |
|    n_updates            | 34990        |
|    policy_gradient_loss | -0.000497    |
|    std                  | 33.2         |
|    value_loss           | 0.51         |
------------------------------------------
Iteration: 3501 | Episodes: 142000 | Median Reward: 39.98 | Max Reward: 48.88
Iteration: 3503 | Episodes: 142100 | Median Reward: 39.52 | Max Reward: 48.88
Iteration: 3506 | Episodes: 142200 | Median Reward: 34.47 | Max Reward: 48.88
Iteration: 3508 | Episodes: 142300 | Median Reward: 37.05 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.4         |
| time/                   |               |
|    fps                  | 550           |
|    iterations           | 3510          |
|    time_elapsed         | 26134         |
|    total_timesteps      | 14376960      |
| train/                  |               |
|    approx_kl            | 2.2393506e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -162          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.01         |
|    n_updates            | 35090         |
|    policy_gradient_loss | -0.000432     |
|    std                  | 33.6          |
|    value_loss           | 0.548         |
-------------------------------------------
Iteration: 3511 | Episodes: 142400 | Median Reward: 35.84 | Max Reward: 48.88
Iteration: 3513 | Episodes: 142500 | Median Reward: 37.40 | Max Reward: 48.88
Iteration: 3516 | Episodes: 142600 | Median Reward: 40.70 | Max Reward: 48.88
Iteration: 3518 | Episodes: 142700 | Median Reward: 41.68 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 3520         |
|    time_elapsed         | 26216        |
|    total_timesteps      | 14417920     |
| train/                  |              |
|    approx_kl            | 9.840945e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -162         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.93        |
|    n_updates            | 35190        |
|    policy_gradient_loss | -0.001       |
|    std                  | 33.9         |
|    value_loss           | 0.404        |
------------------------------------------
Iteration: 3521 | Episodes: 142800 | Median Reward: 38.40 | Max Reward: 48.88
Iteration: 3523 | Episodes: 142900 | Median Reward: 37.93 | Max Reward: 48.88
Iteration: 3526 | Episodes: 143000 | Median Reward: 39.19 | Max Reward: 48.88
Iteration: 3528 | Episodes: 143100 | Median Reward: 36.52 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.9        |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 3530         |
|    time_elapsed         | 26306        |
|    total_timesteps      | 14458880     |
| train/                  |              |
|    approx_kl            | 0.0006659628 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -163         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -8.04        |
|    n_updates            | 35290        |
|    policy_gradient_loss | -0.00188     |
|    std                  | 34.2         |
|    value_loss           | 0.368        |
------------------------------------------
Iteration: 3531 | Episodes: 143200 | Median Reward: 38.40 | Max Reward: 48.88
Iteration: 3533 | Episodes: 143300 | Median Reward: 38.72 | Max Reward: 48.88
Iteration: 3535 | Episodes: 143400 | Median Reward: 39.58 | Max Reward: 48.88
Iteration: 3538 | Episodes: 143500 | Median Reward: 34.04 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.3        |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 3540         |
|    time_elapsed         | 26386        |
|    total_timesteps      | 14499840     |
| train/                  |              |
|    approx_kl            | 0.0006967975 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -163         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.09        |
|    n_updates            | 35390        |
|    policy_gradient_loss | -0.00158     |
|    std                  | 34.5         |
|    value_loss           | 0.157        |
------------------------------------------
Iteration: 3540 | Episodes: 143600 | Median Reward: 36.57 | Max Reward: 48.88
Iteration: 3543 | Episodes: 143700 | Median Reward: 38.57 | Max Reward: 48.88
Iteration: 3545 | Episodes: 143800 | Median Reward: 35.65 | Max Reward: 48.88
Iteration: 3548 | Episodes: 143900 | Median Reward: 34.51 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.5         |
| time/                   |               |
|    fps                  | 549           |
|    iterations           | 3550          |
|    time_elapsed         | 26479         |
|    total_timesteps      | 14540800      |
| train/                  |               |
|    approx_kl            | 0.00090621447 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -163          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.61         |
|    n_updates            | 35490         |
|    policy_gradient_loss | -0.00294      |
|    std                  | 35            |
|    value_loss           | 1.47          |
-------------------------------------------
Iteration: 3550 | Episodes: 144000 | Median Reward: 38.94 | Max Reward: 48.88
Iteration: 3553 | Episodes: 144100 | Median Reward: 39.22 | Max Reward: 48.88
Iteration: 3555 | Episodes: 144200 | Median Reward: 38.73 | Max Reward: 48.88
Iteration: 3558 | Episodes: 144300 | Median Reward: 42.29 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61          |
| time/                   |              |
|    fps                  | 549          |
|    iterations           | 3560         |
|    time_elapsed         | 26559        |
|    total_timesteps      | 14581760     |
| train/                  |              |
|    approx_kl            | 0.0026551995 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -163         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.99        |
|    n_updates            | 35590        |
|    policy_gradient_loss | -0.00355     |
|    std                  | 35.2         |
|    value_loss           | 0.367        |
------------------------------------------
Iteration: 3560 | Episodes: 144400 | Median Reward: 41.82 | Max Reward: 48.88
Iteration: 3563 | Episodes: 144500 | Median Reward: 36.11 | Max Reward: 48.88
Iteration: 3565 | Episodes: 144600 | Median Reward: 36.75 | Max Reward: 48.88
Iteration: 3568 | Episodes: 144700 | Median Reward: 38.04 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.1         |
| time/                   |               |
|    fps                  | 548           |
|    iterations           | 3570          |
|    time_elapsed         | 26642         |
|    total_timesteps      | 14622720      |
| train/                  |               |
|    approx_kl            | 2.1034692e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -163          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8            |
|    n_updates            | 35690         |
|    policy_gradient_loss | -0.000167     |
|    std                  | 35.5          |
|    value_loss           | 0.274         |
-------------------------------------------
Iteration: 3570 | Episodes: 144800 | Median Reward: 36.02 | Max Reward: 48.88
Iteration: 3572 | Episodes: 144900 | Median Reward: 39.09 | Max Reward: 48.88
Iteration: 3575 | Episodes: 145000 | Median Reward: 36.42 | Max Reward: 48.88
Iteration: 3577 | Episodes: 145100 | Median Reward: 38.62 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62          |
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 3580         |
|    time_elapsed         | 26730        |
|    total_timesteps      | 14663680     |
| train/                  |              |
|    approx_kl            | 9.826719e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -164         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.88        |
|    n_updates            | 35790        |
|    policy_gradient_loss | -0.00056     |
|    std                  | 35.9         |
|    value_loss           | 0.704        |
------------------------------------------
Iteration: 3580 | Episodes: 145200 | Median Reward: 39.31 | Max Reward: 48.88
Iteration: 3582 | Episodes: 145300 | Median Reward: 36.64 | Max Reward: 48.88
Iteration: 3585 | Episodes: 145400 | Median Reward: 37.23 | Max Reward: 48.88
Iteration: 3587 | Episodes: 145500 | Median Reward: 37.90 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.7        |
| time/                   |              |
|    fps                  | 548          |
|    iterations           | 3590         |
|    time_elapsed         | 26808        |
|    total_timesteps      | 14704640     |
| train/                  |              |
|    approx_kl            | 0.0001394146 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -164         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.98        |
|    n_updates            | 35890        |
|    policy_gradient_loss | -0.00126     |
|    std                  | 36.3         |
|    value_loss           | 0.417        |
------------------------------------------
Iteration: 3590 | Episodes: 145600 | Median Reward: 35.60 | Max Reward: 48.88
Iteration: 3592 | Episodes: 145700 | Median Reward: 37.94 | Max Reward: 48.88
Iteration: 3595 | Episodes: 145800 | Median Reward: 38.13 | Max Reward: 48.88
Iteration: 3597 | Episodes: 145900 | Median Reward: 38.23 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.8         |
| time/                   |               |
|    fps                  | 548           |
|    iterations           | 3600          |
|    time_elapsed         | 26903         |
|    total_timesteps      | 14745600      |
| train/                  |               |
|    approx_kl            | 3.8031052e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -164          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.02         |
|    n_updates            | 35990         |
|    policy_gradient_loss | -0.000103     |
|    std                  | 36.8          |
|    value_loss           | 0.627         |
-------------------------------------------
Iteration: 3600 | Episodes: 146000 | Median Reward: 35.57 | Max Reward: 48.88
Iteration: 3602 | Episodes: 146100 | Median Reward: 35.41 | Max Reward: 48.88
Iteration: 3605 | Episodes: 146200 | Median Reward: 41.34 | Max Reward: 48.88
Iteration: 3607 | Episodes: 146300 | Median Reward: 38.63 | Max Reward: 48.88
Iteration: 3609 | Episodes: 146400 | Median Reward: 34.63 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.7       |
| time/                   |             |
|    fps                  | 547         |
|    iterations           | 3610        |
|    time_elapsed         | 26983       |
|    total_timesteps      | 14786560    |
| train/                  |             |
|    approx_kl            | 8.28318e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -164        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -8.11       |
|    n_updates            | 36090       |
|    policy_gradient_loss | -0.000209   |
|    std                  | 37          |
|    value_loss           | 0.327       |
-----------------------------------------
Iteration: 3612 | Episodes: 146500 | Median Reward: 38.08 | Max Reward: 48.88
Iteration: 3614 | Episodes: 146600 | Median Reward: 36.81 | Max Reward: 48.88
Iteration: 3617 | Episodes: 146700 | Median Reward: 39.20 | Max Reward: 48.88
Iteration: 3619 | Episodes: 146800 | Median Reward: 42.78 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.1        |
| time/                   |              |
|    fps                  | 547          |
|    iterations           | 3620         |
|    time_elapsed         | 27066        |
|    total_timesteps      | 14827520     |
| train/                  |              |
|    approx_kl            | 0.0013749364 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -164         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.13        |
|    n_updates            | 36190        |
|    policy_gradient_loss | -0.00899     |
|    std                  | 37.3         |
|    value_loss           | 0.423        |
------------------------------------------
Iteration: 3622 | Episodes: 146900 | Median Reward: 34.62 | Max Reward: 48.88
Iteration: 3624 | Episodes: 147000 | Median Reward: 36.04 | Max Reward: 48.88
Iteration: 3627 | Episodes: 147100 | Median Reward: 35.95 | Max Reward: 48.88
Iteration: 3629 | Episodes: 147200 | Median Reward: 37.12 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62           |
| time/                   |               |
|    fps                  | 547           |
|    iterations           | 3630          |
|    time_elapsed         | 27154         |
|    total_timesteps      | 14868480      |
| train/                  |               |
|    approx_kl            | 3.7805716e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -164          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -8.06         |
|    n_updates            | 36290         |
|    policy_gradient_loss | -0.000426     |
|    std                  | 37.6          |
|    value_loss           | 0.554         |
-------------------------------------------
Iteration: 3632 | Episodes: 147300 | Median Reward: 37.89 | Max Reward: 48.88
Iteration: 3634 | Episodes: 147400 | Median Reward: 34.63 | Max Reward: 48.88
Iteration: 3637 | Episodes: 147500 | Median Reward: 42.65 | Max Reward: 48.88
Iteration: 3639 | Episodes: 147600 | Median Reward: 38.79 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.7         |
| time/                   |               |
|    fps                  | 547           |
|    iterations           | 3640          |
|    time_elapsed         | 27233         |
|    total_timesteps      | 14909440      |
| train/                  |               |
|    approx_kl            | 3.5050587e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -165          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.92         |
|    n_updates            | 36390         |
|    policy_gradient_loss | -0.0004       |
|    std                  | 38            |
|    value_loss           | 0.759         |
-------------------------------------------
Iteration: 3642 | Episodes: 147700 | Median Reward: 36.22 | Max Reward: 48.88
Iteration: 3644 | Episodes: 147800 | Median Reward: 38.51 | Max Reward: 48.88
Iteration: 3646 | Episodes: 147900 | Median Reward: 38.92 | Max Reward: 48.88
Iteration: 3649 | Episodes: 148000 | Median Reward: 35.11 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63           |
| time/                   |               |
|    fps                  | 547           |
|    iterations           | 3650          |
|    time_elapsed         | 27326         |
|    total_timesteps      | 14950400      |
| train/                  |               |
|    approx_kl            | 0.00037416772 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -165          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.08         |
|    n_updates            | 36490         |
|    policy_gradient_loss | -0.00119      |
|    std                  | 38.6          |
|    value_loss           | 0.382         |
-------------------------------------------
Iteration: 3651 | Episodes: 148100 | Median Reward: 37.56 | Max Reward: 48.88
Iteration: 3654 | Episodes: 148200 | Median Reward: 39.22 | Max Reward: 48.88
Iteration: 3656 | Episodes: 148300 | Median Reward: 38.42 | Max Reward: 48.88
Iteration: 3659 | Episodes: 148400 | Median Reward: 38.58 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.4         |
| time/                   |               |
|    fps                  | 547           |
|    iterations           | 3660          |
|    time_elapsed         | 27406         |
|    total_timesteps      | 14991360      |
| train/                  |               |
|    approx_kl            | 0.00012885472 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -165          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.72         |
|    n_updates            | 36590         |
|    policy_gradient_loss | -0.00191      |
|    std                  | 39.2          |
|    value_loss           | 0.609         |
-------------------------------------------
Iteration: 3661 | Episodes: 148500 | Median Reward: 38.71 | Max Reward: 48.88
Iteration: 3664 | Episodes: 148600 | Median Reward: 37.26 | Max Reward: 48.88
Iteration: 3666 | Episodes: 148700 | Median Reward: 38.63 | Max Reward: 48.88
Iteration: 3669 | Episodes: 148800 | Median Reward: 40.11 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.5       |
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 3670        |
|    time_elapsed         | 27489       |
|    total_timesteps      | 15032320    |
| train/                  |             |
|    approx_kl            | 0.000728825 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -165        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -8.11       |
|    n_updates            | 36690       |
|    policy_gradient_loss | -0.00285    |
|    std                  | 39.5        |
|    value_loss           | 0.403       |
-----------------------------------------
Iteration: 3671 | Episodes: 148900 | Median Reward: 42.29 | Max Reward: 48.88
Iteration: 3674 | Episodes: 149000 | Median Reward: 40.45 | Max Reward: 48.88
Iteration: 3676 | Episodes: 149100 | Median Reward: 34.43 | Max Reward: 48.88
Iteration: 3679 | Episodes: 149200 | Median Reward: 35.22 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.3        |
| time/                   |              |
|    fps                  | 546          |
|    iterations           | 3680         |
|    time_elapsed         | 27577        |
|    total_timesteps      | 15073280     |
| train/                  |              |
|    approx_kl            | 7.508148e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -166         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.53        |
|    n_updates            | 36790        |
|    policy_gradient_loss | -0.000128    |
|    std                  | 39.9         |
|    value_loss           | 2.05         |
------------------------------------------
Iteration: 3681 | Episodes: 149300 | Median Reward: 36.65 | Max Reward: 48.88
Iteration: 3683 | Episodes: 149400 | Median Reward: 35.85 | Max Reward: 48.88
Iteration: 3686 | Episodes: 149500 | Median Reward: 36.88 | Max Reward: 48.88
Iteration: 3688 | Episodes: 149600 | Median Reward: 31.75 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.9       |
| time/                   |             |
|    fps                  | 546         |
|    iterations           | 3690        |
|    time_elapsed         | 27656       |
|    total_timesteps      | 15114240    |
| train/                  |             |
|    approx_kl            | 0.001206196 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -166        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.92       |
|    n_updates            | 36890       |
|    policy_gradient_loss | -0.00487    |
|    std                  | 40.3        |
|    value_loss           | 0.797       |
-----------------------------------------
Iteration: 3691 | Episodes: 149700 | Median Reward: 42.08 | Max Reward: 48.88
Iteration: 3693 | Episodes: 149800 | Median Reward: 38.52 | Max Reward: 48.88
Iteration: 3696 | Episodes: 149900 | Median Reward: 39.60 | Max Reward: 48.88
Iteration: 3698 | Episodes: 150000 | Median Reward: 36.50 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.2         |
| time/                   |               |
|    fps                  | 546           |
|    iterations           | 3700          |
|    time_elapsed         | 27748         |
|    total_timesteps      | 15155200      |
| train/                  |               |
|    approx_kl            | 0.00017586803 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -166          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.06         |
|    n_updates            | 36990         |
|    policy_gradient_loss | -0.000308     |
|    std                  | 40.7          |
|    value_loss           | 0.736         |
-------------------------------------------
Iteration: 3701 | Episodes: 150100 | Median Reward: 39.40 | Max Reward: 48.88
Iteration: 3703 | Episodes: 150200 | Median Reward: 38.50 | Max Reward: 48.88
Iteration: 3706 | Episodes: 150300 | Median Reward: 39.11 | Max Reward: 48.88
Iteration: 3708 | Episodes: 150400 | Median Reward: 35.71 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62           |
| time/                   |               |
|    fps                  | 546           |
|    iterations           | 3710          |
|    time_elapsed         | 27829         |
|    total_timesteps      | 15196160      |
| train/                  |               |
|    approx_kl            | 4.1045132e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -166          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.99         |
|    n_updates            | 37090         |
|    policy_gradient_loss | -0.000421     |
|    std                  | 41.1          |
|    value_loss           | 0.676         |
-------------------------------------------
Iteration: 3711 | Episodes: 150500 | Median Reward: 36.93 | Max Reward: 48.88
Iteration: 3713 | Episodes: 150600 | Median Reward: 39.47 | Max Reward: 48.88
Iteration: 3715 | Episodes: 150700 | Median Reward: 40.27 | Max Reward: 48.88
Iteration: 3718 | Episodes: 150800 | Median Reward: 38.26 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.1        |
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 3720         |
|    time_elapsed         | 27914        |
|    total_timesteps      | 15237120     |
| train/                  |              |
|    approx_kl            | 0.0006974784 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -166         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -8.26        |
|    n_updates            | 37190        |
|    policy_gradient_loss | -0.00069     |
|    std                  | 41.3         |
|    value_loss           | 0.201        |
------------------------------------------
Iteration: 3720 | Episodes: 150900 | Median Reward: 37.87 | Max Reward: 48.88
Iteration: 3723 | Episodes: 151000 | Median Reward: 33.98 | Max Reward: 48.88
Iteration: 3725 | Episodes: 151100 | Median Reward: 35.19 | Max Reward: 48.88
Iteration: 3728 | Episodes: 151200 | Median Reward: 39.28 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.1        |
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 3730         |
|    time_elapsed         | 28002        |
|    total_timesteps      | 15278080     |
| train/                  |              |
|    approx_kl            | 7.957025e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -167         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -8.14        |
|    n_updates            | 37290        |
|    policy_gradient_loss | -0.00048     |
|    std                  | 42.1         |
|    value_loss           | 0.406        |
------------------------------------------
Iteration: 3730 | Episodes: 151300 | Median Reward: 39.46 | Max Reward: 48.88
Iteration: 3733 | Episodes: 151400 | Median Reward: 35.85 | Max Reward: 48.88
Iteration: 3735 | Episodes: 151500 | Median Reward: 39.44 | Max Reward: 48.88
Iteration: 3738 | Episodes: 151600 | Median Reward: 35.87 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.8        |
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 3740         |
|    time_elapsed         | 28082        |
|    total_timesteps      | 15319040     |
| train/                  |              |
|    approx_kl            | 0.0003907903 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -167         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.19        |
|    n_updates            | 37390        |
|    policy_gradient_loss | -0.00324     |
|    std                  | 42.5         |
|    value_loss           | 0.463        |
------------------------------------------
Iteration: 3740 | Episodes: 151700 | Median Reward: 32.54 | Max Reward: 48.88
Iteration: 3743 | Episodes: 151800 | Median Reward: 34.47 | Max Reward: 48.88
Iteration: 3745 | Episodes: 151900 | Median Reward: 37.58 | Max Reward: 48.88
Iteration: 3748 | Episodes: 152000 | Median Reward: 36.12 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.8         |
| time/                   |               |
|    fps                  | 545           |
|    iterations           | 3750          |
|    time_elapsed         | 28175         |
|    total_timesteps      | 15360000      |
| train/                  |               |
|    approx_kl            | 8.5532665e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -167          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.21         |
|    n_updates            | 37490         |
|    policy_gradient_loss | 1.15e-05      |
|    std                  | 42.9          |
|    value_loss           | 0.537         |
-------------------------------------------
Iteration: 3750 | Episodes: 152100 | Median Reward: 38.27 | Max Reward: 48.88
Iteration: 3752 | Episodes: 152200 | Median Reward: 37.98 | Max Reward: 48.88
Iteration: 3755 | Episodes: 152300 | Median Reward: 39.88 | Max Reward: 48.88
Iteration: 3757 | Episodes: 152400 | Median Reward: 39.31 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.9        |
| time/                   |              |
|    fps                  | 545          |
|    iterations           | 3760         |
|    time_elapsed         | 28256        |
|    total_timesteps      | 15400960     |
| train/                  |              |
|    approx_kl            | 0.0048239375 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -167         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.34        |
|    n_updates            | 37590        |
|    policy_gradient_loss | -0.0182      |
|    std                  | 43.2         |
|    value_loss           | 0.276        |
------------------------------------------
Iteration: 3760 | Episodes: 152500 | Median Reward: 35.66 | Max Reward: 48.88
Iteration: 3762 | Episodes: 152600 | Median Reward: 36.62 | Max Reward: 48.88
Iteration: 3765 | Episodes: 152700 | Median Reward: 33.19 | Max Reward: 48.88
Iteration: 3767 | Episodes: 152800 | Median Reward: 33.63 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.5        |
| time/                   |              |
|    fps                  | 544          |
|    iterations           | 3770         |
|    time_elapsed         | 28339        |
|    total_timesteps      | 15441920     |
| train/                  |              |
|    approx_kl            | 0.0043055136 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -168         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.31        |
|    n_updates            | 37690        |
|    policy_gradient_loss | -0.0119      |
|    std                  | 43.6         |
|    value_loss           | 0.32         |
------------------------------------------
Iteration: 3770 | Episodes: 152900 | Median Reward: 36.28 | Max Reward: 48.88
Iteration: 3772 | Episodes: 153000 | Median Reward: 38.68 | Max Reward: 48.88
Iteration: 3775 | Episodes: 153100 | Median Reward: 41.28 | Max Reward: 48.88
Iteration: 3777 | Episodes: 153200 | Median Reward: 42.62 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.1        |
| time/                   |              |
|    fps                  | 544          |
|    iterations           | 3780         |
|    time_elapsed         | 28428        |
|    total_timesteps      | 15482880     |
| train/                  |              |
|    approx_kl            | 6.388333e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -168         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.06        |
|    n_updates            | 37790        |
|    policy_gradient_loss | -0.000465    |
|    std                  | 43.9         |
|    value_loss           | 0.625        |
------------------------------------------
Iteration: 3780 | Episodes: 153300 | Median Reward: 38.82 | Max Reward: 48.88
Iteration: 3782 | Episodes: 153400 | Median Reward: 36.07 | Max Reward: 48.88
Iteration: 3785 | Episodes: 153500 | Median Reward: 34.97 | Max Reward: 48.88
Iteration: 3787 | Episodes: 153600 | Median Reward: 36.19 | Max Reward: 48.88
Iteration: 3789 | Episodes: 153700 | Median Reward: 33.39 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.4         |
| time/                   |               |
|    fps                  | 544           |
|    iterations           | 3790          |
|    time_elapsed         | 28507         |
|    total_timesteps      | 15523840      |
| train/                  |               |
|    approx_kl            | 0.00012031237 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -168          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.2          |
|    n_updates            | 37890         |
|    policy_gradient_loss | -0.000754     |
|    std                  | 44.3          |
|    value_loss           | 0.38          |
-------------------------------------------
Iteration: 3792 | Episodes: 153800 | Median Reward: 29.92 | Max Reward: 48.88
Iteration: 3794 | Episodes: 153900 | Median Reward: 37.44 | Max Reward: 48.88
Iteration: 3797 | Episodes: 154000 | Median Reward: 36.16 | Max Reward: 48.88
Iteration: 3799 | Episodes: 154100 | Median Reward: 36.59 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.2         |
| time/                   |               |
|    fps                  | 544           |
|    iterations           | 3800          |
|    time_elapsed         | 28601         |
|    total_timesteps      | 15564800      |
| train/                  |               |
|    approx_kl            | 0.00017305792 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -168          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.19         |
|    n_updates            | 37990         |
|    policy_gradient_loss | -0.00179      |
|    std                  | 44.7          |
|    value_loss           | 0.53          |
-------------------------------------------
Iteration: 3802 | Episodes: 154200 | Median Reward: 34.41 | Max Reward: 48.88
Iteration: 3804 | Episodes: 154300 | Median Reward: 36.17 | Max Reward: 48.88
Iteration: 3807 | Episodes: 154400 | Median Reward: 37.16 | Max Reward: 48.88
Iteration: 3809 | Episodes: 154500 | Median Reward: 36.70 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.6        |
| time/                   |              |
|    fps                  | 544          |
|    iterations           | 3810         |
|    time_elapsed         | 28682        |
|    total_timesteps      | 15605760     |
| train/                  |              |
|    approx_kl            | 0.0004698293 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -168         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.04        |
|    n_updates            | 38090        |
|    policy_gradient_loss | -0.0024      |
|    std                  | 45.1         |
|    value_loss           | 0.828        |
------------------------------------------
Iteration: 3812 | Episodes: 154600 | Median Reward: 37.75 | Max Reward: 48.88
Iteration: 3814 | Episodes: 154700 | Median Reward: 33.94 | Max Reward: 48.88
Iteration: 3817 | Episodes: 154800 | Median Reward: 41.08 | Max Reward: 48.88
Iteration: 3819 | Episodes: 154900 | Median Reward: 38.88 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.5         |
| time/                   |               |
|    fps                  | 543           |
|    iterations           | 3820          |
|    time_elapsed         | 28764         |
|    total_timesteps      | 15646720      |
| train/                  |               |
|    approx_kl            | 0.00032450948 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -168          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.26         |
|    n_updates            | 38190         |
|    policy_gradient_loss | -0.00202      |
|    std                  | 45.3          |
|    value_loss           | 0.433         |
-------------------------------------------
Iteration: 3822 | Episodes: 155000 | Median Reward: 35.52 | Max Reward: 48.88
Iteration: 3824 | Episodes: 155100 | Median Reward: 35.69 | Max Reward: 48.88
Iteration: 3826 | Episodes: 155200 | Median Reward: 36.03 | Max Reward: 48.88
Iteration: 3829 | Episodes: 155300 | Median Reward: 39.36 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -64.1          |
| time/                   |                |
|    fps                  | 543            |
|    iterations           | 3830           |
|    time_elapsed         | 28853          |
|    total_timesteps      | 15687680       |
| train/                  |                |
|    approx_kl            | 0.000100788806 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -169           |
|    explained_variance   | 0.999          |
|    learning_rate        | 0.0005         |
|    loss                 | -8.02          |
|    n_updates            | 38290          |
|    policy_gradient_loss | -0.000862      |
|    std                  | 45.8           |
|    value_loss           | 0.492          |
--------------------------------------------
Iteration: 3831 | Episodes: 155400 | Median Reward: 34.82 | Max Reward: 48.88
Iteration: 3834 | Episodes: 155500 | Median Reward: 35.88 | Max Reward: 48.88
Iteration: 3836 | Episodes: 155600 | Median Reward: 33.31 | Max Reward: 48.88
Iteration: 3839 | Episodes: 155700 | Median Reward: 34.40 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.1       |
| time/                   |             |
|    fps                  | 543         |
|    iterations           | 3840        |
|    time_elapsed         | 28932       |
|    total_timesteps      | 15728640    |
| train/                  |             |
|    approx_kl            | 0.019395882 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.4         |
|    entropy_loss         | -169        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -8.4        |
|    n_updates            | 38390       |
|    policy_gradient_loss | -0.0291     |
|    std                  | 45.9        |
|    value_loss           | 0.295       |
-----------------------------------------
Iteration: 3841 | Episodes: 155800 | Median Reward: 34.11 | Max Reward: 48.88
Iteration: 3844 | Episodes: 155900 | Median Reward: 37.16 | Max Reward: 48.88
Iteration: 3846 | Episodes: 156000 | Median Reward: 31.61 | Max Reward: 48.88
Iteration: 3849 | Episodes: 156100 | Median Reward: 35.18 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.3        |
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 3850         |
|    time_elapsed         | 29025        |
|    total_timesteps      | 15769600     |
| train/                  |              |
|    approx_kl            | 3.420416e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -169         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.37        |
|    n_updates            | 38490        |
|    policy_gradient_loss | -0.000379    |
|    std                  | 46.3         |
|    value_loss           | 0.277        |
------------------------------------------
Iteration: 3851 | Episodes: 156200 | Median Reward: 38.19 | Max Reward: 48.88
Iteration: 3854 | Episodes: 156300 | Median Reward: 37.97 | Max Reward: 48.88
Iteration: 3856 | Episodes: 156400 | Median Reward: 37.79 | Max Reward: 48.88
Iteration: 3859 | Episodes: 156500 | Median Reward: 33.21 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.9        |
| time/                   |              |
|    fps                  | 543          |
|    iterations           | 3860         |
|    time_elapsed         | 29106        |
|    total_timesteps      | 15810560     |
| train/                  |              |
|    approx_kl            | 0.0031052923 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -169         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.19        |
|    n_updates            | 38590        |
|    policy_gradient_loss | -0.0104      |
|    std                  | 46.8         |
|    value_loss           | 0.634        |
------------------------------------------
Iteration: 3861 | Episodes: 156600 | Median Reward: 34.28 | Max Reward: 48.88
Iteration: 3863 | Episodes: 156700 | Median Reward: 35.10 | Max Reward: 48.88
Iteration: 3866 | Episodes: 156800 | Median Reward: 32.15 | Max Reward: 48.88
Iteration: 3868 | Episodes: 156900 | Median Reward: 39.41 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.4         |
| time/                   |               |
|    fps                  | 543           |
|    iterations           | 3870          |
|    time_elapsed         | 29188         |
|    total_timesteps      | 15851520      |
| train/                  |               |
|    approx_kl            | 2.7685775e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -169          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -8.13         |
|    n_updates            | 38690         |
|    policy_gradient_loss | 0.000192      |
|    std                  | 47.3          |
|    value_loss           | 0.566         |
-------------------------------------------
Iteration: 3871 | Episodes: 157000 | Median Reward: 38.22 | Max Reward: 48.88
Iteration: 3873 | Episodes: 157100 | Median Reward: 38.02 | Max Reward: 48.88
Iteration: 3876 | Episodes: 157200 | Median Reward: 35.92 | Max Reward: 48.88
Iteration: 3878 | Episodes: 157300 | Median Reward: 34.52 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.7        |
| time/                   |              |
|    fps                  | 542          |
|    iterations           | 3880         |
|    time_elapsed         | 29278        |
|    total_timesteps      | 15892480     |
| train/                  |              |
|    approx_kl            | 0.0007688479 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -170         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.29        |
|    n_updates            | 38790        |
|    policy_gradient_loss | -0.00459     |
|    std                  | 47.5         |
|    value_loss           | 0.645        |
------------------------------------------
Iteration: 3881 | Episodes: 157400 | Median Reward: 39.48 | Max Reward: 48.88
Iteration: 3883 | Episodes: 157500 | Median Reward: 37.40 | Max Reward: 48.88
Iteration: 3886 | Episodes: 157600 | Median Reward: 36.64 | Max Reward: 48.88
Iteration: 3888 | Episodes: 157700 | Median Reward: 38.69 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.8        |
| time/                   |              |
|    fps                  | 542          |
|    iterations           | 3890         |
|    time_elapsed         | 29357        |
|    total_timesteps      | 15933440     |
| train/                  |              |
|    approx_kl            | 7.907074e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -170         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.95        |
|    n_updates            | 38890        |
|    policy_gradient_loss | -0.000104    |
|    std                  | 47.7         |
|    value_loss           | 1.12         |
------------------------------------------
Iteration: 3891 | Episodes: 157800 | Median Reward: 31.52 | Max Reward: 48.88
Iteration: 3893 | Episodes: 157900 | Median Reward: 35.04 | Max Reward: 48.88
Iteration: 3895 | Episodes: 158000 | Median Reward: 36.67 | Max Reward: 48.88
Iteration: 3898 | Episodes: 158100 | Median Reward: 38.46 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.8         |
| time/                   |               |
|    fps                  | 542           |
|    iterations           | 3900          |
|    time_elapsed         | 29451         |
|    total_timesteps      | 15974400      |
| train/                  |               |
|    approx_kl            | 0.00016614288 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -170          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.31         |
|    n_updates            | 38990         |
|    policy_gradient_loss | -0.00147      |
|    std                  | 47.9          |
|    value_loss           | 0.358         |
-------------------------------------------
Iteration: 3900 | Episodes: 158200 | Median Reward: 39.41 | Max Reward: 48.88
Iteration: 3903 | Episodes: 158300 | Median Reward: 42.51 | Max Reward: 48.88
Iteration: 3905 | Episodes: 158400 | Median Reward: 36.57 | Max Reward: 48.88
Iteration: 3908 | Episodes: 158500 | Median Reward: 36.77 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.3         |
| time/                   |               |
|    fps                  | 542           |
|    iterations           | 3910          |
|    time_elapsed         | 29530         |
|    total_timesteps      | 16015360      |
| train/                  |               |
|    approx_kl            | 4.9719572e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -170          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.21         |
|    n_updates            | 39090         |
|    policy_gradient_loss | -0.000219     |
|    std                  | 48.4          |
|    value_loss           | 0.54          |
-------------------------------------------
Iteration: 3910 | Episodes: 158600 | Median Reward: 35.63 | Max Reward: 48.88
Iteration: 3913 | Episodes: 158700 | Median Reward: 35.19 | Max Reward: 48.88
Iteration: 3915 | Episodes: 158800 | Median Reward: 38.53 | Max Reward: 48.88
Iteration: 3918 | Episodes: 158900 | Median Reward: 38.82 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65           |
| time/                   |               |
|    fps                  | 542           |
|    iterations           | 3920          |
|    time_elapsed         | 29611         |
|    total_timesteps      | 16056320      |
| train/                  |               |
|    approx_kl            | 0.00033713997 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -170          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.13         |
|    n_updates            | 39190         |
|    policy_gradient_loss | -0.00298      |
|    std                  | 48.7          |
|    value_loss           | 0.39          |
-------------------------------------------
Iteration: 3920 | Episodes: 159000 | Median Reward: 33.98 | Max Reward: 48.88
Iteration: 3923 | Episodes: 159100 | Median Reward: 37.43 | Max Reward: 48.88
Iteration: 3925 | Episodes: 159200 | Median Reward: 41.67 | Max Reward: 48.88
Iteration: 3928 | Episodes: 159300 | Median Reward: 35.87 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.9         |
| time/                   |               |
|    fps                  | 541           |
|    iterations           | 3930          |
|    time_elapsed         | 29699         |
|    total_timesteps      | 16097280      |
| train/                  |               |
|    approx_kl            | 0.00028521384 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -170          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.4          |
|    n_updates            | 39290         |
|    policy_gradient_loss | -0.000225     |
|    std                  | 49.1          |
|    value_loss           | 0.354         |
-------------------------------------------
Iteration: 3930 | Episodes: 159400 | Median Reward: 36.11 | Max Reward: 48.88
Iteration: 3932 | Episodes: 159500 | Median Reward: 30.97 | Max Reward: 48.88
Iteration: 3935 | Episodes: 159600 | Median Reward: 34.64 | Max Reward: 48.88
Iteration: 3937 | Episodes: 159700 | Median Reward: 37.48 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66           |
| time/                   |               |
|    fps                  | 541           |
|    iterations           | 3940          |
|    time_elapsed         | 29779         |
|    total_timesteps      | 16138240      |
| train/                  |               |
|    approx_kl            | 0.00032946118 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -171          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.35         |
|    n_updates            | 39390         |
|    policy_gradient_loss | -0.00169      |
|    std                  | 50            |
|    value_loss           | 0.561         |
-------------------------------------------
Iteration: 3940 | Episodes: 159800 | Median Reward: 36.35 | Max Reward: 48.88
Iteration: 3942 | Episodes: 159900 | Median Reward: 34.41 | Max Reward: 48.88
Iteration: 3945 | Episodes: 160000 | Median Reward: 33.87 | Max Reward: 48.88
Iteration: 3947 | Episodes: 160100 | Median Reward: 35.16 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68         |
| time/                   |             |
|    fps                  | 541         |
|    iterations           | 3950        |
|    time_elapsed         | 29871       |
|    total_timesteps      | 16179200    |
| train/                  |             |
|    approx_kl            | 0.004273899 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -171        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -8.36       |
|    n_updates            | 39490       |
|    policy_gradient_loss | -0.0148     |
|    std                  | 50.3        |
|    value_loss           | 0.732       |
-----------------------------------------
Iteration: 3950 | Episodes: 160200 | Median Reward: 31.22 | Max Reward: 48.88
Iteration: 3952 | Episodes: 160300 | Median Reward: 31.81 | Max Reward: 48.88
Iteration: 3955 | Episodes: 160400 | Median Reward: 31.24 | Max Reward: 48.88
Iteration: 3957 | Episodes: 160500 | Median Reward: 35.70 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.3        |
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 3960         |
|    time_elapsed         | 29952        |
|    total_timesteps      | 16220160     |
| train/                  |              |
|    approx_kl            | 9.483332e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -171         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.44        |
|    n_updates            | 39590        |
|    policy_gradient_loss | -0.000802    |
|    std                  | 50.9         |
|    value_loss           | 0.368        |
------------------------------------------
Iteration: 3960 | Episodes: 160600 | Median Reward: 36.56 | Max Reward: 48.88
Iteration: 3962 | Episodes: 160700 | Median Reward: 38.97 | Max Reward: 48.88
Iteration: 3965 | Episodes: 160800 | Median Reward: 36.33 | Max Reward: 48.88
Iteration: 3967 | Episodes: 160900 | Median Reward: 31.83 | Max Reward: 48.88
Iteration: 3969 | Episodes: 161000 | Median Reward: 35.57 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.8        |
| time/                   |              |
|    fps                  | 541          |
|    iterations           | 3970         |
|    time_elapsed         | 30035        |
|    total_timesteps      | 16261120     |
| train/                  |              |
|    approx_kl            | 0.0009467177 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -171         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -8.4         |
|    n_updates            | 39690        |
|    policy_gradient_loss | -0.00524     |
|    std                  | 51.4         |
|    value_loss           | 0.351        |
------------------------------------------
Iteration: 3972 | Episodes: 161100 | Median Reward: 35.42 | Max Reward: 48.88
Iteration: 3974 | Episodes: 161200 | Median Reward: 37.38 | Max Reward: 48.88
Iteration: 3977 | Episodes: 161300 | Median Reward: 33.56 | Max Reward: 48.88
Iteration: 3979 | Episodes: 161400 | Median Reward: 30.82 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.7         |
| time/                   |               |
|    fps                  | 541           |
|    iterations           | 3980          |
|    time_elapsed         | 30125         |
|    total_timesteps      | 16302080      |
| train/                  |               |
|    approx_kl            | 0.00032984093 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -172          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.45         |
|    n_updates            | 39790         |
|    policy_gradient_loss | -0.00225      |
|    std                  | 51.8          |
|    value_loss           | 0.36          |
-------------------------------------------
Iteration: 3982 | Episodes: 161500 | Median Reward: 35.65 | Max Reward: 48.88
Iteration: 3984 | Episodes: 161600 | Median Reward: 37.32 | Max Reward: 48.88
Iteration: 3987 | Episodes: 161700 | Median Reward: 30.93 | Max Reward: 48.88
Iteration: 3989 | Episodes: 161800 | Median Reward: 34.59 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.2         |
| time/                   |               |
|    fps                  | 541           |
|    iterations           | 3990          |
|    time_elapsed         | 30203         |
|    total_timesteps      | 16343040      |
| train/                  |               |
|    approx_kl            | 0.00017516877 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -172          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.19         |
|    n_updates            | 39890         |
|    policy_gradient_loss | -0.00131      |
|    std                  | 52.3          |
|    value_loss           | 1.11          |
-------------------------------------------
Iteration: 3992 | Episodes: 161900 | Median Reward: 33.86 | Max Reward: 48.88
Iteration: 3994 | Episodes: 162000 | Median Reward: 34.29 | Max Reward: 48.88
Iteration: 3997 | Episodes: 162100 | Median Reward: 29.88 | Max Reward: 48.88
Iteration: 3999 | Episodes: 162200 | Median Reward: 35.09 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.5        |
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 4000         |
|    time_elapsed         | 30298        |
|    total_timesteps      | 16384000     |
| train/                  |              |
|    approx_kl            | 9.710874e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -172         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.49        |
|    n_updates            | 39990        |
|    policy_gradient_loss | -0.000315    |
|    std                  | 52.6         |
|    value_loss           | 0.299        |
------------------------------------------
Iteration: 4002 | Episodes: 162300 | Median Reward: 28.70 | Max Reward: 48.88
Iteration: 4004 | Episodes: 162400 | Median Reward: 35.96 | Max Reward: 48.88
Iteration: 4006 | Episodes: 162500 | Median Reward: 31.48 | Max Reward: 48.88
Iteration: 4009 | Episodes: 162600 | Median Reward: 39.76 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.9         |
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 4010          |
|    time_elapsed         | 30377         |
|    total_timesteps      | 16424960      |
| train/                  |               |
|    approx_kl            | 5.5099852e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -172          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.27         |
|    n_updates            | 40090         |
|    policy_gradient_loss | -0.000277     |
|    std                  | 52.9          |
|    value_loss           | 0.995         |
-------------------------------------------
Iteration: 4011 | Episodes: 162700 | Median Reward: 32.50 | Max Reward: 48.88
Iteration: 4014 | Episodes: 162800 | Median Reward: 34.32 | Max Reward: 48.88
Iteration: 4016 | Episodes: 162900 | Median Reward: 35.51 | Max Reward: 48.88
Iteration: 4019 | Episodes: 163000 | Median Reward: 37.14 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63          |
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 4020         |
|    time_elapsed         | 30459        |
|    total_timesteps      | 16465920     |
| train/                  |              |
|    approx_kl            | 3.245195e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -172         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.14        |
|    n_updates            | 40190        |
|    policy_gradient_loss | 0.000358     |
|    std                  | 53.3         |
|    value_loss           | 1.18         |
------------------------------------------
Iteration: 4021 | Episodes: 163100 | Median Reward: 36.27 | Max Reward: 48.88
Iteration: 4024 | Episodes: 163200 | Median Reward: 35.70 | Max Reward: 48.88
Iteration: 4026 | Episodes: 163300 | Median Reward: 35.42 | Max Reward: 48.88
Iteration: 4029 | Episodes: 163400 | Median Reward: 33.68 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.6         |
| time/                   |               |
|    fps                  | 540           |
|    iterations           | 4030          |
|    time_elapsed         | 30547         |
|    total_timesteps      | 16506880      |
| train/                  |               |
|    approx_kl            | 0.00040551525 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -172          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -8.49         |
|    n_updates            | 40290         |
|    policy_gradient_loss | -0.00281      |
|    std                  | 53.6          |
|    value_loss           | 0.279         |
-------------------------------------------
Iteration: 4031 | Episodes: 163500 | Median Reward: 34.07 | Max Reward: 48.88
Iteration: 4034 | Episodes: 163600 | Median Reward: 37.45 | Max Reward: 48.88
Iteration: 4036 | Episodes: 163700 | Median Reward: 35.42 | Max Reward: 48.88
Iteration: 4039 | Episodes: 163800 | Median Reward: 34.25 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.1        |
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 4040         |
|    time_elapsed         | 30626        |
|    total_timesteps      | 16547840     |
| train/                  |              |
|    approx_kl            | 0.0003673068 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -173         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.31        |
|    n_updates            | 40390        |
|    policy_gradient_loss | -0.00164     |
|    std                  | 54.1         |
|    value_loss           | 0.752        |
------------------------------------------
Iteration: 4041 | Episodes: 163900 | Median Reward: 36.26 | Max Reward: 48.88
Iteration: 4043 | Episodes: 164000 | Median Reward: 30.38 | Max Reward: 48.88
Iteration: 4046 | Episodes: 164100 | Median Reward: 31.86 | Max Reward: 48.88
Iteration: 4048 | Episodes: 164200 | Median Reward: 35.44 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.7         |
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 4050          |
|    time_elapsed         | 30720         |
|    total_timesteps      | 16588800      |
| train/                  |               |
|    approx_kl            | 2.4818699e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -173          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.4          |
|    n_updates            | 40490         |
|    policy_gradient_loss | -0.000179     |
|    std                  | 54.3          |
|    value_loss           | 0.532         |
-------------------------------------------
Iteration: 4051 | Episodes: 164300 | Median Reward: 35.29 | Max Reward: 48.88
Iteration: 4053 | Episodes: 164400 | Median Reward: 37.97 | Max Reward: 48.88
Iteration: 4056 | Episodes: 164500 | Median Reward: 33.18 | Max Reward: 48.88
Iteration: 4058 | Episodes: 164600 | Median Reward: 39.03 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.3         |
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 4060          |
|    time_elapsed         | 30801         |
|    total_timesteps      | 16629760      |
| train/                  |               |
|    approx_kl            | 0.00010655678 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -173          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.47         |
|    n_updates            | 40590         |
|    policy_gradient_loss | -0.000801     |
|    std                  | 54.8          |
|    value_loss           | 0.366         |
-------------------------------------------
Iteration: 4061 | Episodes: 164700 | Median Reward: 37.58 | Max Reward: 48.88
Iteration: 4063 | Episodes: 164800 | Median Reward: 32.67 | Max Reward: 48.88
Iteration: 4066 | Episodes: 164900 | Median Reward: 33.53 | Max Reward: 48.88
Iteration: 4068 | Episodes: 165000 | Median Reward: 37.96 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.9        |
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 4070         |
|    time_elapsed         | 30884        |
|    total_timesteps      | 16670720     |
| train/                  |              |
|    approx_kl            | 0.0018549446 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -173         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.39        |
|    n_updates            | 40690        |
|    policy_gradient_loss | -0.00862     |
|    std                  | 55.1         |
|    value_loss           | 0.654        |
------------------------------------------
Iteration: 4071 | Episodes: 165100 | Median Reward: 38.49 | Max Reward: 48.88
Iteration: 4073 | Episodes: 165200 | Median Reward: 36.58 | Max Reward: 48.88
Iteration: 4076 | Episodes: 165300 | Median Reward: 37.07 | Max Reward: 48.88
Iteration: 4078 | Episodes: 165400 | Median Reward: 32.76 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.7         |
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 4080          |
|    time_elapsed         | 30969         |
|    total_timesteps      | 16711680      |
| train/                  |               |
|    approx_kl            | 5.6826117e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -173          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.25         |
|    n_updates            | 40790         |
|    policy_gradient_loss | -0.000727     |
|    std                  | 55.4          |
|    value_loss           | 0.672         |
-------------------------------------------
Iteration: 4080 | Episodes: 165500 | Median Reward: 31.40 | Max Reward: 48.88
Iteration: 4083 | Episodes: 165600 | Median Reward: 37.09 | Max Reward: 48.88
Iteration: 4085 | Episodes: 165700 | Median Reward: 33.65 | Max Reward: 48.88
Iteration: 4088 | Episodes: 165800 | Median Reward: 35.80 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.7         |
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 4090          |
|    time_elapsed         | 31043         |
|    total_timesteps      | 16752640      |
| train/                  |               |
|    approx_kl            | 0.00047873848 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -173          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.3          |
|    n_updates            | 40890         |
|    policy_gradient_loss | -0.0014       |
|    std                  | 55.8          |
|    value_loss           | 0.882         |
-------------------------------------------
Iteration: 4090 | Episodes: 165900 | Median Reward: 35.08 | Max Reward: 48.88
Iteration: 4093 | Episodes: 166000 | Median Reward: 35.47 | Max Reward: 48.88
Iteration: 4095 | Episodes: 166100 | Median Reward: 34.13 | Max Reward: 48.88
Iteration: 4098 | Episodes: 166200 | Median Reward: 38.34 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.4        |
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 4100         |
|    time_elapsed         | 31132        |
|    total_timesteps      | 16793600     |
| train/                  |              |
|    approx_kl            | 8.219518e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -174         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.37        |
|    n_updates            | 40990        |
|    policy_gradient_loss | -0.00147     |
|    std                  | 56.5         |
|    value_loss           | 0.598        |
------------------------------------------
Iteration: 4100 | Episodes: 166300 | Median Reward: 31.09 | Max Reward: 48.88
Iteration: 4103 | Episodes: 166400 | Median Reward: 37.03 | Max Reward: 48.88
Iteration: 4105 | Episodes: 166500 | Median Reward: 35.45 | Max Reward: 48.88
Iteration: 4108 | Episodes: 166600 | Median Reward: 30.54 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.4        |
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 4110         |
|    time_elapsed         | 31208        |
|    total_timesteps      | 16834560     |
| train/                  |              |
|    approx_kl            | 0.0002587656 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -174         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.56        |
|    n_updates            | 41090        |
|    policy_gradient_loss | -0.00139     |
|    std                  | 57.1         |
|    value_loss           | 0.352        |
------------------------------------------
Iteration: 4110 | Episodes: 166700 | Median Reward: 34.78 | Max Reward: 48.88
Iteration: 4112 | Episodes: 166800 | Median Reward: 31.61 | Max Reward: 48.88
Iteration: 4115 | Episodes: 166900 | Median Reward: 29.63 | Max Reward: 48.88
Iteration: 4117 | Episodes: 167000 | Median Reward: 29.99 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69          |
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 4120         |
|    time_elapsed         | 31299        |
|    total_timesteps      | 16875520     |
| train/                  |              |
|    approx_kl            | 0.0006055711 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -174         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.24        |
|    n_updates            | 41190        |
|    policy_gradient_loss | -0.002       |
|    std                  | 57.7         |
|    value_loss           | 1.56         |
------------------------------------------
Iteration: 4120 | Episodes: 167100 | Median Reward: 34.57 | Max Reward: 48.88
Iteration: 4122 | Episodes: 167200 | Median Reward: 30.19 | Max Reward: 48.88
Iteration: 4125 | Episodes: 167300 | Median Reward: 31.42 | Max Reward: 48.88
Iteration: 4127 | Episodes: 167400 | Median Reward: 31.43 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67           |
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 4130          |
|    time_elapsed         | 31377         |
|    total_timesteps      | 16916480      |
| train/                  |               |
|    approx_kl            | 0.00030960847 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -174          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.51         |
|    n_updates            | 41290         |
|    policy_gradient_loss | -0.00168      |
|    std                  | 58.5          |
|    value_loss           | 0.89          |
-------------------------------------------
Iteration: 4130 | Episodes: 167500 | Median Reward: 31.73 | Max Reward: 48.88
Iteration: 4132 | Episodes: 167600 | Median Reward: 35.73 | Max Reward: 48.88
Iteration: 4135 | Episodes: 167700 | Median Reward: 33.12 | Max Reward: 48.88
Iteration: 4137 | Episodes: 167800 | Median Reward: 36.74 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66           |
| time/                   |               |
|    fps                  | 539           |
|    iterations           | 4140          |
|    time_elapsed         | 31460         |
|    total_timesteps      | 16957440      |
| train/                  |               |
|    approx_kl            | 2.7097849e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -175          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -8.4          |
|    n_updates            | 41390         |
|    policy_gradient_loss | -0.000189     |
|    std                  | 59.2          |
|    value_loss           | 0.404         |
-------------------------------------------
Iteration: 4140 | Episodes: 167900 | Median Reward: 35.47 | Max Reward: 48.88
Iteration: 4142 | Episodes: 168000 | Median Reward: 36.84 | Max Reward: 48.88
Iteration: 4145 | Episodes: 168100 | Median Reward: 34.85 | Max Reward: 48.88
Iteration: 4147 | Episodes: 168200 | Median Reward: 34.43 | Max Reward: 48.88
Iteration: 4149 | Episodes: 168300 | Median Reward: 32.59 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.6        |
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 4150         |
|    time_elapsed         | 31550        |
|    total_timesteps      | 16998400     |
| train/                  |              |
|    approx_kl            | 0.0026578065 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -175         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.41        |
|    n_updates            | 41490        |
|    policy_gradient_loss | -0.00826     |
|    std                  | 59.8         |
|    value_loss           | 0.588        |
------------------------------------------
Iteration: 4152 | Episodes: 168400 | Median Reward: 38.65 | Max Reward: 48.88
Iteration: 4154 | Episodes: 168500 | Median Reward: 35.87 | Max Reward: 48.88
Iteration: 4157 | Episodes: 168600 | Median Reward: 33.69 | Max Reward: 48.88
Iteration: 4159 | Episodes: 168700 | Median Reward: 34.60 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.3         |
| time/                   |               |
|    fps                  | 538           |
|    iterations           | 4160          |
|    time_elapsed         | 31631         |
|    total_timesteps      | 17039360      |
| train/                  |               |
|    approx_kl            | 5.9698534e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -175          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.49         |
|    n_updates            | 41590         |
|    policy_gradient_loss | -0.000709     |
|    std                  | 60.2          |
|    value_loss           | 0.552         |
-------------------------------------------
Iteration: 4162 | Episodes: 168800 | Median Reward: 36.40 | Max Reward: 48.88
Iteration: 4164 | Episodes: 168900 | Median Reward: 30.62 | Max Reward: 48.88
Iteration: 4167 | Episodes: 169000 | Median Reward: 35.41 | Max Reward: 48.88
Iteration: 4169 | Episodes: 169100 | Median Reward: 35.92 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.7         |
| time/                   |               |
|    fps                  | 538           |
|    iterations           | 4170          |
|    time_elapsed         | 31723         |
|    total_timesteps      | 17080320      |
| train/                  |               |
|    approx_kl            | 0.00027405654 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -175          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.5          |
|    n_updates            | 41690         |
|    policy_gradient_loss | -0.00094      |
|    std                  | 60.8          |
|    value_loss           | 0.707         |
-------------------------------------------
Iteration: 4172 | Episodes: 169200 | Median Reward: 37.06 | Max Reward: 48.88
Iteration: 4174 | Episodes: 169300 | Median Reward: 35.07 | Max Reward: 48.88
Iteration: 4177 | Episodes: 169400 | Median Reward: 33.81 | Max Reward: 48.88
Iteration: 4179 | Episodes: 169500 | Median Reward: 35.52 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.6        |
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 4180         |
|    time_elapsed         | 31802        |
|    total_timesteps      | 17121280     |
| train/                  |              |
|    approx_kl            | 0.0010943873 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -175         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.47        |
|    n_updates            | 41790        |
|    policy_gradient_loss | -0.00497     |
|    std                  | 61.4         |
|    value_loss           | 0.497        |
------------------------------------------
Iteration: 4182 | Episodes: 169600 | Median Reward: 37.41 | Max Reward: 48.88
Iteration: 4184 | Episodes: 169700 | Median Reward: 33.17 | Max Reward: 48.88
Iteration: 4186 | Episodes: 169800 | Median Reward: 30.99 | Max Reward: 48.88
Iteration: 4189 | Episodes: 169900 | Median Reward: 33.50 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.7        |
| time/                   |              |
|    fps                  | 538          |
|    iterations           | 4190         |
|    time_elapsed         | 31885        |
|    total_timesteps      | 17162240     |
| train/                  |              |
|    approx_kl            | 8.453752e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -176         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.42        |
|    n_updates            | 41890        |
|    policy_gradient_loss | 7.32e-05     |
|    std                  | 61.8         |
|    value_loss           | 0.828        |
------------------------------------------
Iteration: 4191 | Episodes: 170000 | Median Reward: 31.02 | Max Reward: 48.88
Iteration: 4194 | Episodes: 170100 | Median Reward: 29.14 | Max Reward: 48.88
Iteration: 4196 | Episodes: 170200 | Median Reward: 32.57 | Max Reward: 48.88
Iteration: 4199 | Episodes: 170300 | Median Reward: 34.21 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.5        |
| time/                   |              |
|    fps                  | 537          |
|    iterations           | 4200         |
|    time_elapsed         | 31979        |
|    total_timesteps      | 17203200     |
| train/                  |              |
|    approx_kl            | 7.781421e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -176         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.63        |
|    n_updates            | 41990        |
|    policy_gradient_loss | -0.000316    |
|    std                  | 62.2         |
|    value_loss           | 0.393        |
------------------------------------------
Iteration: 4201 | Episodes: 170400 | Median Reward: 35.95 | Max Reward: 48.88
Iteration: 4204 | Episodes: 170500 | Median Reward: 33.90 | Max Reward: 48.88
Iteration: 4206 | Episodes: 170600 | Median Reward: 33.59 | Max Reward: 48.88
Iteration: 4209 | Episodes: 170700 | Median Reward: 35.31 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.3         |
| time/                   |               |
|    fps                  | 537           |
|    iterations           | 4210          |
|    time_elapsed         | 32057         |
|    total_timesteps      | 17244160      |
| train/                  |               |
|    approx_kl            | 0.00036136992 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -176          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.6          |
|    n_updates            | 42090         |
|    policy_gradient_loss | -0.000743     |
|    std                  | 62.6          |
|    value_loss           | 0.646         |
-------------------------------------------
Iteration: 4211 | Episodes: 170800 | Median Reward: 37.47 | Max Reward: 48.88
Iteration: 4214 | Episodes: 170900 | Median Reward: 33.95 | Max Reward: 48.88
Iteration: 4216 | Episodes: 171000 | Median Reward: 31.70 | Max Reward: 48.88
Iteration: 4219 | Episodes: 171100 | Median Reward: 30.78 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.1        |
| time/                   |              |
|    fps                  | 537          |
|    iterations           | 4220         |
|    time_elapsed         | 32150        |
|    total_timesteps      | 17285120     |
| train/                  |              |
|    approx_kl            | 9.454481e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -176         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.43        |
|    n_updates            | 42190        |
|    policy_gradient_loss | -0.000378    |
|    std                  | 63.1         |
|    value_loss           | 0.898        |
------------------------------------------
Iteration: 4221 | Episodes: 171200 | Median Reward: 34.52 | Max Reward: 48.88
Iteration: 4223 | Episodes: 171300 | Median Reward: 33.88 | Max Reward: 48.88
Iteration: 4226 | Episodes: 171400 | Median Reward: 33.00 | Max Reward: 48.88
Iteration: 4228 | Episodes: 171500 | Median Reward: 30.40 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.5         |
| time/                   |               |
|    fps                  | 537           |
|    iterations           | 4230          |
|    time_elapsed         | 32233         |
|    total_timesteps      | 17326080      |
| train/                  |               |
|    approx_kl            | 0.00064329023 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -176          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.68         |
|    n_updates            | 42290         |
|    policy_gradient_loss | -0.00543      |
|    std                  | 63.7          |
|    value_loss           | 0.379         |
-------------------------------------------
Iteration: 4231 | Episodes: 171600 | Median Reward: 30.92 | Max Reward: 48.88
Iteration: 4233 | Episodes: 171700 | Median Reward: 35.17 | Max Reward: 48.88
Iteration: 4236 | Episodes: 171800 | Median Reward: 36.62 | Max Reward: 48.88
Iteration: 4238 | Episodes: 171900 | Median Reward: 26.14 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.3         |
| time/                   |               |
|    fps                  | 537           |
|    iterations           | 4240          |
|    time_elapsed         | 32311         |
|    total_timesteps      | 17367040      |
| train/                  |               |
|    approx_kl            | 2.4449211e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -177          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.65         |
|    n_updates            | 42390         |
|    policy_gradient_loss | -0.000138     |
|    std                  | 64.7          |
|    value_loss           | 0.566         |
-------------------------------------------
Iteration: 4241 | Episodes: 172000 | Median Reward: 34.40 | Max Reward: 48.88
Iteration: 4243 | Episodes: 172100 | Median Reward: 31.70 | Max Reward: 48.88
Iteration: 4246 | Episodes: 172200 | Median Reward: 33.58 | Max Reward: 48.88
Iteration: 4248 | Episodes: 172300 | Median Reward: 35.66 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.1         |
| time/                   |               |
|    fps                  | 537           |
|    iterations           | 4250          |
|    time_elapsed         | 32404         |
|    total_timesteps      | 17408000      |
| train/                  |               |
|    approx_kl            | 6.8094116e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -177          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.98         |
|    n_updates            | 42490         |
|    policy_gradient_loss | -0.000231     |
|    std                  | 65            |
|    value_loss           | 2.67          |
-------------------------------------------
Iteration: 4251 | Episodes: 172400 | Median Reward: 34.87 | Max Reward: 48.88
Iteration: 4253 | Episodes: 172500 | Median Reward: 34.64 | Max Reward: 48.88
Iteration: 4256 | Episodes: 172600 | Median Reward: 28.98 | Max Reward: 48.88
Iteration: 4258 | Episodes: 172700 | Median Reward: 34.76 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.7         |
| time/                   |               |
|    fps                  | 537           |
|    iterations           | 4260          |
|    time_elapsed         | 32485         |
|    total_timesteps      | 17448960      |
| train/                  |               |
|    approx_kl            | 3.4886936e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -177          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.7          |
|    n_updates            | 42590         |
|    policy_gradient_loss | -0.00132      |
|    std                  | 65.5          |
|    value_loss           | 0.908         |
-------------------------------------------
Iteration: 4260 | Episodes: 172800 | Median Reward: 33.59 | Max Reward: 48.88
Iteration: 4263 | Episodes: 172900 | Median Reward: 32.66 | Max Reward: 48.88
Iteration: 4265 | Episodes: 173000 | Median Reward: 32.66 | Max Reward: 48.88
Iteration: 4268 | Episodes: 173100 | Median Reward: 35.21 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.3       |
| time/                   |             |
|    fps                  | 536         |
|    iterations           | 4270        |
|    time_elapsed         | 32574       |
|    total_timesteps      | 17489920    |
| train/                  |             |
|    approx_kl            | 8.45756e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -177        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -8.68       |
|    n_updates            | 42690       |
|    policy_gradient_loss | -0.000782   |
|    std                  | 66.2        |
|    value_loss           | 0.484       |
-----------------------------------------
Iteration: 4270 | Episodes: 173200 | Median Reward: 30.11 | Max Reward: 48.88
Iteration: 4273 | Episodes: 173300 | Median Reward: 29.95 | Max Reward: 48.88
Iteration: 4275 | Episodes: 173400 | Median Reward: 38.28 | Max Reward: 48.88
Iteration: 4278 | Episodes: 173500 | Median Reward: 32.68 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.3         |
| time/                   |               |
|    fps                  | 536           |
|    iterations           | 4280          |
|    time_elapsed         | 32657         |
|    total_timesteps      | 17530880      |
| train/                  |               |
|    approx_kl            | 9.1126305e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -177          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -8.67         |
|    n_updates            | 42790         |
|    policy_gradient_loss | -0.000646     |
|    std                  | 66.8          |
|    value_loss           | 0.41          |
-------------------------------------------
Iteration: 4280 | Episodes: 173600 | Median Reward: 32.97 | Max Reward: 48.88
Iteration: 4283 | Episodes: 173700 | Median Reward: 34.61 | Max Reward: 48.88
Iteration: 4285 | Episodes: 173800 | Median Reward: 31.65 | Max Reward: 48.88
Iteration: 4288 | Episodes: 173900 | Median Reward: 33.78 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.2        |
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 4290         |
|    time_elapsed         | 32735        |
|    total_timesteps      | 17571840     |
| train/                  |              |
|    approx_kl            | 0.0005326823 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -178         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.67        |
|    n_updates            | 42890        |
|    policy_gradient_loss | -0.00313     |
|    std                  | 67.3         |
|    value_loss           | 0.507        |
------------------------------------------
Iteration: 4290 | Episodes: 174000 | Median Reward: 30.43 | Max Reward: 48.88
Iteration: 4292 | Episodes: 174100 | Median Reward: 30.37 | Max Reward: 48.88
Iteration: 4295 | Episodes: 174200 | Median Reward: 31.20 | Max Reward: 48.88
Iteration: 4297 | Episodes: 174300 | Median Reward: 34.90 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.8        |
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 4300         |
|    time_elapsed         | 32829        |
|    total_timesteps      | 17612800     |
| train/                  |              |
|    approx_kl            | 0.0006815487 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -178         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.29        |
|    n_updates            | 42990        |
|    policy_gradient_loss | -0.00238     |
|    std                  | 67.7         |
|    value_loss           | 1.25         |
------------------------------------------
Iteration: 4300 | Episodes: 174400 | Median Reward: 25.20 | Max Reward: 48.88
Iteration: 4302 | Episodes: 174500 | Median Reward: 30.94 | Max Reward: 48.88
Iteration: 4305 | Episodes: 174600 | Median Reward: 31.63 | Max Reward: 48.88
Iteration: 4307 | Episodes: 174700 | Median Reward: 29.98 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.2         |
| time/                   |               |
|    fps                  | 536           |
|    iterations           | 4310          |
|    time_elapsed         | 32910         |
|    total_timesteps      | 17653760      |
| train/                  |               |
|    approx_kl            | 0.00010761648 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -178          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.42         |
|    n_updates            | 43090         |
|    policy_gradient_loss | -0.00106      |
|    std                  | 68.1          |
|    value_loss           | 1.02          |
-------------------------------------------
Iteration: 4310 | Episodes: 174800 | Median Reward: 32.02 | Max Reward: 48.88
Iteration: 4312 | Episodes: 174900 | Median Reward: 35.83 | Max Reward: 48.88
Iteration: 4315 | Episodes: 175000 | Median Reward: 34.12 | Max Reward: 48.88
Iteration: 4317 | Episodes: 175100 | Median Reward: 35.19 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.6         |
| time/                   |               |
|    fps                  | 536           |
|    iterations           | 4320          |
|    time_elapsed         | 32999         |
|    total_timesteps      | 17694720      |
| train/                  |               |
|    approx_kl            | 0.00017204191 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -178          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.65         |
|    n_updates            | 43190         |
|    policy_gradient_loss | -0.000306     |
|    std                  | 68.7          |
|    value_loss           | 0.653         |
-------------------------------------------
Iteration: 4320 | Episodes: 175200 | Median Reward: 35.40 | Max Reward: 48.88
Iteration: 4322 | Episodes: 175300 | Median Reward: 35.59 | Max Reward: 48.88
Iteration: 4325 | Episodes: 175400 | Median Reward: 34.33 | Max Reward: 48.88
Iteration: 4327 | Episodes: 175500 | Median Reward: 33.05 | Max Reward: 48.88
Iteration: 4329 | Episodes: 175600 | Median Reward: 30.40 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.9        |
| time/                   |              |
|    fps                  | 536          |
|    iterations           | 4330         |
|    time_elapsed         | 33081        |
|    total_timesteps      | 17735680     |
| train/                  |              |
|    approx_kl            | 3.502624e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -178         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.16        |
|    n_updates            | 43290        |
|    policy_gradient_loss | -0.000387    |
|    std                  | 69.4         |
|    value_loss           | 0.833        |
------------------------------------------
Iteration: 4332 | Episodes: 175700 | Median Reward: 33.76 | Max Reward: 48.88
Iteration: 4334 | Episodes: 175800 | Median Reward: 32.70 | Max Reward: 48.88
Iteration: 4337 | Episodes: 175900 | Median Reward: 33.72 | Max Reward: 48.88
Iteration: 4339 | Episodes: 176000 | Median Reward: 34.85 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.5         |
| time/                   |               |
|    fps                  | 536           |
|    iterations           | 4340          |
|    time_elapsed         | 33159         |
|    total_timesteps      | 17776640      |
| train/                  |               |
|    approx_kl            | 0.00048609616 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -179          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.79         |
|    n_updates            | 43390         |
|    policy_gradient_loss | -0.000427     |
|    std                  | 70.1          |
|    value_loss           | 0.429         |
-------------------------------------------
Iteration: 4342 | Episodes: 176100 | Median Reward: 39.77 | Max Reward: 48.88
Iteration: 4344 | Episodes: 176200 | Median Reward: 35.38 | Max Reward: 48.88
Iteration: 4347 | Episodes: 176300 | Median Reward: 33.02 | Max Reward: 48.88
Iteration: 4349 | Episodes: 176400 | Median Reward: 29.23 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.6        |
| time/                   |              |
|    fps                  | 535          |
|    iterations           | 4350         |
|    time_elapsed         | 33254        |
|    total_timesteps      | 17817600     |
| train/                  |              |
|    approx_kl            | 5.977243e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -179         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.57        |
|    n_updates            | 43490        |
|    policy_gradient_loss | -2.37e-05    |
|    std                  | 70.8         |
|    value_loss           | 0.989        |
------------------------------------------
Iteration: 4352 | Episodes: 176500 | Median Reward: 35.56 | Max Reward: 48.88
Iteration: 4354 | Episodes: 176600 | Median Reward: 34.35 | Max Reward: 48.88
Iteration: 4357 | Episodes: 176700 | Median Reward: 31.89 | Max Reward: 48.88
Iteration: 4359 | Episodes: 176800 | Median Reward: 32.27 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.8         |
| time/                   |               |
|    fps                  | 535           |
|    iterations           | 4360          |
|    time_elapsed         | 33333         |
|    total_timesteps      | 17858560      |
| train/                  |               |
|    approx_kl            | 0.00011734858 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -179          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.7          |
|    n_updates            | 43590         |
|    policy_gradient_loss | -0.000554     |
|    std                  | 71.6          |
|    value_loss           | 0.633         |
-------------------------------------------
Iteration: 4362 | Episodes: 176900 | Median Reward: 34.88 | Max Reward: 48.88
Iteration: 4364 | Episodes: 177000 | Median Reward: 31.07 | Max Reward: 48.88
Iteration: 4366 | Episodes: 177100 | Median Reward: 37.13 | Max Reward: 48.88
Iteration: 4369 | Episodes: 177200 | Median Reward: 26.58 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.8         |
| time/                   |               |
|    fps                  | 535           |
|    iterations           | 4370          |
|    time_elapsed         | 33422         |
|    total_timesteps      | 17899520      |
| train/                  |               |
|    approx_kl            | 0.00061547616 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -179          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.79         |
|    n_updates            | 43690         |
|    policy_gradient_loss | -0.00207      |
|    std                  | 72            |
|    value_loss           | 0.376         |
-------------------------------------------
Iteration: 4371 | Episodes: 177300 | Median Reward: 31.33 | Max Reward: 48.88
Iteration: 4374 | Episodes: 177400 | Median Reward: 34.53 | Max Reward: 48.88
Iteration: 4376 | Episodes: 177500 | Median Reward: 32.48 | Max Reward: 48.88
Iteration: 4379 | Episodes: 177600 | Median Reward: 34.18 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.6         |
| time/                   |               |
|    fps                  | 535           |
|    iterations           | 4380          |
|    time_elapsed         | 33506         |
|    total_timesteps      | 17940480      |
| train/                  |               |
|    approx_kl            | 4.2343017e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -179          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.55         |
|    n_updates            | 43790         |
|    policy_gradient_loss | -0.000575     |
|    std                  | 72.7          |
|    value_loss           | 0.974         |
-------------------------------------------
Iteration: 4381 | Episodes: 177700 | Median Reward: 35.91 | Max Reward: 48.88
Iteration: 4384 | Episodes: 177800 | Median Reward: 31.22 | Max Reward: 48.88
Iteration: 4386 | Episodes: 177900 | Median Reward: 30.27 | Max Reward: 48.88
Iteration: 4389 | Episodes: 178000 | Median Reward: 30.07 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.1        |
| time/                   |              |
|    fps                  | 535          |
|    iterations           | 4390         |
|    time_elapsed         | 33586        |
|    total_timesteps      | 17981440     |
| train/                  |              |
|    approx_kl            | 0.0017681953 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -180         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.72        |
|    n_updates            | 43890        |
|    policy_gradient_loss | -0.00498     |
|    std                  | 73.9         |
|    value_loss           | 0.551        |
------------------------------------------
Iteration: 4391 | Episodes: 178100 | Median Reward: 36.06 | Max Reward: 48.88
Iteration: 4394 | Episodes: 178200 | Median Reward: 30.87 | Max Reward: 48.88
Iteration: 4396 | Episodes: 178300 | Median Reward: 27.33 | Max Reward: 48.88
Iteration: 4399 | Episodes: 178400 | Median Reward: 23.35 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.3         |
| time/                   |               |
|    fps                  | 535           |
|    iterations           | 4400          |
|    time_elapsed         | 33680         |
|    total_timesteps      | 18022400      |
| train/                  |               |
|    approx_kl            | 9.2028844e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -180          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.46         |
|    n_updates            | 43990         |
|    policy_gradient_loss | -0.00141      |
|    std                  | 74.9          |
|    value_loss           | 1.5           |
-------------------------------------------
Iteration: 4401 | Episodes: 178500 | Median Reward: 31.71 | Max Reward: 48.88
Iteration: 4403 | Episodes: 178600 | Median Reward: 33.67 | Max Reward: 48.88
Iteration: 4406 | Episodes: 178700 | Median Reward: 31.33 | Max Reward: 48.88
Iteration: 4408 | Episodes: 178800 | Median Reward: 33.18 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.2         |
| time/                   |               |
|    fps                  | 535           |
|    iterations           | 4410          |
|    time_elapsed         | 33760         |
|    total_timesteps      | 18063360      |
| train/                  |               |
|    approx_kl            | 1.1715645e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -180          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.66         |
|    n_updates            | 44090         |
|    policy_gradient_loss | -7.72e-07     |
|    std                  | 75.9          |
|    value_loss           | 0.941         |
-------------------------------------------
Iteration: 4411 | Episodes: 178900 | Median Reward: 27.78 | Max Reward: 48.88
Iteration: 4413 | Episodes: 179000 | Median Reward: 30.96 | Max Reward: 48.88
Iteration: 4416 | Episodes: 179100 | Median Reward: 32.93 | Max Reward: 48.88
Iteration: 4418 | Episodes: 179200 | Median Reward: 28.94 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69           |
| time/                   |               |
|    fps                  | 534           |
|    iterations           | 4420          |
|    time_elapsed         | 33846         |
|    total_timesteps      | 18104320      |
| train/                  |               |
|    approx_kl            | 0.00087224826 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -181          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.98         |
|    n_updates            | 44190         |
|    policy_gradient_loss | -0.00265      |
|    std                  | 77.3          |
|    value_loss           | 0.469         |
-------------------------------------------
Iteration: 4421 | Episodes: 179300 | Median Reward: 32.67 | Max Reward: 48.88
Iteration: 4423 | Episodes: 179400 | Median Reward: 30.46 | Max Reward: 48.88
Iteration: 4426 | Episodes: 179500 | Median Reward: 33.69 | Max Reward: 48.88
Iteration: 4428 | Episodes: 179600 | Median Reward: 29.52 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.9         |
| time/                   |               |
|    fps                  | 534           |
|    iterations           | 4430          |
|    time_elapsed         | 33926         |
|    total_timesteps      | 18145280      |
| train/                  |               |
|    approx_kl            | 8.0591155e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -181          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.43         |
|    n_updates            | 44290         |
|    policy_gradient_loss | -0.000557     |
|    std                  | 78            |
|    value_loss           | 1.57          |
-------------------------------------------
Iteration: 4431 | Episodes: 179700 | Median Reward: 32.98 | Max Reward: 48.88
Iteration: 4433 | Episodes: 179800 | Median Reward: 26.03 | Max Reward: 48.88
Iteration: 4436 | Episodes: 179900 | Median Reward: 30.37 | Max Reward: 48.88
Iteration: 4438 | Episodes: 180000 | Median Reward: 32.24 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.3         |
| time/                   |               |
|    fps                  | 534           |
|    iterations           | 4440          |
|    time_elapsed         | 34004         |
|    total_timesteps      | 18186240      |
| train/                  |               |
|    approx_kl            | 3.7224396e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -181          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.92         |
|    n_updates            | 44390         |
|    policy_gradient_loss | -7.07e-05     |
|    std                  | 78.7          |
|    value_loss           | 0.67          |
-------------------------------------------
Iteration: 4440 | Episodes: 180100 | Median Reward: 33.87 | Max Reward: 48.88
Iteration: 4443 | Episodes: 180200 | Median Reward: 31.21 | Max Reward: 48.88
Iteration: 4445 | Episodes: 180300 | Median Reward: 34.74 | Max Reward: 48.88
Iteration: 4448 | Episodes: 180400 | Median Reward: 30.81 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.8        |
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 4450         |
|    time_elapsed         | 34096        |
|    total_timesteps      | 18227200     |
| train/                  |              |
|    approx_kl            | 0.0010507945 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -181         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.93        |
|    n_updates            | 44490        |
|    policy_gradient_loss | -0.00463     |
|    std                  | 79.5         |
|    value_loss           | 0.331        |
------------------------------------------
Iteration: 4450 | Episodes: 180500 | Median Reward: 26.65 | Max Reward: 48.88
Iteration: 4453 | Episodes: 180600 | Median Reward: 31.12 | Max Reward: 48.88
Iteration: 4455 | Episodes: 180700 | Median Reward: 35.39 | Max Reward: 48.88
Iteration: 4458 | Episodes: 180800 | Median Reward: 31.68 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.9        |
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 4460         |
|    time_elapsed         | 34172        |
|    total_timesteps      | 18268160     |
| train/                  |              |
|    approx_kl            | 7.611385e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -182         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.88        |
|    n_updates            | 44590        |
|    policy_gradient_loss | -0.000997    |
|    std                  | 80.3         |
|    value_loss           | 0.656        |
------------------------------------------
Iteration: 4460 | Episodes: 180900 | Median Reward: 29.64 | Max Reward: 48.88
Iteration: 4463 | Episodes: 181000 | Median Reward: 32.00 | Max Reward: 48.88
Iteration: 4465 | Episodes: 181100 | Median Reward: 34.79 | Max Reward: 48.88
Iteration: 4468 | Episodes: 181200 | Median Reward: 30.24 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.9         |
| time/                   |               |
|    fps                  | 534           |
|    iterations           | 4470          |
|    time_elapsed         | 34264         |
|    total_timesteps      | 18309120      |
| train/                  |               |
|    approx_kl            | 2.7778122e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -182          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.95         |
|    n_updates            | 44690         |
|    policy_gradient_loss | 0.000577      |
|    std                  | 80.9          |
|    value_loss           | 0.383         |
-------------------------------------------
Iteration: 4470 | Episodes: 181300 | Median Reward: 33.67 | Max Reward: 48.88
Iteration: 4472 | Episodes: 181400 | Median Reward: 31.26 | Max Reward: 48.88
Iteration: 4475 | Episodes: 181500 | Median Reward: 31.62 | Max Reward: 48.88
Iteration: 4477 | Episodes: 181600 | Median Reward: 34.89 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69         |
| time/                   |             |
|    fps                  | 534         |
|    iterations           | 4480        |
|    time_elapsed         | 34344       |
|    total_timesteps      | 18350080    |
| train/                  |             |
|    approx_kl            | 0.003480383 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -182        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -8.62       |
|    n_updates            | 44790       |
|    policy_gradient_loss | -0.00733    |
|    std                  | 81.6        |
|    value_loss           | 1.4         |
-----------------------------------------
Iteration: 4480 | Episodes: 181700 | Median Reward: 30.07 | Max Reward: 48.88
Iteration: 4482 | Episodes: 181800 | Median Reward: 29.36 | Max Reward: 48.88
Iteration: 4485 | Episodes: 181900 | Median Reward: 35.26 | Max Reward: 48.88
Iteration: 4487 | Episodes: 182000 | Median Reward: 30.48 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.3        |
| time/                   |              |
|    fps                  | 534          |
|    iterations           | 4490         |
|    time_elapsed         | 34425        |
|    total_timesteps      | 18391040     |
| train/                  |              |
|    approx_kl            | 0.0006907073 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -182         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.94        |
|    n_updates            | 44890        |
|    policy_gradient_loss | -0.00269     |
|    std                  | 82.1         |
|    value_loss           | 0.445        |
------------------------------------------
Iteration: 4490 | Episodes: 182100 | Median Reward: 30.91 | Max Reward: 48.88
Iteration: 4492 | Episodes: 182200 | Median Reward: 30.17 | Max Reward: 48.88
Iteration: 4495 | Episodes: 182300 | Median Reward: 32.58 | Max Reward: 48.88
Iteration: 4497 | Episodes: 182400 | Median Reward: 30.07 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.6         |
| time/                   |               |
|    fps                  | 534           |
|    iterations           | 4500          |
|    time_elapsed         | 34515         |
|    total_timesteps      | 18432000      |
| train/                  |               |
|    approx_kl            | 0.00020135606 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -182          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.85         |
|    n_updates            | 44990         |
|    policy_gradient_loss | -0.000984     |
|    std                  | 82.7          |
|    value_loss           | 0.71          |
-------------------------------------------
Iteration: 4500 | Episodes: 182500 | Median Reward: 35.11 | Max Reward: 48.88
Iteration: 4502 | Episodes: 182600 | Median Reward: 27.87 | Max Reward: 48.88
Iteration: 4505 | Episodes: 182700 | Median Reward: 33.59 | Max Reward: 48.88
Iteration: 4507 | Episodes: 182800 | Median Reward: 33.33 | Max Reward: 48.88
Iteration: 4509 | Episodes: 182900 | Median Reward: 31.96 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70           |
| time/                   |               |
|    fps                  | 533           |
|    iterations           | 4510          |
|    time_elapsed         | 34596         |
|    total_timesteps      | 18472960      |
| train/                  |               |
|    approx_kl            | 0.00090750743 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -183          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.67         |
|    n_updates            | 45090         |
|    policy_gradient_loss | -0.0022       |
|    std                  | 83.6          |
|    value_loss           | 1.23          |
-------------------------------------------
Iteration: 4512 | Episodes: 183000 | Median Reward: 35.94 | Max Reward: 48.88
Iteration: 4514 | Episodes: 183100 | Median Reward: 33.77 | Max Reward: 48.88
Iteration: 4517 | Episodes: 183200 | Median Reward: 31.13 | Max Reward: 48.88
Iteration: 4519 | Episodes: 183300 | Median Reward: 33.88 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.4        |
| time/                   |              |
|    fps                  | 533          |
|    iterations           | 4520         |
|    time_elapsed         | 34688        |
|    total_timesteps      | 18513920     |
| train/                  |              |
|    approx_kl            | 7.702503e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -183         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.39        |
|    n_updates            | 45190        |
|    policy_gradient_loss | -5.85e-05    |
|    std                  | 84           |
|    value_loss           | 1.84         |
------------------------------------------
Iteration: 4522 | Episodes: 183400 | Median Reward: 32.87 | Max Reward: 48.88
Iteration: 4524 | Episodes: 183500 | Median Reward: 34.84 | Max Reward: 48.88
Iteration: 4527 | Episodes: 183600 | Median Reward: 29.66 | Max Reward: 48.88
Iteration: 4529 | Episodes: 183700 | Median Reward: 32.07 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.8         |
| time/                   |               |
|    fps                  | 533           |
|    iterations           | 4530          |
|    time_elapsed         | 34768         |
|    total_timesteps      | 18554880      |
| train/                  |               |
|    approx_kl            | 0.00082884915 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -183          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.85         |
|    n_updates            | 45290         |
|    policy_gradient_loss | -0.000868     |
|    std                  | 84.7          |
|    value_loss           | 0.67          |
-------------------------------------------
Iteration: 4532 | Episodes: 183800 | Median Reward: 27.63 | Max Reward: 48.88
Iteration: 4534 | Episodes: 183900 | Median Reward: 29.44 | Max Reward: 48.88
Iteration: 4537 | Episodes: 184000 | Median Reward: 35.91 | Max Reward: 48.88
Iteration: 4539 | Episodes: 184100 | Median Reward: 29.61 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.7        |
| time/                   |              |
|    fps                  | 533          |
|    iterations           | 4540         |
|    time_elapsed         | 34849        |
|    total_timesteps      | 18595840     |
| train/                  |              |
|    approx_kl            | 0.0006664209 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -183         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.03        |
|    n_updates            | 45390        |
|    policy_gradient_loss | -0.00508     |
|    std                  | 85.5         |
|    value_loss           | 0.346        |
------------------------------------------
Iteration: 4542 | Episodes: 184200 | Median Reward: 29.29 | Max Reward: 48.88
Iteration: 4544 | Episodes: 184300 | Median Reward: 33.07 | Max Reward: 48.88
Iteration: 4546 | Episodes: 184400 | Median Reward: 28.37 | Max Reward: 48.88
Iteration: 4549 | Episodes: 184500 | Median Reward: 28.22 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.7        |
| time/                   |              |
|    fps                  | 533          |
|    iterations           | 4550         |
|    time_elapsed         | 34940        |
|    total_timesteps      | 18636800     |
| train/                  |              |
|    approx_kl            | 0.0003954229 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -183         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.92        |
|    n_updates            | 45490        |
|    policy_gradient_loss | -0.00407     |
|    std                  | 86.6         |
|    value_loss           | 0.722        |
------------------------------------------
Iteration: 4551 | Episodes: 184600 | Median Reward: 29.73 | Max Reward: 48.88
Iteration: 4554 | Episodes: 184700 | Median Reward: 32.60 | Max Reward: 48.88
Iteration: 4556 | Episodes: 184800 | Median Reward: 28.87 | Max Reward: 48.88
Iteration: 4559 | Episodes: 184900 | Median Reward: 29.71 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72.4        |
| time/                   |              |
|    fps                  | 533          |
|    iterations           | 4560         |
|    time_elapsed         | 35019        |
|    total_timesteps      | 18677760     |
| train/                  |              |
|    approx_kl            | 6.764225e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -184         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.73        |
|    n_updates            | 45590        |
|    policy_gradient_loss | 2.58e-05     |
|    std                  | 87.7         |
|    value_loss           | 1.17         |
------------------------------------------
Iteration: 4561 | Episodes: 185000 | Median Reward: 22.43 | Max Reward: 48.88
Iteration: 4564 | Episodes: 185100 | Median Reward: 30.35 | Max Reward: 48.88
Iteration: 4566 | Episodes: 185200 | Median Reward: 30.98 | Max Reward: 48.88
Iteration: 4569 | Episodes: 185300 | Median Reward: 37.58 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.1         |
| time/                   |               |
|    fps                  | 533           |
|    iterations           | 4570          |
|    time_elapsed         | 35110         |
|    total_timesteps      | 18718720      |
| train/                  |               |
|    approx_kl            | 0.00041549793 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -184          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.85         |
|    n_updates            | 45690         |
|    policy_gradient_loss | -0.00223      |
|    std                  | 88.6          |
|    value_loss           | 0.459         |
-------------------------------------------
Iteration: 4571 | Episodes: 185400 | Median Reward: 31.51 | Max Reward: 48.88
Iteration: 4574 | Episodes: 185500 | Median Reward: 30.29 | Max Reward: 48.88
Iteration: 4576 | Episodes: 185600 | Median Reward: 36.43 | Max Reward: 48.88
Iteration: 4579 | Episodes: 185700 | Median Reward: 36.07 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.6         |
| time/                   |               |
|    fps                  | 533           |
|    iterations           | 4580          |
|    time_elapsed         | 35189         |
|    total_timesteps      | 18759680      |
| train/                  |               |
|    approx_kl            | 2.8642215e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -184          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.01         |
|    n_updates            | 45790         |
|    policy_gradient_loss | -0.000267     |
|    std                  | 89.5          |
|    value_loss           | 0.521         |
-------------------------------------------
Iteration: 4581 | Episodes: 185800 | Median Reward: 33.99 | Max Reward: 48.88
Iteration: 4583 | Episodes: 185900 | Median Reward: 33.06 | Max Reward: 48.88
Iteration: 4586 | Episodes: 186000 | Median Reward: 34.12 | Max Reward: 48.88
Iteration: 4588 | Episodes: 186100 | Median Reward: 33.87 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -72.4         |
| time/                   |               |
|    fps                  | 533           |
|    iterations           | 4590          |
|    time_elapsed         | 35271         |
|    total_timesteps      | 18800640      |
| train/                  |               |
|    approx_kl            | 0.00022833665 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -184          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.66         |
|    n_updates            | 45890         |
|    policy_gradient_loss | -0.00252      |
|    std                  | 89.9          |
|    value_loss           | 1.6           |
-------------------------------------------
Iteration: 4591 | Episodes: 186200 | Median Reward: 24.75 | Max Reward: 48.88
Iteration: 4593 | Episodes: 186300 | Median Reward: 26.29 | Max Reward: 48.88
Iteration: 4596 | Episodes: 186400 | Median Reward: 30.13 | Max Reward: 48.88
Iteration: 4598 | Episodes: 186500 | Median Reward: 28.44 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.6         |
| time/                   |               |
|    fps                  | 532           |
|    iterations           | 4600          |
|    time_elapsed         | 35360         |
|    total_timesteps      | 18841600      |
| train/                  |               |
|    approx_kl            | 0.00019610979 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -184          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.04         |
|    n_updates            | 45990         |
|    policy_gradient_loss | -0.00188      |
|    std                  | 90.6          |
|    value_loss           | 0.477         |
-------------------------------------------
Iteration: 4601 | Episodes: 186600 | Median Reward: 28.21 | Max Reward: 48.88
Iteration: 4603 | Episodes: 186700 | Median Reward: 30.06 | Max Reward: 48.88
Iteration: 4606 | Episodes: 186800 | Median Reward: 29.81 | Max Reward: 48.88
Iteration: 4608 | Episodes: 186900 | Median Reward: 28.24 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.4         |
| time/                   |               |
|    fps                  | 532           |
|    iterations           | 4610          |
|    time_elapsed         | 35440         |
|    total_timesteps      | 18882560      |
| train/                  |               |
|    approx_kl            | 0.00037452945 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -184          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.04         |
|    n_updates            | 46090         |
|    policy_gradient_loss | -0.000816     |
|    std                  | 91.6          |
|    value_loss           | 0.474         |
-------------------------------------------
Iteration: 4611 | Episodes: 187000 | Median Reward: 27.74 | Max Reward: 48.88
Iteration: 4613 | Episodes: 187100 | Median Reward: 21.24 | Max Reward: 48.88
Iteration: 4616 | Episodes: 187200 | Median Reward: 36.31 | Max Reward: 48.88
Iteration: 4618 | Episodes: 187300 | Median Reward: 32.04 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.9        |
| time/                   |              |
|    fps                  | 532          |
|    iterations           | 4620         |
|    time_elapsed         | 35534        |
|    total_timesteps      | 18923520     |
| train/                  |              |
|    approx_kl            | 0.0002509148 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -185         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.99        |
|    n_updates            | 46190        |
|    policy_gradient_loss | -0.00118     |
|    std                  | 92.8         |
|    value_loss           | 0.502        |
------------------------------------------
Iteration: 4620 | Episodes: 187400 | Median Reward: 29.05 | Max Reward: 48.88
Iteration: 4623 | Episodes: 187500 | Median Reward: 29.13 | Max Reward: 48.88
Iteration: 4625 | Episodes: 187600 | Median Reward: 28.68 | Max Reward: 48.88
Iteration: 4628 | Episodes: 187700 | Median Reward: 29.51 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70           |
| time/                   |               |
|    fps                  | 532           |
|    iterations           | 4630          |
|    time_elapsed         | 35614         |
|    total_timesteps      | 18964480      |
| train/                  |               |
|    approx_kl            | 0.00019526458 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -185          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.05         |
|    n_updates            | 46290         |
|    policy_gradient_loss | -0.00217      |
|    std                  | 94.1          |
|    value_loss           | 0.64          |
-------------------------------------------
Iteration: 4630 | Episodes: 187800 | Median Reward: 30.54 | Max Reward: 48.88
Iteration: 4633 | Episodes: 187900 | Median Reward: 28.61 | Max Reward: 48.88
Iteration: 4635 | Episodes: 188000 | Median Reward: 29.96 | Max Reward: 48.88
Iteration: 4638 | Episodes: 188100 | Median Reward: 35.96 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.8         |
| time/                   |               |
|    fps                  | 532           |
|    iterations           | 4640          |
|    time_elapsed         | 35696         |
|    total_timesteps      | 19005440      |
| train/                  |               |
|    approx_kl            | 3.9934414e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -185          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.92         |
|    n_updates            | 46390         |
|    policy_gradient_loss | 0.000163      |
|    std                  | 94.8          |
|    value_loss           | 0.913         |
-------------------------------------------
Iteration: 4640 | Episodes: 188200 | Median Reward: 34.03 | Max Reward: 48.88
Iteration: 4643 | Episodes: 188300 | Median Reward: 31.94 | Max Reward: 48.88
Iteration: 4645 | Episodes: 188400 | Median Reward: 32.18 | Max Reward: 48.88
Iteration: 4648 | Episodes: 188500 | Median Reward: 29.99 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -71.6          |
| time/                   |                |
|    fps                  | 532            |
|    iterations           | 4650           |
|    time_elapsed         | 35785          |
|    total_timesteps      | 19046400       |
| train/                  |                |
|    approx_kl            | 0.000115976814 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -185           |
|    explained_variance   | 0.997          |
|    learning_rate        | 0.0005         |
|    loss                 | -8.53          |
|    n_updates            | 46490          |
|    policy_gradient_loss | 0.000379       |
|    std                  | 96             |
|    value_loss           | 1.46           |
--------------------------------------------
Iteration: 4650 | Episodes: 188600 | Median Reward: 26.55 | Max Reward: 48.88
Iteration: 4653 | Episodes: 188700 | Median Reward: 33.17 | Max Reward: 48.88
Iteration: 4655 | Episodes: 188800 | Median Reward: 34.31 | Max Reward: 48.88
Iteration: 4657 | Episodes: 188900 | Median Reward: 26.46 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.6        |
| time/                   |              |
|    fps                  | 532          |
|    iterations           | 4660         |
|    time_elapsed         | 35862        |
|    total_timesteps      | 19087360     |
| train/                  |              |
|    approx_kl            | 0.0008181857 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -186         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.15        |
|    n_updates            | 46590        |
|    policy_gradient_loss | -0.00282     |
|    std                  | 97.3         |
|    value_loss           | 0.409        |
------------------------------------------
Iteration: 4660 | Episodes: 189000 | Median Reward: 23.18 | Max Reward: 48.88
Iteration: 4662 | Episodes: 189100 | Median Reward: 29.51 | Max Reward: 48.88
Iteration: 4665 | Episodes: 189200 | Median Reward: 28.59 | Max Reward: 48.88
Iteration: 4667 | Episodes: 189300 | Median Reward: 26.96 | Max Reward: 48.88
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -73.7      |
| time/                   |            |
|    fps                  | 532        |
|    iterations           | 4670       |
|    time_elapsed         | 35954      |
|    total_timesteps      | 19128320   |
| train/                  |            |
|    approx_kl            | 0.00208954 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -186       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -9.24      |
|    n_updates            | 46690      |
|    policy_gradient_loss | -0.0059    |
|    std                  | 98.6       |
|    value_loss           | 0.188      |
----------------------------------------
Iteration: 4670 | Episodes: 189400 | Median Reward: 26.92 | Max Reward: 48.88
Iteration: 4672 | Episodes: 189500 | Median Reward: 32.69 | Max Reward: 48.88
Iteration: 4675 | Episodes: 189600 | Median Reward: 25.03 | Max Reward: 48.88
Iteration: 4677 | Episodes: 189700 | Median Reward: 32.07 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.1        |
| time/                   |              |
|    fps                  | 531          |
|    iterations           | 4680         |
|    time_elapsed         | 36033        |
|    total_timesteps      | 19169280     |
| train/                  |              |
|    approx_kl            | 3.311975e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -186         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.11        |
|    n_updates            | 46790        |
|    policy_gradient_loss | -0.000481    |
|    std                  | 99.7         |
|    value_loss           | 0.541        |
------------------------------------------
Iteration: 4680 | Episodes: 189800 | Median Reward: 34.31 | Max Reward: 48.88
Iteration: 4682 | Episodes: 189900 | Median Reward: 30.12 | Max Reward: 48.88
Iteration: 4685 | Episodes: 190000 | Median Reward: 27.39 | Max Reward: 48.88
Iteration: 4687 | Episodes: 190100 | Median Reward: 31.72 | Max Reward: 48.88
Iteration: 4689 | Episodes: 190200 | Median Reward: 31.16 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.9         |
| time/                   |               |
|    fps                  | 531           |
|    iterations           | 4690          |
|    time_elapsed         | 36118         |
|    total_timesteps      | 19210240      |
| train/                  |               |
|    approx_kl            | 0.00013444903 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -186          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.67         |
|    n_updates            | 46890         |
|    policy_gradient_loss | -2.17e-05     |
|    std                  | 101           |
|    value_loss           | 1.86          |
-------------------------------------------
Iteration: 4692 | Episodes: 190300 | Median Reward: 30.09 | Max Reward: 48.88
Iteration: 4694 | Episodes: 190400 | Median Reward: 32.18 | Max Reward: 48.88
Iteration: 4697 | Episodes: 190500 | Median Reward: 28.69 | Max Reward: 48.88
Iteration: 4699 | Episodes: 190600 | Median Reward: 28.69 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.7         |
| time/                   |               |
|    fps                  | 531           |
|    iterations           | 4700          |
|    time_elapsed         | 36206         |
|    total_timesteps      | 19251200      |
| train/                  |               |
|    approx_kl            | 0.00078492734 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -186          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.14         |
|    n_updates            | 46990         |
|    policy_gradient_loss | -0.00341      |
|    std                  | 102           |
|    value_loss           | 0.506         |
-------------------------------------------
Iteration: 4702 | Episodes: 190700 | Median Reward: 32.98 | Max Reward: 48.88
Iteration: 4704 | Episodes: 190800 | Median Reward: 34.68 | Max Reward: 48.88
Iteration: 4707 | Episodes: 190900 | Median Reward: 29.70 | Max Reward: 48.88
Iteration: 4709 | Episodes: 191000 | Median Reward: 29.80 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.3        |
| time/                   |              |
|    fps                  | 531          |
|    iterations           | 4710         |
|    time_elapsed         | 36286        |
|    total_timesteps      | 19292160     |
| train/                  |              |
|    approx_kl            | 0.0001596907 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -187         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.74        |
|    n_updates            | 47090        |
|    policy_gradient_loss | -0.00121     |
|    std                  | 103          |
|    value_loss           | 1.31         |
------------------------------------------
Iteration: 4712 | Episodes: 191100 | Median Reward: 26.66 | Max Reward: 48.88
Iteration: 4714 | Episodes: 191200 | Median Reward: 29.69 | Max Reward: 48.88
Iteration: 4717 | Episodes: 191300 | Median Reward: 28.46 | Max Reward: 48.88
Iteration: 4719 | Episodes: 191400 | Median Reward: 30.34 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.1       |
| time/                   |             |
|    fps                  | 531         |
|    iterations           | 4720        |
|    time_elapsed         | 36378       |
|    total_timesteps      | 19333120    |
| train/                  |             |
|    approx_kl            | 0.000322553 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -187        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.12       |
|    n_updates            | 47190       |
|    policy_gradient_loss | -0.000743   |
|    std                  | 104         |
|    value_loss           | 0.696       |
-----------------------------------------
Iteration: 4722 | Episodes: 191500 | Median Reward: 36.00 | Max Reward: 48.88
Iteration: 4724 | Episodes: 191600 | Median Reward: 28.19 | Max Reward: 48.88
Iteration: 4726 | Episodes: 191700 | Median Reward: 31.72 | Max Reward: 48.88
Iteration: 4729 | Episodes: 191800 | Median Reward: 25.95 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.3        |
| time/                   |              |
|    fps                  | 531          |
|    iterations           | 4730         |
|    time_elapsed         | 36457        |
|    total_timesteps      | 19374080     |
| train/                  |              |
|    approx_kl            | 0.0004932191 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -187         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.19        |
|    n_updates            | 47290        |
|    policy_gradient_loss | -0.00322     |
|    std                  | 105          |
|    value_loss           | 0.366        |
------------------------------------------
Iteration: 4731 | Episodes: 191900 | Median Reward: 25.55 | Max Reward: 48.88
Iteration: 4734 | Episodes: 192000 | Median Reward: 30.76 | Max Reward: 48.88
Iteration: 4736 | Episodes: 192100 | Median Reward: 32.10 | Max Reward: 48.88
Iteration: 4739 | Episodes: 192200 | Median Reward: 32.07 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.8        |
| time/                   |              |
|    fps                  | 531          |
|    iterations           | 4740         |
|    time_elapsed         | 36542        |
|    total_timesteps      | 19415040     |
| train/                  |              |
|    approx_kl            | 0.0001661531 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -188         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.9         |
|    n_updates            | 47390        |
|    policy_gradient_loss | -0.00133     |
|    std                  | 107          |
|    value_loss           | 1.62         |
------------------------------------------
Iteration: 4741 | Episodes: 192300 | Median Reward: 31.35 | Max Reward: 48.88
Iteration: 4744 | Episodes: 192400 | Median Reward: 24.65 | Max Reward: 48.88
Iteration: 4746 | Episodes: 192500 | Median Reward: 28.82 | Max Reward: 48.88
Iteration: 4749 | Episodes: 192600 | Median Reward: 31.23 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.9        |
| time/                   |              |
|    fps                  | 531          |
|    iterations           | 4750         |
|    time_elapsed         | 36627        |
|    total_timesteps      | 19456000     |
| train/                  |              |
|    approx_kl            | 8.344927e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -188         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.21        |
|    n_updates            | 47490        |
|    policy_gradient_loss | 0.000252     |
|    std                  | 108          |
|    value_loss           | 0.465        |
------------------------------------------
Iteration: 4751 | Episodes: 192700 | Median Reward: 31.82 | Max Reward: 48.88
Iteration: 4754 | Episodes: 192800 | Median Reward: 28.22 | Max Reward: 48.88
Iteration: 4756 | Episodes: 192900 | Median Reward: 29.37 | Max Reward: 48.88
Iteration: 4759 | Episodes: 193000 | Median Reward: 27.86 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.3         |
| time/                   |               |
|    fps                  | 531           |
|    iterations           | 4760          |
|    time_elapsed         | 36710         |
|    total_timesteps      | 19496960      |
| train/                  |               |
|    approx_kl            | 0.00027891962 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -188          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.04         |
|    n_updates            | 47590         |
|    policy_gradient_loss | -0.000855     |
|    std                  | 109           |
|    value_loss           | 1.07          |
-------------------------------------------
Iteration: 4761 | Episodes: 193100 | Median Reward: 23.31 | Max Reward: 48.88
Iteration: 4763 | Episodes: 193200 | Median Reward: 30.11 | Max Reward: 48.88
Iteration: 4766 | Episodes: 193300 | Median Reward: 29.37 | Max Reward: 48.88
Iteration: 4768 | Episodes: 193400 | Median Reward: 31.56 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.3         |
| time/                   |               |
|    fps                  | 530           |
|    iterations           | 4770          |
|    time_elapsed         | 36802         |
|    total_timesteps      | 19537920      |
| train/                  |               |
|    approx_kl            | 1.8946841e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -188          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.08         |
|    n_updates            | 47690         |
|    policy_gradient_loss | -0.00027      |
|    std                  | 110           |
|    value_loss           | 0.939         |
-------------------------------------------
Iteration: 4771 | Episodes: 193500 | Median Reward: 30.18 | Max Reward: 48.88
Iteration: 4773 | Episodes: 193600 | Median Reward: 29.54 | Max Reward: 48.88
Iteration: 4776 | Episodes: 193700 | Median Reward: 27.56 | Max Reward: 48.88
Iteration: 4778 | Episodes: 193800 | Median Reward: 26.21 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.6         |
| time/                   |               |
|    fps                  | 530           |
|    iterations           | 4780          |
|    time_elapsed         | 36884         |
|    total_timesteps      | 19578880      |
| train/                  |               |
|    approx_kl            | 0.00013291361 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -188          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.8          |
|    n_updates            | 47790         |
|    policy_gradient_loss | -0.00108      |
|    std                  | 110           |
|    value_loss           | 1.89          |
-------------------------------------------
Iteration: 4781 | Episodes: 193900 | Median Reward: 30.35 | Max Reward: 48.88
Iteration: 4783 | Episodes: 194000 | Median Reward: 27.68 | Max Reward: 48.88
Iteration: 4786 | Episodes: 194100 | Median Reward: 27.70 | Max Reward: 48.88
Iteration: 4788 | Episodes: 194200 | Median Reward: 31.81 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.7        |
| time/                   |              |
|    fps                  | 530          |
|    iterations           | 4790         |
|    time_elapsed         | 36967        |
|    total_timesteps      | 19619840     |
| train/                  |              |
|    approx_kl            | 6.486816e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -188         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.27        |
|    n_updates            | 47890        |
|    policy_gradient_loss | -0.00026     |
|    std                  | 111          |
|    value_loss           | 0.433        |
------------------------------------------
Iteration: 4791 | Episodes: 194300 | Median Reward: 22.51 | Max Reward: 48.88
Iteration: 4793 | Episodes: 194400 | Median Reward: 28.50 | Max Reward: 48.88
Iteration: 4796 | Episodes: 194500 | Median Reward: 26.34 | Max Reward: 48.88
Iteration: 4798 | Episodes: 194600 | Median Reward: 30.81 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.1        |
| time/                   |              |
|    fps                  | 530          |
|    iterations           | 4800         |
|    time_elapsed         | 37056        |
|    total_timesteps      | 19660800     |
| train/                  |              |
|    approx_kl            | 0.0001231602 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -189         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.2         |
|    n_updates            | 47990        |
|    policy_gradient_loss | -0.00107     |
|    std                  | 112          |
|    value_loss           | 0.582        |
------------------------------------------
Iteration: 4800 | Episodes: 194700 | Median Reward: 29.89 | Max Reward: 48.88
Iteration: 4803 | Episodes: 194800 | Median Reward: 32.01 | Max Reward: 48.88
Iteration: 4805 | Episodes: 194900 | Median Reward: 35.84 | Max Reward: 48.88
Iteration: 4808 | Episodes: 195000 | Median Reward: 33.25 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.5         |
| time/                   |               |
|    fps                  | 530           |
|    iterations           | 4810          |
|    time_elapsed         | 37136         |
|    total_timesteps      | 19701760      |
| train/                  |               |
|    approx_kl            | 1.2003817e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -189          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.88         |
|    n_updates            | 48090         |
|    policy_gradient_loss | 0.000111      |
|    std                  | 113           |
|    value_loss           | 1.33          |
-------------------------------------------
Iteration: 4810 | Episodes: 195100 | Median Reward: 29.06 | Max Reward: 48.88
Iteration: 4813 | Episodes: 195200 | Median Reward: 30.90 | Max Reward: 48.88
Iteration: 4815 | Episodes: 195300 | Median Reward: 25.50 | Max Reward: 48.88
Iteration: 4818 | Episodes: 195400 | Median Reward: 28.20 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -72.6         |
| time/                   |               |
|    fps                  | 530           |
|    iterations           | 4820          |
|    time_elapsed         | 37228         |
|    total_timesteps      | 19742720      |
| train/                  |               |
|    approx_kl            | 0.00014444799 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -189          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.8          |
|    n_updates            | 48190         |
|    policy_gradient_loss | -0.0015       |
|    std                  | 115           |
|    value_loss           | 1.54          |
-------------------------------------------
Iteration: 4820 | Episodes: 195500 | Median Reward: 30.01 | Max Reward: 48.88
Iteration: 4823 | Episodes: 195600 | Median Reward: 27.05 | Max Reward: 48.88
Iteration: 4825 | Episodes: 195700 | Median Reward: 26.80 | Max Reward: 48.88
Iteration: 4828 | Episodes: 195800 | Median Reward: 25.05 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.3         |
| time/                   |               |
|    fps                  | 530           |
|    iterations           | 4830          |
|    time_elapsed         | 37308         |
|    total_timesteps      | 19783680      |
| train/                  |               |
|    approx_kl            | 0.00022793548 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -189          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.18         |
|    n_updates            | 48290         |
|    policy_gradient_loss | -0.00133      |
|    std                  | 117           |
|    value_loss           | 0.764         |
-------------------------------------------
Iteration: 4830 | Episodes: 195900 | Median Reward: 34.59 | Max Reward: 48.88
Iteration: 4833 | Episodes: 196000 | Median Reward: 31.30 | Max Reward: 48.88
Iteration: 4835 | Episodes: 196100 | Median Reward: 28.73 | Max Reward: 48.88
Iteration: 4837 | Episodes: 196200 | Median Reward: 31.16 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -73.8        |
| time/                   |              |
|    fps                  | 530          |
|    iterations           | 4840         |
|    time_elapsed         | 37392        |
|    total_timesteps      | 19824640     |
| train/                  |              |
|    approx_kl            | 7.630343e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -190         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.11        |
|    n_updates            | 48390        |
|    policy_gradient_loss | -0.000797    |
|    std                  | 119          |
|    value_loss           | 1.05         |
------------------------------------------
Iteration: 4840 | Episodes: 196300 | Median Reward: 26.73 | Max Reward: 48.88
Iteration: 4842 | Episodes: 196400 | Median Reward: 24.74 | Max Reward: 48.88
Iteration: 4845 | Episodes: 196500 | Median Reward: 29.05 | Max Reward: 48.88
Iteration: 4847 | Episodes: 196600 | Median Reward: 27.30 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.2       |
| time/                   |             |
|    fps                  | 530         |
|    iterations           | 4850        |
|    time_elapsed         | 37480       |
|    total_timesteps      | 19865600    |
| train/                  |             |
|    approx_kl            | 0.001157788 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -190        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.37       |
|    n_updates            | 48490       |
|    policy_gradient_loss | -0.00568    |
|    std                  | 119         |
|    value_loss           | 0.454       |
-----------------------------------------
Iteration: 4850 | Episodes: 196700 | Median Reward: 26.80 | Max Reward: 48.88
Iteration: 4852 | Episodes: 196800 | Median Reward: 28.78 | Max Reward: 48.88
Iteration: 4855 | Episodes: 196900 | Median Reward: 26.28 | Max Reward: 48.88
Iteration: 4857 | Episodes: 197000 | Median Reward: 22.33 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.2         |
| time/                   |               |
|    fps                  | 530           |
|    iterations           | 4860          |
|    time_elapsed         | 37558         |
|    total_timesteps      | 19906560      |
| train/                  |               |
|    approx_kl            | 4.2469444e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -190          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.97         |
|    n_updates            | 48590         |
|    policy_gradient_loss | -0.000275     |
|    std                  | 120           |
|    value_loss           | 1.32          |
-------------------------------------------
Iteration: 4860 | Episodes: 197100 | Median Reward: 29.75 | Max Reward: 48.88
Iteration: 4862 | Episodes: 197200 | Median Reward: 30.82 | Max Reward: 48.88
Iteration: 4865 | Episodes: 197300 | Median Reward: 30.76 | Max Reward: 48.88
Iteration: 4867 | Episodes: 197400 | Median Reward: 29.99 | Max Reward: 48.88
Iteration: 4869 | Episodes: 197500 | Median Reward: 24.63 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.4         |
| time/                   |               |
|    fps                  | 529           |
|    iterations           | 4870          |
|    time_elapsed         | 37650         |
|    total_timesteps      | 19947520      |
| train/                  |               |
|    approx_kl            | 0.00015597863 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -190          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.08         |
|    n_updates            | 48690         |
|    policy_gradient_loss | -0.000779     |
|    std                  | 122           |
|    value_loss           | 1.19          |
-------------------------------------------
Iteration: 4872 | Episodes: 197600 | Median Reward: 32.13 | Max Reward: 48.88
Iteration: 4874 | Episodes: 197700 | Median Reward: 27.53 | Max Reward: 48.88
Iteration: 4877 | Episodes: 197800 | Median Reward: 25.02 | Max Reward: 48.88
Iteration: 4879 | Episodes: 197900 | Median Reward: 30.18 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.6        |
| time/                   |              |
|    fps                  | 529          |
|    iterations           | 4880         |
|    time_elapsed         | 37730        |
|    total_timesteps      | 19988480     |
| train/                  |              |
|    approx_kl            | 0.0004717019 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -191         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.39        |
|    n_updates            | 48790        |
|    policy_gradient_loss | -0.000594    |
|    std                  | 123          |
|    value_loss           | 0.324        |
------------------------------------------
Iteration: 4882 | Episodes: 198000 | Median Reward: 29.59 | Max Reward: 48.88
Iteration: 4884 | Episodes: 198100 | Median Reward: 27.15 | Max Reward: 48.88
Iteration: 4887 | Episodes: 198200 | Median Reward: 30.05 | Max Reward: 48.88
Iteration: 4889 | Episodes: 198300 | Median Reward: 26.23 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.8         |
| time/                   |               |
|    fps                  | 529           |
|    iterations           | 4890          |
|    time_elapsed         | 37814         |
|    total_timesteps      | 20029440      |
| train/                  |               |
|    approx_kl            | 0.00021757549 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -191          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -9            |
|    n_updates            | 48890         |
|    policy_gradient_loss | -0.00181      |
|    std                  | 124           |
|    value_loss           | 1.78          |
-------------------------------------------
Iteration: 4892 | Episodes: 198400 | Median Reward: 14.37 | Max Reward: 48.88
Iteration: 4894 | Episodes: 198500 | Median Reward: 30.77 | Max Reward: 48.88
Iteration: 4897 | Episodes: 198600 | Median Reward: 29.12 | Max Reward: 48.88
Iteration: 4899 | Episodes: 198700 | Median Reward: 25.05 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.2         |
| time/                   |               |
|    fps                  | 529           |
|    iterations           | 4900          |
|    time_elapsed         | 37904         |
|    total_timesteps      | 20070400      |
| train/                  |               |
|    approx_kl            | 0.00017389374 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -191          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.76         |
|    n_updates            | 48990         |
|    policy_gradient_loss | -0.00216      |
|    std                  | 126           |
|    value_loss           | 1.73          |
-------------------------------------------
Iteration: 4902 | Episodes: 198800 | Median Reward: 30.68 | Max Reward: 48.88
Iteration: 4904 | Episodes: 198900 | Median Reward: 34.30 | Max Reward: 48.88
Iteration: 4906 | Episodes: 199000 | Median Reward: 28.82 | Max Reward: 48.88
Iteration: 4909 | Episodes: 199100 | Median Reward: 26.51 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.2         |
| time/                   |               |
|    fps                  | 529           |
|    iterations           | 4910          |
|    time_elapsed         | 37982         |
|    total_timesteps      | 20111360      |
| train/                  |               |
|    approx_kl            | 0.00020752821 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -191          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.41         |
|    n_updates            | 49090         |
|    policy_gradient_loss | -0.00115      |
|    std                  | 127           |
|    value_loss           | 0.414         |
-------------------------------------------
Iteration: 4911 | Episodes: 199200 | Median Reward: 26.10 | Max Reward: 48.88
Iteration: 4914 | Episodes: 199300 | Median Reward: 26.50 | Max Reward: 48.88
Iteration: 4916 | Episodes: 199400 | Median Reward: 28.03 | Max Reward: 48.88
Iteration: 4919 | Episodes: 199500 | Median Reward: 31.44 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.3         |
| time/                   |               |
|    fps                  | 529           |
|    iterations           | 4920          |
|    time_elapsed         | 38073         |
|    total_timesteps      | 20152320      |
| train/                  |               |
|    approx_kl            | 0.00021128137 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -192          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.47         |
|    n_updates            | 49190         |
|    policy_gradient_loss | -0.000584     |
|    std                  | 128           |
|    value_loss           | 0.481         |
-------------------------------------------
Iteration: 4921 | Episodes: 199600 | Median Reward: 30.53 | Max Reward: 48.88
Iteration: 4924 | Episodes: 199700 | Median Reward: 31.61 | Max Reward: 48.88
Iteration: 4926 | Episodes: 199800 | Median Reward: 22.35 | Max Reward: 48.88
Iteration: 4929 | Episodes: 199900 | Median Reward: 24.64 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.2        |
| time/                   |              |
|    fps                  | 529          |
|    iterations           | 4930         |
|    time_elapsed         | 38154        |
|    total_timesteps      | 20193280     |
| train/                  |              |
|    approx_kl            | 7.608216e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -192         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.18        |
|    n_updates            | 49290        |
|    policy_gradient_loss | -0.000552    |
|    std                  | 130          |
|    value_loss           | 0.847        |
------------------------------------------
Iteration: 4931 | Episodes: 200000 | Median Reward: 23.89 | Max Reward: 48.88
Iteration: 4934 | Episodes: 200100 | Median Reward: 29.98 | Max Reward: 48.88
Iteration: 4936 | Episodes: 200200 | Median Reward: 24.97 | Max Reward: 48.88
Iteration: 4939 | Episodes: 200300 | Median Reward: 23.33 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.9         |
| time/                   |               |
|    fps                  | 529           |
|    iterations           | 4940          |
|    time_elapsed         | 38238         |
|    total_timesteps      | 20234240      |
| train/                  |               |
|    approx_kl            | 3.2576674e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -192          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.31         |
|    n_updates            | 49390         |
|    policy_gradient_loss | -0.000428     |
|    std                  | 132           |
|    value_loss           | 0.643         |
-------------------------------------------
Iteration: 4941 | Episodes: 200400 | Median Reward: 27.79 | Max Reward: 48.88
Iteration: 4943 | Episodes: 200500 | Median Reward: 33.15 | Max Reward: 48.88
Iteration: 4946 | Episodes: 200600 | Median Reward: 27.10 | Max Reward: 48.88
Iteration: 4948 | Episodes: 200700 | Median Reward: 24.84 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.5        |
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 4950         |
|    time_elapsed         | 38328        |
|    total_timesteps      | 20275200     |
| train/                  |              |
|    approx_kl            | 3.019825e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -192         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.54        |
|    n_updates            | 49490        |
|    policy_gradient_loss | -0.000225    |
|    std                  | 133          |
|    value_loss           | 3            |
------------------------------------------
Iteration: 4951 | Episodes: 200800 | Median Reward: 29.93 | Max Reward: 48.88
Iteration: 4953 | Episodes: 200900 | Median Reward: 24.78 | Max Reward: 48.88
Iteration: 4956 | Episodes: 201000 | Median Reward: 24.44 | Max Reward: 48.88
Iteration: 4958 | Episodes: 201100 | Median Reward: 26.47 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.4         |
| time/                   |               |
|    fps                  | 528           |
|    iterations           | 4960          |
|    time_elapsed         | 38406         |
|    total_timesteps      | 20316160      |
| train/                  |               |
|    approx_kl            | 0.00013244478 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -193          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.43         |
|    n_updates            | 49590         |
|    policy_gradient_loss | -0.000904     |
|    std                  | 135           |
|    value_loss           | 0.572         |
-------------------------------------------
Iteration: 4961 | Episodes: 201200 | Median Reward: 27.42 | Max Reward: 48.88
Iteration: 4963 | Episodes: 201300 | Median Reward: 27.98 | Max Reward: 48.88
Iteration: 4966 | Episodes: 201400 | Median Reward: 21.05 | Max Reward: 48.88
Iteration: 4968 | Episodes: 201500 | Median Reward: 27.64 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -73.1        |
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 4970         |
|    time_elapsed         | 38500        |
|    total_timesteps      | 20357120     |
| train/                  |              |
|    approx_kl            | 0.0007956347 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -193         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.46        |
|    n_updates            | 49690        |
|    policy_gradient_loss | -0.00398     |
|    std                  | 136          |
|    value_loss           | 0.657        |
------------------------------------------
Iteration: 4971 | Episodes: 201600 | Median Reward: 30.51 | Max Reward: 48.88
Iteration: 4973 | Episodes: 201700 | Median Reward: 30.23 | Max Reward: 48.88
Iteration: 4976 | Episodes: 201800 | Median Reward: 29.81 | Max Reward: 48.88
Iteration: 4978 | Episodes: 201900 | Median Reward: 27.89 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.1         |
| time/                   |               |
|    fps                  | 528           |
|    iterations           | 4980          |
|    time_elapsed         | 38581         |
|    total_timesteps      | 20398080      |
| train/                  |               |
|    approx_kl            | 0.00012392663 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -193          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.55         |
|    n_updates            | 49790         |
|    policy_gradient_loss | -0.00135      |
|    std                  | 137           |
|    value_loss           | 0.458         |
-------------------------------------------
Iteration: 4980 | Episodes: 202000 | Median Reward: 24.79 | Max Reward: 48.88
Iteration: 4983 | Episodes: 202100 | Median Reward: 29.60 | Max Reward: 48.88
Iteration: 4985 | Episodes: 202200 | Median Reward: 29.03 | Max Reward: 48.88
Iteration: 4988 | Episodes: 202300 | Median Reward: 31.46 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -73          |
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 4990         |
|    time_elapsed         | 38663        |
|    total_timesteps      | 20439040     |
| train/                  |              |
|    approx_kl            | 0.0005138018 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -193         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.46        |
|    n_updates            | 49890        |
|    policy_gradient_loss | -0.00336     |
|    std                  | 139          |
|    value_loss           | 0.808        |
------------------------------------------
Iteration: 4990 | Episodes: 202400 | Median Reward: 29.06 | Max Reward: 48.88
Iteration: 4993 | Episodes: 202500 | Median Reward: 24.07 | Max Reward: 48.88
Iteration: 4995 | Episodes: 202600 | Median Reward: 25.71 | Max Reward: 48.88
Iteration: 4998 | Episodes: 202700 | Median Reward: 21.89 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.9       |
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 5000        |
|    time_elapsed         | 38751       |
|    total_timesteps      | 20480000    |
| train/                  |             |
|    approx_kl            | 8.01346e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -194        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.38       |
|    n_updates            | 49990       |
|    policy_gradient_loss | -0.00126    |
|    std                  | 141         |
|    value_loss           | 1.04        |
-----------------------------------------
Iteration: 5000 | Episodes: 202800 | Median Reward: 27.81 | Max Reward: 48.88
Iteration: 5003 | Episodes: 202900 | Median Reward: 23.80 | Max Reward: 48.88
Iteration: 5005 | Episodes: 203000 | Median Reward: 22.31 | Max Reward: 48.88
Iteration: 5008 | Episodes: 203100 | Median Reward: 25.19 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.5         |
| time/                   |               |
|    fps                  | 528           |
|    iterations           | 5010          |
|    time_elapsed         | 38829         |
|    total_timesteps      | 20520960      |
| train/                  |               |
|    approx_kl            | 0.00033122126 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -194          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.44         |
|    n_updates            | 50090         |
|    policy_gradient_loss | -0.00153      |
|    std                  | 144           |
|    value_loss           | 0.703         |
-------------------------------------------
Iteration: 5010 | Episodes: 203200 | Median Reward: 29.16 | Max Reward: 48.88
Iteration: 5013 | Episodes: 203300 | Median Reward: 25.44 | Max Reward: 48.88
Iteration: 5015 | Episodes: 203400 | Median Reward: 27.87 | Max Reward: 48.88
Iteration: 5017 | Episodes: 203500 | Median Reward: 30.89 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.4        |
| time/                   |              |
|    fps                  | 528          |
|    iterations           | 5020         |
|    time_elapsed         | 38922        |
|    total_timesteps      | 20561920     |
| train/                  |              |
|    approx_kl            | 0.0009422661 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -194         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.28        |
|    n_updates            | 50190        |
|    policy_gradient_loss | -0.00217     |
|    std                  | 146          |
|    value_loss           | 1.32         |
------------------------------------------
Iteration: 5020 | Episodes: 203600 | Median Reward: 22.46 | Max Reward: 48.88
Iteration: 5022 | Episodes: 203700 | Median Reward: 26.86 | Max Reward: 48.88
Iteration: 5025 | Episodes: 203800 | Median Reward: 24.87 | Max Reward: 48.88
Iteration: 5027 | Episodes: 203900 | Median Reward: 22.94 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.6       |
| time/                   |             |
|    fps                  | 528         |
|    iterations           | 5030        |
|    time_elapsed         | 39002       |
|    total_timesteps      | 20602880    |
| train/                  |             |
|    approx_kl            | 9.29512e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -195        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.29       |
|    n_updates            | 50290       |
|    policy_gradient_loss | -0.000427   |
|    std                  | 148         |
|    value_loss           | 1.14        |
-----------------------------------------
Iteration: 5030 | Episodes: 204000 | Median Reward: 29.16 | Max Reward: 48.88
Iteration: 5032 | Episodes: 204100 | Median Reward: 21.95 | Max Reward: 48.88
Iteration: 5035 | Episodes: 204200 | Median Reward: 26.31 | Max Reward: 48.88
Iteration: 5037 | Episodes: 204300 | Median Reward: 26.84 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.1         |
| time/                   |               |
|    fps                  | 528           |
|    iterations           | 5040          |
|    time_elapsed         | 39086         |
|    total_timesteps      | 20643840      |
| train/                  |               |
|    approx_kl            | 0.00039253396 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -195          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.57         |
|    n_updates            | 50390         |
|    policy_gradient_loss | -0.00191      |
|    std                  | 150           |
|    value_loss           | 0.54          |
-------------------------------------------
Iteration: 5040 | Episodes: 204400 | Median Reward: 25.77 | Max Reward: 48.88
Iteration: 5042 | Episodes: 204500 | Median Reward: 23.96 | Max Reward: 48.88
Iteration: 5045 | Episodes: 204600 | Median Reward: 31.29 | Max Reward: 48.88
Iteration: 5047 | Episodes: 204700 | Median Reward: 32.31 | Max Reward: 48.88
Iteration: 5049 | Episodes: 204800 | Median Reward: 30.68 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -72            |
| time/                   |                |
|    fps                  | 527            |
|    iterations           | 5050           |
|    time_elapsed         | 39175          |
|    total_timesteps      | 20684800       |
| train/                  |                |
|    approx_kl            | 0.000106175765 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -195           |
|    explained_variance   | 0.998          |
|    learning_rate        | 0.0005         |
|    loss                 | -9.11          |
|    n_updates            | 50490          |
|    policy_gradient_loss | -0.000913      |
|    std                  | 151            |
|    value_loss           | 1.29           |
--------------------------------------------
Iteration: 5052 | Episodes: 204900 | Median Reward: 34.10 | Max Reward: 48.88
Iteration: 5054 | Episodes: 205000 | Median Reward: 26.49 | Max Reward: 48.88
Iteration: 5057 | Episodes: 205100 | Median Reward: 26.19 | Max Reward: 48.88
Iteration: 5059 | Episodes: 205200 | Median Reward: 27.15 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.3         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 5060          |
|    time_elapsed         | 39254         |
|    total_timesteps      | 20725760      |
| train/                  |               |
|    approx_kl            | 0.00020767123 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -196          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.52         |
|    n_updates            | 50590         |
|    policy_gradient_loss | -0.00157      |
|    std                  | 153           |
|    value_loss           | 0.693         |
-------------------------------------------
Iteration: 5062 | Episodes: 205300 | Median Reward: 25.83 | Max Reward: 48.88
Iteration: 5064 | Episodes: 205400 | Median Reward: 27.91 | Max Reward: 48.88
Iteration: 5067 | Episodes: 205500 | Median Reward: 27.33 | Max Reward: 48.88
Iteration: 5069 | Episodes: 205600 | Median Reward: 28.53 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.5         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 5070          |
|    time_elapsed         | 39345         |
|    total_timesteps      | 20766720      |
| train/                  |               |
|    approx_kl            | 7.9107136e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -196          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.46         |
|    n_updates            | 50690         |
|    policy_gradient_loss | -0.00174      |
|    std                  | 155           |
|    value_loss           | 0.806         |
-------------------------------------------
Iteration: 5072 | Episodes: 205700 | Median Reward: 22.87 | Max Reward: 48.88
Iteration: 5074 | Episodes: 205800 | Median Reward: 26.70 | Max Reward: 48.88
Iteration: 5077 | Episodes: 205900 | Median Reward: 31.47 | Max Reward: 48.88
Iteration: 5079 | Episodes: 206000 | Median Reward: 25.49 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.6         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 5080          |
|    time_elapsed         | 39424         |
|    total_timesteps      | 20807680      |
| train/                  |               |
|    approx_kl            | 0.00014702408 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -196          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.28         |
|    n_updates            | 50790         |
|    policy_gradient_loss | -0.00143      |
|    std                  | 157           |
|    value_loss           | 1.77          |
-------------------------------------------
Iteration: 5082 | Episodes: 206100 | Median Reward: 25.93 | Max Reward: 48.88
Iteration: 5084 | Episodes: 206200 | Median Reward: 17.76 | Max Reward: 48.88
Iteration: 5086 | Episodes: 206300 | Median Reward: 26.07 | Max Reward: 48.88
Iteration: 5089 | Episodes: 206400 | Median Reward: 26.09 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.7        |
| time/                   |              |
|    fps                  | 527          |
|    iterations           | 5090         |
|    time_elapsed         | 39505        |
|    total_timesteps      | 20848640     |
| train/                  |              |
|    approx_kl            | 0.0018486055 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -196         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.17        |
|    n_updates            | 50890        |
|    policy_gradient_loss | -0.00513     |
|    std                  | 158          |
|    value_loss           | 1.71         |
------------------------------------------
Iteration: 5091 | Episodes: 206500 | Median Reward: 24.44 | Max Reward: 48.88
Iteration: 5094 | Episodes: 206600 | Median Reward: 22.60 | Max Reward: 48.88
Iteration: 5096 | Episodes: 206700 | Median Reward: 26.13 | Max Reward: 48.88
Iteration: 5099 | Episodes: 206800 | Median Reward: 30.62 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.6         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 5100          |
|    time_elapsed         | 39590         |
|    total_timesteps      | 20889600      |
| train/                  |               |
|    approx_kl            | 0.00015321665 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -196          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.51         |
|    n_updates            | 50990         |
|    policy_gradient_loss | -0.00125      |
|    std                  | 160           |
|    value_loss           | 0.888         |
-------------------------------------------
Iteration: 5101 | Episodes: 206900 | Median Reward: 25.06 | Max Reward: 48.88
Iteration: 5104 | Episodes: 207000 | Median Reward: 27.13 | Max Reward: 48.88
Iteration: 5106 | Episodes: 207100 | Median Reward: 25.29 | Max Reward: 48.88
Iteration: 5109 | Episodes: 207200 | Median Reward: 22.91 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.7         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 5110          |
|    time_elapsed         | 39667         |
|    total_timesteps      | 20930560      |
| train/                  |               |
|    approx_kl            | 0.00022140871 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -197          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.61         |
|    n_updates            | 51090         |
|    policy_gradient_loss | -0.00219      |
|    std                  | 162           |
|    value_loss           | 0.708         |
-------------------------------------------
Iteration: 5111 | Episodes: 207300 | Median Reward: 27.84 | Max Reward: 48.88
Iteration: 5114 | Episodes: 207400 | Median Reward: 29.44 | Max Reward: 48.88
Iteration: 5116 | Episodes: 207500 | Median Reward: 24.36 | Max Reward: 48.88
Iteration: 5119 | Episodes: 207600 | Median Reward: 28.94 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.1         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 5120          |
|    time_elapsed         | 39756         |
|    total_timesteps      | 20971520      |
| train/                  |               |
|    approx_kl            | 0.00022288272 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -197          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.38         |
|    n_updates            | 51190         |
|    policy_gradient_loss | -0.00116      |
|    std                  | 164           |
|    value_loss           | 1.63          |
-------------------------------------------
Iteration: 5121 | Episodes: 207700 | Median Reward: 29.30 | Max Reward: 48.88
Iteration: 5123 | Episodes: 207800 | Median Reward: 26.53 | Max Reward: 48.88
Iteration: 5126 | Episodes: 207900 | Median Reward: 29.80 | Max Reward: 48.88
Iteration: 5128 | Episodes: 208000 | Median Reward: 24.37 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76          |
| time/                   |              |
|    fps                  | 527          |
|    iterations           | 5130         |
|    time_elapsed         | 39833        |
|    total_timesteps      | 21012480     |
| train/                  |              |
|    approx_kl            | 8.590464e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -197         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.44        |
|    n_updates            | 51290        |
|    policy_gradient_loss | -0.000622    |
|    std                  | 166          |
|    value_loss           | 1.42         |
------------------------------------------
Iteration: 5131 | Episodes: 208100 | Median Reward: 29.53 | Max Reward: 48.88
Iteration: 5133 | Episodes: 208200 | Median Reward: 24.42 | Max Reward: 48.88
Iteration: 5136 | Episodes: 208300 | Median Reward: 21.70 | Max Reward: 48.88
Iteration: 5138 | Episodes: 208400 | Median Reward: 26.32 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.2        |
| time/                   |              |
|    fps                  | 527          |
|    iterations           | 5140         |
|    time_elapsed         | 39911        |
|    total_timesteps      | 21053440     |
| train/                  |              |
|    approx_kl            | 7.185727e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -197         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.61        |
|    n_updates            | 51390        |
|    policy_gradient_loss | -0.000699    |
|    std                  | 168          |
|    value_loss           | 0.646        |
------------------------------------------
Iteration: 5141 | Episodes: 208500 | Median Reward: 20.78 | Max Reward: 48.88
Iteration: 5143 | Episodes: 208600 | Median Reward: 20.78 | Max Reward: 48.88
Iteration: 5146 | Episodes: 208700 | Median Reward: 27.85 | Max Reward: 48.88
Iteration: 5148 | Episodes: 208800 | Median Reward: 25.64 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.5         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 5150          |
|    time_elapsed         | 40000         |
|    total_timesteps      | 21094400      |
| train/                  |               |
|    approx_kl            | 0.00020352541 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -198          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.64         |
|    n_updates            | 51490         |
|    policy_gradient_loss | -0.00181      |
|    std                  | 170           |
|    value_loss           | 0.626         |
-------------------------------------------
Iteration: 5151 | Episodes: 208900 | Median Reward: 26.42 | Max Reward: 48.88
Iteration: 5153 | Episodes: 209000 | Median Reward: 22.96 | Max Reward: 48.88
Iteration: 5156 | Episodes: 209100 | Median Reward: 25.08 | Max Reward: 48.88
Iteration: 5158 | Episodes: 209200 | Median Reward: 28.90 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.3         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 5160          |
|    time_elapsed         | 40075         |
|    total_timesteps      | 21135360      |
| train/                  |               |
|    approx_kl            | 5.5838536e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -198          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.36         |
|    n_updates            | 51590         |
|    policy_gradient_loss | -0.000348     |
|    std                  | 174           |
|    value_loss           | 1.64          |
-------------------------------------------
Iteration: 5160 | Episodes: 209300 | Median Reward: 26.08 | Max Reward: 48.88
Iteration: 5163 | Episodes: 209400 | Median Reward: 33.17 | Max Reward: 48.88
Iteration: 5165 | Episodes: 209500 | Median Reward: 22.31 | Max Reward: 48.88
Iteration: 5168 | Episodes: 209600 | Median Reward: 25.34 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.8         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 5170          |
|    time_elapsed         | 40163         |
|    total_timesteps      | 21176320      |
| train/                  |               |
|    approx_kl            | 0.00034449206 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -198          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.66         |
|    n_updates            | 51690         |
|    policy_gradient_loss | -0.00312      |
|    std                  | 175           |
|    value_loss           | 0.736         |
-------------------------------------------
Iteration: 5170 | Episodes: 209700 | Median Reward: 22.81 | Max Reward: 48.88
Iteration: 5173 | Episodes: 209800 | Median Reward: 28.84 | Max Reward: 48.88
Iteration: 5175 | Episodes: 209900 | Median Reward: 26.73 | Max Reward: 48.88
Iteration: 5178 | Episodes: 210000 | Median Reward: 28.25 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.6        |
| time/                   |              |
|    fps                  | 527          |
|    iterations           | 5180         |
|    time_elapsed         | 40244        |
|    total_timesteps      | 21217280     |
| train/                  |              |
|    approx_kl            | 8.260048e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -199         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.57        |
|    n_updates            | 51790        |
|    policy_gradient_loss | -0.000507    |
|    std                  | 177          |
|    value_loss           | 0.904        |
------------------------------------------
Iteration: 5180 | Episodes: 210100 | Median Reward: 21.59 | Max Reward: 48.88
Iteration: 5183 | Episodes: 210200 | Median Reward: 18.31 | Max Reward: 48.88
Iteration: 5185 | Episodes: 210300 | Median Reward: 31.92 | Max Reward: 48.88
Iteration: 5188 | Episodes: 210400 | Median Reward: 26.44 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72          |
| time/                   |              |
|    fps                  | 527          |
|    iterations           | 5190         |
|    time_elapsed         | 40320        |
|    total_timesteps      | 21258240     |
| train/                  |              |
|    approx_kl            | 0.0012340278 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -199         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.47        |
|    n_updates            | 51890        |
|    policy_gradient_loss | -0.00964     |
|    std                  | 180          |
|    value_loss           | 1.4          |
------------------------------------------
Iteration: 5190 | Episodes: 210500 | Median Reward: 26.81 | Max Reward: 48.88
Iteration: 5193 | Episodes: 210600 | Median Reward: 31.60 | Max Reward: 48.88
Iteration: 5195 | Episodes: 210700 | Median Reward: 21.45 | Max Reward: 48.88
Iteration: 5197 | Episodes: 210800 | Median Reward: 22.66 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.3        |
| time/                   |              |
|    fps                  | 527          |
|    iterations           | 5200         |
|    time_elapsed         | 40412        |
|    total_timesteps      | 21299200     |
| train/                  |              |
|    approx_kl            | 4.881239e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -199         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.67        |
|    n_updates            | 51990        |
|    policy_gradient_loss | -0.000277    |
|    std                  | 182          |
|    value_loss           | 0.794        |
------------------------------------------
Iteration: 5200 | Episodes: 210900 | Median Reward: 24.65 | Max Reward: 48.88
Iteration: 5202 | Episodes: 211000 | Median Reward: 29.47 | Max Reward: 48.88
Iteration: 5205 | Episodes: 211100 | Median Reward: 22.20 | Max Reward: 48.88
Iteration: 5207 | Episodes: 211200 | Median Reward: 26.66 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.2         |
| time/                   |               |
|    fps                  | 527           |
|    iterations           | 5210          |
|    time_elapsed         | 40490         |
|    total_timesteps      | 21340160      |
| train/                  |               |
|    approx_kl            | 0.00018411459 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -199          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.59         |
|    n_updates            | 52090         |
|    policy_gradient_loss | -0.000702     |
|    std                  | 185           |
|    value_loss           | 0.961         |
-------------------------------------------
Iteration: 5210 | Episodes: 211300 | Median Reward: 25.67 | Max Reward: 48.88
Iteration: 5212 | Episodes: 211400 | Median Reward: 22.18 | Max Reward: 48.88
Iteration: 5215 | Episodes: 211500 | Median Reward: 22.04 | Max Reward: 48.88
Iteration: 5217 | Episodes: 211600 | Median Reward: 22.39 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.5        |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5220         |
|    time_elapsed         | 40571        |
|    total_timesteps      | 21381120     |
| train/                  |              |
|    approx_kl            | 0.0005311944 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -200         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.73        |
|    n_updates            | 52190        |
|    policy_gradient_loss | -0.00206     |
|    std                  | 187          |
|    value_loss           | 0.663        |
------------------------------------------
Iteration: 5220 | Episodes: 211700 | Median Reward: 26.13 | Max Reward: 48.88
Iteration: 5222 | Episodes: 211800 | Median Reward: 23.52 | Max Reward: 48.88
Iteration: 5225 | Episodes: 211900 | Median Reward: 26.15 | Max Reward: 48.88
Iteration: 5227 | Episodes: 212000 | Median Reward: 24.48 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.9         |
| time/                   |               |
|    fps                  | 526           |
|    iterations           | 5230          |
|    time_elapsed         | 40658         |
|    total_timesteps      | 21422080      |
| train/                  |               |
|    approx_kl            | 3.7082922e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -200          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.79         |
|    n_updates            | 52290         |
|    policy_gradient_loss | 0.00016       |
|    std                  | 189           |
|    value_loss           | 0.56          |
-------------------------------------------
Iteration: 5230 | Episodes: 212100 | Median Reward: 28.07 | Max Reward: 48.88
Iteration: 5232 | Episodes: 212200 | Median Reward: 26.80 | Max Reward: 48.88
Iteration: 5234 | Episodes: 212300 | Median Reward: 31.22 | Max Reward: 48.88
Iteration: 5237 | Episodes: 212400 | Median Reward: 23.36 | Max Reward: 48.88
Iteration: 5239 | Episodes: 212500 | Median Reward: 21.58 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.8         |
| time/                   |               |
|    fps                  | 526           |
|    iterations           | 5240          |
|    time_elapsed         | 40734         |
|    total_timesteps      | 21463040      |
| train/                  |               |
|    approx_kl            | 0.00060797075 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -200          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.79         |
|    n_updates            | 52390         |
|    policy_gradient_loss | -0.00308      |
|    std                  | 193           |
|    value_loss           | 0.582         |
-------------------------------------------
Iteration: 5242 | Episodes: 212600 | Median Reward: 18.91 | Max Reward: 48.88
Iteration: 5244 | Episodes: 212700 | Median Reward: 30.16 | Max Reward: 48.88
Iteration: 5247 | Episodes: 212800 | Median Reward: 29.70 | Max Reward: 48.88
Iteration: 5249 | Episodes: 212900 | Median Reward: 26.31 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75          |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5250         |
|    time_elapsed         | 40824        |
|    total_timesteps      | 21504000     |
| train/                  |              |
|    approx_kl            | 0.0011736095 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -200         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.84        |
|    n_updates            | 52490        |
|    policy_gradient_loss | -0.008       |
|    std                  | 196          |
|    value_loss           | 0.59         |
------------------------------------------
Iteration: 5252 | Episodes: 213000 | Median Reward: 29.80 | Max Reward: 48.88
Iteration: 5254 | Episodes: 213100 | Median Reward: 27.13 | Max Reward: 48.88
Iteration: 5257 | Episodes: 213200 | Median Reward: 26.89 | Max Reward: 48.88
Iteration: 5259 | Episodes: 213300 | Median Reward: 19.76 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.3        |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5260         |
|    time_elapsed         | 40902        |
|    total_timesteps      | 21544960     |
| train/                  |              |
|    approx_kl            | 0.0001254992 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -201         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.01        |
|    n_updates            | 52590        |
|    policy_gradient_loss | -0.00161     |
|    std                  | 199          |
|    value_loss           | 2.37         |
------------------------------------------
Iteration: 5262 | Episodes: 213400 | Median Reward: 21.79 | Max Reward: 48.88
Iteration: 5264 | Episodes: 213500 | Median Reward: 21.38 | Max Reward: 48.88
Iteration: 5266 | Episodes: 213600 | Median Reward: 28.61 | Max Reward: 48.88
Iteration: 5269 | Episodes: 213700 | Median Reward: 24.18 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.8        |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5270         |
|    time_elapsed         | 40980        |
|    total_timesteps      | 21585920     |
| train/                  |              |
|    approx_kl            | 0.0002889805 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -201         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.79        |
|    n_updates            | 52690        |
|    policy_gradient_loss | -0.00189     |
|    std                  | 199          |
|    value_loss           | 0.743        |
------------------------------------------
Iteration: 5271 | Episodes: 213800 | Median Reward: 16.03 | Max Reward: 48.88
Iteration: 5274 | Episodes: 213900 | Median Reward: 20.94 | Max Reward: 48.88
Iteration: 5276 | Episodes: 214000 | Median Reward: 25.47 | Max Reward: 48.88
Iteration: 5279 | Episodes: 214100 | Median Reward: 24.88 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.2        |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5280         |
|    time_elapsed         | 41069        |
|    total_timesteps      | 21626880     |
| train/                  |              |
|    approx_kl            | 0.0061015654 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -201         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.74        |
|    n_updates            | 52790        |
|    policy_gradient_loss | -0.0168      |
|    std                  | 200          |
|    value_loss           | 1.07         |
------------------------------------------
Iteration: 5281 | Episodes: 214200 | Median Reward: 19.21 | Max Reward: 48.88
Iteration: 5284 | Episodes: 214300 | Median Reward: 26.04 | Max Reward: 48.88
Iteration: 5286 | Episodes: 214400 | Median Reward: 28.84 | Max Reward: 48.88
Iteration: 5289 | Episodes: 214500 | Median Reward: 29.69 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.5        |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5290         |
|    time_elapsed         | 41146        |
|    total_timesteps      | 21667840     |
| train/                  |              |
|    approx_kl            | 0.0003140596 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -201         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.94        |
|    n_updates            | 52890        |
|    policy_gradient_loss | -0.00206     |
|    std                  | 202          |
|    value_loss           | 0.366        |
------------------------------------------
Iteration: 5291 | Episodes: 214600 | Median Reward: 23.48 | Max Reward: 48.88
Iteration: 5294 | Episodes: 214700 | Median Reward: 16.69 | Max Reward: 48.88
Iteration: 5296 | Episodes: 214800 | Median Reward: 23.81 | Max Reward: 48.88
Iteration: 5299 | Episodes: 214900 | Median Reward: 21.20 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.2         |
| time/                   |               |
|    fps                  | 526           |
|    iterations           | 5300          |
|    time_elapsed         | 41231         |
|    total_timesteps      | 21708800      |
| train/                  |               |
|    approx_kl            | 0.00016456997 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -202          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.63         |
|    n_updates            | 52990         |
|    policy_gradient_loss | -0.00148      |
|    std                  | 205           |
|    value_loss           | 1.21          |
-------------------------------------------
Iteration: 5301 | Episodes: 215000 | Median Reward: 25.14 | Max Reward: 48.88
Iteration: 5303 | Episodes: 215100 | Median Reward: 16.45 | Max Reward: 48.88
Iteration: 5306 | Episodes: 215200 | Median Reward: 29.57 | Max Reward: 48.88
Iteration: 5308 | Episodes: 215300 | Median Reward: 20.06 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.9        |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5310         |
|    time_elapsed         | 41314        |
|    total_timesteps      | 21749760     |
| train/                  |              |
|    approx_kl            | 0.0001880462 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -202         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.64        |
|    n_updates            | 53090        |
|    policy_gradient_loss | -0.00154     |
|    std                  | 205          |
|    value_loss           | 1.01         |
------------------------------------------
Iteration: 5311 | Episodes: 215400 | Median Reward: 8.56 | Max Reward: 48.88
Iteration: 5313 | Episodes: 215500 | Median Reward: 22.36 | Max Reward: 48.88
Iteration: 5316 | Episodes: 215600 | Median Reward: 23.95 | Max Reward: 48.88
Iteration: 5318 | Episodes: 215700 | Median Reward: 26.41 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72.1        |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5320         |
|    time_elapsed         | 41392        |
|    total_timesteps      | 21790720     |
| train/                  |              |
|    approx_kl            | 0.0004814177 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -202         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.94        |
|    n_updates            | 53190        |
|    policy_gradient_loss | -0.00342     |
|    std                  | 208          |
|    value_loss           | 0.459        |
------------------------------------------
Iteration: 5321 | Episodes: 215800 | Median Reward: 28.25 | Max Reward: 48.88
Iteration: 5323 | Episodes: 215900 | Median Reward: 22.55 | Max Reward: 48.88
Iteration: 5326 | Episodes: 216000 | Median Reward: 31.83 | Max Reward: 48.88
Iteration: 5328 | Episodes: 216100 | Median Reward: 28.54 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.5         |
| time/                   |               |
|    fps                  | 526           |
|    iterations           | 5330          |
|    time_elapsed         | 41481         |
|    total_timesteps      | 21831680      |
| train/                  |               |
|    approx_kl            | 0.00028415813 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -202          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.98         |
|    n_updates            | 53290         |
|    policy_gradient_loss | -0.00138      |
|    std                  | 210           |
|    value_loss           | 0.377         |
-------------------------------------------
Iteration: 5331 | Episodes: 216200 | Median Reward: 31.85 | Max Reward: 48.88
Iteration: 5333 | Episodes: 216300 | Median Reward: 19.96 | Max Reward: 48.88
Iteration: 5336 | Episodes: 216400 | Median Reward: 23.57 | Max Reward: 48.88
Iteration: 5338 | Episodes: 216500 | Median Reward: 15.48 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.2        |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5340         |
|    time_elapsed         | 41557        |
|    total_timesteps      | 21872640     |
| train/                  |              |
|    approx_kl            | 0.0002077749 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -202         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.48        |
|    n_updates            | 53390        |
|    policy_gradient_loss | -0.000412    |
|    std                  | 212          |
|    value_loss           | 1.5          |
------------------------------------------
Iteration: 5340 | Episodes: 216600 | Median Reward: 26.59 | Max Reward: 48.88
Iteration: 5343 | Episodes: 216700 | Median Reward: 25.47 | Max Reward: 48.88
Iteration: 5345 | Episodes: 216800 | Median Reward: 26.19 | Max Reward: 48.88
Iteration: 5348 | Episodes: 216900 | Median Reward: 26.24 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.8         |
| time/                   |               |
|    fps                  | 526           |
|    iterations           | 5350          |
|    time_elapsed         | 41638         |
|    total_timesteps      | 21913600      |
| train/                  |               |
|    approx_kl            | 0.00019836215 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -203          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.66         |
|    n_updates            | 53490         |
|    policy_gradient_loss | -0.000653     |
|    std                  | 213           |
|    value_loss           | 1.17          |
-------------------------------------------
Iteration: 5350 | Episodes: 217000 | Median Reward: 25.64 | Max Reward: 48.88
Iteration: 5353 | Episodes: 217100 | Median Reward: 25.91 | Max Reward: 48.88
Iteration: 5355 | Episodes: 217200 | Median Reward: 25.45 | Max Reward: 48.88
Iteration: 5358 | Episodes: 217300 | Median Reward: 27.13 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77          |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5360         |
|    time_elapsed         | 41725        |
|    total_timesteps      | 21954560     |
| train/                  |              |
|    approx_kl            | 0.0011515346 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -203         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.48        |
|    n_updates            | 53590        |
|    policy_gradient_loss | -0.0064      |
|    std                  | 217          |
|    value_loss           | 1.79         |
------------------------------------------
Iteration: 5360 | Episodes: 217400 | Median Reward: 22.82 | Max Reward: 48.88
Iteration: 5363 | Episodes: 217500 | Median Reward: 20.57 | Max Reward: 48.88
Iteration: 5365 | Episodes: 217600 | Median Reward: 24.98 | Max Reward: 48.88
Iteration: 5368 | Episodes: 217700 | Median Reward: 23.83 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72.1        |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5370         |
|    time_elapsed         | 41803        |
|    total_timesteps      | 21995520     |
| train/                  |              |
|    approx_kl            | 0.0032346176 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -203         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.1        |
|    n_updates            | 53690        |
|    policy_gradient_loss | -0.00884     |
|    std                  | 220          |
|    value_loss           | 0.263        |
------------------------------------------
Iteration: 5370 | Episodes: 217800 | Median Reward: 27.43 | Max Reward: 48.88
Iteration: 5373 | Episodes: 217900 | Median Reward: 25.27 | Max Reward: 48.88
Iteration: 5375 | Episodes: 218000 | Median Reward: 27.49 | Max Reward: 48.88
Iteration: 5377 | Episodes: 218100 | Median Reward: 29.43 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.3        |
| time/                   |              |
|    fps                  | 526          |
|    iterations           | 5380         |
|    time_elapsed         | 41890        |
|    total_timesteps      | 22036480     |
| train/                  |              |
|    approx_kl            | 0.0018156262 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -204         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -10          |
|    n_updates            | 53790        |
|    policy_gradient_loss | -0.0116      |
|    std                  | 223          |
|    value_loss           | 0.433        |
------------------------------------------
Iteration: 5380 | Episodes: 218200 | Median Reward: 21.84 | Max Reward: 48.88
Iteration: 5382 | Episodes: 218300 | Median Reward: 28.34 | Max Reward: 48.88
Iteration: 5385 | Episodes: 218400 | Median Reward: 29.60 | Max Reward: 48.88
Iteration: 5387 | Episodes: 218500 | Median Reward: 27.59 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.9         |
| time/                   |               |
|    fps                  | 526           |
|    iterations           | 5390          |
|    time_elapsed         | 41970         |
|    total_timesteps      | 22077440      |
| train/                  |               |
|    approx_kl            | 3.8104597e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -204          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.4          |
|    n_updates            | 53890         |
|    policy_gradient_loss | -0.000479     |
|    std                  | 225           |
|    value_loss           | 1.89          |
-------------------------------------------
Iteration: 5390 | Episodes: 218600 | Median Reward: 18.75 | Max Reward: 48.88
Iteration: 5392 | Episodes: 218700 | Median Reward: 23.01 | Max Reward: 48.88
Iteration: 5395 | Episodes: 218800 | Median Reward: 23.13 | Max Reward: 48.88
Iteration: 5397 | Episodes: 218900 | Median Reward: 21.19 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.1         |
| time/                   |               |
|    fps                  | 526           |
|    iterations           | 5400          |
|    time_elapsed         | 42047         |
|    total_timesteps      | 22118400      |
| train/                  |               |
|    approx_kl            | 4.8534363e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -204          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.88         |
|    n_updates            | 53990         |
|    policy_gradient_loss | -0.000775     |
|    std                  | 228           |
|    value_loss           | 0.856         |
-------------------------------------------
Iteration: 5400 | Episodes: 219000 | Median Reward: 19.20 | Max Reward: 48.88
Iteration: 5402 | Episodes: 219100 | Median Reward: 29.57 | Max Reward: 48.88
Iteration: 5405 | Episodes: 219200 | Median Reward: 22.17 | Max Reward: 48.88
Iteration: 5407 | Episodes: 219300 | Median Reward: 22.47 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.1         |
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 5410          |
|    time_elapsed         | 42136         |
|    total_timesteps      | 22159360      |
| train/                  |               |
|    approx_kl            | 0.00020983232 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -204          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.98         |
|    n_updates            | 54090         |
|    policy_gradient_loss | -0.000559     |
|    std                  | 231           |
|    value_loss           | 0.799         |
-------------------------------------------
Iteration: 5410 | Episodes: 219400 | Median Reward: 26.25 | Max Reward: 48.88
Iteration: 5412 | Episodes: 219500 | Median Reward: 23.45 | Max Reward: 48.88
Iteration: 5414 | Episodes: 219600 | Median Reward: 25.39 | Max Reward: 48.88
Iteration: 5417 | Episodes: 219700 | Median Reward: 26.25 | Max Reward: 48.88
Iteration: 5419 | Episodes: 219800 | Median Reward: 19.98 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76         |
| time/                   |             |
|    fps                  | 525         |
|    iterations           | 5420        |
|    time_elapsed         | 42212       |
|    total_timesteps      | 22200320    |
| train/                  |             |
|    approx_kl            | 0.004619021 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -205        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.1       |
|    n_updates            | 54190       |
|    policy_gradient_loss | -0.012      |
|    std                  | 235         |
|    value_loss           | 0.402       |
-----------------------------------------
Iteration: 5422 | Episodes: 219900 | Median Reward: 22.62 | Max Reward: 48.88
Iteration: 5424 | Episodes: 220000 | Median Reward: 24.35 | Max Reward: 48.88
Iteration: 5427 | Episodes: 220100 | Median Reward: 27.45 | Max Reward: 48.88
Iteration: 5429 | Episodes: 220200 | Median Reward: 28.52 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.6         |
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 5430          |
|    time_elapsed         | 42296         |
|    total_timesteps      | 22241280      |
| train/                  |               |
|    approx_kl            | 6.5883854e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -205          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.1         |
|    n_updates            | 54290         |
|    policy_gradient_loss | -0.000687     |
|    std                  | 237           |
|    value_loss           | 0.463         |
-------------------------------------------
Iteration: 5432 | Episodes: 220300 | Median Reward: 25.38 | Max Reward: 48.88
Iteration: 5434 | Episodes: 220400 | Median Reward: 23.52 | Max Reward: 48.88
Iteration: 5437 | Episodes: 220500 | Median Reward: 23.52 | Max Reward: 48.88
Iteration: 5439 | Episodes: 220600 | Median Reward: 24.76 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78           |
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 5440          |
|    time_elapsed         | 42378         |
|    total_timesteps      | 22282240      |
| train/                  |               |
|    approx_kl            | 0.00013606591 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -205          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.72         |
|    n_updates            | 54390         |
|    policy_gradient_loss | -0.000581     |
|    std                  | 242           |
|    value_loss           | 1.33          |
-------------------------------------------
Iteration: 5442 | Episodes: 220700 | Median Reward: 23.99 | Max Reward: 48.88
Iteration: 5444 | Episodes: 220800 | Median Reward: 21.06 | Max Reward: 48.88
Iteration: 5446 | Episodes: 220900 | Median Reward: 22.82 | Max Reward: 48.88
Iteration: 5449 | Episodes: 221000 | Median Reward: 26.39 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.1        |
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 5450         |
|    time_elapsed         | 42455        |
|    total_timesteps      | 22323200     |
| train/                  |              |
|    approx_kl            | 7.802865e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -206         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.1        |
|    n_updates            | 54490        |
|    policy_gradient_loss | -0.000566    |
|    std                  | 243          |
|    value_loss           | 0.542        |
------------------------------------------
Iteration: 5451 | Episodes: 221100 | Median Reward: 27.63 | Max Reward: 48.88
Iteration: 5454 | Episodes: 221200 | Median Reward: 25.99 | Max Reward: 48.88
Iteration: 5456 | Episodes: 221300 | Median Reward: 15.69 | Max Reward: 48.88
Iteration: 5459 | Episodes: 221400 | Median Reward: 22.67 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.4        |
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 5460         |
|    time_elapsed         | 42544        |
|    total_timesteps      | 22364160     |
| train/                  |              |
|    approx_kl            | 0.0001494624 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -206         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.1        |
|    n_updates            | 54590        |
|    policy_gradient_loss | -0.000943    |
|    std                  | 245          |
|    value_loss           | 0.545        |
------------------------------------------
Iteration: 5461 | Episodes: 221500 | Median Reward: 21.88 | Max Reward: 48.88
Iteration: 5464 | Episodes: 221600 | Median Reward: 24.19 | Max Reward: 48.88
Iteration: 5466 | Episodes: 221700 | Median Reward: 20.27 | Max Reward: 48.88
Iteration: 5469 | Episodes: 221800 | Median Reward: 18.13 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.5        |
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 5470         |
|    time_elapsed         | 42622        |
|    total_timesteps      | 22405120     |
| train/                  |              |
|    approx_kl            | 6.267932e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -206         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.91        |
|    n_updates            | 54690        |
|    policy_gradient_loss | -0.000414    |
|    std                  | 247          |
|    value_loss           | 3.3          |
------------------------------------------
Iteration: 5471 | Episodes: 221900 | Median Reward: 29.37 | Max Reward: 48.88
Iteration: 5474 | Episodes: 222000 | Median Reward: 22.99 | Max Reward: 48.88
Iteration: 5476 | Episodes: 222100 | Median Reward: 26.86 | Max Reward: 48.88
Iteration: 5479 | Episodes: 222200 | Median Reward: 27.12 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.6         |
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 5480          |
|    time_elapsed         | 42704         |
|    total_timesteps      | 22446080      |
| train/                  |               |
|    approx_kl            | 3.7901787e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -206          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.89         |
|    n_updates            | 54790         |
|    policy_gradient_loss | -8.77e-05     |
|    std                  | 249           |
|    value_loss           | 1.04          |
-------------------------------------------
Iteration: 5481 | Episodes: 222300 | Median Reward: 22.14 | Max Reward: 48.88
Iteration: 5483 | Episodes: 222400 | Median Reward: 26.16 | Max Reward: 48.88
Iteration: 5486 | Episodes: 222500 | Median Reward: 24.04 | Max Reward: 48.88
Iteration: 5488 | Episodes: 222600 | Median Reward: 25.85 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.8         |
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 5490          |
|    time_elapsed         | 42792         |
|    total_timesteps      | 22487040      |
| train/                  |               |
|    approx_kl            | 0.00024165679 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -206          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.97         |
|    n_updates            | 54890         |
|    policy_gradient_loss | -0.00202      |
|    std                  | 252           |
|    value_loss           | 0.868         |
-------------------------------------------
Iteration: 5491 | Episodes: 222700 | Median Reward: 22.72 | Max Reward: 48.88
Iteration: 5493 | Episodes: 222800 | Median Reward: 21.20 | Max Reward: 48.88
Iteration: 5496 | Episodes: 222900 | Median Reward: 26.51 | Max Reward: 48.88
Iteration: 5498 | Episodes: 223000 | Median Reward: 25.35 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.5        |
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 5500         |
|    time_elapsed         | 42870        |
|    total_timesteps      | 22528000     |
| train/                  |              |
|    approx_kl            | 0.0008764418 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -207         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.1        |
|    n_updates            | 54990        |
|    policy_gradient_loss | -0.00324     |
|    std                  | 255          |
|    value_loss           | 0.685        |
------------------------------------------
Iteration: 5501 | Episodes: 223100 | Median Reward: 19.20 | Max Reward: 48.88
Iteration: 5503 | Episodes: 223200 | Median Reward: 27.49 | Max Reward: 48.88
Iteration: 5506 | Episodes: 223300 | Median Reward: 26.85 | Max Reward: 48.88
Iteration: 5508 | Episodes: 223400 | Median Reward: 15.01 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.7        |
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 5510         |
|    time_elapsed         | 42958        |
|    total_timesteps      | 22568960     |
| train/                  |              |
|    approx_kl            | 0.0003956073 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -207         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -10          |
|    n_updates            | 55090        |
|    policy_gradient_loss | -0.00078     |
|    std                  | 258          |
|    value_loss           | 1.16         |
------------------------------------------
Iteration: 5511 | Episodes: 223500 | Median Reward: 17.58 | Max Reward: 48.88
Iteration: 5513 | Episodes: 223600 | Median Reward: 25.26 | Max Reward: 48.88
Iteration: 5516 | Episodes: 223700 | Median Reward: 21.15 | Max Reward: 48.88
Iteration: 5518 | Episodes: 223800 | Median Reward: 20.75 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.7         |
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 5520          |
|    time_elapsed         | 43037         |
|    total_timesteps      | 22609920      |
| train/                  |               |
|    approx_kl            | 4.8855727e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -207          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.1         |
|    n_updates            | 55190         |
|    policy_gradient_loss | -0.000726     |
|    std                  | 260           |
|    value_loss           | 0.853         |
-------------------------------------------
Iteration: 5520 | Episodes: 223900 | Median Reward: 28.69 | Max Reward: 48.88
Iteration: 5523 | Episodes: 224000 | Median Reward: 23.95 | Max Reward: 48.88
Iteration: 5525 | Episodes: 224100 | Median Reward: 24.64 | Max Reward: 48.88
Iteration: 5528 | Episodes: 224200 | Median Reward: 27.39 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.3        |
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 5530         |
|    time_elapsed         | 43115        |
|    total_timesteps      | 22650880     |
| train/                  |              |
|    approx_kl            | 0.0005151232 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -207         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.2        |
|    n_updates            | 55290        |
|    policy_gradient_loss | -0.00225     |
|    std                  | 262          |
|    value_loss           | 0.382        |
------------------------------------------
Iteration: 5530 | Episodes: 224300 | Median Reward: 16.30 | Max Reward: 48.88
Iteration: 5533 | Episodes: 224400 | Median Reward: 23.84 | Max Reward: 48.88
Iteration: 5535 | Episodes: 224500 | Median Reward: 19.35 | Max Reward: 48.88
Iteration: 5538 | Episodes: 224600 | Median Reward: 26.24 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -81.9        |
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 5540         |
|    time_elapsed         | 43206        |
|    total_timesteps      | 22691840     |
| train/                  |              |
|    approx_kl            | 0.0004218762 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -207         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.83        |
|    n_updates            | 55390        |
|    policy_gradient_loss | 0.00016      |
|    std                  | 265          |
|    value_loss           | 1.54         |
------------------------------------------
Iteration: 5540 | Episodes: 224700 | Median Reward: 15.67 | Max Reward: 48.88
Iteration: 5543 | Episodes: 224800 | Median Reward: 26.87 | Max Reward: 48.88
Iteration: 5545 | Episodes: 224900 | Median Reward: 22.53 | Max Reward: 48.88
Iteration: 5548 | Episodes: 225000 | Median Reward: 24.52 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.2         |
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 5550          |
|    time_elapsed         | 43285         |
|    total_timesteps      | 22732800      |
| train/                  |               |
|    approx_kl            | 2.3733199e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -208          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.2         |
|    n_updates            | 55490         |
|    policy_gradient_loss | -0.000208     |
|    std                  | 268           |
|    value_loss           | 0.389         |
-------------------------------------------
Iteration: 5550 | Episodes: 225100 | Median Reward: 30.59 | Max Reward: 48.88
Iteration: 5553 | Episodes: 225200 | Median Reward: 23.50 | Max Reward: 48.88
Iteration: 5555 | Episodes: 225300 | Median Reward: 21.21 | Max Reward: 48.88
Iteration: 5557 | Episodes: 225400 | Median Reward: 17.31 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.9         |
| time/                   |               |
|    fps                  | 525           |
|    iterations           | 5560          |
|    time_elapsed         | 43366         |
|    total_timesteps      | 22773760      |
| train/                  |               |
|    approx_kl            | 0.00014633659 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -208          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -10           |
|    n_updates            | 55590         |
|    policy_gradient_loss | -0.00114      |
|    std                  | 271           |
|    value_loss           | 1.32          |
-------------------------------------------
Iteration: 5560 | Episodes: 225500 | Median Reward: 20.78 | Max Reward: 48.88
Iteration: 5562 | Episodes: 225600 | Median Reward: 23.22 | Max Reward: 48.88
Iteration: 5565 | Episodes: 225700 | Median Reward: 28.99 | Max Reward: 48.88
Iteration: 5567 | Episodes: 225800 | Median Reward: 21.34 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.8        |
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 5570         |
|    time_elapsed         | 43453        |
|    total_timesteps      | 22814720     |
| train/                  |              |
|    approx_kl            | 0.0023255642 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -208         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.74        |
|    n_updates            | 55690        |
|    policy_gradient_loss | -0.00711     |
|    std                  | 273          |
|    value_loss           | 1.49         |
------------------------------------------
Iteration: 5570 | Episodes: 225900 | Median Reward: 21.28 | Max Reward: 48.88
Iteration: 5572 | Episodes: 226000 | Median Reward: 22.90 | Max Reward: 48.88
Iteration: 5575 | Episodes: 226100 | Median Reward: 19.91 | Max Reward: 48.88
Iteration: 5577 | Episodes: 226200 | Median Reward: 25.35 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.7        |
| time/                   |              |
|    fps                  | 525          |
|    iterations           | 5580         |
|    time_elapsed         | 43532        |
|    total_timesteps      | 22855680     |
| train/                  |              |
|    approx_kl            | 8.877304e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -208         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.1        |
|    n_updates            | 55790        |
|    policy_gradient_loss | -0.00052     |
|    std                  | 274          |
|    value_loss           | 0.781        |
------------------------------------------
Iteration: 5580 | Episodes: 226300 | Median Reward: 24.54 | Max Reward: 48.88
Iteration: 5582 | Episodes: 226400 | Median Reward: 20.61 | Max Reward: 48.88
Iteration: 5585 | Episodes: 226500 | Median Reward: 23.90 | Max Reward: 48.88
Iteration: 5587 | Episodes: 226600 | Median Reward: 24.04 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.8         |
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 5590          |
|    time_elapsed         | 43620         |
|    total_timesteps      | 22896640      |
| train/                  |               |
|    approx_kl            | 0.00045169872 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -209          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.87         |
|    n_updates            | 55890         |
|    policy_gradient_loss | -0.00303      |
|    std                  | 277           |
|    value_loss           | 1.57          |
-------------------------------------------
Iteration: 5590 | Episodes: 226700 | Median Reward: 18.87 | Max Reward: 48.88
Iteration: 5592 | Episodes: 226800 | Median Reward: 24.21 | Max Reward: 48.88
Iteration: 5594 | Episodes: 226900 | Median Reward: 19.09 | Max Reward: 48.88
Iteration: 5597 | Episodes: 227000 | Median Reward: 20.82 | Max Reward: 48.88
Iteration: 5599 | Episodes: 227100 | Median Reward: 27.19 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.5        |
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 5600         |
|    time_elapsed         | 43700        |
|    total_timesteps      | 22937600     |
| train/                  |              |
|    approx_kl            | 0.0008697249 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -209         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.1        |
|    n_updates            | 55990        |
|    policy_gradient_loss | -0.00355     |
|    std                  | 278          |
|    value_loss           | 1.08         |
------------------------------------------
Iteration: 5602 | Episodes: 227200 | Median Reward: 18.53 | Max Reward: 48.88
Iteration: 5604 | Episodes: 227300 | Median Reward: 24.06 | Max Reward: 48.88
Iteration: 5607 | Episodes: 227400 | Median Reward: 25.38 | Max Reward: 48.88
Iteration: 5609 | Episodes: 227500 | Median Reward: 23.07 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.1         |
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 5610          |
|    time_elapsed         | 43778         |
|    total_timesteps      | 22978560      |
| train/                  |               |
|    approx_kl            | 0.00059399346 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -209          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.1         |
|    n_updates            | 56090         |
|    policy_gradient_loss | -0.00359      |
|    std                  | 280           |
|    value_loss           | 0.804         |
-------------------------------------------
Iteration: 5612 | Episodes: 227600 | Median Reward: 22.08 | Max Reward: 48.88
Iteration: 5614 | Episodes: 227700 | Median Reward: 22.26 | Max Reward: 48.88
Iteration: 5617 | Episodes: 227800 | Median Reward: 18.90 | Max Reward: 48.88
Iteration: 5619 | Episodes: 227900 | Median Reward: 19.13 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.4         |
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 5620          |
|    time_elapsed         | 43868         |
|    total_timesteps      | 23019520      |
| train/                  |               |
|    approx_kl            | 0.00023146451 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -209          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.59         |
|    n_updates            | 56190         |
|    policy_gradient_loss | -0.00177      |
|    std                  | 282           |
|    value_loss           | 2.11          |
-------------------------------------------
Iteration: 5622 | Episodes: 228000 | Median Reward: 25.63 | Max Reward: 48.88
Iteration: 5624 | Episodes: 228100 | Median Reward: 22.00 | Max Reward: 48.88
Iteration: 5627 | Episodes: 228200 | Median Reward: 14.88 | Max Reward: 48.88
Iteration: 5629 | Episodes: 228300 | Median Reward: 22.63 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80          |
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 5630         |
|    time_elapsed         | 43945        |
|    total_timesteps      | 23060480     |
| train/                  |              |
|    approx_kl            | 6.068588e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -209         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.2        |
|    n_updates            | 56290        |
|    policy_gradient_loss | -0.000498    |
|    std                  | 285          |
|    value_loss           | 0.627        |
------------------------------------------
Iteration: 5631 | Episodes: 228400 | Median Reward: 19.92 | Max Reward: 48.88
Iteration: 5634 | Episodes: 228500 | Median Reward: 27.67 | Max Reward: 48.88
Iteration: 5636 | Episodes: 228600 | Median Reward: 24.44 | Max Reward: 48.88
Iteration: 5639 | Episodes: 228700 | Median Reward: 23.72 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.3        |
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 5640         |
|    time_elapsed         | 44028        |
|    total_timesteps      | 23101440     |
| train/                  |              |
|    approx_kl            | 3.431781e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -210         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.3        |
|    n_updates            | 56390        |
|    policy_gradient_loss | -0.00101     |
|    std                  | 288          |
|    value_loss           | 0.85         |
------------------------------------------
Iteration: 5641 | Episodes: 228800 | Median Reward: 30.76 | Max Reward: 48.88
Iteration: 5644 | Episodes: 228900 | Median Reward: 22.13 | Max Reward: 48.88
Iteration: 5646 | Episodes: 229000 | Median Reward: 19.25 | Max Reward: 48.88
Iteration: 5649 | Episodes: 229100 | Median Reward: 23.60 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.4        |
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 5650         |
|    time_elapsed         | 44111        |
|    total_timesteps      | 23142400     |
| train/                  |              |
|    approx_kl            | 0.0001218885 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -210         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.3        |
|    n_updates            | 56490        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 290          |
|    value_loss           | 0.423        |
------------------------------------------
Iteration: 5651 | Episodes: 229200 | Median Reward: 20.37 | Max Reward: 48.88
Iteration: 5654 | Episodes: 229300 | Median Reward: 20.83 | Max Reward: 48.88
Iteration: 5656 | Episodes: 229400 | Median Reward: 22.91 | Max Reward: 48.88
Iteration: 5659 | Episodes: 229500 | Median Reward: 23.21 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.2         |
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 5660          |
|    time_elapsed         | 44189         |
|    total_timesteps      | 23183360      |
| train/                  |               |
|    approx_kl            | 0.00023652642 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -210          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.2         |
|    n_updates            | 56590         |
|    policy_gradient_loss | -0.00144      |
|    std                  | 294           |
|    value_loss           | 0.728         |
-------------------------------------------
Iteration: 5661 | Episodes: 229600 | Median Reward: 22.92 | Max Reward: 48.88
Iteration: 5663 | Episodes: 229700 | Median Reward: 25.83 | Max Reward: 48.88
Iteration: 5666 | Episodes: 229800 | Median Reward: 23.60 | Max Reward: 48.88
Iteration: 5668 | Episodes: 229900 | Median Reward: 26.27 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.9        |
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 5670         |
|    time_elapsed         | 44278        |
|    total_timesteps      | 23224320     |
| train/                  |              |
|    approx_kl            | 0.0008546491 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -210         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.2        |
|    n_updates            | 56690        |
|    policy_gradient_loss | -0.00419     |
|    std                  | 299          |
|    value_loss           | 1.01         |
------------------------------------------
Iteration: 5671 | Episodes: 230000 | Median Reward: 23.53 | Max Reward: 48.88
Iteration: 5673 | Episodes: 230100 | Median Reward: 24.54 | Max Reward: 48.88
Iteration: 5676 | Episodes: 230200 | Median Reward: 24.98 | Max Reward: 48.88
Iteration: 5678 | Episodes: 230300 | Median Reward: 29.60 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.9         |
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 5680          |
|    time_elapsed         | 44356         |
|    total_timesteps      | 23265280      |
| train/                  |               |
|    approx_kl            | 0.00044886014 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -211          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.3         |
|    n_updates            | 56790         |
|    policy_gradient_loss | -0.00293      |
|    std                  | 301           |
|    value_loss           | 0.727         |
-------------------------------------------
Iteration: 5681 | Episodes: 230400 | Median Reward: 22.13 | Max Reward: 48.88
Iteration: 5683 | Episodes: 230500 | Median Reward: 20.33 | Max Reward: 48.88
Iteration: 5686 | Episodes: 230600 | Median Reward: 20.28 | Max Reward: 48.88
Iteration: 5688 | Episodes: 230700 | Median Reward: 21.39 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.8        |
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 5690         |
|    time_elapsed         | 44435        |
|    total_timesteps      | 23306240     |
| train/                  |              |
|    approx_kl            | 0.0006422879 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -211         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.2        |
|    n_updates            | 56890        |
|    policy_gradient_loss | -0.00357     |
|    std                  | 304          |
|    value_loss           | 1.07         |
------------------------------------------
Iteration: 5691 | Episodes: 230800 | Median Reward: 20.16 | Max Reward: 48.88
Iteration: 5693 | Episodes: 230900 | Median Reward: 22.46 | Max Reward: 48.88
Iteration: 5696 | Episodes: 231000 | Median Reward: 18.29 | Max Reward: 48.88
Iteration: 5698 | Episodes: 231100 | Median Reward: 17.39 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84.8        |
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 5700         |
|    time_elapsed         | 44525        |
|    total_timesteps      | 23347200     |
| train/                  |              |
|    approx_kl            | 6.832616e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -211         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.41        |
|    n_updates            | 56990        |
|    policy_gradient_loss | -0.000763    |
|    std                  | 306          |
|    value_loss           | 2.73         |
------------------------------------------
Iteration: 5700 | Episodes: 231200 | Median Reward: 16.85 | Max Reward: 48.88
Iteration: 5703 | Episodes: 231300 | Median Reward: 22.42 | Max Reward: 48.88
Iteration: 5705 | Episodes: 231400 | Median Reward: 28.96 | Max Reward: 48.88
Iteration: 5708 | Episodes: 231500 | Median Reward: 20.04 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -91            |
| time/                   |                |
|    fps                  | 524            |
|    iterations           | 5710           |
|    time_elapsed         | 44604          |
|    total_timesteps      | 23388160       |
| train/                  |                |
|    approx_kl            | 1.00287725e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -211           |
|    explained_variance   | 0.993          |
|    learning_rate        | 0.0005         |
|    loss                 | -9.46          |
|    n_updates            | 57090          |
|    policy_gradient_loss | 0.000127       |
|    std                  | 309            |
|    value_loss           | 2.47           |
--------------------------------------------
Iteration: 5710 | Episodes: 231600 | Median Reward: 15.07 | Max Reward: 48.88
Iteration: 5713 | Episodes: 231700 | Median Reward: 16.14 | Max Reward: 48.88
Iteration: 5715 | Episodes: 231800 | Median Reward: 17.87 | Max Reward: 48.88
Iteration: 5718 | Episodes: 231900 | Median Reward: 19.46 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.8         |
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 5720          |
|    time_elapsed         | 44688         |
|    total_timesteps      | 23429120      |
| train/                  |               |
|    approx_kl            | 0.00022542424 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -211          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.89         |
|    n_updates            | 57190         |
|    policy_gradient_loss | -0.00181      |
|    std                  | 310           |
|    value_loss           | 1.56          |
-------------------------------------------
Iteration: 5720 | Episodes: 232000 | Median Reward: 21.63 | Max Reward: 48.88
Iteration: 5723 | Episodes: 232100 | Median Reward: 21.36 | Max Reward: 48.88
Iteration: 5725 | Episodes: 232200 | Median Reward: 21.90 | Max Reward: 48.88
Iteration: 5728 | Episodes: 232300 | Median Reward: 13.37 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80          |
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 5730         |
|    time_elapsed         | 44770        |
|    total_timesteps      | 23470080     |
| train/                  |              |
|    approx_kl            | 8.579006e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -211         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.4        |
|    n_updates            | 57290        |
|    policy_gradient_loss | -0.000861    |
|    std                  | 312          |
|    value_loss           | 0.528        |
------------------------------------------
Iteration: 5730 | Episodes: 232400 | Median Reward: 22.59 | Max Reward: 48.88
Iteration: 5733 | Episodes: 232500 | Median Reward: 26.04 | Max Reward: 48.88
Iteration: 5735 | Episodes: 232600 | Median Reward: 24.00 | Max Reward: 48.88
Iteration: 5737 | Episodes: 232700 | Median Reward: 17.81 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.7        |
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 5740         |
|    time_elapsed         | 44846        |
|    total_timesteps      | 23511040     |
| train/                  |              |
|    approx_kl            | 0.0010790897 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -212         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.3        |
|    n_updates            | 57390        |
|    policy_gradient_loss | -0.00562     |
|    std                  | 316          |
|    value_loss           | 0.791        |
------------------------------------------
Iteration: 5740 | Episodes: 232800 | Median Reward: 26.15 | Max Reward: 48.88
Iteration: 5742 | Episodes: 232900 | Median Reward: 31.87 | Max Reward: 48.88
Iteration: 5745 | Episodes: 233000 | Median Reward: 21.17 | Max Reward: 48.88
Iteration: 5747 | Episodes: 233100 | Median Reward: 26.59 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.1       |
| time/                   |             |
|    fps                  | 524         |
|    iterations           | 5750        |
|    time_elapsed         | 44936       |
|    total_timesteps      | 23552000    |
| train/                  |             |
|    approx_kl            | 0.004679816 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -212        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.9        |
|    n_updates            | 57490       |
|    policy_gradient_loss | -0.0126     |
|    std                  | 322         |
|    value_loss           | 1.85        |
-----------------------------------------
Iteration: 5750 | Episodes: 233200 | Median Reward: 20.08 | Max Reward: 48.88
Iteration: 5752 | Episodes: 233300 | Median Reward: 20.84 | Max Reward: 48.88
Iteration: 5755 | Episodes: 233400 | Median Reward: 16.71 | Max Reward: 48.88
Iteration: 5757 | Episodes: 233500 | Median Reward: 26.89 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.9         |
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 5760          |
|    time_elapsed         | 45014         |
|    total_timesteps      | 23592960      |
| train/                  |               |
|    approx_kl            | 0.00033529394 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -212          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.5         |
|    n_updates            | 57590         |
|    policy_gradient_loss | -0.00237      |
|    std                  | 323           |
|    value_loss           | 0.359         |
-------------------------------------------
Iteration: 5760 | Episodes: 233600 | Median Reward: 19.96 | Max Reward: 48.88
Iteration: 5762 | Episodes: 233700 | Median Reward: 23.64 | Max Reward: 48.88
Iteration: 5765 | Episodes: 233800 | Median Reward: 25.28 | Max Reward: 48.88
Iteration: 5767 | Episodes: 233900 | Median Reward: 25.11 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.1         |
| time/                   |               |
|    fps                  | 524           |
|    iterations           | 5770          |
|    time_elapsed         | 45095         |
|    total_timesteps      | 23633920      |
| train/                  |               |
|    approx_kl            | 0.00017044067 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -212          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.4         |
|    n_updates            | 57690         |
|    policy_gradient_loss | -0.00087      |
|    std                  | 326           |
|    value_loss           | 0.534         |
-------------------------------------------
Iteration: 5770 | Episodes: 234000 | Median Reward: 26.53 | Max Reward: 48.88
Iteration: 5772 | Episodes: 234100 | Median Reward: 26.26 | Max Reward: 48.88
Iteration: 5774 | Episodes: 234200 | Median Reward: 20.14 | Max Reward: 48.88
Iteration: 5777 | Episodes: 234300 | Median Reward: 20.01 | Max Reward: 48.88
Iteration: 5779 | Episodes: 234400 | Median Reward: 15.08 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.6        |
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 5780         |
|    time_elapsed         | 45180        |
|    total_timesteps      | 23674880     |
| train/                  |              |
|    approx_kl            | 0.0006457317 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -213         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.3        |
|    n_updates            | 57790        |
|    policy_gradient_loss | -0.0028      |
|    std                  | 329          |
|    value_loss           | 0.764        |
------------------------------------------
Iteration: 5782 | Episodes: 234500 | Median Reward: 20.40 | Max Reward: 48.88
Iteration: 5784 | Episodes: 234600 | Median Reward: 21.51 | Max Reward: 48.88
Iteration: 5787 | Episodes: 234700 | Median Reward: 24.53 | Max Reward: 48.88
Iteration: 5789 | Episodes: 234800 | Median Reward: 17.28 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84.8        |
| time/                   |              |
|    fps                  | 524          |
|    iterations           | 5790         |
|    time_elapsed         | 45257        |
|    total_timesteps      | 23715840     |
| train/                  |              |
|    approx_kl            | 0.0013150321 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -213         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.95        |
|    n_updates            | 57890        |
|    policy_gradient_loss | -0.00685     |
|    std                  | 336          |
|    value_loss           | 1.63         |
------------------------------------------
Iteration: 5792 | Episodes: 234900 | Median Reward: 19.10 | Max Reward: 48.88
Iteration: 5794 | Episodes: 235000 | Median Reward: 15.18 | Max Reward: 48.88
Iteration: 5797 | Episodes: 235100 | Median Reward: 21.83 | Max Reward: 48.88
Iteration: 5799 | Episodes: 235200 | Median Reward: 15.58 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.8         |
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 5800          |
|    time_elapsed         | 45347         |
|    total_timesteps      | 23756800      |
| train/                  |               |
|    approx_kl            | 0.00028659648 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -213          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.1         |
|    n_updates            | 57990         |
|    policy_gradient_loss | -0.00334      |
|    std                  | 340           |
|    value_loss           | 1.63          |
-------------------------------------------
Iteration: 5802 | Episodes: 235300 | Median Reward: 19.56 | Max Reward: 48.88
Iteration: 5804 | Episodes: 235400 | Median Reward: 25.56 | Max Reward: 48.88
Iteration: 5807 | Episodes: 235500 | Median Reward: 17.03 | Max Reward: 48.88
Iteration: 5809 | Episodes: 235600 | Median Reward: 23.42 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.6        |
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 5810         |
|    time_elapsed         | 45426        |
|    total_timesteps      | 23797760     |
| train/                  |              |
|    approx_kl            | 0.0003640633 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -213         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.3        |
|    n_updates            | 58090        |
|    policy_gradient_loss | -0.00237     |
|    std                  | 344          |
|    value_loss           | 0.966        |
------------------------------------------
Iteration: 5811 | Episodes: 235700 | Median Reward: 21.82 | Max Reward: 48.88
Iteration: 5814 | Episodes: 235800 | Median Reward: 24.67 | Max Reward: 48.88
Iteration: 5816 | Episodes: 235900 | Median Reward: 27.60 | Max Reward: 48.88
Iteration: 5819 | Episodes: 236000 | Median Reward: 15.06 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.4         |
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 5820          |
|    time_elapsed         | 45504         |
|    total_timesteps      | 23838720      |
| train/                  |               |
|    approx_kl            | 0.00014549441 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -214          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -10           |
|    n_updates            | 58190         |
|    policy_gradient_loss | -0.00167      |
|    std                  | 346           |
|    value_loss           | 1.79          |
-------------------------------------------
Iteration: 5821 | Episodes: 236100 | Median Reward: 15.02 | Max Reward: 48.88
Iteration: 5824 | Episodes: 236200 | Median Reward: 18.28 | Max Reward: 48.88
Iteration: 5826 | Episodes: 236300 | Median Reward: 13.78 | Max Reward: 48.88
Iteration: 5829 | Episodes: 236400 | Median Reward: 20.66 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.9         |
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 5830          |
|    time_elapsed         | 45592         |
|    total_timesteps      | 23879680      |
| train/                  |               |
|    approx_kl            | 5.0891686e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -214          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.3         |
|    n_updates            | 58290         |
|    policy_gradient_loss | -0.000381     |
|    std                  | 350           |
|    value_loss           | 0.919         |
-------------------------------------------
Iteration: 5831 | Episodes: 236500 | Median Reward: 26.55 | Max Reward: 48.88
Iteration: 5834 | Episodes: 236600 | Median Reward: 24.88 | Max Reward: 48.88
Iteration: 5836 | Episodes: 236700 | Median Reward: 24.56 | Max Reward: 48.88
Iteration: 5839 | Episodes: 236800 | Median Reward: 23.95 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.4         |
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 5840          |
|    time_elapsed         | 45669         |
|    total_timesteps      | 23920640      |
| train/                  |               |
|    approx_kl            | 0.00034098176 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -214          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.5         |
|    n_updates            | 58390         |
|    policy_gradient_loss | -0.00161      |
|    std                  | 353           |
|    value_loss           | 0.841         |
-------------------------------------------
Iteration: 5841 | Episodes: 236900 | Median Reward: 23.81 | Max Reward: 48.88
Iteration: 5843 | Episodes: 237000 | Median Reward: 19.99 | Max Reward: 48.88
Iteration: 5846 | Episodes: 237100 | Median Reward: 21.39 | Max Reward: 48.88
Iteration: 5848 | Episodes: 237200 | Median Reward: 25.73 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -75.6       |
| time/                   |             |
|    fps                  | 523         |
|    iterations           | 5850        |
|    time_elapsed         | 45755       |
|    total_timesteps      | 23961600    |
| train/                  |             |
|    approx_kl            | 0.001185054 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -214        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.6       |
|    n_updates            | 58490       |
|    policy_gradient_loss | -0.00418    |
|    std                  | 358         |
|    value_loss           | 0.315       |
-----------------------------------------
Iteration: 5851 | Episodes: 237300 | Median Reward: 24.45 | Max Reward: 48.88
Iteration: 5853 | Episodes: 237400 | Median Reward: 22.77 | Max Reward: 48.88
Iteration: 5856 | Episodes: 237500 | Median Reward: 20.74 | Max Reward: 48.88
Iteration: 5858 | Episodes: 237600 | Median Reward: 20.92 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.9        |
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 5860         |
|    time_elapsed         | 45837        |
|    total_timesteps      | 24002560     |
| train/                  |              |
|    approx_kl            | 0.0027920757 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -215         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.6        |
|    n_updates            | 58590        |
|    policy_gradient_loss | -0.00925     |
|    std                  | 359          |
|    value_loss           | 0.428        |
------------------------------------------
Iteration: 5861 | Episodes: 237700 | Median Reward: 28.35 | Max Reward: 48.88
Iteration: 5863 | Episodes: 237800 | Median Reward: 28.45 | Max Reward: 48.88
Iteration: 5866 | Episodes: 237900 | Median Reward: 18.41 | Max Reward: 48.88
Iteration: 5868 | Episodes: 238000 | Median Reward: 24.53 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.1        |
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 5870         |
|    time_elapsed         | 45913        |
|    total_timesteps      | 24043520     |
| train/                  |              |
|    approx_kl            | 0.0001624426 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -215         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.3        |
|    n_updates            | 58690        |
|    policy_gradient_loss | -0.00232     |
|    std                  | 363          |
|    value_loss           | 1.36         |
------------------------------------------
Iteration: 5871 | Episodes: 238100 | Median Reward: 21.44 | Max Reward: 48.88
Iteration: 5873 | Episodes: 238200 | Median Reward: 15.81 | Max Reward: 48.88
Iteration: 5876 | Episodes: 238300 | Median Reward: 24.94 | Max Reward: 48.88
Iteration: 5878 | Episodes: 238400 | Median Reward: 15.84 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.2        |
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 5880         |
|    time_elapsed         | 46003        |
|    total_timesteps      | 24084480     |
| train/                  |              |
|    approx_kl            | 0.0009129568 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -215         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.4        |
|    n_updates            | 58790        |
|    policy_gradient_loss | -0.00423     |
|    std                  | 367          |
|    value_loss           | 0.804        |
------------------------------------------
Iteration: 5880 | Episodes: 238500 | Median Reward: 18.60 | Max Reward: 48.88
Iteration: 5883 | Episodes: 238600 | Median Reward: 17.76 | Max Reward: 48.88
Iteration: 5885 | Episodes: 238700 | Median Reward: 22.13 | Max Reward: 48.88
Iteration: 5888 | Episodes: 238800 | Median Reward: 27.40 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.6         |
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 5890          |
|    time_elapsed         | 46080         |
|    total_timesteps      | 24125440      |
| train/                  |               |
|    approx_kl            | 0.00013200441 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -215          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.6         |
|    n_updates            | 58890         |
|    policy_gradient_loss | 0.00146       |
|    std                  | 370           |
|    value_loss           | 0.469         |
-------------------------------------------
Iteration: 5890 | Episodes: 238900 | Median Reward: 27.36 | Max Reward: 48.88
Iteration: 5893 | Episodes: 239000 | Median Reward: 19.03 | Max Reward: 48.88
Iteration: 5895 | Episodes: 239100 | Median Reward: 16.31 | Max Reward: 48.88
Iteration: 5898 | Episodes: 239200 | Median Reward: 17.65 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.7         |
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 5900          |
|    time_elapsed         | 46164         |
|    total_timesteps      | 24166400      |
| train/                  |               |
|    approx_kl            | 0.00019217067 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -216          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.5         |
|    n_updates            | 58990         |
|    policy_gradient_loss | -0.00159      |
|    std                  | 379           |
|    value_loss           | 0.771         |
-------------------------------------------
Iteration: 5900 | Episodes: 239300 | Median Reward: 23.43 | Max Reward: 48.88
Iteration: 5903 | Episodes: 239400 | Median Reward: 22.64 | Max Reward: 48.88
Iteration: 5905 | Episodes: 239500 | Median Reward: 25.71 | Max Reward: 48.88
Iteration: 5908 | Episodes: 239600 | Median Reward: 20.26 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -79.1         |
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 5910          |
|    time_elapsed         | 46248         |
|    total_timesteps      | 24207360      |
| train/                  |               |
|    approx_kl            | 0.00015325935 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -216          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.6          |
|    n_updates            | 59090         |
|    policy_gradient_loss | -0.00054      |
|    std                  | 383           |
|    value_loss           | 2.66          |
-------------------------------------------
Iteration: 5910 | Episodes: 239700 | Median Reward: 19.15 | Max Reward: 48.88
Iteration: 5913 | Episodes: 239800 | Median Reward: 24.48 | Max Reward: 48.88
Iteration: 5915 | Episodes: 239900 | Median Reward: 17.06 | Max Reward: 48.88
Iteration: 5917 | Episodes: 240000 | Median Reward: 28.30 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.4       |
| time/                   |             |
|    fps                  | 523         |
|    iterations           | 5920        |
|    time_elapsed         | 46327       |
|    total_timesteps      | 24248320    |
| train/                  |             |
|    approx_kl            | 0.002154571 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -216        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.5       |
|    n_updates            | 59190       |
|    policy_gradient_loss | -0.00827    |
|    std                  | 388         |
|    value_loss           | 1.09        |
-----------------------------------------
Iteration: 5920 | Episodes: 240100 | Median Reward: 17.78 | Max Reward: 48.88
Iteration: 5922 | Episodes: 240200 | Median Reward: 19.42 | Max Reward: 48.88
Iteration: 5925 | Episodes: 240300 | Median Reward: 17.72 | Max Reward: 48.88
Iteration: 5927 | Episodes: 240400 | Median Reward: 22.13 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -81.4        |
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 5930         |
|    time_elapsed         | 46425        |
|    total_timesteps      | 24289280     |
| train/                  |              |
|    approx_kl            | 8.549515e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -216         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.2        |
|    n_updates            | 59290        |
|    policy_gradient_loss | -0.000375    |
|    std                  | 395          |
|    value_loss           | 1.48         |
------------------------------------------
Iteration: 5930 | Episodes: 240500 | Median Reward: 15.36 | Max Reward: 48.88
Iteration: 5932 | Episodes: 240600 | Median Reward: 19.19 | Max Reward: 48.88
Iteration: 5935 | Episodes: 240700 | Median Reward: 23.02 | Max Reward: 48.88
Iteration: 5937 | Episodes: 240800 | Median Reward: 19.69 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78          |
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 5940         |
|    time_elapsed         | 46502        |
|    total_timesteps      | 24330240     |
| train/                  |              |
|    approx_kl            | 7.015548e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -217         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.5        |
|    n_updates            | 59390        |
|    policy_gradient_loss | 0.0002       |
|    std                  | 399          |
|    value_loss           | 0.786        |
------------------------------------------
Iteration: 5940 | Episodes: 240900 | Median Reward: 26.73 | Max Reward: 48.88
Iteration: 5942 | Episodes: 241000 | Median Reward: 25.34 | Max Reward: 48.88
Iteration: 5945 | Episodes: 241100 | Median Reward: 23.42 | Max Reward: 48.88
Iteration: 5947 | Episodes: 241200 | Median Reward: 18.83 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.8         |
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 5950          |
|    time_elapsed         | 46580         |
|    total_timesteps      | 24371200      |
| train/                  |               |
|    approx_kl            | 0.00011712505 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -217          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.4         |
|    n_updates            | 59490         |
|    policy_gradient_loss | -0.000648     |
|    std                  | 400           |
|    value_loss           | 1.21          |
-------------------------------------------
Iteration: 5950 | Episodes: 241300 | Median Reward: 20.35 | Max Reward: 48.88
Iteration: 5952 | Episodes: 241400 | Median Reward: 21.25 | Max Reward: 48.88
Iteration: 5954 | Episodes: 241500 | Median Reward: 18.35 | Max Reward: 48.88
Iteration: 5957 | Episodes: 241600 | Median Reward: 16.15 | Max Reward: 48.88
Iteration: 5959 | Episodes: 241700 | Median Reward: 23.32 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -79.6         |
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 5960          |
|    time_elapsed         | 46667         |
|    total_timesteps      | 24412160      |
| train/                  |               |
|    approx_kl            | 3.8526603e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -217          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.42         |
|    n_updates            | 59590         |
|    policy_gradient_loss | -0.00056      |
|    std                  | 405           |
|    value_loss           | 3.25          |
-------------------------------------------
Iteration: 5962 | Episodes: 241800 | Median Reward: 25.15 | Max Reward: 48.88
Iteration: 5964 | Episodes: 241900 | Median Reward: 23.03 | Max Reward: 48.88
Iteration: 5967 | Episodes: 242000 | Median Reward: 13.94 | Max Reward: 48.88
Iteration: 5969 | Episodes: 242100 | Median Reward: 22.67 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -81.2        |
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 5970         |
|    time_elapsed         | 46743        |
|    total_timesteps      | 24453120     |
| train/                  |              |
|    approx_kl            | 0.0002939339 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -217         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.5        |
|    n_updates            | 59690        |
|    policy_gradient_loss | -0.000941    |
|    std                  | 410          |
|    value_loss           | 1.2          |
------------------------------------------
Iteration: 5972 | Episodes: 242200 | Median Reward: 19.88 | Max Reward: 48.88
Iteration: 5974 | Episodes: 242300 | Median Reward: 24.79 | Max Reward: 48.88
Iteration: 5977 | Episodes: 242400 | Median Reward: 21.92 | Max Reward: 48.88
Iteration: 5979 | Episodes: 242500 | Median Reward: 25.08 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74.6       |
| time/                   |             |
|    fps                  | 523         |
|    iterations           | 5980        |
|    time_elapsed         | 46831       |
|    total_timesteps      | 24494080    |
| train/                  |             |
|    approx_kl            | 5.06847e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -217        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.5       |
|    n_updates            | 59790       |
|    policy_gradient_loss | -0.000249   |
|    std                  | 412         |
|    value_loss           | 1.18        |
-----------------------------------------
Iteration: 5982 | Episodes: 242600 | Median Reward: 22.30 | Max Reward: 48.88
Iteration: 5984 | Episodes: 242700 | Median Reward: 18.42 | Max Reward: 48.88
Iteration: 5987 | Episodes: 242800 | Median Reward: 21.85 | Max Reward: 48.88
Iteration: 5989 | Episodes: 242900 | Median Reward: 20.03 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.3         |
| time/                   |               |
|    fps                  | 523           |
|    iterations           | 5990          |
|    time_elapsed         | 46910         |
|    total_timesteps      | 24535040      |
| train/                  |               |
|    approx_kl            | 5.0913106e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -218          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.5         |
|    n_updates            | 59890         |
|    policy_gradient_loss | -0.000824     |
|    std                  | 414           |
|    value_loss           | 0.82          |
-------------------------------------------
Iteration: 5991 | Episodes: 243000 | Median Reward: 13.06 | Max Reward: 48.88
Iteration: 5994 | Episodes: 243100 | Median Reward: 20.20 | Max Reward: 48.88
Iteration: 5996 | Episodes: 243200 | Median Reward: 24.34 | Max Reward: 48.88
Iteration: 5999 | Episodes: 243300 | Median Reward: 14.05 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.2        |
| time/                   |              |
|    fps                  | 523          |
|    iterations           | 6000         |
|    time_elapsed         | 46986        |
|    total_timesteps      | 24576000     |
| train/                  |              |
|    approx_kl            | 0.0019421303 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -218         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.7        |
|    n_updates            | 59990        |
|    policy_gradient_loss | -0.00903     |
|    std                  | 420          |
|    value_loss           | 0.583        |
------------------------------------------
Iteration: 6001 | Episodes: 243400 | Median Reward: 10.53 | Max Reward: 48.88
Iteration: 6004 | Episodes: 243500 | Median Reward: 17.26 | Max Reward: 48.88
Iteration: 6006 | Episodes: 243600 | Median Reward: 18.09 | Max Reward: 48.88
Iteration: 6009 | Episodes: 243700 | Median Reward: 22.91 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.8        |
| time/                   |              |
|    fps                  | 522          |
|    iterations           | 6010         |
|    time_elapsed         | 47076        |
|    total_timesteps      | 24616960     |
| train/                  |              |
|    approx_kl            | 0.0002552204 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -218         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.3        |
|    n_updates            | 60090        |
|    policy_gradient_loss | -0.00117     |
|    std                  | 424          |
|    value_loss           | 1.36         |
------------------------------------------
Iteration: 6011 | Episodes: 243800 | Median Reward: 22.82 | Max Reward: 48.88
Iteration: 6014 | Episodes: 243900 | Median Reward: 11.52 | Max Reward: 48.88
Iteration: 6016 | Episodes: 244000 | Median Reward: 22.16 | Max Reward: 48.88
Iteration: 6019 | Episodes: 244100 | Median Reward: 20.74 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.7         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6020          |
|    time_elapsed         | 47152         |
|    total_timesteps      | 24657920      |
| train/                  |               |
|    approx_kl            | 0.00026646574 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -218          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.7         |
|    n_updates            | 60190         |
|    policy_gradient_loss | -0.00288      |
|    std                  | 429           |
|    value_loss           | 0.919         |
-------------------------------------------
Iteration: 6021 | Episodes: 244200 | Median Reward: 22.29 | Max Reward: 48.88
Iteration: 6023 | Episodes: 244300 | Median Reward: 29.52 | Max Reward: 48.88
Iteration: 6026 | Episodes: 244400 | Median Reward: 20.92 | Max Reward: 48.88
Iteration: 6028 | Episodes: 244500 | Median Reward: 22.09 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.6        |
| time/                   |              |
|    fps                  | 522          |
|    iterations           | 6030         |
|    time_elapsed         | 47237        |
|    total_timesteps      | 24698880     |
| train/                  |              |
|    approx_kl            | 0.0001278134 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -218         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.5        |
|    n_updates            | 60290        |
|    policy_gradient_loss | -0.00152     |
|    std                  | 434          |
|    value_loss           | 0.942        |
------------------------------------------
Iteration: 6031 | Episodes: 244600 | Median Reward: 17.77 | Max Reward: 48.88
Iteration: 6033 | Episodes: 244700 | Median Reward: 15.42 | Max Reward: 48.88
Iteration: 6036 | Episodes: 244800 | Median Reward: 21.15 | Max Reward: 48.88
Iteration: 6038 | Episodes: 244900 | Median Reward: 19.29 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.9         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6040          |
|    time_elapsed         | 47320         |
|    total_timesteps      | 24739840      |
| train/                  |               |
|    approx_kl            | 9.4609044e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -219          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.2         |
|    n_updates            | 60390         |
|    policy_gradient_loss | -0.000612     |
|    std                  | 440           |
|    value_loss           | 1.69          |
-------------------------------------------
Iteration: 6041 | Episodes: 245000 | Median Reward: 12.69 | Max Reward: 48.88
Iteration: 6043 | Episodes: 245100 | Median Reward: 18.48 | Max Reward: 48.88
Iteration: 6046 | Episodes: 245200 | Median Reward: 19.21 | Max Reward: 48.88
Iteration: 6048 | Episodes: 245300 | Median Reward: 15.46 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.7        |
| time/                   |              |
|    fps                  | 522          |
|    iterations           | 6050         |
|    time_elapsed         | 47397        |
|    total_timesteps      | 24780800     |
| train/                  |              |
|    approx_kl            | 0.0007759208 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -219         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.4        |
|    n_updates            | 60490        |
|    policy_gradient_loss | -0.00366     |
|    std                  | 444          |
|    value_loss           | 1.47         |
------------------------------------------
Iteration: 6051 | Episodes: 245400 | Median Reward: 21.70 | Max Reward: 48.88
Iteration: 6053 | Episodes: 245500 | Median Reward: 26.39 | Max Reward: 48.88
Iteration: 6056 | Episodes: 245600 | Median Reward: 17.04 | Max Reward: 48.88
Iteration: 6058 | Episodes: 245700 | Median Reward: 19.07 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -79.7         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6060          |
|    time_elapsed         | 47485         |
|    total_timesteps      | 24821760      |
| train/                  |               |
|    approx_kl            | 0.00027277943 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -219          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.3         |
|    n_updates            | 60590         |
|    policy_gradient_loss | -0.00168      |
|    std                  | 447           |
|    value_loss           | 1.8           |
-------------------------------------------
Iteration: 6060 | Episodes: 245800 | Median Reward: 17.89 | Max Reward: 48.88
Iteration: 6063 | Episodes: 245900 | Median Reward: 20.93 | Max Reward: 48.88
Iteration: 6065 | Episodes: 246000 | Median Reward: 17.81 | Max Reward: 48.88
Iteration: 6068 | Episodes: 246100 | Median Reward: 14.66 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.3        |
| time/                   |              |
|    fps                  | 522          |
|    iterations           | 6070         |
|    time_elapsed         | 47563        |
|    total_timesteps      | 24862720     |
| train/                  |              |
|    approx_kl            | 0.0005371973 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -219         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.6        |
|    n_updates            | 60690        |
|    policy_gradient_loss | -0.00294     |
|    std                  | 453          |
|    value_loss           | 0.976        |
------------------------------------------
Iteration: 6070 | Episodes: 246200 | Median Reward: 20.84 | Max Reward: 48.88
Iteration: 6073 | Episodes: 246300 | Median Reward: 23.17 | Max Reward: 48.88
Iteration: 6075 | Episodes: 246400 | Median Reward: 20.53 | Max Reward: 48.88
Iteration: 6078 | Episodes: 246500 | Median Reward: 15.95 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.8         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6080          |
|    time_elapsed         | 47645         |
|    total_timesteps      | 24903680      |
| train/                  |               |
|    approx_kl            | 2.2810418e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -219          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.8         |
|    n_updates            | 60790         |
|    policy_gradient_loss | 3.62e-05      |
|    std                  | 460           |
|    value_loss           | 0.718         |
-------------------------------------------
Iteration: 6080 | Episodes: 246600 | Median Reward: 22.79 | Max Reward: 48.88
Iteration: 6083 | Episodes: 246700 | Median Reward: 26.03 | Max Reward: 48.88
Iteration: 6085 | Episodes: 246800 | Median Reward: 22.40 | Max Reward: 48.88
Iteration: 6088 | Episodes: 246900 | Median Reward: 21.78 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.8        |
| time/                   |              |
|    fps                  | 522          |
|    iterations           | 6090         |
|    time_elapsed         | 47731        |
|    total_timesteps      | 24944640     |
| train/                  |              |
|    approx_kl            | 3.431861e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -220         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.2        |
|    n_updates            | 60890        |
|    policy_gradient_loss | -0.000597    |
|    std                  | 467          |
|    value_loss           | 2.11         |
------------------------------------------
Iteration: 6090 | Episodes: 247000 | Median Reward: 25.81 | Max Reward: 48.88
Iteration: 6093 | Episodes: 247100 | Median Reward: 22.04 | Max Reward: 48.88
Iteration: 6095 | Episodes: 247200 | Median Reward: 15.84 | Max Reward: 48.88
Iteration: 6097 | Episodes: 247300 | Median Reward: 15.57 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.6         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6100          |
|    time_elapsed         | 47808         |
|    total_timesteps      | 24985600      |
| train/                  |               |
|    approx_kl            | 0.00037359353 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -220          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.6         |
|    n_updates            | 60990         |
|    policy_gradient_loss | -0.00024      |
|    std                  | 470           |
|    value_loss           | 0.839         |
-------------------------------------------
Iteration: 6100 | Episodes: 247400 | Median Reward: 19.77 | Max Reward: 48.88
Iteration: 6102 | Episodes: 247500 | Median Reward: 8.03 | Max Reward: 48.88
Iteration: 6105 | Episodes: 247600 | Median Reward: 24.41 | Max Reward: 48.88
Iteration: 6107 | Episodes: 247700 | Median Reward: 17.30 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82          |
| time/                   |              |
|    fps                  | 522          |
|    iterations           | 6110         |
|    time_elapsed         | 47897        |
|    total_timesteps      | 25026560     |
| train/                  |              |
|    approx_kl            | 2.047696e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -220         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.8        |
|    n_updates            | 61090        |
|    policy_gradient_loss | 9.08e-05     |
|    std                  | 476          |
|    value_loss           | 0.85         |
------------------------------------------
Iteration: 6110 | Episodes: 247800 | Median Reward: 21.56 | Max Reward: 48.88
Iteration: 6112 | Episodes: 247900 | Median Reward: 18.41 | Max Reward: 48.88
Iteration: 6115 | Episodes: 248000 | Median Reward: 26.87 | Max Reward: 48.88
Iteration: 6117 | Episodes: 248100 | Median Reward: 19.72 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.3         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6120          |
|    time_elapsed         | 47974         |
|    total_timesteps      | 25067520      |
| train/                  |               |
|    approx_kl            | 0.00087374425 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -220          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.8         |
|    n_updates            | 61190         |
|    policy_gradient_loss | -0.00192      |
|    std                  | 480           |
|    value_loss           | 0.687         |
-------------------------------------------
Iteration: 6120 | Episodes: 248200 | Median Reward: 20.64 | Max Reward: 48.88
Iteration: 6122 | Episodes: 248300 | Median Reward: 11.49 | Max Reward: 48.88
Iteration: 6125 | Episodes: 248400 | Median Reward: 23.46 | Max Reward: 48.88
Iteration: 6127 | Episodes: 248500 | Median Reward: 20.49 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.5        |
| time/                   |              |
|    fps                  | 522          |
|    iterations           | 6130         |
|    time_elapsed         | 48059        |
|    total_timesteps      | 25108480     |
| train/                  |              |
|    approx_kl            | 0.0017032749 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -221         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.9        |
|    n_updates            | 61290        |
|    policy_gradient_loss | -0.00674     |
|    std                  | 486          |
|    value_loss           | 0.545        |
------------------------------------------
Iteration: 6130 | Episodes: 248600 | Median Reward: 24.70 | Max Reward: 48.88
Iteration: 6132 | Episodes: 248700 | Median Reward: 22.05 | Max Reward: 48.88
Iteration: 6134 | Episodes: 248800 | Median Reward: 25.54 | Max Reward: 48.88
Iteration: 6137 | Episodes: 248900 | Median Reward: 25.96 | Max Reward: 48.88
Iteration: 6139 | Episodes: 249000 | Median Reward: 24.86 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.9         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6140          |
|    time_elapsed         | 48147         |
|    total_timesteps      | 25149440      |
| train/                  |               |
|    approx_kl            | 0.00017895989 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -221          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.5         |
|    n_updates            | 61390         |
|    policy_gradient_loss | -0.000415     |
|    std                  | 489           |
|    value_loss           | 1.36          |
-------------------------------------------
Iteration: 6142 | Episodes: 249100 | Median Reward: 24.96 | Max Reward: 48.88
Iteration: 6144 | Episodes: 249200 | Median Reward: 14.66 | Max Reward: 48.88
Iteration: 6147 | Episodes: 249300 | Median Reward: 22.81 | Max Reward: 48.88
Iteration: 6149 | Episodes: 249400 | Median Reward: 22.40 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.2        |
| time/                   |              |
|    fps                  | 522          |
|    iterations           | 6150         |
|    time_elapsed         | 48230        |
|    total_timesteps      | 25190400     |
| train/                  |              |
|    approx_kl            | 0.0006978288 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -221         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.5        |
|    n_updates            | 61490        |
|    policy_gradient_loss | -0.00586     |
|    std                  | 492          |
|    value_loss           | 1.26         |
------------------------------------------
Iteration: 6152 | Episodes: 249500 | Median Reward: 22.24 | Max Reward: 48.88
Iteration: 6154 | Episodes: 249600 | Median Reward: 25.26 | Max Reward: 48.88
Iteration: 6157 | Episodes: 249700 | Median Reward: 22.98 | Max Reward: 48.88
Iteration: 6159 | Episodes: 249800 | Median Reward: 24.64 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -79.7         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6160          |
|    time_elapsed         | 48315         |
|    total_timesteps      | 25231360      |
| train/                  |               |
|    approx_kl            | 0.00016816106 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -221          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.9         |
|    n_updates            | 61590         |
|    policy_gradient_loss | -0.00212      |
|    std                  | 493           |
|    value_loss           | 0.372         |
-------------------------------------------
Iteration: 6162 | Episodes: 249900 | Median Reward: 24.39 | Max Reward: 48.88
Iteration: 6164 | Episodes: 250000 | Median Reward: 22.25 | Max Reward: 48.88
Iteration: 6167 | Episodes: 250100 | Median Reward: 16.87 | Max Reward: 48.88
Iteration: 6169 | Episodes: 250200 | Median Reward: 22.79 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.8         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6170          |
|    time_elapsed         | 48398         |
|    total_timesteps      | 25272320      |
| train/                  |               |
|    approx_kl            | 0.00074301334 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -221          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.8         |
|    n_updates            | 61690         |
|    policy_gradient_loss | -0.00361      |
|    std                  | 500           |
|    value_loss           | 0.671         |
-------------------------------------------
Iteration: 6171 | Episodes: 250300 | Median Reward: 15.54 | Max Reward: 48.88
Iteration: 6174 | Episodes: 250400 | Median Reward: 22.56 | Max Reward: 48.88
Iteration: 6176 | Episodes: 250500 | Median Reward: 23.39 | Max Reward: 48.88
Iteration: 6179 | Episodes: 250600 | Median Reward: 20.10 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.2         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6180          |
|    time_elapsed         | 48476         |
|    total_timesteps      | 25313280      |
| train/                  |               |
|    approx_kl            | 0.00035464385 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -222          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.9         |
|    n_updates            | 61790         |
|    policy_gradient_loss | -0.00155      |
|    std                  | 504           |
|    value_loss           | 0.382         |
-------------------------------------------
Iteration: 6181 | Episodes: 250700 | Median Reward: 23.62 | Max Reward: 48.88
Iteration: 6184 | Episodes: 250800 | Median Reward: 23.30 | Max Reward: 48.88
Iteration: 6186 | Episodes: 250900 | Median Reward: 22.93 | Max Reward: 48.88
Iteration: 6189 | Episodes: 251000 | Median Reward: 27.08 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.2         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6190          |
|    time_elapsed         | 48565         |
|    total_timesteps      | 25354240      |
| train/                  |               |
|    approx_kl            | 0.00013175767 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -222          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.8         |
|    n_updates            | 61890         |
|    policy_gradient_loss | 0.000362      |
|    std                  | 510           |
|    value_loss           | 0.658         |
-------------------------------------------
Iteration: 6191 | Episodes: 251100 | Median Reward: 16.21 | Max Reward: 48.88
Iteration: 6194 | Episodes: 251200 | Median Reward: 19.14 | Max Reward: 48.88
Iteration: 6196 | Episodes: 251300 | Median Reward: 19.37 | Max Reward: 48.88
Iteration: 6199 | Episodes: 251400 | Median Reward: 20.21 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.3         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6200          |
|    time_elapsed         | 48642         |
|    total_timesteps      | 25395200      |
| train/                  |               |
|    approx_kl            | 1.4646095e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -222          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.9         |
|    n_updates            | 61990         |
|    policy_gradient_loss | -0.000136     |
|    std                  | 517           |
|    value_loss           | 0.527         |
-------------------------------------------
Iteration: 6201 | Episodes: 251500 | Median Reward: 23.32 | Max Reward: 48.88
Iteration: 6204 | Episodes: 251600 | Median Reward: 13.46 | Max Reward: 48.88
Iteration: 6206 | Episodes: 251700 | Median Reward: 15.97 | Max Reward: 48.88
Iteration: 6208 | Episodes: 251800 | Median Reward: 15.74 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.4         |
| time/                   |               |
|    fps                  | 522           |
|    iterations           | 6210          |
|    time_elapsed         | 48724         |
|    total_timesteps      | 25436160      |
| train/                  |               |
|    approx_kl            | 0.00060335174 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -222          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.9         |
|    n_updates            | 62090         |
|    policy_gradient_loss | -0.00365      |
|    std                  | 521           |
|    value_loss           | 0.828         |
-------------------------------------------
Iteration: 6211 | Episodes: 251900 | Median Reward: 19.03 | Max Reward: 48.88
Iteration: 6213 | Episodes: 252000 | Median Reward: 20.20 | Max Reward: 48.88
Iteration: 6216 | Episodes: 252100 | Median Reward: 14.22 | Max Reward: 48.88
Iteration: 6218 | Episodes: 252200 | Median Reward: 15.40 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.7         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6220          |
|    time_elapsed         | 48810         |
|    total_timesteps      | 25477120      |
| train/                  |               |
|    approx_kl            | 0.00016474948 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -223          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.4         |
|    n_updates            | 62190         |
|    policy_gradient_loss | -0.0013       |
|    std                  | 528           |
|    value_loss           | 1.6           |
-------------------------------------------
Iteration: 6221 | Episodes: 252300 | Median Reward: 27.21 | Max Reward: 48.88
Iteration: 6223 | Episodes: 252400 | Median Reward: 22.65 | Max Reward: 48.88
Iteration: 6226 | Episodes: 252500 | Median Reward: 15.33 | Max Reward: 48.88
Iteration: 6228 | Episodes: 252600 | Median Reward: 17.70 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -79           |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6230          |
|    time_elapsed         | 48887         |
|    total_timesteps      | 25518080      |
| train/                  |               |
|    approx_kl            | 6.2761654e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -223          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.9         |
|    n_updates            | 62290         |
|    policy_gradient_loss | -0.000345     |
|    std                  | 533           |
|    value_loss           | 0.599         |
-------------------------------------------
Iteration: 6231 | Episodes: 252700 | Median Reward: 25.91 | Max Reward: 48.88
Iteration: 6233 | Episodes: 252800 | Median Reward: 24.76 | Max Reward: 48.88
Iteration: 6236 | Episodes: 252900 | Median Reward: 16.80 | Max Reward: 48.88
Iteration: 6238 | Episodes: 253000 | Median Reward: 18.14 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.5        |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6240         |
|    time_elapsed         | 48981        |
|    total_timesteps      | 25559040     |
| train/                  |              |
|    approx_kl            | 0.0016055654 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -223         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.8        |
|    n_updates            | 62390        |
|    policy_gradient_loss | -0.00415     |
|    std                  | 540          |
|    value_loss           | 0.799        |
------------------------------------------
Iteration: 6240 | Episodes: 253100 | Median Reward: 16.28 | Max Reward: 48.88
Iteration: 6243 | Episodes: 253200 | Median Reward: 26.37 | Max Reward: 48.88
Iteration: 6245 | Episodes: 253300 | Median Reward: 18.81 | Max Reward: 48.88
Iteration: 6248 | Episodes: 253400 | Median Reward: 18.00 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.9         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6250          |
|    time_elapsed         | 49060         |
|    total_timesteps      | 25600000      |
| train/                  |               |
|    approx_kl            | 0.00021067605 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -223          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.8         |
|    n_updates            | 62490         |
|    policy_gradient_loss | -0.00194      |
|    std                  | 545           |
|    value_loss           | 1.15          |
-------------------------------------------
Iteration: 6250 | Episodes: 253500 | Median Reward: 16.80 | Max Reward: 48.88
Iteration: 6253 | Episodes: 253600 | Median Reward: 21.65 | Max Reward: 48.88
Iteration: 6255 | Episodes: 253700 | Median Reward: 19.18 | Max Reward: 48.88
Iteration: 6258 | Episodes: 253800 | Median Reward: 17.90 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.5        |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6260         |
|    time_elapsed         | 49138        |
|    total_timesteps      | 25640960     |
| train/                  |              |
|    approx_kl            | 0.0010706296 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -223         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.9        |
|    n_updates            | 62590        |
|    policy_gradient_loss | -0.00723     |
|    std                  | 547          |
|    value_loss           | 0.696        |
------------------------------------------
Iteration: 6260 | Episodes: 253900 | Median Reward: 22.06 | Max Reward: 48.88
Iteration: 6263 | Episodes: 254000 | Median Reward: 18.66 | Max Reward: 48.88
Iteration: 6265 | Episodes: 254100 | Median Reward: 22.41 | Max Reward: 48.88
Iteration: 6268 | Episodes: 254200 | Median Reward: 23.71 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.1         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6270          |
|    time_elapsed         | 49228         |
|    total_timesteps      | 25681920      |
| train/                  |               |
|    approx_kl            | 2.5388057e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -224          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.3         |
|    n_updates            | 62690         |
|    policy_gradient_loss | 2.94e-05      |
|    std                  | 551           |
|    value_loss           | 2.13          |
-------------------------------------------
Iteration: 6270 | Episodes: 254300 | Median Reward: 23.44 | Max Reward: 48.88
Iteration: 6273 | Episodes: 254400 | Median Reward: 19.04 | Max Reward: 48.88
Iteration: 6275 | Episodes: 254500 | Median Reward: 19.61 | Max Reward: 48.88
Iteration: 6277 | Episodes: 254600 | Median Reward: 28.73 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.2         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6280          |
|    time_elapsed         | 49306         |
|    total_timesteps      | 25722880      |
| train/                  |               |
|    approx_kl            | 0.00014698203 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -224          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.8         |
|    n_updates            | 62790         |
|    policy_gradient_loss | -0.00164      |
|    std                  | 558           |
|    value_loss           | 1.12          |
-------------------------------------------
Iteration: 6280 | Episodes: 254700 | Median Reward: 16.41 | Max Reward: 48.88
Iteration: 6282 | Episodes: 254800 | Median Reward: 19.64 | Max Reward: 48.88
Iteration: 6285 | Episodes: 254900 | Median Reward: 19.55 | Max Reward: 48.88
Iteration: 6287 | Episodes: 255000 | Median Reward: 25.28 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.3        |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6290         |
|    time_elapsed         | 49390        |
|    total_timesteps      | 25763840     |
| train/                  |              |
|    approx_kl            | 0.0007303846 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -224         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.9        |
|    n_updates            | 62890        |
|    policy_gradient_loss | -0.00393     |
|    std                  | 565          |
|    value_loss           | 0.777        |
------------------------------------------
Iteration: 6290 | Episodes: 255100 | Median Reward: 21.97 | Max Reward: 48.88
Iteration: 6292 | Episodes: 255200 | Median Reward: 26.93 | Max Reward: 48.88
Iteration: 6295 | Episodes: 255300 | Median Reward: 16.18 | Max Reward: 48.88
Iteration: 6297 | Episodes: 255400 | Median Reward: 17.14 | Max Reward: 48.88
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -77.8      |
| time/                   |            |
|    fps                  | 521        |
|    iterations           | 6300       |
|    time_elapsed         | 49472      |
|    total_timesteps      | 25804800   |
| train/                  |            |
|    approx_kl            | 0.00425867 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -224       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -11.1      |
|    n_updates            | 62990      |
|    policy_gradient_loss | -0.0099    |
|    std                  | 572        |
|    value_loss           | 0.457      |
----------------------------------------
Iteration: 6300 | Episodes: 255500 | Median Reward: 25.07 | Max Reward: 48.88
Iteration: 6302 | Episodes: 255600 | Median Reward: 14.56 | Max Reward: 48.88
Iteration: 6305 | Episodes: 255700 | Median Reward: 22.89 | Max Reward: 48.88
Iteration: 6307 | Episodes: 255800 | Median Reward: 20.72 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82          |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6310         |
|    time_elapsed         | 49547        |
|    total_timesteps      | 25845760     |
| train/                  |              |
|    approx_kl            | 0.0001228236 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -225         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.6        |
|    n_updates            | 63090        |
|    policy_gradient_loss | -0.00205     |
|    std                  | 576          |
|    value_loss           | 1.46         |
------------------------------------------
Iteration: 6310 | Episodes: 255900 | Median Reward: 21.89 | Max Reward: 48.88
Iteration: 6312 | Episodes: 256000 | Median Reward: 19.87 | Max Reward: 48.88
Iteration: 6314 | Episodes: 256100 | Median Reward: 21.69 | Max Reward: 48.88
Iteration: 6317 | Episodes: 256200 | Median Reward: 14.45 | Max Reward: 48.88
Iteration: 6319 | Episodes: 256300 | Median Reward: 15.52 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -81          |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6320         |
|    time_elapsed         | 49639        |
|    total_timesteps      | 25886720     |
| train/                  |              |
|    approx_kl            | 0.0007892174 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -225         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11          |
|    n_updates            | 63190        |
|    policy_gradient_loss | -0.00492     |
|    std                  | 581          |
|    value_loss           | 0.716        |
------------------------------------------
Iteration: 6322 | Episodes: 256400 | Median Reward: 15.92 | Max Reward: 48.88
Iteration: 6324 | Episodes: 256500 | Median Reward: 21.81 | Max Reward: 48.88
Iteration: 6327 | Episodes: 256600 | Median Reward: 18.01 | Max Reward: 48.88
Iteration: 6329 | Episodes: 256700 | Median Reward: 18.87 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.2         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6330          |
|    time_elapsed         | 49715         |
|    total_timesteps      | 25927680      |
| train/                  |               |
|    approx_kl            | 6.4408465e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -225          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.9         |
|    n_updates            | 63290         |
|    policy_gradient_loss | -0.000153     |
|    std                  | 586           |
|    value_loss           | 1.05          |
-------------------------------------------
Iteration: 6332 | Episodes: 256800 | Median Reward: 24.04 | Max Reward: 48.88
Iteration: 6334 | Episodes: 256900 | Median Reward: 19.29 | Max Reward: 48.88
Iteration: 6337 | Episodes: 257000 | Median Reward: 15.74 | Max Reward: 48.88
Iteration: 6339 | Episodes: 257100 | Median Reward: 22.55 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.7        |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6340         |
|    time_elapsed         | 49796        |
|    total_timesteps      | 25968640     |
| train/                  |              |
|    approx_kl            | 0.0004448996 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -225         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11          |
|    n_updates            | 63390        |
|    policy_gradient_loss | -0.000888    |
|    std                  | 594          |
|    value_loss           | 1.15         |
------------------------------------------
Iteration: 6342 | Episodes: 257200 | Median Reward: 21.36 | Max Reward: 48.88
Iteration: 6344 | Episodes: 257300 | Median Reward: 20.87 | Max Reward: 48.88
Iteration: 6347 | Episodes: 257400 | Median Reward: 20.89 | Max Reward: 48.88
Iteration: 6349 | Episodes: 257500 | Median Reward: 21.42 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -81.1        |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6350         |
|    time_elapsed         | 49877        |
|    total_timesteps      | 26009600     |
| train/                  |              |
|    approx_kl            | 7.138378e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -226         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11          |
|    n_updates            | 63490        |
|    policy_gradient_loss | -0.000842    |
|    std                  | 602          |
|    value_loss           | 0.671        |
------------------------------------------
Iteration: 6351 | Episodes: 257600 | Median Reward: 27.22 | Max Reward: 48.88
Iteration: 6354 | Episodes: 257700 | Median Reward: 21.91 | Max Reward: 48.88
Iteration: 6356 | Episodes: 257800 | Median Reward: 16.62 | Max Reward: 48.88
Iteration: 6359 | Episodes: 257900 | Median Reward: 21.03 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.4        |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6360         |
|    time_elapsed         | 49952        |
|    total_timesteps      | 26050560     |
| train/                  |              |
|    approx_kl            | 7.128925e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -226         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11          |
|    n_updates            | 63590        |
|    policy_gradient_loss | -0.000494    |
|    std                  | 610          |
|    value_loss           | 0.904        |
------------------------------------------
Iteration: 6361 | Episodes: 258000 | Median Reward: 22.87 | Max Reward: 48.88
Iteration: 6364 | Episodes: 258100 | Median Reward: 22.44 | Max Reward: 48.88
Iteration: 6366 | Episodes: 258200 | Median Reward: 20.92 | Max Reward: 48.88
Iteration: 6369 | Episodes: 258300 | Median Reward: 17.50 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.8        |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6370         |
|    time_elapsed         | 50041        |
|    total_timesteps      | 26091520     |
| train/                  |              |
|    approx_kl            | 0.0006085061 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -226         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.9        |
|    n_updates            | 63690        |
|    policy_gradient_loss | -0.00298     |
|    std                  | 619          |
|    value_loss           | 1.32         |
------------------------------------------
Iteration: 6371 | Episodes: 258400 | Median Reward: 21.35 | Max Reward: 48.88
Iteration: 6374 | Episodes: 258500 | Median Reward: 19.44 | Max Reward: 48.88
Iteration: 6376 | Episodes: 258600 | Median Reward: 18.14 | Max Reward: 48.88
Iteration: 6379 | Episodes: 258700 | Median Reward: 26.95 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -77.6         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6380          |
|    time_elapsed         | 50117         |
|    total_timesteps      | 26132480      |
| train/                  |               |
|    approx_kl            | 0.00013604606 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -227          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -11.2         |
|    n_updates            | 63790         |
|    policy_gradient_loss | -0.0016       |
|    std                  | 623           |
|    value_loss           | 0.357         |
-------------------------------------------
Iteration: 6381 | Episodes: 258800 | Median Reward: 14.80 | Max Reward: 48.88
Iteration: 6384 | Episodes: 258900 | Median Reward: 18.13 | Max Reward: 48.88
Iteration: 6386 | Episodes: 259000 | Median Reward: 19.88 | Max Reward: 48.88
Iteration: 6388 | Episodes: 259100 | Median Reward: 17.45 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.8         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6390          |
|    time_elapsed         | 50199         |
|    total_timesteps      | 26173440      |
| train/                  |               |
|    approx_kl            | 4.7328707e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -227          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.9         |
|    n_updates            | 63890         |
|    policy_gradient_loss | -0.00114      |
|    std                  | 624           |
|    value_loss           | 1.19          |
-------------------------------------------
Iteration: 6391 | Episodes: 259200 | Median Reward: 22.68 | Max Reward: 48.88
Iteration: 6393 | Episodes: 259300 | Median Reward: 12.26 | Max Reward: 48.88
Iteration: 6396 | Episodes: 259400 | Median Reward: 19.89 | Max Reward: 48.88
Iteration: 6398 | Episodes: 259500 | Median Reward: 22.13 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.3         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6400          |
|    time_elapsed         | 50283         |
|    total_timesteps      | 26214400      |
| train/                  |               |
|    approx_kl            | 1.4838355e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -227          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.6         |
|    n_updates            | 63990         |
|    policy_gradient_loss | 5.75e-05      |
|    std                  | 628           |
|    value_loss           | 2.01          |
-------------------------------------------
Iteration: 6401 | Episodes: 259600 | Median Reward: 25.86 | Max Reward: 48.88
Iteration: 6403 | Episodes: 259700 | Median Reward: 23.41 | Max Reward: 48.88
Iteration: 6406 | Episodes: 259800 | Median Reward: 19.35 | Max Reward: 48.88
Iteration: 6408 | Episodes: 259900 | Median Reward: 17.98 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79          |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6410         |
|    time_elapsed         | 50358        |
|    total_timesteps      | 26255360     |
| train/                  |              |
|    approx_kl            | 0.0015368874 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -227         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.1        |
|    n_updates            | 64090        |
|    policy_gradient_loss | -0.00735     |
|    std                  | 631          |
|    value_loss           | 0.606        |
------------------------------------------
Iteration: 6411 | Episodes: 260000 | Median Reward: 18.41 | Max Reward: 48.88
Iteration: 6413 | Episodes: 260100 | Median Reward: 18.53 | Max Reward: 48.88
Iteration: 6416 | Episodes: 260200 | Median Reward: 15.92 | Max Reward: 48.88
Iteration: 6418 | Episodes: 260300 | Median Reward: 20.20 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.7         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6420          |
|    time_elapsed         | 50445         |
|    total_timesteps      | 26296320      |
| train/                  |               |
|    approx_kl            | 6.0376507e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -227          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.4         |
|    n_updates            | 64190         |
|    policy_gradient_loss | -0.000894     |
|    std                  | 635           |
|    value_loss           | 1.82          |
-------------------------------------------
Iteration: 6420 | Episodes: 260400 | Median Reward: 16.07 | Max Reward: 48.88
Iteration: 6423 | Episodes: 260500 | Median Reward: 18.05 | Max Reward: 48.88
Iteration: 6425 | Episodes: 260600 | Median Reward: 17.10 | Max Reward: 48.88
Iteration: 6428 | Episodes: 260700 | Median Reward: 24.56 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82          |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6430         |
|    time_elapsed         | 50522        |
|    total_timesteps      | 26337280     |
| train/                  |              |
|    approx_kl            | 0.0001317391 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -228         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.1        |
|    n_updates            | 64290        |
|    policy_gradient_loss | -0.00165     |
|    std                  | 643          |
|    value_loss           | 0.714        |
------------------------------------------
Iteration: 6430 | Episodes: 260800 | Median Reward: 20.05 | Max Reward: 48.88
Iteration: 6433 | Episodes: 260900 | Median Reward: 20.80 | Max Reward: 48.88
Iteration: 6435 | Episodes: 261000 | Median Reward: 20.63 | Max Reward: 48.88
Iteration: 6438 | Episodes: 261100 | Median Reward: 17.93 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.8         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6440          |
|    time_elapsed         | 50601         |
|    total_timesteps      | 26378240      |
| train/                  |               |
|    approx_kl            | 1.9286948e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -228          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.1         |
|    n_updates            | 64390         |
|    policy_gradient_loss | -0.00026      |
|    std                  | 647           |
|    value_loss           | 1.23          |
-------------------------------------------
Iteration: 6440 | Episodes: 261200 | Median Reward: 17.43 | Max Reward: 48.88
Iteration: 6443 | Episodes: 261300 | Median Reward: 20.39 | Max Reward: 48.88
Iteration: 6445 | Episodes: 261400 | Median Reward: 18.04 | Max Reward: 48.88
Iteration: 6448 | Episodes: 261500 | Median Reward: 17.52 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.5         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6450          |
|    time_elapsed         | 50686         |
|    total_timesteps      | 26419200      |
| train/                  |               |
|    approx_kl            | 0.00023671362 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -228          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11           |
|    n_updates            | 64490         |
|    policy_gradient_loss | -0.00192      |
|    std                  | 654           |
|    value_loss           | 0.998         |
-------------------------------------------
Iteration: 6450 | Episodes: 261600 | Median Reward: 22.46 | Max Reward: 48.88
Iteration: 6453 | Episodes: 261700 | Median Reward: 20.46 | Max Reward: 48.88
Iteration: 6455 | Episodes: 261800 | Median Reward: 16.42 | Max Reward: 48.88
Iteration: 6457 | Episodes: 261900 | Median Reward: 22.49 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.4         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6460          |
|    time_elapsed         | 50762         |
|    total_timesteps      | 26460160      |
| train/                  |               |
|    approx_kl            | 0.00012881015 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -228          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.8         |
|    n_updates            | 64590         |
|    policy_gradient_loss | -0.00171      |
|    std                  | 659           |
|    value_loss           | 1.46          |
-------------------------------------------
Iteration: 6460 | Episodes: 262000 | Median Reward: 13.80 | Max Reward: 48.88
Iteration: 6462 | Episodes: 262100 | Median Reward: 16.63 | Max Reward: 48.88
Iteration: 6465 | Episodes: 262200 | Median Reward: 16.06 | Max Reward: 48.88
Iteration: 6467 | Episodes: 262300 | Median Reward: 28.26 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.7        |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6470         |
|    time_elapsed         | 50850        |
|    total_timesteps      | 26501120     |
| train/                  |              |
|    approx_kl            | 0.0006958846 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -228         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.2        |
|    n_updates            | 64690        |
|    policy_gradient_loss | -0.0058      |
|    std                  | 664          |
|    value_loss           | 0.55         |
------------------------------------------
Iteration: 6470 | Episodes: 262400 | Median Reward: 21.41 | Max Reward: 48.88
Iteration: 6472 | Episodes: 262500 | Median Reward: 19.66 | Max Reward: 48.88
Iteration: 6475 | Episodes: 262600 | Median Reward: 17.51 | Max Reward: 48.88
Iteration: 6477 | Episodes: 262700 | Median Reward: 15.56 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.7        |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6480         |
|    time_elapsed         | 50927        |
|    total_timesteps      | 26542080     |
| train/                  |              |
|    approx_kl            | 9.992687e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -229         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0005       |
|    loss                 | -10          |
|    n_updates            | 64790        |
|    policy_gradient_loss | -0.000914    |
|    std                  | 671          |
|    value_loss           | 3.2          |
------------------------------------------
Iteration: 6480 | Episodes: 262800 | Median Reward: 19.16 | Max Reward: 48.88
Iteration: 6482 | Episodes: 262900 | Median Reward: 18.08 | Max Reward: 48.88
Iteration: 6485 | Episodes: 263000 | Median Reward: 22.14 | Max Reward: 48.88
Iteration: 6487 | Episodes: 263100 | Median Reward: 20.09 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84.6        |
| time/                   |              |
|    fps                  | 521          |
|    iterations           | 6490         |
|    time_elapsed         | 51005        |
|    total_timesteps      | 26583040     |
| train/                  |              |
|    approx_kl            | 0.0029483158 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -229         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.3        |
|    n_updates            | 64890        |
|    policy_gradient_loss | -0.00679     |
|    std                  | 672          |
|    value_loss           | 0.453        |
------------------------------------------
Iteration: 6490 | Episodes: 263200 | Median Reward: 16.63 | Max Reward: 48.88
Iteration: 6492 | Episodes: 263300 | Median Reward: 18.38 | Max Reward: 48.88
Iteration: 6494 | Episodes: 263400 | Median Reward: 22.86 | Max Reward: 48.88
Iteration: 6497 | Episodes: 263500 | Median Reward: 16.90 | Max Reward: 48.88
Iteration: 6499 | Episodes: 263600 | Median Reward: 21.94 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -79.4         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6500          |
|    time_elapsed         | 51092         |
|    total_timesteps      | 26624000      |
| train/                  |               |
|    approx_kl            | 0.00027961144 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -229          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.2         |
|    n_updates            | 64990         |
|    policy_gradient_loss | -0.002        |
|    std                  | 679           |
|    value_loss           | 0.734         |
-------------------------------------------
Iteration: 6502 | Episodes: 263700 | Median Reward: 13.73 | Max Reward: 48.88
Iteration: 6504 | Episodes: 263800 | Median Reward: 13.54 | Max Reward: 48.88
Iteration: 6507 | Episodes: 263900 | Median Reward: 21.04 | Max Reward: 48.88
Iteration: 6509 | Episodes: 264000 | Median Reward: 12.76 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.1         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6510          |
|    time_elapsed         | 51167         |
|    total_timesteps      | 26664960      |
| train/                  |               |
|    approx_kl            | 4.8053596e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -229          |
|    explained_variance   | 0.991         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.7         |
|    n_updates            | 65090         |
|    policy_gradient_loss | -0.000435     |
|    std                  | 686           |
|    value_loss           | 2.57          |
-------------------------------------------
Iteration: 6512 | Episodes: 264100 | Median Reward: 18.64 | Max Reward: 48.88
Iteration: 6514 | Episodes: 264200 | Median Reward: 20.98 | Max Reward: 48.88
Iteration: 6517 | Episodes: 264300 | Median Reward: 15.12 | Max Reward: 48.88
Iteration: 6519 | Episodes: 264400 | Median Reward: 18.02 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.7         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6520          |
|    time_elapsed         | 51253         |
|    total_timesteps      | 26705920      |
| train/                  |               |
|    approx_kl            | 3.9176026e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -229          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -11           |
|    n_updates            | 65190         |
|    policy_gradient_loss | -0.000371     |
|    std                  | 694           |
|    value_loss           | 1.51          |
-------------------------------------------
Iteration: 6522 | Episodes: 264500 | Median Reward: 16.75 | Max Reward: 48.88
Iteration: 6524 | Episodes: 264600 | Median Reward: 20.71 | Max Reward: 48.88
Iteration: 6527 | Episodes: 264700 | Median Reward: 22.72 | Max Reward: 48.88
Iteration: 6529 | Episodes: 264800 | Median Reward: 12.73 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.2         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6530          |
|    time_elapsed         | 51330         |
|    total_timesteps      | 26746880      |
| train/                  |               |
|    approx_kl            | 7.1337665e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -230          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.4         |
|    n_updates            | 65290         |
|    policy_gradient_loss | -0.000961     |
|    std                  | 698           |
|    value_loss           | 2.3           |
-------------------------------------------
Iteration: 6531 | Episodes: 264900 | Median Reward: 11.85 | Max Reward: 48.88
Iteration: 6534 | Episodes: 265000 | Median Reward: 16.57 | Max Reward: 48.88
Iteration: 6536 | Episodes: 265100 | Median Reward: 16.48 | Max Reward: 48.88
Iteration: 6539 | Episodes: 265200 | Median Reward: 15.38 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88.8         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6540          |
|    time_elapsed         | 51407         |
|    total_timesteps      | 26787840      |
| train/                  |               |
|    approx_kl            | 0.00027753666 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -230          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.3         |
|    n_updates            | 65390         |
|    policy_gradient_loss | -0.00137      |
|    std                  | 707           |
|    value_loss           | 0.699         |
-------------------------------------------
Iteration: 6541 | Episodes: 265300 | Median Reward: 14.14 | Max Reward: 48.88
Iteration: 6544 | Episodes: 265400 | Median Reward: 26.33 | Max Reward: 48.88
Iteration: 6546 | Episodes: 265500 | Median Reward: 8.59 | Max Reward: 48.88
Iteration: 6549 | Episodes: 265600 | Median Reward: 18.79 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -79.9         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6550          |
|    time_elapsed         | 51495         |
|    total_timesteps      | 26828800      |
| train/                  |               |
|    approx_kl            | 0.00017685353 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -230          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.8         |
|    n_updates            | 65490         |
|    policy_gradient_loss | -0.000862     |
|    std                  | 716           |
|    value_loss           | 1.6           |
-------------------------------------------
Iteration: 6551 | Episodes: 265700 | Median Reward: 11.26 | Max Reward: 48.88
Iteration: 6554 | Episodes: 265800 | Median Reward: 20.62 | Max Reward: 48.88
Iteration: 6556 | Episodes: 265900 | Median Reward: 20.63 | Max Reward: 48.88
Iteration: 6559 | Episodes: 266000 | Median Reward: 14.64 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.8         |
| time/                   |               |
|    fps                  | 521           |
|    iterations           | 6560          |
|    time_elapsed         | 51569         |
|    total_timesteps      | 26869760      |
| train/                  |               |
|    approx_kl            | 0.00097560824 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -231          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.4         |
|    n_updates            | 65590         |
|    policy_gradient_loss | -0.0056       |
|    std                  | 726           |
|    value_loss           | 0.419         |
-------------------------------------------
Iteration: 6561 | Episodes: 266100 | Median Reward: 11.88 | Max Reward: 48.88
Iteration: 6564 | Episodes: 266200 | Median Reward: 25.07 | Max Reward: 48.88
Iteration: 6566 | Episodes: 266300 | Median Reward: 17.92 | Max Reward: 48.88
Iteration: 6568 | Episodes: 266400 | Median Reward: 12.20 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86           |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6570          |
|    time_elapsed         | 51656         |
|    total_timesteps      | 26910720      |
| train/                  |               |
|    approx_kl            | 0.00029307476 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -231          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.3         |
|    n_updates            | 65690         |
|    policy_gradient_loss | -0.00211      |
|    std                  | 735           |
|    value_loss           | 0.67          |
-------------------------------------------
Iteration: 6571 | Episodes: 266500 | Median Reward: 15.58 | Max Reward: 48.88
Iteration: 6573 | Episodes: 266600 | Median Reward: 15.10 | Max Reward: 48.88
Iteration: 6576 | Episodes: 266700 | Median Reward: 15.29 | Max Reward: 48.88
Iteration: 6578 | Episodes: 266800 | Median Reward: 13.54 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -86.1        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6580         |
|    time_elapsed         | 51735        |
|    total_timesteps      | 26951680     |
| train/                  |              |
|    approx_kl            | 0.0007156951 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -231         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.8        |
|    n_updates            | 65790        |
|    policy_gradient_loss | -0.00291     |
|    std                  | 743          |
|    value_loss           | 2.34         |
------------------------------------------
Iteration: 6581 | Episodes: 266900 | Median Reward: 12.52 | Max Reward: 48.88
Iteration: 6583 | Episodes: 267000 | Median Reward: 13.24 | Max Reward: 48.88
Iteration: 6586 | Episodes: 267100 | Median Reward: 19.36 | Max Reward: 48.88
Iteration: 6588 | Episodes: 267200 | Median Reward: 21.76 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.8       |
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 6590        |
|    time_elapsed         | 51813       |
|    total_timesteps      | 26992640    |
| train/                  |             |
|    approx_kl            | 0.002141831 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -231        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.3       |
|    n_updates            | 65890       |
|    policy_gradient_loss | -0.0108     |
|    std                  | 752         |
|    value_loss           | 0.814       |
-----------------------------------------
Iteration: 6591 | Episodes: 267300 | Median Reward: 15.61 | Max Reward: 48.88
Iteration: 6593 | Episodes: 267400 | Median Reward: 26.34 | Max Reward: 48.88
Iteration: 6596 | Episodes: 267500 | Median Reward: 18.79 | Max Reward: 48.88
Iteration: 6598 | Episodes: 267600 | Median Reward: 22.58 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.1         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6600          |
|    time_elapsed         | 51903         |
|    total_timesteps      | 27033600      |
| train/                  |               |
|    approx_kl            | 0.00013440185 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -232          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.3         |
|    n_updates            | 65990         |
|    policy_gradient_loss | -0.000954     |
|    std                  | 763           |
|    value_loss           | 0.783         |
-------------------------------------------
Iteration: 6601 | Episodes: 267700 | Median Reward: 16.32 | Max Reward: 48.88
Iteration: 6603 | Episodes: 267800 | Median Reward: 17.54 | Max Reward: 48.88
Iteration: 6605 | Episodes: 267900 | Median Reward: 12.82 | Max Reward: 48.88
Iteration: 6608 | Episodes: 268000 | Median Reward: 16.57 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.9         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6610          |
|    time_elapsed         | 51979         |
|    total_timesteps      | 27074560      |
| train/                  |               |
|    approx_kl            | 5.9806436e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -232          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.7         |
|    n_updates            | 66090         |
|    policy_gradient_loss | -0.000509     |
|    std                  | 769           |
|    value_loss           | 1.75          |
-------------------------------------------
Iteration: 6610 | Episodes: 268100 | Median Reward: 16.02 | Max Reward: 48.88
Iteration: 6613 | Episodes: 268200 | Median Reward: 19.29 | Max Reward: 48.88
Iteration: 6615 | Episodes: 268300 | Median Reward: 16.48 | Max Reward: 48.88
Iteration: 6618 | Episodes: 268400 | Median Reward: 15.77 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.8        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6620         |
|    time_elapsed         | 52062        |
|    total_timesteps      | 27115520     |
| train/                  |              |
|    approx_kl            | 4.556129e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -232         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.2        |
|    n_updates            | 66190        |
|    policy_gradient_loss | -4.36e-05    |
|    std                  | 771          |
|    value_loss           | 0.936        |
------------------------------------------
Iteration: 6620 | Episodes: 268500 | Median Reward: 10.05 | Max Reward: 48.88
Iteration: 6623 | Episodes: 268600 | Median Reward: 20.37 | Max Reward: 48.88
Iteration: 6625 | Episodes: 268700 | Median Reward: 17.80 | Max Reward: 48.88
Iteration: 6628 | Episodes: 268800 | Median Reward: 20.34 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.1        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6630         |
|    time_elapsed         | 52142        |
|    total_timesteps      | 27156480     |
| train/                  |              |
|    approx_kl            | 8.574518e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -232         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.1        |
|    n_updates            | 66290        |
|    policy_gradient_loss | -0.000473    |
|    std                  | 778          |
|    value_loss           | 1.23         |
------------------------------------------
Iteration: 6630 | Episodes: 268900 | Median Reward: 18.76 | Max Reward: 48.88
Iteration: 6633 | Episodes: 269000 | Median Reward: 14.72 | Max Reward: 48.88
Iteration: 6635 | Episodes: 269100 | Median Reward: 19.21 | Max Reward: 48.88
Iteration: 6637 | Episodes: 269200 | Median Reward: 16.26 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82           |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6640          |
|    time_elapsed         | 52216         |
|    total_timesteps      | 27197440      |
| train/                  |               |
|    approx_kl            | 0.00013995933 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -233          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.3         |
|    n_updates            | 66390         |
|    policy_gradient_loss | -0.000972     |
|    std                  | 786           |
|    value_loss           | 1.08          |
-------------------------------------------
Iteration: 6640 | Episodes: 269300 | Median Reward: 20.02 | Max Reward: 48.88
Iteration: 6642 | Episodes: 269400 | Median Reward: 20.80 | Max Reward: 48.88
Iteration: 6645 | Episodes: 269500 | Median Reward: 26.92 | Max Reward: 48.88
Iteration: 6647 | Episodes: 269600 | Median Reward: 24.11 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.8         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6650          |
|    time_elapsed         | 52303         |
|    total_timesteps      | 27238400      |
| train/                  |               |
|    approx_kl            | 0.00050109497 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -233          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11           |
|    n_updates            | 66490         |
|    policy_gradient_loss | -0.0031       |
|    std                  | 794           |
|    value_loss           | 1.59          |
-------------------------------------------
Iteration: 6650 | Episodes: 269700 | Median Reward: 18.46 | Max Reward: 48.88
Iteration: 6652 | Episodes: 269800 | Median Reward: 10.14 | Max Reward: 48.88
Iteration: 6655 | Episodes: 269900 | Median Reward: 11.11 | Max Reward: 48.88
Iteration: 6657 | Episodes: 270000 | Median Reward: 15.02 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -86.1       |
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 6660        |
|    time_elapsed         | 52378       |
|    total_timesteps      | 27279360    |
| train/                  |             |
|    approx_kl            | 8.42243e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -233        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.9       |
|    n_updates            | 66590       |
|    policy_gradient_loss | -0.000495   |
|    std                  | 801         |
|    value_loss           | 2           |
-----------------------------------------
Iteration: 6660 | Episodes: 270100 | Median Reward: 18.05 | Max Reward: 48.88
Iteration: 6662 | Episodes: 270200 | Median Reward: 14.41 | Max Reward: 48.88
Iteration: 6665 | Episodes: 270300 | Median Reward: 16.70 | Max Reward: 48.88
Iteration: 6667 | Episodes: 270400 | Median Reward: 22.47 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.7        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6670         |
|    time_elapsed         | 52461        |
|    total_timesteps      | 27320320     |
| train/                  |              |
|    approx_kl            | 0.0018421758 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -233         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.5        |
|    n_updates            | 66690        |
|    policy_gradient_loss | -0.00345     |
|    std                  | 810          |
|    value_loss           | 0.623        |
------------------------------------------
Iteration: 6670 | Episodes: 270500 | Median Reward: 16.11 | Max Reward: 48.88
Iteration: 6672 | Episodes: 270600 | Median Reward: 20.62 | Max Reward: 48.88
Iteration: 6674 | Episodes: 270700 | Median Reward: 20.82 | Max Reward: 48.88
Iteration: 6677 | Episodes: 270800 | Median Reward: 21.51 | Max Reward: 48.88
Iteration: 6679 | Episodes: 270900 | Median Reward: 18.87 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.7        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6680         |
|    time_elapsed         | 52542        |
|    total_timesteps      | 27361280     |
| train/                  |              |
|    approx_kl            | 0.0016417656 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -234         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.1        |
|    n_updates            | 66790        |
|    policy_gradient_loss | -0.0064      |
|    std                  | 816          |
|    value_loss           | 1.94         |
------------------------------------------
Iteration: 6682 | Episodes: 271000 | Median Reward: 21.84 | Max Reward: 48.88
Iteration: 6684 | Episodes: 271100 | Median Reward: 14.96 | Max Reward: 48.88
Iteration: 6687 | Episodes: 271200 | Median Reward: 9.62 | Max Reward: 48.88
Iteration: 6689 | Episodes: 271300 | Median Reward: 6.50 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.3         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6690          |
|    time_elapsed         | 52617         |
|    total_timesteps      | 27402240      |
| train/                  |               |
|    approx_kl            | 0.00018468112 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -234          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.3         |
|    n_updates            | 66890         |
|    policy_gradient_loss | -0.00227      |
|    std                  | 816           |
|    value_loss           | 1.18          |
-------------------------------------------
Iteration: 6692 | Episodes: 271400 | Median Reward: 14.12 | Max Reward: 48.88
Iteration: 6694 | Episodes: 271500 | Median Reward: 20.31 | Max Reward: 48.88
Iteration: 6697 | Episodes: 271600 | Median Reward: 20.93 | Max Reward: 48.88
Iteration: 6699 | Episodes: 271700 | Median Reward: 18.94 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.8         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6700          |
|    time_elapsed         | 52705         |
|    total_timesteps      | 27443200      |
| train/                  |               |
|    approx_kl            | 0.00075111154 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -234          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.3         |
|    n_updates            | 66990         |
|    policy_gradient_loss | -0.00131      |
|    std                  | 821           |
|    value_loss           | 0.778         |
-------------------------------------------
Iteration: 6702 | Episodes: 271800 | Median Reward: 18.16 | Max Reward: 48.88
Iteration: 6704 | Episodes: 271900 | Median Reward: 15.96 | Max Reward: 48.88
Iteration: 6707 | Episodes: 272000 | Median Reward: 11.33 | Max Reward: 48.88
Iteration: 6709 | Episodes: 272100 | Median Reward: 18.79 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.9         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6710          |
|    time_elapsed         | 52781         |
|    total_timesteps      | 27484160      |
| train/                  |               |
|    approx_kl            | 3.4581957e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -234          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -11           |
|    n_updates            | 67090         |
|    policy_gradient_loss | 0.000529      |
|    std                  | 827           |
|    value_loss           | 1.57          |
-------------------------------------------
Iteration: 6711 | Episodes: 272200 | Median Reward: 18.33 | Max Reward: 48.88
Iteration: 6714 | Episodes: 272300 | Median Reward: 13.73 | Max Reward: 48.88
Iteration: 6716 | Episodes: 272400 | Median Reward: 18.20 | Max Reward: 48.88
Iteration: 6719 | Episodes: 272500 | Median Reward: 14.37 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.6         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6720          |
|    time_elapsed         | 52863         |
|    total_timesteps      | 27525120      |
| train/                  |               |
|    approx_kl            | 0.00036284825 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -234          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.3         |
|    n_updates            | 67190         |
|    policy_gradient_loss | -0.00262      |
|    std                  | 838           |
|    value_loss           | 1.24          |
-------------------------------------------
Iteration: 6721 | Episodes: 272600 | Median Reward: 16.44 | Max Reward: 48.88
Iteration: 6724 | Episodes: 272700 | Median Reward: 20.30 | Max Reward: 48.88
Iteration: 6726 | Episodes: 272800 | Median Reward: 9.32 | Max Reward: 48.88
Iteration: 6729 | Episodes: 272900 | Median Reward: 6.90 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.3        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6730         |
|    time_elapsed         | 52946        |
|    total_timesteps      | 27566080     |
| train/                  |              |
|    approx_kl            | 7.901987e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -235         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.2        |
|    n_updates            | 67290        |
|    policy_gradient_loss | -2.96e-05    |
|    std                  | 847          |
|    value_loss           | 1.43         |
------------------------------------------
Iteration: 6731 | Episodes: 273000 | Median Reward: 14.39 | Max Reward: 48.88
Iteration: 6734 | Episodes: 273100 | Median Reward: 7.17 | Max Reward: 48.88
Iteration: 6736 | Episodes: 273200 | Median Reward: 22.71 | Max Reward: 48.88
Iteration: 6739 | Episodes: 273300 | Median Reward: 20.42 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.4         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6740          |
|    time_elapsed         | 53021         |
|    total_timesteps      | 27607040      |
| train/                  |               |
|    approx_kl            | 0.00023227959 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -235          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.4         |
|    n_updates            | 67390         |
|    policy_gradient_loss | -0.00204      |
|    std                  | 851           |
|    value_loss           | 1.14          |
-------------------------------------------
Iteration: 6741 | Episodes: 273400 | Median Reward: 17.79 | Max Reward: 48.88
Iteration: 6744 | Episodes: 273500 | Median Reward: 17.71 | Max Reward: 48.88
Iteration: 6746 | Episodes: 273600 | Median Reward: 9.96 | Max Reward: 48.88
Iteration: 6748 | Episodes: 273700 | Median Reward: 13.17 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.8         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6750          |
|    time_elapsed         | 53111         |
|    total_timesteps      | 27648000      |
| train/                  |               |
|    approx_kl            | 9.8800374e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -235          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.5         |
|    n_updates            | 67490         |
|    policy_gradient_loss | -0.000952     |
|    std                  | 858           |
|    value_loss           | 0.956         |
-------------------------------------------
Iteration: 6751 | Episodes: 273800 | Median Reward: 19.32 | Max Reward: 48.88
Iteration: 6753 | Episodes: 273900 | Median Reward: 17.85 | Max Reward: 48.88
Iteration: 6756 | Episodes: 274000 | Median Reward: 21.75 | Max Reward: 48.88
Iteration: 6758 | Episodes: 274100 | Median Reward: 16.81 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.7         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6760          |
|    time_elapsed         | 53187         |
|    total_timesteps      | 27688960      |
| train/                  |               |
|    approx_kl            | 8.8691755e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -235          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.4         |
|    n_updates            | 67590         |
|    policy_gradient_loss | -0.000887     |
|    std                  | 860           |
|    value_loss           | 1.01          |
-------------------------------------------
Iteration: 6761 | Episodes: 274200 | Median Reward: 15.59 | Max Reward: 48.88
Iteration: 6763 | Episodes: 274300 | Median Reward: 12.40 | Max Reward: 48.88
Iteration: 6766 | Episodes: 274400 | Median Reward: 20.21 | Max Reward: 48.88
Iteration: 6768 | Episodes: 274500 | Median Reward: 16.60 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -86.8          |
| time/                   |                |
|    fps                  | 520            |
|    iterations           | 6770           |
|    time_elapsed         | 53268          |
|    total_timesteps      | 27729920       |
| train/                  |                |
|    approx_kl            | 0.000108118984 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -235           |
|    explained_variance   | 0.997          |
|    learning_rate        | 0.0005         |
|    loss                 | -11.3          |
|    n_updates            | 67690          |
|    policy_gradient_loss | -0.000764      |
|    std                  | 867            |
|    value_loss           | 1.22           |
--------------------------------------------
Iteration: 6771 | Episodes: 274600 | Median Reward: 13.85 | Max Reward: 48.88
Iteration: 6773 | Episodes: 274700 | Median Reward: 9.42 | Max Reward: 48.88
Iteration: 6776 | Episodes: 274800 | Median Reward: 22.81 | Max Reward: 48.88
Iteration: 6778 | Episodes: 274900 | Median Reward: 20.67 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.9         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6780          |
|    time_elapsed         | 53350         |
|    total_timesteps      | 27770880      |
| train/                  |               |
|    approx_kl            | 0.00017646074 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -236          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.6         |
|    n_updates            | 67790         |
|    policy_gradient_loss | -0.000821     |
|    std                  | 876           |
|    value_loss           | 0.601         |
-------------------------------------------
Iteration: 6781 | Episodes: 275000 | Median Reward: 24.25 | Max Reward: 48.88
Iteration: 6783 | Episodes: 275100 | Median Reward: 13.28 | Max Reward: 48.88
Iteration: 6785 | Episodes: 275200 | Median Reward: 16.33 | Max Reward: 48.88
Iteration: 6788 | Episodes: 275300 | Median Reward: 15.68 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.9        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6790         |
|    time_elapsed         | 53426        |
|    total_timesteps      | 27811840     |
| train/                  |              |
|    approx_kl            | 0.0001402048 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -236         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.3        |
|    n_updates            | 67890        |
|    policy_gradient_loss | -0.000337    |
|    std                  | 885          |
|    value_loss           | 1.19         |
------------------------------------------
Iteration: 6790 | Episodes: 275400 | Median Reward: 17.17 | Max Reward: 48.88
Iteration: 6793 | Episodes: 275500 | Median Reward: 20.06 | Max Reward: 48.88
Iteration: 6795 | Episodes: 275600 | Median Reward: 4.30 | Max Reward: 48.88
Iteration: 6798 | Episodes: 275700 | Median Reward: 15.20 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.8        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6800         |
|    time_elapsed         | 53514        |
|    total_timesteps      | 27852800     |
| train/                  |              |
|    approx_kl            | 0.0012558106 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -236         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.3        |
|    n_updates            | 67990        |
|    policy_gradient_loss | -0.00614     |
|    std                  | 898          |
|    value_loss           | 1.49         |
------------------------------------------
Iteration: 6800 | Episodes: 275800 | Median Reward: 22.48 | Max Reward: 48.88
Iteration: 6803 | Episodes: 275900 | Median Reward: 16.75 | Max Reward: 48.88
Iteration: 6805 | Episodes: 276000 | Median Reward: 24.58 | Max Reward: 48.88
Iteration: 6808 | Episodes: 276100 | Median Reward: 18.30 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84           |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6810          |
|    time_elapsed         | 53591         |
|    total_timesteps      | 27893760      |
| train/                  |               |
|    approx_kl            | 0.00014472922 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -236          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.1         |
|    n_updates            | 68090         |
|    policy_gradient_loss | -3.62e-05     |
|    std                  | 907           |
|    value_loss           | 2.03          |
-------------------------------------------
Iteration: 6810 | Episodes: 276200 | Median Reward: 15.55 | Max Reward: 48.88
Iteration: 6813 | Episodes: 276300 | Median Reward: 14.93 | Max Reward: 48.88
Iteration: 6815 | Episodes: 276400 | Median Reward: 12.21 | Max Reward: 48.88
Iteration: 6817 | Episodes: 276500 | Median Reward: 12.17 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.1        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6820         |
|    time_elapsed         | 53672        |
|    total_timesteps      | 27934720     |
| train/                  |              |
|    approx_kl            | 0.0019826728 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -237         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.2        |
|    n_updates            | 68190        |
|    policy_gradient_loss | -0.0082      |
|    std                  | 915          |
|    value_loss           | 1.55         |
------------------------------------------
Iteration: 6820 | Episodes: 276600 | Median Reward: 28.02 | Max Reward: 48.88
Iteration: 6822 | Episodes: 276700 | Median Reward: 19.99 | Max Reward: 48.88
Iteration: 6825 | Episodes: 276800 | Median Reward: 19.21 | Max Reward: 48.88
Iteration: 6827 | Episodes: 276900 | Median Reward: 20.70 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.5         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6830          |
|    time_elapsed         | 53756         |
|    total_timesteps      | 27975680      |
| train/                  |               |
|    approx_kl            | 4.7012174e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -237          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.5         |
|    n_updates            | 68290         |
|    policy_gradient_loss | 0.000327      |
|    std                  | 923           |
|    value_loss           | 0.924         |
-------------------------------------------
Iteration: 6830 | Episodes: 277000 | Median Reward: 16.94 | Max Reward: 48.88
Iteration: 6832 | Episodes: 277100 | Median Reward: 16.87 | Max Reward: 48.88
Iteration: 6835 | Episodes: 277200 | Median Reward: 13.49 | Max Reward: 48.88
Iteration: 6837 | Episodes: 277300 | Median Reward: 16.80 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.7        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6840         |
|    time_elapsed         | 53831        |
|    total_timesteps      | 28016640     |
| train/                  |              |
|    approx_kl            | 0.0002623031 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -237         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.3        |
|    n_updates            | 68390        |
|    policy_gradient_loss | -0.00116     |
|    std                  | 931          |
|    value_loss           | 3.59         |
------------------------------------------
Iteration: 6840 | Episodes: 277400 | Median Reward: 12.14 | Max Reward: 48.88
Iteration: 6842 | Episodes: 277500 | Median Reward: 18.99 | Max Reward: 48.88
Iteration: 6845 | Episodes: 277600 | Median Reward: 16.62 | Max Reward: 48.88
Iteration: 6847 | Episodes: 277700 | Median Reward: 24.01 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.5         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6850          |
|    time_elapsed         | 53919         |
|    total_timesteps      | 28057600      |
| train/                  |               |
|    approx_kl            | 0.00035309631 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -237          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.5         |
|    n_updates            | 68490         |
|    policy_gradient_loss | -0.00325      |
|    std                  | 938           |
|    value_loss           | 0.958         |
-------------------------------------------
Iteration: 6850 | Episodes: 277800 | Median Reward: 16.02 | Max Reward: 48.88
Iteration: 6852 | Episodes: 277900 | Median Reward: 18.96 | Max Reward: 48.88
Iteration: 6854 | Episodes: 278000 | Median Reward: 19.41 | Max Reward: 48.88
Iteration: 6857 | Episodes: 278100 | Median Reward: 17.35 | Max Reward: 48.88
Iteration: 6859 | Episodes: 278200 | Median Reward: 10.84 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88.6         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6860          |
|    time_elapsed         | 53995         |
|    total_timesteps      | 28098560      |
| train/                  |               |
|    approx_kl            | 0.00043713447 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -237          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.6         |
|    n_updates            | 68590         |
|    policy_gradient_loss | -0.00364      |
|    std                  | 944           |
|    value_loss           | 0.674         |
-------------------------------------------
Iteration: 6862 | Episodes: 278300 | Median Reward: 19.26 | Max Reward: 48.88
Iteration: 6864 | Episodes: 278400 | Median Reward: 15.14 | Max Reward: 48.88
Iteration: 6867 | Episodes: 278500 | Median Reward: 18.24 | Max Reward: 48.88
Iteration: 6869 | Episodes: 278600 | Median Reward: 11.79 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.2        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6870         |
|    time_elapsed         | 54075        |
|    total_timesteps      | 28139520     |
| train/                  |              |
|    approx_kl            | 4.658039e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -238         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.7        |
|    n_updates            | 68690        |
|    policy_gradient_loss | -0.00022     |
|    std                  | 955          |
|    value_loss           | 0.587        |
------------------------------------------
Iteration: 6872 | Episodes: 278700 | Median Reward: 18.44 | Max Reward: 48.88
Iteration: 6874 | Episodes: 278800 | Median Reward: 16.55 | Max Reward: 48.88
Iteration: 6877 | Episodes: 278900 | Median Reward: 15.84 | Max Reward: 48.88
Iteration: 6879 | Episodes: 279000 | Median Reward: 19.14 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -79.3         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6880          |
|    time_elapsed         | 54158         |
|    total_timesteps      | 28180480      |
| train/                  |               |
|    approx_kl            | 0.00011728557 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -238          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.6         |
|    n_updates            | 68790         |
|    policy_gradient_loss | -0.00123      |
|    std                  | 965           |
|    value_loss           | 0.719         |
-------------------------------------------
Iteration: 6882 | Episodes: 279100 | Median Reward: 11.46 | Max Reward: 48.88
Iteration: 6884 | Episodes: 279200 | Median Reward: 9.48 | Max Reward: 48.88
Iteration: 6887 | Episodes: 279300 | Median Reward: 26.98 | Max Reward: 48.88
Iteration: 6889 | Episodes: 279400 | Median Reward: 18.02 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -81.2        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6890         |
|    time_elapsed         | 54233        |
|    total_timesteps      | 28221440     |
| train/                  |              |
|    approx_kl            | 0.0001430609 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -238         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.4        |
|    n_updates            | 68890        |
|    policy_gradient_loss | -0.00156     |
|    std                  | 977          |
|    value_loss           | 0.968        |
------------------------------------------
Iteration: 6891 | Episodes: 279500 | Median Reward: 16.96 | Max Reward: 48.88
Iteration: 6894 | Episodes: 279600 | Median Reward: 22.12 | Max Reward: 48.88
Iteration: 6896 | Episodes: 279700 | Median Reward: 19.40 | Max Reward: 48.88
Iteration: 6899 | Episodes: 279800 | Median Reward: 11.69 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.5        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6900         |
|    time_elapsed         | 54321        |
|    total_timesteps      | 28262400     |
| train/                  |              |
|    approx_kl            | 0.0005385523 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -239         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.5        |
|    n_updates            | 68990        |
|    policy_gradient_loss | -0.00327     |
|    std                  | 986          |
|    value_loss           | 1.17         |
------------------------------------------
Iteration: 6901 | Episodes: 279900 | Median Reward: 14.82 | Max Reward: 48.88
Iteration: 6904 | Episodes: 280000 | Median Reward: 18.72 | Max Reward: 48.88
Iteration: 6906 | Episodes: 280100 | Median Reward: 17.79 | Max Reward: 48.88
Iteration: 6909 | Episodes: 280200 | Median Reward: 25.45 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.8        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6910         |
|    time_elapsed         | 54396        |
|    total_timesteps      | 28303360     |
| train/                  |              |
|    approx_kl            | 6.068981e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -239         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.7        |
|    n_updates            | 69090        |
|    policy_gradient_loss | -0.000807    |
|    std                  | 996          |
|    value_loss           | 0.735        |
------------------------------------------
Iteration: 6911 | Episodes: 280300 | Median Reward: 16.77 | Max Reward: 48.88
Iteration: 6914 | Episodes: 280400 | Median Reward: 18.71 | Max Reward: 48.88
Iteration: 6916 | Episodes: 280500 | Median Reward: 15.99 | Max Reward: 48.88
Iteration: 6919 | Episodes: 280600 | Median Reward: 16.02 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.5         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6920          |
|    time_elapsed         | 54478         |
|    total_timesteps      | 28344320      |
| train/                  |               |
|    approx_kl            | 0.00020118857 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -239          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.9         |
|    n_updates            | 69190         |
|    policy_gradient_loss | -0.00226      |
|    std                  | 1.01e+03      |
|    value_loss           | 2.74          |
-------------------------------------------
Iteration: 6921 | Episodes: 280700 | Median Reward: 15.22 | Max Reward: 48.88
Iteration: 6924 | Episodes: 280800 | Median Reward: 14.13 | Max Reward: 48.88
Iteration: 6926 | Episodes: 280900 | Median Reward: 17.87 | Max Reward: 48.88
Iteration: 6928 | Episodes: 281000 | Median Reward: 16.68 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.1         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6930          |
|    time_elapsed         | 54560         |
|    total_timesteps      | 28385280      |
| train/                  |               |
|    approx_kl            | 0.00018765146 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -239          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.7         |
|    n_updates            | 69290         |
|    policy_gradient_loss | -0.00263      |
|    std                  | 1.02e+03      |
|    value_loss           | 0.815         |
-------------------------------------------
Iteration: 6931 | Episodes: 281100 | Median Reward: 15.46 | Max Reward: 48.88
Iteration: 6933 | Episodes: 281200 | Median Reward: 17.68 | Max Reward: 48.88
Iteration: 6936 | Episodes: 281300 | Median Reward: 13.44 | Max Reward: 48.88
Iteration: 6938 | Episodes: 281400 | Median Reward: 15.81 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.6         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6940          |
|    time_elapsed         | 54637         |
|    total_timesteps      | 28426240      |
| train/                  |               |
|    approx_kl            | 1.4121208e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -240          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.5         |
|    n_updates            | 69390         |
|    policy_gradient_loss | -3.1e-07      |
|    std                  | 1.03e+03      |
|    value_loss           | 1.16          |
-------------------------------------------
Iteration: 6941 | Episodes: 281500 | Median Reward: 17.64 | Max Reward: 48.88
Iteration: 6943 | Episodes: 281600 | Median Reward: 15.95 | Max Reward: 48.88
Iteration: 6946 | Episodes: 281700 | Median Reward: 14.39 | Max Reward: 48.88
Iteration: 6948 | Episodes: 281800 | Median Reward: 13.67 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89.7         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6950          |
|    time_elapsed         | 54724         |
|    total_timesteps      | 28467200      |
| train/                  |               |
|    approx_kl            | 5.0487404e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -240          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.5         |
|    n_updates            | 69490         |
|    policy_gradient_loss | -0.000221     |
|    std                  | 1.05e+03      |
|    value_loss           | 1.23          |
-------------------------------------------
Iteration: 6951 | Episodes: 281900 | Median Reward: 15.88 | Max Reward: 48.88
Iteration: 6953 | Episodes: 282000 | Median Reward: 17.60 | Max Reward: 48.88
Iteration: 6956 | Episodes: 282100 | Median Reward: 18.55 | Max Reward: 48.88
Iteration: 6958 | Episodes: 282200 | Median Reward: 16.87 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.4        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 6960         |
|    time_elapsed         | 54802        |
|    total_timesteps      | 28508160     |
| train/                  |              |
|    approx_kl            | 0.0016026208 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -240         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.7        |
|    n_updates            | 69590        |
|    policy_gradient_loss | -0.00667     |
|    std                  | 1.06e+03     |
|    value_loss           | 0.833        |
------------------------------------------
Iteration: 6961 | Episodes: 282300 | Median Reward: 15.59 | Max Reward: 48.88
Iteration: 6963 | Episodes: 282400 | Median Reward: 5.20 | Max Reward: 48.88
Iteration: 6965 | Episodes: 282500 | Median Reward: 9.46 | Max Reward: 48.88
Iteration: 6968 | Episodes: 282600 | Median Reward: 20.62 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.2         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6970          |
|    time_elapsed         | 54883         |
|    total_timesteps      | 28549120      |
| train/                  |               |
|    approx_kl            | 0.00019938551 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -240          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.4         |
|    n_updates            | 69690         |
|    policy_gradient_loss | -0.0016       |
|    std                  | 1.08e+03      |
|    value_loss           | 1.54          |
-------------------------------------------
Iteration: 6970 | Episodes: 282700 | Median Reward: 11.48 | Max Reward: 48.88
Iteration: 6973 | Episodes: 282800 | Median Reward: 20.53 | Max Reward: 48.88
Iteration: 6975 | Episodes: 282900 | Median Reward: 16.95 | Max Reward: 48.88
Iteration: 6978 | Episodes: 283000 | Median Reward: 17.90 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.8         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6980          |
|    time_elapsed         | 54963         |
|    total_timesteps      | 28590080      |
| train/                  |               |
|    approx_kl            | 0.00084632676 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -241          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.7         |
|    n_updates            | 69790         |
|    policy_gradient_loss | -0.00447      |
|    std                  | 1.08e+03      |
|    value_loss           | 0.872         |
-------------------------------------------
Iteration: 6980 | Episodes: 283100 | Median Reward: 19.61 | Max Reward: 48.88
Iteration: 6983 | Episodes: 283200 | Median Reward: 9.70 | Max Reward: 48.88
Iteration: 6985 | Episodes: 283300 | Median Reward: 15.39 | Max Reward: 48.88
Iteration: 6988 | Episodes: 283400 | Median Reward: 12.12 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.7         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 6990          |
|    time_elapsed         | 55037         |
|    total_timesteps      | 28631040      |
| train/                  |               |
|    approx_kl            | 0.00015791244 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -241          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.2         |
|    n_updates            | 69890         |
|    policy_gradient_loss | -0.000638     |
|    std                  | 1.1e+03       |
|    value_loss           | 2.18          |
-------------------------------------------
Iteration: 6990 | Episodes: 283500 | Median Reward: 18.87 | Max Reward: 48.88
Iteration: 6993 | Episodes: 283600 | Median Reward: 10.18 | Max Reward: 48.88
Iteration: 6995 | Episodes: 283700 | Median Reward: 15.05 | Max Reward: 48.88
Iteration: 6997 | Episodes: 283800 | Median Reward: 12.84 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88          |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 7000         |
|    time_elapsed         | 55122        |
|    total_timesteps      | 28672000     |
| train/                  |              |
|    approx_kl            | 6.168462e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -241         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.4        |
|    n_updates            | 69990        |
|    policy_gradient_loss | -0.000384    |
|    std                  | 1.11e+03     |
|    value_loss           | 1.69         |
------------------------------------------
Iteration: 7000 | Episodes: 283900 | Median Reward: 16.89 | Max Reward: 48.88
Iteration: 7002 | Episodes: 284000 | Median Reward: 25.03 | Max Reward: 48.88
Iteration: 7005 | Episodes: 284100 | Median Reward: 12.89 | Max Reward: 48.88
Iteration: 7007 | Episodes: 284200 | Median Reward: 17.95 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -84.8       |
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 7010        |
|    time_elapsed         | 55197       |
|    total_timesteps      | 28712960    |
| train/                  |             |
|    approx_kl            | 0.002028809 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -241        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.8       |
|    n_updates            | 70090       |
|    policy_gradient_loss | -0.00619    |
|    std                  | 1.11e+03    |
|    value_loss           | 0.788       |
-----------------------------------------
Iteration: 7010 | Episodes: 284300 | Median Reward: 14.19 | Max Reward: 48.88
Iteration: 7012 | Episodes: 284400 | Median Reward: 11.33 | Max Reward: 48.88
Iteration: 7015 | Episodes: 284500 | Median Reward: 10.08 | Max Reward: 48.88
Iteration: 7017 | Episodes: 284600 | Median Reward: 16.99 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.9         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 7020          |
|    time_elapsed         | 55280         |
|    total_timesteps      | 28753920      |
| train/                  |               |
|    approx_kl            | 0.00046015703 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -241          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.4         |
|    n_updates            | 70190         |
|    policy_gradient_loss | -0.00283      |
|    std                  | 1.11e+03      |
|    value_loss           | 1.63          |
-------------------------------------------
Iteration: 7020 | Episodes: 284700 | Median Reward: 13.41 | Max Reward: 48.88
Iteration: 7022 | Episodes: 284800 | Median Reward: 23.09 | Max Reward: 48.88
Iteration: 7025 | Episodes: 284900 | Median Reward: 19.20 | Max Reward: 48.88
Iteration: 7027 | Episodes: 285000 | Median Reward: 12.62 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.6         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 7030          |
|    time_elapsed         | 55358         |
|    total_timesteps      | 28794880      |
| train/                  |               |
|    approx_kl            | 0.00023173299 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -242          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.7         |
|    n_updates            | 70290         |
|    policy_gradient_loss | -0.00111      |
|    std                  | 1.13e+03      |
|    value_loss           | 1.11          |
-------------------------------------------
Iteration: 7030 | Episodes: 285100 | Median Reward: 22.44 | Max Reward: 48.88
Iteration: 7032 | Episodes: 285200 | Median Reward: 10.00 | Max Reward: 48.88
Iteration: 7034 | Episodes: 285300 | Median Reward: 3.36 | Max Reward: 48.88
Iteration: 7037 | Episodes: 285400 | Median Reward: 10.85 | Max Reward: 48.88
Iteration: 7039 | Episodes: 285500 | Median Reward: 12.67 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -85.7       |
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 7040        |
|    time_elapsed         | 55434       |
|    total_timesteps      | 28835840    |
| train/                  |             |
|    approx_kl            | 0.007477347 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -242        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.8       |
|    n_updates            | 70390       |
|    policy_gradient_loss | -0.02       |
|    std                  | 1.13e+03    |
|    value_loss           | 1.04        |
-----------------------------------------
Iteration: 7042 | Episodes: 285600 | Median Reward: 18.86 | Max Reward: 48.88
Iteration: 7044 | Episodes: 285700 | Median Reward: 15.71 | Max Reward: 48.88
Iteration: 7047 | Episodes: 285800 | Median Reward: 11.72 | Max Reward: 48.88
Iteration: 7049 | Episodes: 285900 | Median Reward: 17.46 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.5         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 7050          |
|    time_elapsed         | 55524         |
|    total_timesteps      | 28876800      |
| train/                  |               |
|    approx_kl            | 0.00018708436 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -242          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.4         |
|    n_updates            | 70490         |
|    policy_gradient_loss | -0.000672     |
|    std                  | 1.15e+03      |
|    value_loss           | 1.64          |
-------------------------------------------
Iteration: 7052 | Episodes: 286000 | Median Reward: 11.75 | Max Reward: 48.88
Iteration: 7054 | Episodes: 286100 | Median Reward: 15.84 | Max Reward: 48.88
Iteration: 7057 | Episodes: 286200 | Median Reward: 16.38 | Max Reward: 48.88
Iteration: 7059 | Episodes: 286300 | Median Reward: 15.54 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.2        |
| time/                   |              |
|    fps                  | 520          |
|    iterations           | 7060         |
|    time_elapsed         | 55598        |
|    total_timesteps      | 28917760     |
| train/                  |              |
|    approx_kl            | 8.444561e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -242         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.8        |
|    n_updates            | 70590        |
|    policy_gradient_loss | -0.000529    |
|    std                  | 1.16e+03     |
|    value_loss           | 1.01         |
------------------------------------------
Iteration: 7062 | Episodes: 286400 | Median Reward: 9.41 | Max Reward: 48.88
Iteration: 7064 | Episodes: 286500 | Median Reward: 15.11 | Max Reward: 48.88
Iteration: 7067 | Episodes: 286600 | Median Reward: 19.87 | Max Reward: 48.88
Iteration: 7069 | Episodes: 286700 | Median Reward: 13.98 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.5         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 7070          |
|    time_elapsed         | 55684         |
|    total_timesteps      | 28958720      |
| train/                  |               |
|    approx_kl            | 1.5934434e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -243          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.5         |
|    n_updates            | 70690         |
|    policy_gradient_loss | 3.81e-05      |
|    std                  | 1.17e+03      |
|    value_loss           | 1.46          |
-------------------------------------------
Iteration: 7071 | Episodes: 286800 | Median Reward: 17.28 | Max Reward: 48.88
Iteration: 7074 | Episodes: 286900 | Median Reward: 20.23 | Max Reward: 48.88
Iteration: 7076 | Episodes: 287000 | Median Reward: 18.68 | Max Reward: 48.88
Iteration: 7079 | Episodes: 287100 | Median Reward: 14.65 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -84.4       |
| time/                   |             |
|    fps                  | 520         |
|    iterations           | 7080        |
|    time_elapsed         | 55762       |
|    total_timesteps      | 28999680    |
| train/                  |             |
|    approx_kl            | 0.000533844 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -243        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.9       |
|    n_updates            | 70790       |
|    policy_gradient_loss | -0.00218    |
|    std                  | 1.19e+03    |
|    value_loss           | 0.684       |
-----------------------------------------
Iteration: 7081 | Episodes: 287200 | Median Reward: 15.55 | Max Reward: 48.88
Iteration: 7084 | Episodes: 287300 | Median Reward: 12.87 | Max Reward: 48.88
Iteration: 7086 | Episodes: 287400 | Median Reward: 20.51 | Max Reward: 48.88
Iteration: 7089 | Episodes: 287500 | Median Reward: 16.81 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.5         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 7090          |
|    time_elapsed         | 55836         |
|    total_timesteps      | 29040640      |
| train/                  |               |
|    approx_kl            | 1.1757453e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -243          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.8         |
|    n_updates            | 70890         |
|    policy_gradient_loss | -0.000175     |
|    std                  | 1.2e+03       |
|    value_loss           | 0.799         |
-------------------------------------------
Iteration: 7091 | Episodes: 287600 | Median Reward: 14.19 | Max Reward: 48.88
Iteration: 7094 | Episodes: 287700 | Median Reward: 18.13 | Max Reward: 48.88
Iteration: 7096 | Episodes: 287800 | Median Reward: 26.32 | Max Reward: 48.88
Iteration: 7099 | Episodes: 287900 | Median Reward: 21.09 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.2         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 7100          |
|    time_elapsed         | 55924         |
|    total_timesteps      | 29081600      |
| train/                  |               |
|    approx_kl            | 0.00020301866 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -243          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.6         |
|    n_updates            | 70990         |
|    policy_gradient_loss | -0.000214     |
|    std                  | 1.21e+03      |
|    value_loss           | 1.35          |
-------------------------------------------
Iteration: 7101 | Episodes: 288000 | Median Reward: 7.48 | Max Reward: 48.88
Iteration: 7104 | Episodes: 288100 | Median Reward: 13.51 | Max Reward: 48.88
Iteration: 7106 | Episodes: 288200 | Median Reward: 12.40 | Max Reward: 48.88
Iteration: 7108 | Episodes: 288300 | Median Reward: 23.81 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.9         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 7110          |
|    time_elapsed         | 55999         |
|    total_timesteps      | 29122560      |
| train/                  |               |
|    approx_kl            | 0.00019562157 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -244          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.5         |
|    n_updates            | 71090         |
|    policy_gradient_loss | -0.000529     |
|    std                  | 1.23e+03      |
|    value_loss           | 2.21          |
-------------------------------------------
Iteration: 7111 | Episodes: 288400 | Median Reward: 16.19 | Max Reward: 48.88
Iteration: 7113 | Episodes: 288500 | Median Reward: 12.25 | Max Reward: 48.88
Iteration: 7116 | Episodes: 288600 | Median Reward: 15.02 | Max Reward: 48.88
Iteration: 7118 | Episodes: 288700 | Median Reward: 17.66 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.2         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 7120          |
|    time_elapsed         | 56082         |
|    total_timesteps      | 29163520      |
| train/                  |               |
|    approx_kl            | 2.8841721e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -244          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.5         |
|    n_updates            | 71190         |
|    policy_gradient_loss | 0.000214      |
|    std                  | 1.23e+03      |
|    value_loss           | 1.93          |
-------------------------------------------
Iteration: 7121 | Episodes: 288800 | Median Reward: 17.32 | Max Reward: 48.88
Iteration: 7123 | Episodes: 288900 | Median Reward: 11.23 | Max Reward: 48.88
Iteration: 7126 | Episodes: 289000 | Median Reward: 12.54 | Max Reward: 48.88
Iteration: 7128 | Episodes: 289100 | Median Reward: 13.91 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83           |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 7130          |
|    time_elapsed         | 56158         |
|    total_timesteps      | 29204480      |
| train/                  |               |
|    approx_kl            | 1.7837287e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -244          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12           |
|    n_updates            | 71290         |
|    policy_gradient_loss | -0.000181     |
|    std                  | 1.24e+03      |
|    value_loss           | 0.495         |
-------------------------------------------
Iteration: 7131 | Episodes: 289200 | Median Reward: 20.79 | Max Reward: 48.88
Iteration: 7133 | Episodes: 289300 | Median Reward: 20.11 | Max Reward: 48.88
Iteration: 7136 | Episodes: 289400 | Median Reward: 19.72 | Max Reward: 48.88
Iteration: 7138 | Episodes: 289500 | Median Reward: 13.09 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.7         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 7140          |
|    time_elapsed         | 56235         |
|    total_timesteps      | 29245440      |
| train/                  |               |
|    approx_kl            | 0.00024902337 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -244          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.9         |
|    n_updates            | 71390         |
|    policy_gradient_loss | -0.000255     |
|    std                  | 1.26e+03      |
|    value_loss           | 0.877         |
-------------------------------------------
Iteration: 7141 | Episodes: 289600 | Median Reward: 18.17 | Max Reward: 48.88
Iteration: 7143 | Episodes: 289700 | Median Reward: 18.40 | Max Reward: 48.88
Iteration: 7145 | Episodes: 289800 | Median Reward: 5.25 | Max Reward: 48.88
Iteration: 7148 | Episodes: 289900 | Median Reward: 11.22 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.7        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7150         |
|    time_elapsed         | 56320        |
|    total_timesteps      | 29286400     |
| train/                  |              |
|    approx_kl            | 0.0005738343 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -245         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12          |
|    n_updates            | 71490        |
|    policy_gradient_loss | -0.00302     |
|    std                  | 1.27e+03     |
|    value_loss           | 0.555        |
------------------------------------------
Iteration: 7150 | Episodes: 290000 | Median Reward: 19.49 | Max Reward: 48.88
Iteration: 7153 | Episodes: 290100 | Median Reward: 14.64 | Max Reward: 48.88
Iteration: 7155 | Episodes: 290200 | Median Reward: 16.27 | Max Reward: 48.88
Iteration: 7158 | Episodes: 290300 | Median Reward: 20.82 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.9         |
| time/                   |               |
|    fps                  | 520           |
|    iterations           | 7160          |
|    time_elapsed         | 56396         |
|    total_timesteps      | 29327360      |
| train/                  |               |
|    approx_kl            | 4.6475005e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -245          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.8         |
|    n_updates            | 71590         |
|    policy_gradient_loss | -0.000458     |
|    std                  | 1.28e+03      |
|    value_loss           | 1.14          |
-------------------------------------------
Iteration: 7160 | Episodes: 290400 | Median Reward: 16.23 | Max Reward: 48.88
Iteration: 7163 | Episodes: 290500 | Median Reward: 18.95 | Max Reward: 48.88
Iteration: 7165 | Episodes: 290600 | Median Reward: 15.84 | Max Reward: 48.88
Iteration: 7168 | Episodes: 290700 | Median Reward: 13.02 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.1         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7170          |
|    time_elapsed         | 56481         |
|    total_timesteps      | 29368320      |
| train/                  |               |
|    approx_kl            | 4.3389562e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -245          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.7         |
|    n_updates            | 71690         |
|    policy_gradient_loss | -0.000452     |
|    std                  | 1.29e+03      |
|    value_loss           | 1.3           |
-------------------------------------------
Iteration: 7170 | Episodes: 290800 | Median Reward: 14.64 | Max Reward: 48.88
Iteration: 7173 | Episodes: 290900 | Median Reward: 14.23 | Max Reward: 48.88
Iteration: 7175 | Episodes: 291000 | Median Reward: 15.09 | Max Reward: 48.88
Iteration: 7178 | Episodes: 291100 | Median Reward: 20.55 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -86.1        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7180         |
|    time_elapsed         | 56556        |
|    total_timesteps      | 29409280     |
| train/                  |              |
|    approx_kl            | 0.0001607216 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -245         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.9        |
|    n_updates            | 71790        |
|    policy_gradient_loss | -0.000531    |
|    std                  | 1.3e+03      |
|    value_loss           | 1            |
------------------------------------------
Iteration: 7180 | Episodes: 291200 | Median Reward: 6.67 | Max Reward: 48.88
Iteration: 7182 | Episodes: 291300 | Median Reward: 16.67 | Max Reward: 48.88
Iteration: 7185 | Episodes: 291400 | Median Reward: 11.85 | Max Reward: 48.88
Iteration: 7187 | Episodes: 291500 | Median Reward: 18.01 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -93.6         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7190          |
|    time_elapsed         | 56637         |
|    total_timesteps      | 29450240      |
| train/                  |               |
|    approx_kl            | 0.00038999802 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -245          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.4         |
|    n_updates            | 71890         |
|    policy_gradient_loss | -0.00266      |
|    std                  | 1.31e+03      |
|    value_loss           | 2.07          |
-------------------------------------------
Iteration: 7190 | Episodes: 291600 | Median Reward: 11.89 | Max Reward: 48.88
Iteration: 7192 | Episodes: 291700 | Median Reward: 15.44 | Max Reward: 48.88
Iteration: 7195 | Episodes: 291800 | Median Reward: 18.79 | Max Reward: 48.88
Iteration: 7197 | Episodes: 291900 | Median Reward: 8.00 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88.6        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7200         |
|    time_elapsed         | 56718        |
|    total_timesteps      | 29491200     |
| train/                  |              |
|    approx_kl            | 0.0031583514 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -246         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.9        |
|    n_updates            | 71990        |
|    policy_gradient_loss | -0.00956     |
|    std                  | 1.32e+03     |
|    value_loss           | 1.01         |
------------------------------------------
Iteration: 7200 | Episodes: 292000 | Median Reward: 14.71 | Max Reward: 48.88
Iteration: 7202 | Episodes: 292100 | Median Reward: 11.95 | Max Reward: 48.88
Iteration: 7205 | Episodes: 292200 | Median Reward: 16.16 | Max Reward: 48.88
Iteration: 7207 | Episodes: 292300 | Median Reward: 21.89 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -82.7       |
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 7210        |
|    time_elapsed         | 56793       |
|    total_timesteps      | 29532160    |
| train/                  |             |
|    approx_kl            | 7.30624e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -246        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.8       |
|    n_updates            | 72090       |
|    policy_gradient_loss | -0.000706   |
|    std                  | 1.33e+03    |
|    value_loss           | 1.27        |
-----------------------------------------
Iteration: 7210 | Episodes: 292400 | Median Reward: 17.18 | Max Reward: 48.88
Iteration: 7212 | Episodes: 292500 | Median Reward: 17.03 | Max Reward: 48.88
Iteration: 7214 | Episodes: 292600 | Median Reward: 16.38 | Max Reward: 48.88
Iteration: 7217 | Episodes: 292700 | Median Reward: 11.10 | Max Reward: 48.88
Iteration: 7219 | Episodes: 292800 | Median Reward: 10.77 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.7         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7220          |
|    time_elapsed         | 56880         |
|    total_timesteps      | 29573120      |
| train/                  |               |
|    approx_kl            | 0.00024331965 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -246          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12           |
|    n_updates            | 72190         |
|    policy_gradient_loss | -0.00178      |
|    std                  | 1.35e+03      |
|    value_loss           | 0.722         |
-------------------------------------------
Iteration: 7222 | Episodes: 292900 | Median Reward: 14.81 | Max Reward: 48.88
Iteration: 7224 | Episodes: 293000 | Median Reward: 18.94 | Max Reward: 48.88
Iteration: 7227 | Episodes: 293100 | Median Reward: 7.25 | Max Reward: 48.88
Iteration: 7229 | Episodes: 293200 | Median Reward: 15.43 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.8         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7230          |
|    time_elapsed         | 56955         |
|    total_timesteps      | 29614080      |
| train/                  |               |
|    approx_kl            | 0.00015559388 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -246          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.6         |
|    n_updates            | 72290         |
|    policy_gradient_loss | -0.000245     |
|    std                  | 1.36e+03      |
|    value_loss           | 1.52          |
-------------------------------------------
Iteration: 7232 | Episodes: 293300 | Median Reward: 19.03 | Max Reward: 48.88
Iteration: 7234 | Episodes: 293400 | Median Reward: 16.71 | Max Reward: 48.88
Iteration: 7237 | Episodes: 293500 | Median Reward: 11.58 | Max Reward: 48.88
Iteration: 7239 | Episodes: 293600 | Median Reward: 15.72 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.3         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7240          |
|    time_elapsed         | 57038         |
|    total_timesteps      | 29655040      |
| train/                  |               |
|    approx_kl            | 0.00015247238 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -246          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.7         |
|    n_updates            | 72390         |
|    policy_gradient_loss | -0.00118      |
|    std                  | 1.37e+03      |
|    value_loss           | 1.75          |
-------------------------------------------
Iteration: 7242 | Episodes: 293700 | Median Reward: 8.49 | Max Reward: 48.88
Iteration: 7244 | Episodes: 293800 | Median Reward: 17.91 | Max Reward: 48.88
Iteration: 7247 | Episodes: 293900 | Median Reward: 19.81 | Max Reward: 48.88
Iteration: 7249 | Episodes: 294000 | Median Reward: 11.57 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88.8         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7250          |
|    time_elapsed         | 57117         |
|    total_timesteps      | 29696000      |
| train/                  |               |
|    approx_kl            | 4.8675472e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -247          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.9         |
|    n_updates            | 72490         |
|    policy_gradient_loss | -0.00026      |
|    std                  | 1.38e+03      |
|    value_loss           | 0.993         |
-------------------------------------------
Iteration: 7251 | Episodes: 294100 | Median Reward: 14.91 | Max Reward: 48.88
Iteration: 7254 | Episodes: 294200 | Median Reward: 13.74 | Max Reward: 48.88
Iteration: 7256 | Episodes: 294300 | Median Reward: 18.75 | Max Reward: 48.88
Iteration: 7259 | Episodes: 294400 | Median Reward: 15.03 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.8         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7260          |
|    time_elapsed         | 57193         |
|    total_timesteps      | 29736960      |
| train/                  |               |
|    approx_kl            | 0.00040592355 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -247          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.9         |
|    n_updates            | 72590         |
|    policy_gradient_loss | -0.00308      |
|    std                  | 1.4e+03       |
|    value_loss           | 1.32          |
-------------------------------------------
Iteration: 7261 | Episodes: 294500 | Median Reward: 14.87 | Max Reward: 48.88
Iteration: 7264 | Episodes: 294600 | Median Reward: 24.10 | Max Reward: 48.88
Iteration: 7266 | Episodes: 294700 | Median Reward: 11.05 | Max Reward: 48.88
Iteration: 7269 | Episodes: 294800 | Median Reward: 19.16 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.4         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7270          |
|    time_elapsed         | 57280         |
|    total_timesteps      | 29777920      |
| train/                  |               |
|    approx_kl            | 0.00048264913 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -247          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.8         |
|    n_updates            | 72690         |
|    policy_gradient_loss | -0.00468      |
|    std                  | 1.41e+03      |
|    value_loss           | 1.45          |
-------------------------------------------
Iteration: 7271 | Episodes: 294900 | Median Reward: 11.85 | Max Reward: 48.88
Iteration: 7274 | Episodes: 295000 | Median Reward: 18.03 | Max Reward: 48.88
Iteration: 7276 | Episodes: 295100 | Median Reward: 22.73 | Max Reward: 48.88
Iteration: 7279 | Episodes: 295200 | Median Reward: 20.84 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.3         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7280          |
|    time_elapsed         | 57353         |
|    total_timesteps      | 29818880      |
| train/                  |               |
|    approx_kl            | 0.00013392758 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -247          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.7         |
|    n_updates            | 72790         |
|    policy_gradient_loss | 8.6e-05       |
|    std                  | 1.42e+03      |
|    value_loss           | 1.72          |
-------------------------------------------
Iteration: 7281 | Episodes: 295300 | Median Reward: 14.71 | Max Reward: 48.88
Iteration: 7284 | Episodes: 295400 | Median Reward: 11.00 | Max Reward: 48.88
Iteration: 7286 | Episodes: 295500 | Median Reward: 23.16 | Max Reward: 48.88
Iteration: 7288 | Episodes: 295600 | Median Reward: 22.82 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.1         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7290          |
|    time_elapsed         | 57437         |
|    total_timesteps      | 29859840      |
| train/                  |               |
|    approx_kl            | 0.00011310386 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -248          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.3         |
|    n_updates            | 72890         |
|    policy_gradient_loss | -0.00163      |
|    std                  | 1.44e+03      |
|    value_loss           | 0.503         |
-------------------------------------------
Iteration: 7291 | Episodes: 295700 | Median Reward: 15.87 | Max Reward: 48.88
Iteration: 7293 | Episodes: 295800 | Median Reward: 10.91 | Max Reward: 48.88
Iteration: 7296 | Episodes: 295900 | Median Reward: 14.21 | Max Reward: 48.88
Iteration: 7298 | Episodes: 296000 | Median Reward: 17.98 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.9         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7300          |
|    time_elapsed         | 57516         |
|    total_timesteps      | 29900800      |
| train/                  |               |
|    approx_kl            | 0.00022303758 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -248          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12           |
|    n_updates            | 72990         |
|    policy_gradient_loss | -0.00223      |
|    std                  | 1.45e+03      |
|    value_loss           | 0.952         |
-------------------------------------------
Iteration: 7301 | Episodes: 296100 | Median Reward: 14.96 | Max Reward: 48.88
Iteration: 7303 | Episodes: 296200 | Median Reward: 16.85 | Max Reward: 48.88
Iteration: 7306 | Episodes: 296300 | Median Reward: 21.96 | Max Reward: 48.88
Iteration: 7308 | Episodes: 296400 | Median Reward: 7.17 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.2         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7310          |
|    time_elapsed         | 57591         |
|    total_timesteps      | 29941760      |
| train/                  |               |
|    approx_kl            | 0.00032466077 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -248          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.1         |
|    n_updates            | 73090         |
|    policy_gradient_loss | -0.000877     |
|    std                  | 1.46e+03      |
|    value_loss           | 0.636         |
-------------------------------------------
Iteration: 7311 | Episodes: 296500 | Median Reward: 14.89 | Max Reward: 48.88
Iteration: 7313 | Episodes: 296600 | Median Reward: 15.10 | Max Reward: 48.88
Iteration: 7316 | Episodes: 296700 | Median Reward: 17.95 | Max Reward: 48.88
Iteration: 7318 | Episodes: 296800 | Median Reward: 16.20 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.7         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7320          |
|    time_elapsed         | 57678         |
|    total_timesteps      | 29982720      |
| train/                  |               |
|    approx_kl            | 0.00014010086 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -248          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -12           |
|    n_updates            | 73190         |
|    policy_gradient_loss | -0.00168      |
|    std                  | 1.47e+03      |
|    value_loss           | 1.15          |
-------------------------------------------
Iteration: 7321 | Episodes: 296900 | Median Reward: 14.26 | Max Reward: 48.88
Iteration: 7323 | Episodes: 297000 | Median Reward: 21.21 | Max Reward: 48.88
Iteration: 7325 | Episodes: 297100 | Median Reward: 19.00 | Max Reward: 48.88
Iteration: 7328 | Episodes: 297200 | Median Reward: 11.97 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.2        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7330         |
|    time_elapsed         | 57754        |
|    total_timesteps      | 30023680     |
| train/                  |              |
|    approx_kl            | 0.0003182391 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -248         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.9        |
|    n_updates            | 73290        |
|    policy_gradient_loss | -0.00326     |
|    std                  | 1.48e+03     |
|    value_loss           | 1.41         |
------------------------------------------
Iteration: 7330 | Episodes: 297300 | Median Reward: 19.50 | Max Reward: 48.88
Iteration: 7333 | Episodes: 297400 | Median Reward: 20.13 | Max Reward: 48.88
Iteration: 7335 | Episodes: 297500 | Median Reward: 13.05 | Max Reward: 48.88
Iteration: 7338 | Episodes: 297600 | Median Reward: 14.05 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -88          |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7340         |
|    time_elapsed         | 57838        |
|    total_timesteps      | 30064640     |
| train/                  |              |
|    approx_kl            | 9.311567e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -248         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.1        |
|    n_updates            | 73390        |
|    policy_gradient_loss | -0.000195    |
|    std                  | 1.49e+03     |
|    value_loss           | 0.75         |
------------------------------------------
Iteration: 7340 | Episodes: 297700 | Median Reward: 12.06 | Max Reward: 48.88
Iteration: 7343 | Episodes: 297800 | Median Reward: 14.56 | Max Reward: 48.88
Iteration: 7345 | Episodes: 297900 | Median Reward: 14.36 | Max Reward: 48.88
Iteration: 7348 | Episodes: 298000 | Median Reward: 11.26 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.4        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7350         |
|    time_elapsed         | 57918        |
|    total_timesteps      | 30105600     |
| train/                  |              |
|    approx_kl            | 2.586667e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -249         |
|    explained_variance   | 0.854        |
|    learning_rate        | 0.0005       |
|    loss                 | 1.2          |
|    n_updates            | 73490        |
|    policy_gradient_loss | -0.000143    |
|    std                  | 1.51e+03     |
|    value_loss           | 64.3         |
------------------------------------------
Iteration: 7350 | Episodes: 298100 | Median Reward: 17.55 | Max Reward: 48.88
Iteration: 7353 | Episodes: 298200 | Median Reward: 20.54 | Max Reward: 48.88
Iteration: 7355 | Episodes: 298300 | Median Reward: 13.79 | Max Reward: 48.88
Iteration: 7358 | Episodes: 298400 | Median Reward: 14.22 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -79.8       |
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 7360        |
|    time_elapsed         | 57994       |
|    total_timesteps      | 30146560    |
| train/                  |             |
|    approx_kl            | 4.21633e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -249        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.8       |
|    n_updates            | 73590       |
|    policy_gradient_loss | -0.000351   |
|    std                  | 1.52e+03    |
|    value_loss           | 1.45        |
-----------------------------------------
Iteration: 7360 | Episodes: 298500 | Median Reward: 21.68 | Max Reward: 48.88
Iteration: 7362 | Episodes: 298600 | Median Reward: 23.43 | Max Reward: 48.88
Iteration: 7365 | Episodes: 298700 | Median Reward: 18.83 | Max Reward: 48.88
Iteration: 7367 | Episodes: 298800 | Median Reward: 21.67 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.1         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7370          |
|    time_elapsed         | 58081         |
|    total_timesteps      | 30187520      |
| train/                  |               |
|    approx_kl            | 0.00096108136 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -249          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.1         |
|    n_updates            | 73690         |
|    policy_gradient_loss | -0.00456      |
|    std                  | 1.53e+03      |
|    value_loss           | 0.844         |
-------------------------------------------
Iteration: 7370 | Episodes: 298900 | Median Reward: 8.28 | Max Reward: 48.88
Iteration: 7372 | Episodes: 299000 | Median Reward: 17.27 | Max Reward: 48.88
Iteration: 7375 | Episodes: 299100 | Median Reward: 22.44 | Max Reward: 48.88
Iteration: 7377 | Episodes: 299200 | Median Reward: 19.79 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83           |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7380          |
|    time_elapsed         | 58157         |
|    total_timesteps      | 30228480      |
| train/                  |               |
|    approx_kl            | 0.00042935062 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -249          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -12           |
|    n_updates            | 73790         |
|    policy_gradient_loss | -0.00222      |
|    std                  | 1.54e+03      |
|    value_loss           | 1.32          |
-------------------------------------------
Iteration: 7380 | Episodes: 299300 | Median Reward: 17.75 | Max Reward: 48.88
Iteration: 7382 | Episodes: 299400 | Median Reward: 15.30 | Max Reward: 48.88
Iteration: 7385 | Episodes: 299500 | Median Reward: 14.78 | Max Reward: 48.88
Iteration: 7387 | Episodes: 299600 | Median Reward: 12.96 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -90.3         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7390          |
|    time_elapsed         | 58240         |
|    total_timesteps      | 30269440      |
| train/                  |               |
|    approx_kl            | 0.00010628911 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -249          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.2         |
|    n_updates            | 73890         |
|    policy_gradient_loss | -0.00217      |
|    std                  | 1.55e+03      |
|    value_loss           | 0.901         |
-------------------------------------------
Iteration: 7390 | Episodes: 299700 | Median Reward: 7.45 | Max Reward: 48.88
Iteration: 7392 | Episodes: 299800 | Median Reward: 15.12 | Max Reward: 48.88
Iteration: 7394 | Episodes: 299900 | Median Reward: 20.09 | Max Reward: 48.88
Iteration: 7397 | Episodes: 300000 | Median Reward: 16.19 | Max Reward: 48.88
Iteration: 7399 | Episodes: 300100 | Median Reward: 10.16 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -92.8         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7400          |
|    time_elapsed         | 58318         |
|    total_timesteps      | 30310400      |
| train/                  |               |
|    approx_kl            | 0.00038519537 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -250          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.8         |
|    n_updates            | 73990         |
|    policy_gradient_loss | -0.0018       |
|    std                  | 1.56e+03      |
|    value_loss           | 1.47          |
-------------------------------------------
Iteration: 7402 | Episodes: 300200 | Median Reward: 14.72 | Max Reward: 48.88
Iteration: 7404 | Episodes: 300300 | Median Reward: 6.72 | Max Reward: 48.88
Iteration: 7407 | Episodes: 300400 | Median Reward: 21.54 | Max Reward: 48.88
Iteration: 7409 | Episodes: 300500 | Median Reward: 18.99 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.9        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7410         |
|    time_elapsed         | 58397        |
|    total_timesteps      | 30351360     |
| train/                  |              |
|    approx_kl            | 7.222075e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.2        |
|    n_updates            | 74090        |
|    policy_gradient_loss | -0.00095     |
|    std                  | 1.58e+03     |
|    value_loss           | 5.52         |
------------------------------------------
Iteration: 7412 | Episodes: 300600 | Median Reward: 11.38 | Max Reward: 48.88
Iteration: 7414 | Episodes: 300700 | Median Reward: 16.34 | Max Reward: 48.88
Iteration: 7417 | Episodes: 300800 | Median Reward: 21.66 | Max Reward: 48.88
Iteration: 7419 | Episodes: 300900 | Median Reward: 9.80 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -88.4          |
| time/                   |                |
|    fps                  | 519            |
|    iterations           | 7420           |
|    time_elapsed         | 58488          |
|    total_timesteps      | 30392320       |
| train/                  |                |
|    approx_kl            | 0.000108262815 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -250           |
|    explained_variance   | 0.998          |
|    learning_rate        | 0.0005         |
|    loss                 | -12.2          |
|    n_updates            | 74190          |
|    policy_gradient_loss | -0.00217       |
|    std                  | 1.59e+03       |
|    value_loss           | 0.783          |
--------------------------------------------
Iteration: 7422 | Episodes: 301000 | Median Reward: 14.53 | Max Reward: 48.88
Iteration: 7424 | Episodes: 301100 | Median Reward: 19.81 | Max Reward: 48.88
Iteration: 7427 | Episodes: 301200 | Median Reward: 9.73 | Max Reward: 48.88
Iteration: 7429 | Episodes: 301300 | Median Reward: 15.23 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.6        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7430         |
|    time_elapsed         | 58564        |
|    total_timesteps      | 30433280     |
| train/                  |              |
|    approx_kl            | 4.231831e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.1        |
|    n_updates            | 74290        |
|    policy_gradient_loss | -0.000458    |
|    std                  | 1.59e+03     |
|    value_loss           | 0.959        |
------------------------------------------
Iteration: 7431 | Episodes: 301400 | Median Reward: 18.36 | Max Reward: 48.88
Iteration: 7434 | Episodes: 301500 | Median Reward: 9.31 | Max Reward: 48.88
Iteration: 7436 | Episodes: 301600 | Median Reward: 16.17 | Max Reward: 48.88
Iteration: 7439 | Episodes: 301700 | Median Reward: 12.19 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -91.6        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7440         |
|    time_elapsed         | 58646        |
|    total_timesteps      | 30474240     |
| train/                  |              |
|    approx_kl            | 0.0009157338 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.3        |
|    n_updates            | 74390        |
|    policy_gradient_loss | -0.00342     |
|    std                  | 1.61e+03     |
|    value_loss           | 0.767        |
------------------------------------------
Iteration: 7441 | Episodes: 301800 | Median Reward: 7.01 | Max Reward: 48.88
Iteration: 7444 | Episodes: 301900 | Median Reward: 17.15 | Max Reward: 48.88
Iteration: 7446 | Episodes: 302000 | Median Reward: 11.36 | Max Reward: 48.88
Iteration: 7449 | Episodes: 302100 | Median Reward: 15.01 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.2        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7450         |
|    time_elapsed         | 58724        |
|    total_timesteps      | 30515200     |
| train/                  |              |
|    approx_kl            | 0.0017047521 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -251         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.9        |
|    n_updates            | 74490        |
|    policy_gradient_loss | -0.00852     |
|    std                  | 1.63e+03     |
|    value_loss           | 1.62         |
------------------------------------------
Iteration: 7451 | Episodes: 302200 | Median Reward: 18.97 | Max Reward: 48.88
Iteration: 7454 | Episodes: 302300 | Median Reward: 18.13 | Max Reward: 48.88
Iteration: 7456 | Episodes: 302400 | Median Reward: 18.38 | Max Reward: 48.88
Iteration: 7459 | Episodes: 302500 | Median Reward: 15.48 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.3         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7460          |
|    time_elapsed         | 58800         |
|    total_timesteps      | 30556160      |
| train/                  |               |
|    approx_kl            | 0.00010250302 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -251          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -12           |
|    n_updates            | 74590         |
|    policy_gradient_loss | -0.00222      |
|    std                  | 1.65e+03      |
|    value_loss           | 1.16          |
-------------------------------------------
Iteration: 7461 | Episodes: 302600 | Median Reward: 20.10 | Max Reward: 48.88
Iteration: 7464 | Episodes: 302700 | Median Reward: 16.75 | Max Reward: 48.88
Iteration: 7466 | Episodes: 302800 | Median Reward: 19.40 | Max Reward: 48.88
Iteration: 7468 | Episodes: 302900 | Median Reward: 14.75 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.3         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7470          |
|    time_elapsed         | 58887         |
|    total_timesteps      | 30597120      |
| train/                  |               |
|    approx_kl            | 0.00045575402 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -251          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.3         |
|    n_updates            | 74690         |
|    policy_gradient_loss | -0.00127      |
|    std                  | 1.66e+03      |
|    value_loss           | 0.802         |
-------------------------------------------
Iteration: 7471 | Episodes: 303000 | Median Reward: 24.04 | Max Reward: 48.88
Iteration: 7473 | Episodes: 303100 | Median Reward: 14.49 | Max Reward: 48.88
Iteration: 7476 | Episodes: 303200 | Median Reward: 12.72 | Max Reward: 48.88
Iteration: 7478 | Episodes: 303300 | Median Reward: 19.38 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.6         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7480          |
|    time_elapsed         | 58963         |
|    total_timesteps      | 30638080      |
| train/                  |               |
|    approx_kl            | 1.7420098e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -251          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.2         |
|    n_updates            | 74790         |
|    policy_gradient_loss | -0.000137     |
|    std                  | 1.69e+03      |
|    value_loss           | 0.898         |
-------------------------------------------
Iteration: 7481 | Episodes: 303400 | Median Reward: 21.63 | Max Reward: 48.88
Iteration: 7483 | Episodes: 303500 | Median Reward: 13.86 | Max Reward: 48.88
Iteration: 7486 | Episodes: 303600 | Median Reward: 18.39 | Max Reward: 48.88
Iteration: 7488 | Episodes: 303700 | Median Reward: 14.34 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -81.6          |
| time/                   |                |
|    fps                  | 519            |
|    iterations           | 7490           |
|    time_elapsed         | 59048          |
|    total_timesteps      | 30679040       |
| train/                  |                |
|    approx_kl            | 0.000111948146 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -252           |
|    explained_variance   | 0.993          |
|    learning_rate        | 0.0005         |
|    loss                 | -11.2          |
|    n_updates            | 74890          |
|    policy_gradient_loss | -0.000623      |
|    std                  | 1.7e+03        |
|    value_loss           | 3.18           |
--------------------------------------------
Iteration: 7491 | Episodes: 303800 | Median Reward: 18.55 | Max Reward: 48.88
Iteration: 7493 | Episodes: 303900 | Median Reward: 15.49 | Max Reward: 48.88
Iteration: 7496 | Episodes: 304000 | Median Reward: 10.38 | Max Reward: 48.88
Iteration: 7498 | Episodes: 304100 | Median Reward: 18.65 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.9        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7500         |
|    time_elapsed         | 59127        |
|    total_timesteps      | 30720000     |
| train/                  |              |
|    approx_kl            | 4.127345e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -252         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.4        |
|    n_updates            | 74990        |
|    policy_gradient_loss | -0.000182    |
|    std                  | 1.72e+03     |
|    value_loss           | 0.558        |
------------------------------------------
Iteration: 7501 | Episodes: 304200 | Median Reward: 10.72 | Max Reward: 48.88
Iteration: 7503 | Episodes: 304300 | Median Reward: 10.38 | Max Reward: 48.88
Iteration: 7505 | Episodes: 304400 | Median Reward: 15.55 | Max Reward: 48.88
Iteration: 7508 | Episodes: 304500 | Median Reward: 10.30 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.4         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7510          |
|    time_elapsed         | 59204         |
|    total_timesteps      | 30760960      |
| train/                  |               |
|    approx_kl            | 0.00013302591 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -252          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.3         |
|    n_updates            | 75090         |
|    policy_gradient_loss | -0.00123      |
|    std                  | 1.73e+03      |
|    value_loss           | 0.916         |
-------------------------------------------
Iteration: 7510 | Episodes: 304600 | Median Reward: 12.58 | Max Reward: 48.88
Iteration: 7513 | Episodes: 304700 | Median Reward: 21.85 | Max Reward: 48.88
Iteration: 7515 | Episodes: 304800 | Median Reward: 13.36 | Max Reward: 48.88
Iteration: 7518 | Episodes: 304900 | Median Reward: 19.07 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.7         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7520          |
|    time_elapsed         | 59291         |
|    total_timesteps      | 30801920      |
| train/                  |               |
|    approx_kl            | 0.00024901045 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -252          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.8         |
|    n_updates            | 75190         |
|    policy_gradient_loss | -0.00124      |
|    std                  | 1.74e+03      |
|    value_loss           | 2.14          |
-------------------------------------------
Iteration: 7520 | Episodes: 305000 | Median Reward: 22.16 | Max Reward: 48.88
Iteration: 7523 | Episodes: 305100 | Median Reward: 11.80 | Max Reward: 48.88
Iteration: 7525 | Episodes: 305200 | Median Reward: 16.80 | Max Reward: 48.88
Iteration: 7528 | Episodes: 305300 | Median Reward: 15.12 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.8        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7530         |
|    time_elapsed         | 59365        |
|    total_timesteps      | 30842880     |
| train/                  |              |
|    approx_kl            | 0.0011080411 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -252         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.3        |
|    n_updates            | 75290        |
|    policy_gradient_loss | -0.00318     |
|    std                  | 1.77e+03     |
|    value_loss           | 0.856        |
------------------------------------------
Iteration: 7530 | Episodes: 305400 | Median Reward: 17.39 | Max Reward: 48.88
Iteration: 7533 | Episodes: 305500 | Median Reward: 20.44 | Max Reward: 48.88
Iteration: 7535 | Episodes: 305600 | Median Reward: 21.35 | Max Reward: 48.88
Iteration: 7538 | Episodes: 305700 | Median Reward: 17.67 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.8         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7540          |
|    time_elapsed         | 59450         |
|    total_timesteps      | 30883840      |
| train/                  |               |
|    approx_kl            | 0.00029825539 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -253          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -12           |
|    n_updates            | 75390         |
|    policy_gradient_loss | -0.00199      |
|    std                  | 1.78e+03      |
|    value_loss           | 1.69          |
-------------------------------------------
Iteration: 7540 | Episodes: 305800 | Median Reward: 20.10 | Max Reward: 48.88
Iteration: 7542 | Episodes: 305900 | Median Reward: 13.88 | Max Reward: 48.88
Iteration: 7545 | Episodes: 306000 | Median Reward: 14.24 | Max Reward: 48.88
Iteration: 7547 | Episodes: 306100 | Median Reward: 17.81 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.3         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7550          |
|    time_elapsed         | 59531         |
|    total_timesteps      | 30924800      |
| train/                  |               |
|    approx_kl            | 2.6617345e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -253          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.3         |
|    n_updates            | 75490         |
|    policy_gradient_loss | -0.00042      |
|    std                  | 1.79e+03      |
|    value_loss           | 0.805         |
-------------------------------------------
Iteration: 7550 | Episodes: 306200 | Median Reward: 22.48 | Max Reward: 48.88
Iteration: 7552 | Episodes: 306300 | Median Reward: 19.44 | Max Reward: 48.88
Iteration: 7555 | Episodes: 306400 | Median Reward: 17.99 | Max Reward: 48.88
Iteration: 7557 | Episodes: 306500 | Median Reward: 18.08 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.7         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7560          |
|    time_elapsed         | 59611         |
|    total_timesteps      | 30965760      |
| train/                  |               |
|    approx_kl            | 0.00013648093 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -253          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.1         |
|    n_updates            | 75590         |
|    policy_gradient_loss | -0.00152      |
|    std                  | 1.81e+03      |
|    value_loss           | 1.78          |
-------------------------------------------
Iteration: 7560 | Episodes: 306600 | Median Reward: 13.84 | Max Reward: 48.88
Iteration: 7562 | Episodes: 306700 | Median Reward: 8.28 | Max Reward: 48.88
Iteration: 7565 | Episodes: 306800 | Median Reward: 23.90 | Max Reward: 48.88
Iteration: 7567 | Episodes: 306900 | Median Reward: 19.47 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.7         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7570          |
|    time_elapsed         | 59702         |
|    total_timesteps      | 31006720      |
| train/                  |               |
|    approx_kl            | 0.00031789747 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -253          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.1         |
|    n_updates            | 75690         |
|    policy_gradient_loss | -0.00325      |
|    std                  | 1.82e+03      |
|    value_loss           | 1.47          |
-------------------------------------------
Iteration: 7570 | Episodes: 307000 | Median Reward: 15.42 | Max Reward: 48.88
Iteration: 7572 | Episodes: 307100 | Median Reward: 19.54 | Max Reward: 48.88
Iteration: 7574 | Episodes: 307200 | Median Reward: 19.15 | Max Reward: 48.88
Iteration: 7577 | Episodes: 307300 | Median Reward: 16.55 | Max Reward: 48.88
Iteration: 7579 | Episodes: 307400 | Median Reward: 15.29 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.2        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7580         |
|    time_elapsed         | 59777        |
|    total_timesteps      | 31047680     |
| train/                  |              |
|    approx_kl            | 0.0002650154 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -254         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.3        |
|    n_updates            | 75790        |
|    policy_gradient_loss | -0.00163     |
|    std                  | 1.85e+03     |
|    value_loss           | 1.2          |
------------------------------------------
Iteration: 7582 | Episodes: 307500 | Median Reward: 8.40 | Max Reward: 48.88
Iteration: 7584 | Episodes: 307600 | Median Reward: 22.03 | Max Reward: 48.88
Iteration: 7587 | Episodes: 307700 | Median Reward: 13.71 | Max Reward: 48.88
Iteration: 7589 | Episodes: 307800 | Median Reward: 11.42 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.2         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7590          |
|    time_elapsed         | 59863         |
|    total_timesteps      | 31088640      |
| train/                  |               |
|    approx_kl            | 0.00012975914 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -254          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.1         |
|    n_updates            | 75890         |
|    policy_gradient_loss | -0.000986     |
|    std                  | 1.87e+03      |
|    value_loss           | 1.41          |
-------------------------------------------
Iteration: 7592 | Episodes: 307900 | Median Reward: 19.40 | Max Reward: 48.88
Iteration: 7594 | Episodes: 308000 | Median Reward: 19.06 | Max Reward: 48.88
Iteration: 7597 | Episodes: 308100 | Median Reward: 9.93 | Max Reward: 48.88
Iteration: 7599 | Episodes: 308200 | Median Reward: 17.10 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.3        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7600         |
|    time_elapsed         | 59938        |
|    total_timesteps      | 31129600     |
| train/                  |              |
|    approx_kl            | 0.0002968693 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -254         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.4        |
|    n_updates            | 75990        |
|    policy_gradient_loss | -0.00399     |
|    std                  | 1.88e+03     |
|    value_loss           | 0.828        |
------------------------------------------
Iteration: 7602 | Episodes: 308300 | Median Reward: 19.34 | Max Reward: 48.88
Iteration: 7604 | Episodes: 308400 | Median Reward: 20.35 | Max Reward: 48.88
Iteration: 7607 | Episodes: 308500 | Median Reward: 16.09 | Max Reward: 48.88
Iteration: 7609 | Episodes: 308600 | Median Reward: 13.04 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -88.4       |
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 7610        |
|    time_elapsed         | 60016       |
|    total_timesteps      | 31170560    |
| train/                  |             |
|    approx_kl            | 0.000483108 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -254        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.1       |
|    n_updates            | 76090       |
|    policy_gradient_loss | -0.00253    |
|    std                  | 1.92e+03    |
|    value_loss           | 1.55        |
-----------------------------------------
Iteration: 7611 | Episodes: 308700 | Median Reward: 15.58 | Max Reward: 48.88
Iteration: 7614 | Episodes: 308800 | Median Reward: 16.76 | Max Reward: 48.88
Iteration: 7616 | Episodes: 308900 | Median Reward: 15.99 | Max Reward: 48.88
Iteration: 7619 | Episodes: 309000 | Median Reward: 19.31 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.9        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7620         |
|    time_elapsed         | 60101        |
|    total_timesteps      | 31211520     |
| train/                  |              |
|    approx_kl            | 7.143515e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -255         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.1        |
|    n_updates            | 76190        |
|    policy_gradient_loss | -0.000459    |
|    std                  | 1.94e+03     |
|    value_loss           | 1.51         |
------------------------------------------
Iteration: 7621 | Episodes: 309100 | Median Reward: 15.04 | Max Reward: 48.88
Iteration: 7624 | Episodes: 309200 | Median Reward: 15.17 | Max Reward: 48.88
Iteration: 7626 | Episodes: 309300 | Median Reward: 13.59 | Max Reward: 48.88
Iteration: 7629 | Episodes: 309400 | Median Reward: 17.30 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.5         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7630          |
|    time_elapsed         | 60176         |
|    total_timesteps      | 31252480      |
| train/                  |               |
|    approx_kl            | 0.00027610877 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -255          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.5         |
|    n_updates            | 76290         |
|    policy_gradient_loss | -0.00209      |
|    std                  | 1.96e+03      |
|    value_loss           | 0.634         |
-------------------------------------------
Iteration: 7631 | Episodes: 309500 | Median Reward: 11.82 | Max Reward: 48.88
Iteration: 7634 | Episodes: 309600 | Median Reward: 18.58 | Max Reward: 48.88
Iteration: 7636 | Episodes: 309700 | Median Reward: 12.00 | Max Reward: 48.88
Iteration: 7639 | Episodes: 309800 | Median Reward: 15.12 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.7        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7640         |
|    time_elapsed         | 60262        |
|    total_timesteps      | 31293440     |
| train/                  |              |
|    approx_kl            | 0.0002885519 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -255         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.3        |
|    n_updates            | 76390        |
|    policy_gradient_loss | -0.00227     |
|    std                  | 1.97e+03     |
|    value_loss           | 1.15         |
------------------------------------------
Iteration: 7641 | Episodes: 309900 | Median Reward: 16.39 | Max Reward: 48.88
Iteration: 7644 | Episodes: 310000 | Median Reward: 18.93 | Max Reward: 48.88
Iteration: 7646 | Episodes: 310100 | Median Reward: 13.55 | Max Reward: 48.88
Iteration: 7648 | Episodes: 310200 | Median Reward: 16.24 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.5         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7650          |
|    time_elapsed         | 60339         |
|    total_timesteps      | 31334400      |
| train/                  |               |
|    approx_kl            | 0.00026858054 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -255          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.2         |
|    n_updates            | 76490         |
|    policy_gradient_loss | -0.00126      |
|    std                  | 1.99e+03      |
|    value_loss           | 1.53          |
-------------------------------------------
Iteration: 7651 | Episodes: 310300 | Median Reward: 11.90 | Max Reward: 48.88
Iteration: 7653 | Episodes: 310400 | Median Reward: 14.60 | Max Reward: 48.88
Iteration: 7656 | Episodes: 310500 | Median Reward: 12.50 | Max Reward: 48.88
Iteration: 7658 | Episodes: 310600 | Median Reward: 15.50 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85           |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7660          |
|    time_elapsed         | 60417         |
|    total_timesteps      | 31375360      |
| train/                  |               |
|    approx_kl            | 0.00014156723 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -256          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.6         |
|    n_updates            | 76590         |
|    policy_gradient_loss | -0.00101      |
|    std                  | 2e+03         |
|    value_loss           | 0.566         |
-------------------------------------------
Iteration: 7661 | Episodes: 310700 | Median Reward: 13.67 | Max Reward: 48.88
Iteration: 7663 | Episodes: 310800 | Median Reward: 9.75 | Max Reward: 48.88
Iteration: 7666 | Episodes: 310900 | Median Reward: 9.87 | Max Reward: 48.88
Iteration: 7668 | Episodes: 311000 | Median Reward: 21.55 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.3         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7670          |
|    time_elapsed         | 60500         |
|    total_timesteps      | 31416320      |
| train/                  |               |
|    approx_kl            | 0.00021800454 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -256          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.8         |
|    n_updates            | 76690         |
|    policy_gradient_loss | -0.00274      |
|    std                  | 2.03e+03      |
|    value_loss           | 2.56          |
-------------------------------------------
Iteration: 7671 | Episodes: 311100 | Median Reward: 17.09 | Max Reward: 48.88
Iteration: 7673 | Episodes: 311200 | Median Reward: 7.81 | Max Reward: 48.88
Iteration: 7676 | Episodes: 311300 | Median Reward: 11.75 | Max Reward: 48.88
Iteration: 7678 | Episodes: 311400 | Median Reward: 10.40 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.8         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7680          |
|    time_elapsed         | 60575         |
|    total_timesteps      | 31457280      |
| train/                  |               |
|    approx_kl            | 0.00025553376 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -256          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.4         |
|    n_updates            | 76790         |
|    policy_gradient_loss | -0.00285      |
|    std                  | 2.04e+03      |
|    value_loss           | 1.1           |
-------------------------------------------
Iteration: 7681 | Episodes: 311500 | Median Reward: 17.35 | Max Reward: 48.88
Iteration: 7683 | Episodes: 311600 | Median Reward: 12.40 | Max Reward: 48.88
Iteration: 7685 | Episodes: 311700 | Median Reward: 6.12 | Max Reward: 48.88
Iteration: 7688 | Episodes: 311800 | Median Reward: 10.61 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85           |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7690          |
|    time_elapsed         | 60660         |
|    total_timesteps      | 31498240      |
| train/                  |               |
|    approx_kl            | 0.00013422375 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -256          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.2         |
|    n_updates            | 76890         |
|    policy_gradient_loss | -0.0022       |
|    std                  | 2.07e+03      |
|    value_loss           | 1.39          |
-------------------------------------------
Iteration: 7690 | Episodes: 311900 | Median Reward: 15.09 | Max Reward: 48.88
Iteration: 7693 | Episodes: 312000 | Median Reward: 5.89 | Max Reward: 48.88
Iteration: 7695 | Episodes: 312100 | Median Reward: 16.15 | Max Reward: 48.88
Iteration: 7698 | Episodes: 312200 | Median Reward: 13.22 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88.4         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7700          |
|    time_elapsed         | 60736         |
|    total_timesteps      | 31539200      |
| train/                  |               |
|    approx_kl            | 0.00020280454 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -257          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.4         |
|    n_updates            | 76990         |
|    policy_gradient_loss | -0.000346     |
|    std                  | 2.1e+03       |
|    value_loss           | 0.8           |
-------------------------------------------
Iteration: 7700 | Episodes: 312300 | Median Reward: 10.66 | Max Reward: 48.88
Iteration: 7703 | Episodes: 312400 | Median Reward: 16.44 | Max Reward: 48.88
Iteration: 7705 | Episodes: 312500 | Median Reward: 11.15 | Max Reward: 48.88
Iteration: 7708 | Episodes: 312600 | Median Reward: 15.90 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -88.3          |
| time/                   |                |
|    fps                  | 519            |
|    iterations           | 7710           |
|    time_elapsed         | 60815          |
|    total_timesteps      | 31580160       |
| train/                  |                |
|    approx_kl            | 0.000109134184 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -257           |
|    explained_variance   | 0.995          |
|    learning_rate        | 0.0005         |
|    loss                 | -12.4          |
|    n_updates            | 77090          |
|    policy_gradient_loss | -0.0019        |
|    std                  | 2.12e+03       |
|    value_loss           | 1.63           |
--------------------------------------------
Iteration: 7710 | Episodes: 312700 | Median Reward: 15.70 | Max Reward: 48.88
Iteration: 7713 | Episodes: 312800 | Median Reward: 16.31 | Max Reward: 48.88
Iteration: 7715 | Episodes: 312900 | Median Reward: 14.20 | Max Reward: 48.88
Iteration: 7718 | Episodes: 313000 | Median Reward: 10.42 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -82.7         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7720          |
|    time_elapsed         | 60897         |
|    total_timesteps      | 31621120      |
| train/                  |               |
|    approx_kl            | 0.00023717937 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -257          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.3         |
|    n_updates            | 77190         |
|    policy_gradient_loss | 0.00129       |
|    std                  | 2.15e+03      |
|    value_loss           | 1.19          |
-------------------------------------------
Iteration: 7720 | Episodes: 313100 | Median Reward: 24.94 | Max Reward: 48.88
Iteration: 7722 | Episodes: 313200 | Median Reward: 20.13 | Max Reward: 48.88
Iteration: 7725 | Episodes: 313300 | Median Reward: 12.49 | Max Reward: 48.88
Iteration: 7727 | Episodes: 313400 | Median Reward: 21.56 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91.3         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7730          |
|    time_elapsed         | 60972         |
|    total_timesteps      | 31662080      |
| train/                  |               |
|    approx_kl            | 0.00025046765 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -257          |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.8         |
|    n_updates            | 77290         |
|    policy_gradient_loss | -0.00117      |
|    std                  | 2.16e+03      |
|    value_loss           | 2.73          |
-------------------------------------------
Iteration: 7730 | Episodes: 313500 | Median Reward: 8.99 | Max Reward: 48.88
Iteration: 7732 | Episodes: 313600 | Median Reward: 12.16 | Max Reward: 48.88
Iteration: 7735 | Episodes: 313700 | Median Reward: 15.81 | Max Reward: 48.88
Iteration: 7737 | Episodes: 313800 | Median Reward: 2.28 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -91           |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7740          |
|    time_elapsed         | 61059         |
|    total_timesteps      | 31703040      |
| train/                  |               |
|    approx_kl            | 5.6718185e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -258          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.5         |
|    n_updates            | 77390         |
|    policy_gradient_loss | -0.00011      |
|    std                  | 2.18e+03      |
|    value_loss           | 1.11          |
-------------------------------------------
Iteration: 7740 | Episodes: 313900 | Median Reward: 8.72 | Max Reward: 48.88
Iteration: 7742 | Episodes: 314000 | Median Reward: 19.83 | Max Reward: 48.88
Iteration: 7745 | Episodes: 314100 | Median Reward: 19.04 | Max Reward: 48.88
Iteration: 7747 | Episodes: 314200 | Median Reward: 14.23 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -86.3        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7750         |
|    time_elapsed         | 61135        |
|    total_timesteps      | 31744000     |
| train/                  |              |
|    approx_kl            | 0.0010659436 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -258         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.6        |
|    n_updates            | 77490        |
|    policy_gradient_loss | -0.00674     |
|    std                  | 2.21e+03     |
|    value_loss           | 0.907        |
------------------------------------------
Iteration: 7750 | Episodes: 314300 | Median Reward: 13.86 | Max Reward: 48.88
Iteration: 7752 | Episodes: 314400 | Median Reward: 15.94 | Max Reward: 48.88
Iteration: 7755 | Episodes: 314500 | Median Reward: 16.25 | Max Reward: 48.88
Iteration: 7757 | Episodes: 314600 | Median Reward: 15.68 | Max Reward: 48.88
Iteration: 7759 | Episodes: 314700 | Median Reward: 21.28 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.1         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7760          |
|    time_elapsed         | 61215         |
|    total_timesteps      | 31784960      |
| train/                  |               |
|    approx_kl            | 0.00018593101 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -258          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.7         |
|    n_updates            | 77590         |
|    policy_gradient_loss | -0.00189      |
|    std                  | 2.24e+03      |
|    value_loss           | 0.476         |
-------------------------------------------
Iteration: 7762 | Episodes: 314800 | Median Reward: 12.67 | Max Reward: 48.88
Iteration: 7764 | Episodes: 314900 | Median Reward: 15.55 | Max Reward: 48.88
Iteration: 7767 | Episodes: 315000 | Median Reward: 11.68 | Max Reward: 48.88
Iteration: 7769 | Episodes: 315100 | Median Reward: 16.19 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.7         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7770          |
|    time_elapsed         | 61297         |
|    total_timesteps      | 31825920      |
| train/                  |               |
|    approx_kl            | 0.00017655813 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -259          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.4         |
|    n_updates            | 77690         |
|    policy_gradient_loss | -0.00101      |
|    std                  | 2.27e+03      |
|    value_loss           | 1.58          |
-------------------------------------------
Iteration: 7772 | Episodes: 315200 | Median Reward: 22.78 | Max Reward: 48.88
Iteration: 7774 | Episodes: 315300 | Median Reward: 20.38 | Max Reward: 48.88
Iteration: 7777 | Episodes: 315400 | Median Reward: 17.37 | Max Reward: 48.88
Iteration: 7779 | Episodes: 315500 | Median Reward: 14.33 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.6         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7780          |
|    time_elapsed         | 61371         |
|    total_timesteps      | 31866880      |
| train/                  |               |
|    approx_kl            | 0.00089051866 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -259          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.7         |
|    n_updates            | 77790         |
|    policy_gradient_loss | -0.00615      |
|    std                  | 2.29e+03      |
|    value_loss           | 0.821         |
-------------------------------------------
Iteration: 7782 | Episodes: 315600 | Median Reward: 10.56 | Max Reward: 48.88
Iteration: 7784 | Episodes: 315700 | Median Reward: 8.73 | Max Reward: 48.88
Iteration: 7787 | Episodes: 315800 | Median Reward: 4.50 | Max Reward: 48.88
Iteration: 7789 | Episodes: 315900 | Median Reward: 13.99 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -90          |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7790         |
|    time_elapsed         | 61457        |
|    total_timesteps      | 31907840     |
| train/                  |              |
|    approx_kl            | 0.0007056205 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -259         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.6        |
|    n_updates            | 77890        |
|    policy_gradient_loss | -0.00327     |
|    std                  | 2.32e+03     |
|    value_loss           | 0.924        |
------------------------------------------
Iteration: 7791 | Episodes: 316000 | Median Reward: 13.99 | Max Reward: 48.88
Iteration: 7794 | Episodes: 316100 | Median Reward: 20.34 | Max Reward: 48.88
Iteration: 7796 | Episodes: 316200 | Median Reward: 20.39 | Max Reward: 48.88
Iteration: 7799 | Episodes: 316300 | Median Reward: 14.75 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.8         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7800          |
|    time_elapsed         | 61532         |
|    total_timesteps      | 31948800      |
| train/                  |               |
|    approx_kl            | 0.00035724198 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -259          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.7         |
|    n_updates            | 77990         |
|    policy_gradient_loss | -0.00201      |
|    std                  | 2.34e+03      |
|    value_loss           | 0.711         |
-------------------------------------------
Iteration: 7801 | Episodes: 316400 | Median Reward: 16.30 | Max Reward: 48.88
Iteration: 7804 | Episodes: 316500 | Median Reward: 15.76 | Max Reward: 48.88
Iteration: 7806 | Episodes: 316600 | Median Reward: 12.07 | Max Reward: 48.88
Iteration: 7809 | Episodes: 316700 | Median Reward: 12.29 | Max Reward: 48.88
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -89.5       |
| time/                   |             |
|    fps                  | 519         |
|    iterations           | 7810        |
|    time_elapsed         | 61614       |
|    total_timesteps      | 31989760    |
| train/                  |             |
|    approx_kl            | 0.005726978 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -260        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.8       |
|    n_updates            | 78090       |
|    policy_gradient_loss | -0.013      |
|    std                  | 2.37e+03    |
|    value_loss           | 0.59        |
-----------------------------------------
Iteration: 7811 | Episodes: 316800 | Median Reward: 4.87 | Max Reward: 48.88
Iteration: 7814 | Episodes: 316900 | Median Reward: 17.30 | Max Reward: 48.88
Iteration: 7816 | Episodes: 317000 | Median Reward: 13.51 | Max Reward: 48.88
Iteration: 7819 | Episodes: 317100 | Median Reward: 24.86 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.6         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7820          |
|    time_elapsed         | 61694         |
|    total_timesteps      | 32030720      |
| train/                  |               |
|    approx_kl            | 0.00028690742 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -260          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.4         |
|    n_updates            | 78190         |
|    policy_gradient_loss | -0.00137      |
|    std                  | 2.39e+03      |
|    value_loss           | 1.52          |
-------------------------------------------
Iteration: 7821 | Episodes: 317200 | Median Reward: 24.55 | Max Reward: 48.88
Iteration: 7824 | Episodes: 317300 | Median Reward: 14.38 | Max Reward: 48.88
Iteration: 7826 | Episodes: 317400 | Median Reward: 12.93 | Max Reward: 48.88
Iteration: 7828 | Episodes: 317500 | Median Reward: 9.36 | Max Reward: 48.88
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -88.1          |
| time/                   |                |
|    fps                  | 519            |
|    iterations           | 7830           |
|    time_elapsed         | 61769          |
|    total_timesteps      | 32071680       |
| train/                  |                |
|    approx_kl            | 0.000112993905 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -260           |
|    explained_variance   | 0.993          |
|    learning_rate        | 0.0005         |
|    loss                 | -12.3          |
|    n_updates            | 78290          |
|    policy_gradient_loss | -0.000432      |
|    std                  | 2.4e+03        |
|    value_loss           | 2.03           |
--------------------------------------------
Iteration: 7831 | Episodes: 317600 | Median Reward: 10.26 | Max Reward: 48.88
Iteration: 7833 | Episodes: 317700 | Median Reward: 9.04 | Max Reward: 48.88
Iteration: 7836 | Episodes: 317800 | Median Reward: 14.48 | Max Reward: 48.88
Iteration: 7838 | Episodes: 317900 | Median Reward: 13.03 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.4         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7840          |
|    time_elapsed         | 61855         |
|    total_timesteps      | 32112640      |
| train/                  |               |
|    approx_kl            | 0.00021106994 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -260          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.6         |
|    n_updates            | 78390         |
|    policy_gradient_loss | -0.000409     |
|    std                  | 2.42e+03      |
|    value_loss           | 1.27          |
-------------------------------------------
Iteration: 7841 | Episodes: 318000 | Median Reward: 12.70 | Max Reward: 48.88
Iteration: 7843 | Episodes: 318100 | Median Reward: 14.87 | Max Reward: 48.88
Iteration: 7846 | Episodes: 318200 | Median Reward: 15.47 | Max Reward: 48.88
Iteration: 7848 | Episodes: 318300 | Median Reward: 14.01 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.1        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7850         |
|    time_elapsed         | 61931        |
|    total_timesteps      | 32153600     |
| train/                  |              |
|    approx_kl            | 0.0014034645 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -261         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.4        |
|    n_updates            | 78490        |
|    policy_gradient_loss | -0.00504     |
|    std                  | 2.46e+03     |
|    value_loss           | 1.41         |
------------------------------------------
Iteration: 7851 | Episodes: 318400 | Median Reward: 15.79 | Max Reward: 48.88
Iteration: 7853 | Episodes: 318500 | Median Reward: 20.12 | Max Reward: 48.88
Iteration: 7856 | Episodes: 318600 | Median Reward: 10.55 | Max Reward: 48.88
Iteration: 7858 | Episodes: 318700 | Median Reward: 15.54 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87.5        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7860         |
|    time_elapsed         | 62014        |
|    total_timesteps      | 32194560     |
| train/                  |              |
|    approx_kl            | 5.448819e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -261         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.3        |
|    n_updates            | 78590        |
|    policy_gradient_loss | -0.00114     |
|    std                  | 2.48e+03     |
|    value_loss           | 1.22         |
------------------------------------------
Iteration: 7861 | Episodes: 318800 | Median Reward: 15.22 | Max Reward: 48.88
Iteration: 7863 | Episodes: 318900 | Median Reward: 15.55 | Max Reward: 48.88
Iteration: 7865 | Episodes: 319000 | Median Reward: 14.68 | Max Reward: 48.88
Iteration: 7868 | Episodes: 319100 | Median Reward: 20.82 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.5         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7870          |
|    time_elapsed         | 62092         |
|    total_timesteps      | 32235520      |
| train/                  |               |
|    approx_kl            | 0.00074839906 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -261          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.7         |
|    n_updates            | 78690         |
|    policy_gradient_loss | -0.003        |
|    std                  | 2.51e+03      |
|    value_loss           | 0.747         |
-------------------------------------------
Iteration: 7870 | Episodes: 319200 | Median Reward: 14.56 | Max Reward: 48.88
Iteration: 7873 | Episodes: 319300 | Median Reward: 13.58 | Max Reward: 48.88
Iteration: 7875 | Episodes: 319400 | Median Reward: 14.82 | Max Reward: 48.88
Iteration: 7878 | Episodes: 319500 | Median Reward: 13.33 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83          |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7880         |
|    time_elapsed         | 62165        |
|    total_timesteps      | 32276480     |
| train/                  |              |
|    approx_kl            | 0.0003339158 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -261         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.9        |
|    n_updates            | 78790        |
|    policy_gradient_loss | -0.00126     |
|    std                  | 2.54e+03     |
|    value_loss           | 0.409        |
------------------------------------------
Iteration: 7880 | Episodes: 319600 | Median Reward: 15.34 | Max Reward: 48.88
Iteration: 7883 | Episodes: 319700 | Median Reward: 12.85 | Max Reward: 48.88
Iteration: 7885 | Episodes: 319800 | Median Reward: 12.11 | Max Reward: 48.88
Iteration: 7888 | Episodes: 319900 | Median Reward: 15.18 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -79.1         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7890          |
|    time_elapsed         | 62252         |
|    total_timesteps      | 32317440      |
| train/                  |               |
|    approx_kl            | 7.9065925e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -262          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.4         |
|    n_updates            | 78890         |
|    policy_gradient_loss | -0.00134      |
|    std                  | 2.57e+03      |
|    value_loss           | 1.43          |
-------------------------------------------
Iteration: 7890 | Episodes: 320000 | Median Reward: 25.84 | Max Reward: 48.88
Iteration: 7893 | Episodes: 320100 | Median Reward: 10.05 | Max Reward: 48.88
Iteration: 7895 | Episodes: 320200 | Median Reward: 20.99 | Max Reward: 48.88
Iteration: 7898 | Episodes: 320300 | Median Reward: 19.17 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.5        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7900         |
|    time_elapsed         | 62328        |
|    total_timesteps      | 32358400     |
| train/                  |              |
|    approx_kl            | 0.0002606942 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -262         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.2        |
|    n_updates            | 78990        |
|    policy_gradient_loss | 0.000215     |
|    std                  | 2.59e+03     |
|    value_loss           | 2.26         |
------------------------------------------
Iteration: 7900 | Episodes: 320400 | Median Reward: 13.81 | Max Reward: 48.88
Iteration: 7902 | Episodes: 320500 | Median Reward: 13.56 | Max Reward: 48.88
Iteration: 7905 | Episodes: 320600 | Median Reward: 13.08 | Max Reward: 48.88
Iteration: 7907 | Episodes: 320700 | Median Reward: 10.59 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.1         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7910          |
|    time_elapsed         | 62412         |
|    total_timesteps      | 32399360      |
| train/                  |               |
|    approx_kl            | 0.00028867892 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -262          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.5         |
|    n_updates            | 79090         |
|    policy_gradient_loss | -0.00256      |
|    std                  | 2.64e+03      |
|    value_loss           | 1.72          |
-------------------------------------------
Iteration: 7910 | Episodes: 320800 | Median Reward: 9.46 | Max Reward: 48.88
Iteration: 7912 | Episodes: 320900 | Median Reward: 12.74 | Max Reward: 48.88
Iteration: 7915 | Episodes: 321000 | Median Reward: 14.50 | Max Reward: 48.88
Iteration: 7917 | Episodes: 321100 | Median Reward: 21.30 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.2         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7920          |
|    time_elapsed         | 62489         |
|    total_timesteps      | 32440320      |
| train/                  |               |
|    approx_kl            | 0.00034603078 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -262          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.6         |
|    n_updates            | 79190         |
|    policy_gradient_loss | -0.00135      |
|    std                  | 2.67e+03      |
|    value_loss           | 1.37          |
-------------------------------------------
Iteration: 7920 | Episodes: 321200 | Median Reward: 11.53 | Max Reward: 48.88
Iteration: 7922 | Episodes: 321300 | Median Reward: 13.01 | Max Reward: 48.88
Iteration: 7925 | Episodes: 321400 | Median Reward: 11.16 | Max Reward: 48.88
Iteration: 7927 | Episodes: 321500 | Median Reward: 12.92 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -89.4        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7930         |
|    time_elapsed         | 62565        |
|    total_timesteps      | 32481280     |
| train/                  |              |
|    approx_kl            | 0.0005768529 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -263         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -12          |
|    n_updates            | 79290        |
|    policy_gradient_loss | -0.00238     |
|    std                  | 2.71e+03     |
|    value_loss           | 2.63         |
------------------------------------------
Iteration: 7930 | Episodes: 321600 | Median Reward: 14.01 | Max Reward: 48.88
Iteration: 7932 | Episodes: 321700 | Median Reward: 15.34 | Max Reward: 48.88
Iteration: 7935 | Episodes: 321800 | Median Reward: 15.66 | Max Reward: 48.88
Iteration: 7937 | Episodes: 321900 | Median Reward: 11.93 | Max Reward: 48.88
Iteration: 7939 | Episodes: 322000 | Median Reward: 17.32 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.2        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7940         |
|    time_elapsed         | 62649        |
|    total_timesteps      | 32522240     |
| train/                  |              |
|    approx_kl            | 0.0018672489 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -263         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13          |
|    n_updates            | 79390        |
|    policy_gradient_loss | -0.00393     |
|    std                  | 2.73e+03     |
|    value_loss           | 0.407        |
------------------------------------------
Iteration: 7942 | Episodes: 322100 | Median Reward: 20.01 | Max Reward: 48.88
Iteration: 7944 | Episodes: 322200 | Median Reward: 15.19 | Max Reward: 48.88
Iteration: 7947 | Episodes: 322300 | Median Reward: 17.80 | Max Reward: 48.88
Iteration: 7949 | Episodes: 322400 | Median Reward: 13.31 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84.8        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7950         |
|    time_elapsed         | 62725        |
|    total_timesteps      | 32563200     |
| train/                  |              |
|    approx_kl            | 0.0009052193 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -263         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.4        |
|    n_updates            | 79490        |
|    policy_gradient_loss | -0.00245     |
|    std                  | 2.77e+03     |
|    value_loss           | 1.8          |
------------------------------------------
Iteration: 7952 | Episodes: 322500 | Median Reward: 9.46 | Max Reward: 48.88
Iteration: 7954 | Episodes: 322600 | Median Reward: 16.59 | Max Reward: 48.88
Iteration: 7957 | Episodes: 322700 | Median Reward: 9.95 | Max Reward: 48.88
Iteration: 7959 | Episodes: 322800 | Median Reward: 15.57 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84.8        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7960         |
|    time_elapsed         | 62811        |
|    total_timesteps      | 32604160     |
| train/                  |              |
|    approx_kl            | 0.0021078314 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -263         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.9        |
|    n_updates            | 79590        |
|    policy_gradient_loss | -0.00604     |
|    std                  | 2.79e+03     |
|    value_loss           | 0.534        |
------------------------------------------
Iteration: 7962 | Episodes: 322900 | Median Reward: 13.77 | Max Reward: 48.88
Iteration: 7964 | Episodes: 323000 | Median Reward: 11.21 | Max Reward: 48.88
Iteration: 7967 | Episodes: 323100 | Median Reward: 14.18 | Max Reward: 48.88
Iteration: 7969 | Episodes: 323200 | Median Reward: 12.85 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.3         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 7970          |
|    time_elapsed         | 62888         |
|    total_timesteps      | 32645120      |
| train/                  |               |
|    approx_kl            | 0.00024276535 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -263          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.6         |
|    n_updates            | 79690         |
|    policy_gradient_loss | -0.00334      |
|    std                  | 2.81e+03      |
|    value_loss           | 1.48          |
-------------------------------------------
Iteration: 7971 | Episodes: 323300 | Median Reward: 5.46 | Max Reward: 48.88
Iteration: 7974 | Episodes: 323400 | Median Reward: 15.85 | Max Reward: 48.88
Iteration: 7976 | Episodes: 323500 | Median Reward: 17.60 | Max Reward: 48.88
Iteration: 7979 | Episodes: 323600 | Median Reward: 14.19 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87          |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7980         |
|    time_elapsed         | 62966        |
|    total_timesteps      | 32686080     |
| train/                  |              |
|    approx_kl            | 0.0015156446 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -264         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.3        |
|    n_updates            | 79790        |
|    policy_gradient_loss | -0.00655     |
|    std                  | 2.84e+03     |
|    value_loss           | 2.18         |
------------------------------------------
Iteration: 7981 | Episodes: 323700 | Median Reward: 24.60 | Max Reward: 48.88
Iteration: 7984 | Episodes: 323800 | Median Reward: 12.87 | Max Reward: 48.88
Iteration: 7986 | Episodes: 323900 | Median Reward: 16.78 | Max Reward: 48.88
Iteration: 7989 | Episodes: 324000 | Median Reward: 14.81 | Max Reward: 48.88
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -87.1        |
| time/                   |              |
|    fps                  | 519          |
|    iterations           | 7990         |
|    time_elapsed         | 63051        |
|    total_timesteps      | 32727040     |
| train/                  |              |
|    approx_kl            | 0.0006933145 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -264         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.7        |
|    n_updates            | 79890        |
|    policy_gradient_loss | -0.00307     |
|    std                  | 2.87e+03     |
|    value_loss           | 1.17         |
------------------------------------------
Iteration: 7991 | Episodes: 324100 | Median Reward: 13.75 | Max Reward: 48.88
Iteration: 7994 | Episodes: 324200 | Median Reward: 11.81 | Max Reward: 48.88
Iteration: 7996 | Episodes: 324300 | Median Reward: 12.61 | Max Reward: 48.88
Iteration: 7999 | Episodes: 324400 | Median Reward: 12.37 | Max Reward: 48.88
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88.4         |
| time/                   |               |
|    fps                  | 519           |
|    iterations           | 8000          |
|    time_elapsed         | 63127         |
|    total_timesteps      | 32768000      |
| train/                  |               |
|    approx_kl            | 0.00028463747 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -264          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.6         |
|    n_updates            | 79990         |
|    policy_gradient_loss | -0.00261      |
|    std                  | 2.9e+03       |
|    value_loss           | 1.49          |
-------------------------------------------
Training End | Episodes: 324432 | Median Reward: 15.99 | Max Reward: 48.88
Plot saved as fig_code1_1.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> s[K(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> sree[K[K[K[Kx[Kscreeb [K[K[K[K[K[Kexit()
exit

Script done on 2024-10-18 18:05:18-04:00 [COMMAND_EXIT_CODE="0"]
