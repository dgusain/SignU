Script started on 2024-10-22 22:37:02-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/41" COLUMNS="282" LINES="75"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> conda activate mujoco_Tes[K[K[Ktest
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> nano code1_1_basic_custom_logs[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kvidia-smi
Tue Oct 22 22:37:41 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    Off  | 00000000:01:00.0 Off |                  Off |
| 38%   65C    P2   186W / 230W |   5101MiB / 24564MiB |     87%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    Off  | 00000000:24:00.0 Off |                  Off |
| 37%   63C    P2   176W / 230W |   4943MiB / 24564MiB |     94%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A5000    Off  | 00000000:41:00.0 Off |                  Off |
| 39%   65C    P2   179W / 230W |   5117MiB / 24564MiB |     89%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A5000    Off  | 00000000:61:00.0 Off |                  Off |
| 35%   62C    P2   192W / 230W |   7162MiB / 24564MiB |     96%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA RTX A5000    Off  | 00000000:81:00.0 Off |                  Off |
| 36%   63C    P2   196W / 230W |   6702MiB / 24564MiB |     95%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA RTX A5000    Off  | 00000000:A1:00.0 Off |                  Off |
| 34%   61C    P2   178W / 230W |   5173MiB / 24564MiB |     91%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA RTX A5000    Off  | 00000000:C1:00.0 Off |                  Off |
| 31%   59C    P2   183W / 230W |   5227MiB / 24564MiB |     92%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA RTX A5000    Off  | 00000000:E1:00.0 Off |                  Off |
| 31%   60C    P2   185W / 230W |   9816MiB / 24564MiB |     93%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A   2761459      C   ...3/envs/YOLOV11/bin/python     5095MiB |
|    1   N/A  N/A   2761460      C   ...3/envs/YOLOV11/bin/python     4941MiB |
|    2   N/A  N/A   2761461      C   ...3/envs/YOLOV11/bin/python     5113MiB |
|    3   N/A  N/A   1036369      C   python                           1697MiB |
|    3   N/A  N/A   2761462      C   ...3/envs/YOLOV11/bin/python     5461MiB |
|    4   N/A  N/A   1079799      C   python                           1687MiB |
|    4   N/A  N/A   2761463      C   ...3/envs/YOLOV11/bin/python     5011MiB |
|    5   N/A  N/A   2761464      C   ...3/envs/YOLOV11/bin/python     5169MiB |
|    6   N/A  N/A   2761465      C   ...3/envs/YOLOV11/bin/python     5223MiB |
|    7   N/A  N/A   1164278      C   python                           4523MiB |
|    7   N/A  N/A   2761466      C   ...3/envs/YOLOV11/bin/python     5289MiB |
+-----------------------------------------------------------------------------+
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> b[KNANO [K[K[K[Knano cust[K[K[Kd[Kode1_1_basic_custom.py
[?2004h[?1049h[22;0;0t[1;75r(B[m[4l[?7h[39;49m[?1h=[?1h=[?25l[39;49m(B[m[H[2J[73;135H(B[0;7m[ Reading... ](B[m[73;133H(B[0;7m[ Read 366 lines ](B[m[H(B[0;7m  GNU nano 4.8                                                                                                                     code1_1_basic_custom.py                                                                                                                                [1;281H(B[m[74d(B[0;7m^G(B[m Get Help[18G(B[0;7m^O(B[m Write Out     (B[0;7m^W(B[m Where Is[52G(B[0;7m^K(B[m Cut Text[69G(B[0;7m^J(B[m Justify[86G(B[0;7m^C(B[m Cur Pos[103G(B[0;7mM-U(B[m Undo[74;120H(B[0;7mM-A(B[m Mark Text    (B[0;7mM-](B[m To Bracket   (B[0;7mM-Q(B[m Previous     (B[0;7m^B(B[m Back[74;188H(B[0;7m^â—€(B[m Prev Word     (B[0;7m^A(B[m Home[74;222H(B[0;7m^P(B[m Prev Line     (B[0;7mM-â–²(B[m Scroll Up    (B[0;7m^â–²(B[m Prev Block[75d(B[0;7m^X(B[m Exit[75;18H(B[0;7m^R(B[m Read File     (B[0;7m^\(B[m Replace[52G(B[0;7m^U(B[m Paste Text    (B[0;7m^T(B[m To Spell[86G(B[0;7m^_(B[m Go To Line    (B[0;7mM-E(B[m Redo[75;120H(B[0;7mM-6(B[m Copy Text    (B[0;7m^Q(B[m Where Was     (B[0;7mM-W(B[m Next[75;171H(B[0;7m^F(B[m Forward[188G(B[0;7m^â–¶(B[m Next Word     (B[0;7m^E(B[m End[75;222H(B[0;7m^N(B[m Next Line     (B[0;7mM-â–¼(B[m Scroll Down  (B[0;7m^â–¼(B[m Next Block[73d[2d(B[0;1m[36mimport[39m(B[m gymnasium (B[0;1m[36mas[39m(B[m gym[3d(B[0;1m[36mfrom[39m(B[m gymnasium (B[0;1m[36mimport[39m(B[m spaces[4d(B[0;1m[36mfrom[39m(B[m statistics (B[0;1m[36mimport[39m(B[m median[5d(B[0;1m[36mimport[39m(B[m numpy (B[0;1m[36mas[39m(B[m np[6d(B[0;1m[36mimport[39m(B[m mujoco_py[7d(B[0;1m[36mimport[39m(B[m os[8d(B[0;1m[36mimport[39m(B[m torch[9d(B[0;1m[36mfrom[39m(B[m torch (B[0;1m[36mimport[39m(B[m nn[10d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.vec_env (B[0;1m[36mimport[39m(B[m DummyVecEnv, VecMonitor[11d(B[0;1m[36mfrom[39m(B[m sb3_contrib (B[0;1m[36mimport[39m(B[m RecurrentPPO[12d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.callbacks (B[0;1m[36mimport[39m(B[m BaseCallback[13d(B[0;1m[36mfrom[39m(B[m sb3_contrib.ppo_recurrent.policies (B[0;1m[36mimport[39m(B[m MlpLstmPolicy[14d(B[0;1m[36mfrom[39m(B[m stable_baselines3.common.torch_layers (B[0;1m[36mimport[39m(B[m BaseFeaturesExtractor[15d(B[0;1m[36mimport[39m(B[m logging[16d(B[0;1m[36mimport[39m(B[m matplotlib.pyplot (B[0;1m[36mas[39m(B[m plt[42m  [18d(B[0;1m[31m# Set up logging[19d[39m(B[mlogging.basicConfig(level=logging.INFO)[21d(B[0;1m[36mclass[39m(B[m CustomFE(BaseFeaturesExtractor):[22;5H(B[0;1m[32m"""[23d    Custom feature extractor for HandEnv.[24d    Increases network depth and incorporates layer normalization.[25d    """[27;5H[36mdef[34m __init__[39m(B[m(self, observation_space: gym.Space, features_dim: int = 256):[28;8H(B[0;1m[31m # Ensure the observation space is as expected[29;9H[36massert[39m(B[m isinstance(observation_space, spaces.Box), (B[0;1m[32m"Observation space must be of type Box"[31;9H[39m(B[msuper(CustomFE, self).__init__(observation_space, features_dim)[33;8H(B[0;1m[31m # Define the network architecture[34;9H[39m(B[mself.net = nn.Sequential([35;13Hnn.Linear(observation_space.shape[0], 128), (B[0;1m[31m # input:24, first layer: 128[36;13H[39m(B[mnn.ReLU(),[37;13Hnn.LayerNorm(128),[38;13Hnn.Linear(128, 256),[39;13Hnn.ReLU(),[40;13Hnn.LayerNorm(256),[41;13Hnn.Linear(256, features_dim),[42;13Hnn.ReLU(),[43;13Hnn.LayerNorm(features_dim)[44;9H)[46;5H(B[0;1m[36mdef[34m forward[39m(B[m(self, observations: torch.Tensor) -> torch.Tensor:[47;9H(B[0;1m[36mreturn[39m(B[m self.net(observations)[49d(B[0;1m[36mclass[39m(B[m CustomACLstmPolicy(MlpLstmPolicy):[50;5H(B[0;1m[32m"""[51d    Custom Actor-Critic Policy with LSTM for RecurrentPPO.[52d    Integrates the CustomFeaturesExtractor.[53d    """[55;5H[36mdef[34m __init__[39m(B[m(self, *args, **kwargs):[56;9Hsuper(CustomACLstmPolicy, self).__init__([57;13H*args,[58;13H**kwargs,[59;13Hfeatures_extractor_class=CustomFE,[60;13Hfeatures_extractor_kwargs=dict(features_dim=256),[61;13Hnet_arch=[dict(pi=[256, 256], vf=[256, 256])], (B[0;1m[31m # Corrected bracket[62;13H[39m(B[mactivation_fn=nn.ReLU,[63;13Hlstm_hidden_size=128 (B[0;1m[31m # Size of LSTM hidden state[64;9H[39m(B[m)[66d(B[0;1m[36mclass[39m(B[m HandEnv(gym.Env):[67;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self):[68;9Hsuper(HandEnv, self).__init__()[69;9Hxml_path = os.path.expanduser((B[0;1m[32m'/home/easgrad/dgusain/Bot_hand/bot_hand.xml'[39m(B[m)[70;9Hlogging.info(f(B[0;1m[32m"Attempting to load Mujoco model from: {xml_path}"[39m(B[m)[72;9H(B[0;1m[36mif[39m(B[m (B[0;1m[36mnot[39m(B[m os.path.isfile(xml_path):[2d[?12l[?25h[?25l[10d[?12l[?25h[?25l[18d[?12l[?25h[?25l[26d[?12l[?25h[?25l[73d[K[50d[?12l[?25h[?25l[58d[?12l[?25h[?25l[66d[?12l[?25h[?25l[2;73r[73;1H[18S[1;75r[55;13Hlogging.error(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[56;13H(B[0;1m[36mraise[39m(B[m FileNotFoundError(f(B[0;1m[32m"XML file not found at {xml_path}."[39m(B[m)[58;9H(B[0;1m[36mtry[39m(B[m:[59dself.model = mujoco_py.load_model_from_path(xml_path)[60;13Hself.sim = mujoco_py.MjSim(self.model)[61;13Hlogging.info((B[0;1m[32m"Mujoco model loaded successfully."[39m(B[m)[62;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[63;13Hlogging.error(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[64;13H(B[0;1m[36mraise[39m(B[m RuntimeError(f(B[0;1m[32m"Failed to load Mujoco model: {e}"[39m(B[m)[66;9Hjoint_weights = [[67;13H0.5, (B[0;1m[31m # ID 0: WristJoint1[68;13H[39m(B[m0.5, (B[0;1m[31m # ID 1: WristJoint0[69;13H[39m(B[m3.0, (B[0;1m[31m # ID 2: ForeFingerJoint3 (MCP)[70;13H[39m(B[m2.5, (B[0;1m[31m # ID 3: ForeFingerJoint2 (MCP proximal)[71;13H[39m(B[m2.0, (B[0;1m[31m # ID 4: ForeFingerJoint1 (PIP)[72;13H[39m(B[m1.0, (B[0;1m[31m # ID 5: ForeFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;13H3.0, (B[0;1m[31m # ID 6: MiddleFingerJoint3 (MCP)[66;13H[39m(B[m2.5, (B[0;1m[31m # ID 7: MiddleFingerJoint2 (MCP proximal)[67;13H[39m(B[m2.0, (B[0;1m[31m # ID 8: MiddleFingerJoint1 (PIP)[68;13H[39m(B[m1.0, (B[0;1m[31m # ID 9: MiddleFingerJoint0 (DIP)[69;13H[39m(B[m3.0, (B[0;1m[31m # ID 10: RingFingerJoint3 (MCP)[70;13H[39m(B[m2.5, (B[0;1m[31m # ID 11: RingFingerJoint2 (MCP proximal)[71;13H[39m(B[m2.0, (B[0;1m[31m # ID 12: RingFingerJoint1 (PIP)[72;13H[39m(B[m1.0, (B[0;1m[31m # ID 13: RingFingerJoint0 (DIP)[39m(B[m[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;13H1.5, (B[0;1m[31m # ID 14: LittleFingerJoint4 (CMC Joint rotation)[66;13H[39m(B[m3.0, (B[0;1m[31m # ID 15: LittleFingerJoint3 (MCP)[67;13H[39m(B[m2.5, (B[0;1m[31m # ID 16: LittleFingerJoint2 (MCP proximal)[68;13H[39m(B[m2.0, (B[0;1m[31m # ID 17: LittleFingerJoint1 (PIP)[69;13H[39m(B[m1.0, (B[0;1m[31m # ID 18: LittleFingerJoint0 (DIP)[70;13H[39m(B[m1.5, (B[0;1m[31m # ID 19: ThumbJoint4 (CMC Joint abduction/adduction)[71;13H[39m(B[m1.5, (B[0;1m[31m # ID 20: ThumbJoint3 (CMC Joint flexion/extension)[72;13H[39m(B[m1.5, (B[0;1m[31m # ID 21: ThumbJoint2 (CMC Joint rotation)[39m(B[m[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;13H3.0, (B[0;1m[31m # ID 22: ThumbJoint1 (MCP)[66;13H[39m(B[m0.5, (B[0;1m[31m # ID 23: ThumbJoint0 (IP)[67;13H[39m(B[m0.0  (B[0;1m[31m # ID 24: None[68;9H[39m(B[m][69d(B[0;1m[31m # Converting joint weights to a tensor[70;9H[39m(B[mself.joint_weights = torch.tensor(joint_weights, dtype=torch.float32)[71;8H(B[0;1m[31m # Normalize the weights so that their sum equals 1[72;9H[39m(B[mself.joint_weights /= self.joint_weights.sum()[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[66;9Hself.initial_state = self.sim.get_state()[67;8H(B[0;1m[31m # Define observation and action spaces[68;9H[39m(B[mself.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(24,), dtype=np.float32)[69;9Hself.target_threshold = 99[70;9Hself.max_steps = 100[71;9Hself.steps_taken = 0[72d[?12l[?25h[?25l7[2;73r8[73d[32S[1;75r[41;8H(B[0;1m[31m # Initialize actuator ranges[42;9H[39m(B[mactuator_ranges = self.sim.model.actuator_ctrlrange[43;9Hself.actuator_min = torch.tensor(actuator_ranges[:, 0], dtype=torch.float32)[44;9Hself.actuator_max = torch.tensor(actuator_ranges[:, 1], dtype=torch.float32)[46;9Hself.action_space = spaces.Box(low=-1, high=1, shape=(self.actuator_min.shape[0],), dtype=np.float32)[47;9Hself.ground_truth_quats = torch.tensor(self.get_ground_truth_quaternion(), dtype=torch.float32)[49;5H(B[0;1m[36mdef[34m reset[39m(B[m(self, seed=(B[0;1m[35mNone[39m(B[m, options=(B[0;1m[35mNone[39m(B[m):[50;9Hsuper().reset(seed=seed)[51;8H(B[0;1m[31m #self.sim.reset()[52;9H[39m(B[mself.sim.set_state(self.initial_state)[53;9Hself.sim.forward()[54;9Hself.steps_taken = 0[55;9Hobs = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[56;9H(B[0;1m[36mreturn[39m(B[m obs, {}[58;5H(B[0;1m[36mdef[34m step[39m(B[m(self, action: np.ndarray):[59;9Haction_tensor = torch.from_numpy(action).float()[60;9Hrescaled_action = self.actuator_min + (action_tensor + 1) * (self.actuator_max - self.actuator_min) / 2[61;9Hself.sim.data.ctrl[:] = rescaled_action.numpy()[62;9Hself.sim.step()[64;8H(B[0;1m[31m # Fetch state as a tensor[65;9H[39m(B[mstate = torch.from_numpy(self.sim.data.qpos[:24].astype(np.float32))[66d[42m        [67d(B[0;1m[31m # Calculate done condition[68;9H[39m(B[mdone = self.calculate_done(state, flag=(B[0;1m[35mFalse[39m(B[m) (B[0;1m[36mor[39m(B[m self.steps_taken >= self.max_steps[69;9H(B[0;1m[36mif[39m(B[m done:[70;13Hreward = self.calculate_reward(state, flag=(B[0;1m[35mTrue[39m(B[m).item()[71;9H(B[0;1m[36melse[39m(B[m:[72dreward = -1.0 (B[0;1m[31m # Penalize extra steps[39m(B[m[?12l[?25h[?25l7[2;73r8[73d[24S[1;75r[50;9Hself.steps_taken += 1[51;9Htruncated = (B[0;1m[35mFalse[52;9H[39m(B[minfo = {}[54;9H(B[0;1m[36mreturn[39m(B[m state.numpy(), reward, done, truncated, info[56;5H(B[0;1m[36mdef[34m calculate_done[39m(B[m(self, state: torch.Tensor, flag: bool) -> bool:[57;9H(B[0;1m[36mif[39m(B[m flag:[58;13H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mFalse[59;9H[39m(B[mconfidence = self.calculate_confidence(state)[60;9H(B[0;1m[36mreturn[39m(B[m confidence >= self.target_threshold[62;5H(B[0;1m[36mdef[34m calculate_reward[39m(B[m(self, state: torch.Tensor, flag: bool) -> torch.Tensor:[63;9Hconfidence = self.calculate_confidence(state)[64;9H(B[0;1m[36mif[39m(B[m flag:[65;13H(B[0;1m[36mreturn[39m(B[m confidence - 50 (B[0;1m[31m # Reward calculation[66;9H[36melse[39m(B[m:[67d(B[0;1m[36mif[39m(B[m confidence > 85:[68;17H(B[0;1m[36mreturn[39m(B[m (confidence - 50) / 2.0 (B[0;1m[31m # Encouraging the model in the right direction[69;13H[36melse[39m(B[m:[70d(B[0;1m[36mreturn[39m(B[m (confidence - 50) / 4.0 (B[0;1m[31m # Lesser reward[72;5H[36mdef[34m calculate_confidence[39m(B[m(self, state: torch.Tensor) -> torch.Tensor:[?12l[?25h[?25l7[2;73r8[73d[16S[1;75r[57;9Hrendered_quat = self.get_rendered_pose_quaternion()[58;9Hrendered_quat = self.normalize_quaternion(rendered_quat)[59;9Hgt_quat = self.normalize_quaternion(self.ground_truth_quats)[60;9Hsimilarity = torch.abs(torch.sum(rendered_quat * gt_quat, dim=1)) * 100[61;9Hweighted_similarity = similarity * self.joint_weights[62;9Havg_confidence = weighted_similarity.sum()[63;9H(B[0;1m[36mreturn[39m(B[m avg_confidence[65;5H(B[0;1m[36mdef[34m get_rendered_pose_quaternion[39m(B[m(self) -> torch.Tensor:[66;9Hquaternions = [][67;9H(B[0;1m[36mfor[39m(B[m joint (B[0;1m[36min[39m(B[m range(self.model.njnt):[68;13Hq = self.sim.data.qpos[self.model.jnt_qposadr[joint]:self.model.jnt_qposadr[joint] + 4][69;13Hquaternions.append(q)[70;8H(B[0;1m[31m # Convert list of arrays to a single NumPy array[71;9H[39m(B[mquaternions_np = np.array(quaternions, dtype=np.float32)[72;8H(B[0;1m[31m # Convert NumPy array to PyTorch tensor[39m(B[m[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;9H(B[0;1m[36mreturn[39m(B[m torch.from_numpy(quaternions_np)[67;5H(B[0;1m[36mdef[34m normalize_quaternion[39m(B[m(self, q: torch.Tensor) -> torch.Tensor:[68;9Hnorm = torch.norm(q, dim=1, keepdim=(B[0;1m[35mTrue[39m(B[m)[69;9H(B[0;1m[36mreturn[39m(B[m q / norm.clamp(min=1e-8) (B[0;1m[31m # Prevent division by zero[71;5H[36mdef[34m get_ground_truth_quaternion[39m(B[m(self) -> np.ndarray:[72;9Hground_truth_quats = [[?12l[?25h[?25l[2;9Hrendered_quat = self.normalize_quaternion(rendered_quat)[3;8H gt_quat = self.normalize_quaternion(self.ground_truth_quats)[4;10Himilarity = torch.abs(torch.sum(rendered_quat * gt_quat, dim=1)) * 100[5;9Hweighted_similarity = similarity * self.joint_weights[6;9Havg_confidence = weighted_similarity.sum()[7;9H(B[0;1m[36mreturn[39m(B[m avg_confidence[K[8d[K[9;5H(B[0;1m[36mdef[34m get_rendered_pose_quaternion[39m(B[m(self) -> torch.Tensor:[10;5H    quaternions = [][K[11;9H(B[0;1m[36mfor[39m(B[m joint (B[0;1m[36min[39m(B[m range(self.model.njnt):[K[12;9H    q = self.sim.data.qpos[self.model.jnt_qposadr[joint]:self.model.jnt_qposadr[joint] + 4][K[13;9H    quaternions.append(q)[K[14;8H(B[0;1m[31m # Convert list of arrays to a single NumPy array[15;9H[39m(B[mquaternions_np = np.array(quaternions, dtype=np.float32)[16;11H(B[0;1m[31mConvert NumPy array to PyTorch tensor[17;9H[36mreturn[39m(B[m torch.from_numpy(quaternions_np)[K[18d[K[19;5H(B[0;1m[36mdef[34m normalize_quaternion[39m(B[m(self, q: torch.Tensor) -> torch.Tensor:[20;9Hnorm = torch.norm(q, dim=1, keepdim=(B[0;1m[35mTrue[39m(B[m)[K[21;9H(B[0;1m[36mreturn[39m(B[m q / norm.clamp(min=1e-8) (B[0;1m[31m # Prevent division by zero[22d[39m(B[m[K[23;5H(B[0;1m[36mdef[34m get_ground_truth_quaternion[39m(B[m(self) -> np.ndarray:[24;9Hground_truth_quats = [[K[25;13H[6.32658406e-04,  1.25473697e-03, -9.99718591e-03,  1.56702220e+00],[26;9H    [1.25473697e-03, -9.99718591e-03,  1.56702220e+00,  1.56730258e+00],[27;9H    [-0.00999719,  1.5670222,   1.56730258,  1.5674143],[28;9H    [1.5670222,   1.56730258,  1.5674143,  -0.00999801],[29;13H[1.56730258,  1.5674143,  -0.00999801,  1.56701926],[30;9H    [1.5674143,  -0.00999801,  1.56701926,  1.56730194],[31;13H[-0.00999801,  1.56701926,  1.56730194,  1.5674143],[32;12H[1K [1.56701926,  1.56730194,  1.5674143,  -0.00999756],[K[33;9H    [1.56730194,  1.5674143,  -0.00999756,  1.56701717],[34;13H[1.5674143,  -0.00999756,  1.56701717,  1.56730177],[35;9H    [-0.00999756,  1.56701717,  1.56730177,  1.56741429],[36;9H    [1.56701717, 1.56730177, 1.56741429, 0.00868252],[37;13H[1.56730177,  1.56741429,  0.00868252, -0.01000805],[38;12H[1K [1.56741429,  0.00868252, -0.01000805,  1.56708349],[K[39;9H    [0.00868252, -0.01000805,  1.56708349,  1.56730911],[40;9H    [-0.01000805,  1.56708349,  1.56730911,  1.56741508],[41;13H[1.56708349, 1.56730911, 1.56741508, 0.49916711],[42;9H    [1.56730911, 1.56741508, 0.49916711, 0.50022545],[43;13H[1.56741508, 0.49916711, 0.50022545, 0.25330455],[44;13H[4.99167109e-01, 5.00225452e-01, 2.53304550e-01, 4.08440608e-04],[K[45;13H[5.00225452e-01,  2.53304550e-01,  4.08440608e-04, -9.00000689e-01],[46;13H[2.53304550e-01,  4.08440608e-04, -9.00000689e-01,  1.00000000e-01],[47;13H[4.08440608e-04, -9.00000689e-01,  1.00000000e-01, -1.00000000e-01],[48;12H[1K [-0.90000069,  0.1,        -0.1,         0.01463168],[K[49;9H    [0.1,        -0.1,         0.01463168,  1.0]   [50;9H][K[51d(B[0;1m[36mreturn[39m(B[m np.array(ground_truth_quats, dtype=np.float32)[K[52d[K[53d(B[0;1m[36mclass[39m(B[m RewardCallback(BaseCallback):[K[54;5H(B[0;1m[32m"""[39m(B[m[K[55d(B[0;1m[32m    Custom callback for logging average rewards over every 100 episodes, episode numbers, and iteration numbers.[56d    Also stores average rewards for plotting.[57d    """[39m(B[m[K[58d[K[59;5H(B[0;1m[36mdef[34m __init__[39m(B[m(self, avg_interval=100):[K[60;9Hsuper(RewardCallback, self).__init__()[K[61;9Hself.episode_num = 0[K[62;8H self.max_reward = -np.inf[K[63;9Hself.avg_interval = avg_interval (B[0;1m[31m # Number of episodes per average - not hyperparameter[64;8H[39m(B[m self.sum_rewards = 0.0[K[65;9Hself.count_rewards = 0[K[66;9Hself.iteration_num = 0[42m     [67;5H[49m(B[m    self.rewards_list = [][K[68;9Hself.median_rewards = {}[K[69d[K[70;5H(B[0;1m[36mdef[34m _on_rollout_end[39m(B[m(self) -> bool:[71;5H    (B[0;1m[32m"""[39m(B[m[K[72d(B[0;1m[32m        Called at the end of a rollout.[39m(B[m[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;1H(B[0;1m[32m        Used to track the number of iterations.[66d        """[67;9H[39m(B[mself.iteration_num += 1[68;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[70;5H[36mdef[34m _on_step[39m(B[m(self) -> bool:[71;9H(B[0;1m[32m"""[72d        Called at every step. Checks if any episode has finished and logs the reward.[39m(B[m[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;1H(B[0;1m[32m        """[66;8H[31m # Retrieve 'dones' and 'rewards' from the current step[67;9H[39m(B[mdones = self.locals.get((B[0;1m[32m'dones'[39m(B[m, [])[68;9Hrewards = self.locals.get((B[0;1m[32m'rewards'[39m(B[m, [])[70;8H(B[0;1m[31m # Check if any of the environments are done[71;9H[36mif[39m(B[m np.any(dones):[72;13H(B[0;1m[36mfor[39m(B[m done, reward (B[0;1m[36min[39m(B[m zip(dones, rewards):[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;17H(B[0;1m[36mif[39m(B[m done:[66;21Hself.episode_num += 1[67;21H(B[0;1m[36mif[39m(B[m reward > self.max_reward:[68;25Hself.max_reward = reward[69;21Hself.rewards_list.append(reward)[70;21H(B[0;1m[36mif[39m(B[m len(self.rewards_list) == self.avg_interval:[71;25Hmedian_reward = median(self.rewards_list)[72;25Hblock_num = self.episode_num // self.avg_interval[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;25Hself.median_rewards[block_num] = median_reward[66;25Hprint(f(B[0;1m[32m"Iteration: {self.iteration_num} | Episodes: {block_num * self.avg_interval} | Median Reward: {median_reward:.2f} | Max Reward: {self.max_reward:.2f}"[39m(B[m)[67;24H(B[0;1m[31m # Reset sum and count[68;25H[39m(B[mself.rewards_list = [][69;9H(B[0;1m[36mreturn[39m(B[m (B[0;1m[35mTrue[71;5H[36mdef[34m _on_training_end[39m(B[m(self) -> (B[0;1m[35mNone[39m(B[m:[72;9H(B[0;1m[32m"""[39m(B[m[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;1H(B[0;1m[32m        Called at the end of training. If there are remaining episodes, compute and store the average.[66d        """[67;9H[36mif[39m(B[m len(self.rewards_list) > 0:[68;13Hmedian_reward = median(self.rewards_list)[69;13Hblock_num = self.episode_num // self.avg_interval + 1[70;13Hself.median_rewards[block_num] = median_reward[71;13Hcurrent_max = max(self.rewards_list)[72;13H(B[0;1m[36mif[39m(B[m current_max > self.max_reward:[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;17Hself.max_reward = current_max[42m           [66;13H[49m(B[mprint(f(B[0;1m[32m"Training End | Episodes: {self.episode_num} | Median Reward: {median_reward:.2f} | Max Reward: {self.max_reward:.2f}"[39m(B[m)[67;13Hself.rewards_list = [][69d(B[0;1m[36mdef[34m make_env[39m(B[m():[70;5H(B[0;1m[32m"""Utility function to create a HandEnv without Monitor."""[71;5H[36mdef[34m _init[39m(B[m():[72;9H(B[0;1m[36mtry[39m(B[m:[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;13Henv = HandEnv()[66;13H(B[0;1m[36mreturn[39m(B[m env[67;9H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[68;13Hlogging.error(f(B[0;1m[32m"Failed to initialize HandEnv: {e}"[39m(B[m)[69;13H(B[0;1m[36mraise[39m(B[m e[70;5H(B[0;1m[36mreturn[39m(B[m _init[72d(B[0;1m[36mdef[34m train_on_gpu[39m(B[m(gpu_id: int, num_envs: int, total_timesteps: int, num_steps: int, save_interval: int):[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;5Hos.environ[(B[0;1m[32m'CUDA_VISIBLE_DEVICES'[39m(B[m] = str(gpu_id)[66;5Hdevice = (B[0;1m[32m'cuda'[39m(B[m (B[0;1m[36mif[39m(B[m torch.cuda.is_available() (B[0;1m[36melse[39m(B[m (B[0;1m[32m'cpu'[67;5H[39m(B[mprint(f(B[0;1m[32m"GPU {gpu_id}: Using {device} device"[39m(B[m)[68;5Henv = DummyVecEnv([make_env() (B[0;1m[36mfor[39m(B[m _ (B[0;1m[36min[39m(B[m range(num_envs)])[42m  [69;5H[49m(B[menv = VecMonitor(env) (B[0;1m[31m # Use VecMonitor instead of individual Monitor wrappers[71;5H[39m(B[mmodel = RecurrentPPO([72;9HCustomACLstmPolicy, (B[0;1m[31m # Use the custom policy[39m(B[m[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;9Henv,[66;9Hverbose=1,[67;9Hdevice=device,[68;9Hent_coef=0.05,[68;36H(B[0;1m[31m # Adjusted entropy coefficient[69;9H[39m(B[mlearning_rate=0.0005,[70;9Hclip_range=0.4,[71;9Hn_steps=num_steps,[71;36H(B[0;1m[31m # Steps per environment per update[72;9H[39m(B[mbatch_size=4096,[72;36H(B[0;1m[31m # Adjusted batch size[39m(B[m[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;9Hgamma=0.99,[66;9Hgae_lambda=0.95,[67;9Hmax_grad_norm=0.5,[68;9Hvf_coef=0.5,[69;9Huse_sde=(B[0;1m[35mTrue[39m(B[m,[69;36H(B[0;1m[31m # Use State Dependent Exploration for better exploration[70;8H # Removed lstm_hidden_size=128[71;5H[39m(B[m)[72d[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;5Hcallback = RewardCallback(avg_interval=100)[67;5H(B[0;1m[36mtry[39m(B[m:[68dmodel.learn(total_timesteps=total_timesteps, callback=callback, log_interval=10)[69;5H(B[0;1m[36mexcept[39m(B[m Exception (B[0;1m[36mas[39m(B[m e:[70;9Hlogging.error(f(B[0;1m[32m"Error during training: {e}"[39m(B[m)[71;9H(B[0;1m[36mraise[39m(B[m e[72d[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;5Hmodel.save(f(B[0;1m[32m"/home/easgrad/dgusain/Bot_hand/agents/agent_code1_1_basic_gpu{gpu_id}"[39m(B[m)[67;5H(B[0;1m[36mreturn[39m(B[m callback (B[0;1m[31m # Return the callback to access episode_rewards[69d[36mdef[34m main[39m(B[m():[70;5Hgpu_id = 0[42m          [71;5H[49m(B[mnum_envs = 4[42m        [72;5H[49m(B[mn_iter = 4000[42m         [49m(B[m[?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[65;5Hn_steps = 1024     (B[0;1m[31m # Steps per environment per update[66;5H[39m(B[mtotal_timesteps = n_iter * n_steps * num_envs (B[0;1m[31m # Total training timesteps[67;5H[39m(B[msave_interval = 25[42m  [69;5H[49m(B[mcallback = train_on_gpu(gpu_id, num_envs, total_timesteps, n_steps, save_interval)[70;5Hblock_numbers = list(callback.median_rewards.keys())[71;5Hblock_rewards = list(callback.median_rewards.values())[72;5Hepisode_numbers = [block_num * 100 (B[0;1m[36mfor[39m(B[m block_num (B[0;1m[36min[39m(B[m block_numbers][?12l[?25h[?25l7[2;73r8[73d[8S[1;75r[66;5Hplt.figure(figsize=(10, 6))[67;5Hplt.plot(episode_numbers, block_rewards, marker=(B[0;1m[32m'o'[39m(B[m, linestyle=(B[0;1m[32m'-'[39m(B[m, color=(B[0;1m[32m'b'[39m(B[m)[68;5Hplt.xlabel((B[0;1m[32m'Episode Number'[39m(B[m)[69;5Hplt.ylabel((B[0;1m[32m'Median Reward (per 100 episodes)'[39m(B[m)[70;5Hplt.title((B[0;1m[32m'Median Reward vs Episode Number'[39m(B[m)[71;5Hplt.grid((B[0;1m[35mTrue[39m(B[m)[72;5Hplt.savefig((B[0;1m[32m'/home/easgrad/dgusain/Bot_hand/figs/fig_code1_1_basic.png'[39m(B[m)[42m  [49m(B[m[?12l[?25h[?25l7[2;73r8[73d[6S[1;75r[67;5Hplt.close()[42m [68;5H[49m(B[mprint((B[0;1m[32m"Plot saved as fig_code1_1_basic.png"[39m(B[m)[70d(B[0;1m[36mif[39m(B[m __name__ == (B[0;1m[32m"__main__"[39m(B[m:[71;5Hmain()[72d[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[?12l[?25h[?25l[64d[?12l[?25h[?25l[56d[?12l[?25h[?25l[48d[?12l[?25h[?25l[40d[?12l[?25h[?25l[32d[?12l[?25h[?25l[40d[?12l[?25h[?25l[48d[?12l[?25h[?25l[56d[?12l[?25h[?25l[64d[?12l[?25h[?25l[72d[?12l[?25h[?25l[64d[?12l[?25h[?25l[56d[?12l[?25h[?25l[48d[?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25l [?12l[?25h[?25lg[?12l[?25h[?25lp[?12l[?25h[?25lu[?12l[?25h[?25l_[?12l[?25h[?25li[?12l[?25h[?25ld[?12l[?25h[?25l [?12l[?25h[?25l=[?12l[?25h[?25l [?12l[?25h[?25l0[?12l[?25h[?25l[1;273H(B[0;7mModified(B[m[48;13H[42m [49m(B[m[1P[?12l[?25h[?25l 5[48;24H[42m [48;15H[49m(B[m[?12l[?25h[?25l[73d(B[0;7mSave modified buffer?                                                                                                                                                                                                                                                                     [74;1H Y(B[m Yes[K[75d(B[0;7m N(B[m No  [75;17H(B[0;7m^C(B[m Cancel[K[73;23H[?12l[?25h[?25l[74d(B[0;7m^G(B[m Get Help[74;71H(B[0;7mM-D(B[m DOS Format[74;141H(B[0;7mM-A(B[m Append[74;211H(B[0;7mM-B(B[m Backup File[75d(B[0;7m^C(B[m Cancel[17G         [75;71H(B[0;7mM-M(B[m Mac Format[75;141H(B[0;7mM-P(B[m Prepend[75;211H(B[0;7m^T(B[m To Files[73d(B[0;7mFile Name to Write: code1_1_basic_custom.py(B[m[73;44H[?12l[?25h[?25l[73;134H[1K (B[0;7m[ Writing... ](B[m[K[1;273H(B[0;7m        (B[m[73;132H(B[0;7m[ Wrote 366 lines ](B[m[J[75d[?12l[?25h[75;1H[?1049l[23;0;0t[?1l>[?2004l(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> clear
[H[2J(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python code1_1_basic_custom.py
GPU 5: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
Iteration: 2 | Episodes: 100 | Median Reward: 18.49 | Max Reward: 36.04
Iteration: 4 | Episodes: 200 | Median Reward: 12.58 | Max Reward: 36.04
Iteration: 7 | Episodes: 300 | Median Reward: 11.85 | Max Reward: 36.04
Iteration: 9 | Episodes: 400 | Median Reward: 12.56 | Max Reward: 36.04
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -85         |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 10          |
|    time_elapsed         | 197         |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.031859145 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.4         |
|    entropy_loss         | -70.4       |
|    explained_variance   | 0.0169      |
|    learning_rate        | 0.0005      |
|    loss                 | 90.5        |
|    n_updates            | 90          |
|    policy_gradient_loss | -3.55e-05   |
|    std                  | 1           |
|    value_loss           | 196         |
-----------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 22.62 | Max Reward: 38.30
Iteration: 14 | Episodes: 600 | Median Reward: 16.58 | Max Reward: 38.30
Iteration: 17 | Episodes: 700 | Median Reward: 16.62 | Max Reward: 38.30
Iteration: 19 | Episodes: 800 | Median Reward: 14.54 | Max Reward: 39.84
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -83.7      |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 20         |
|    time_elapsed         | 390        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.02264142 |
|    clip_fraction        | 0.00022    |
|    clip_range           | 0.4        |
|    entropy_loss         | -76.4      |
|    explained_variance   | 0.351      |
|    learning_rate        | 0.0005     |
|    loss                 | 65.2       |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.025     |
|    std                  | 1          |
|    value_loss           | 158        |
----------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 22.25 | Max Reward: 39.84
Iteration: 24 | Episodes: 1000 | Median Reward: 19.19 | Max Reward: 39.84
Iteration: 27 | Episodes: 1100 | Median Reward: 18.22 | Max Reward: 39.84
Iteration: 29 | Episodes: 1200 | Median Reward: 16.67 | Max Reward: 39.84
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -85.8      |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 30         |
|    time_elapsed         | 587        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.06453745 |
|    clip_fraction        | 0.0539     |
|    clip_range           | 0.4        |
|    entropy_loss         | -90.1      |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0005     |
|    loss                 | 9.86       |
|    n_updates            | 290        |
|    policy_gradient_loss | 0.0305     |
|    std                  | 1          |
|    value_loss           | 51.1       |
----------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 17.30 | Max Reward: 39.84
Iteration: 34 | Episodes: 1400 | Median Reward: 17.40 | Max Reward: 39.84
Iteration: 36 | Episodes: 1500 | Median Reward: 17.95 | Max Reward: 39.84
Iteration: 39 | Episodes: 1600 | Median Reward: 19.75 | Max Reward: 39.84
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -81.4      |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 40         |
|    time_elapsed         | 790        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.10602534 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.4        |
|    entropy_loss         | -110       |
|    explained_variance   | 0.937      |
|    learning_rate        | 0.0005     |
|    loss                 | 2.27       |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0562    |
|    std                  | 1          |
|    value_loss           | 20.4       |
----------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 23.64 | Max Reward: 39.84
Iteration: 44 | Episodes: 1800 | Median Reward: 24.27 | Max Reward: 40.22
Iteration: 46 | Episodes: 1900 | Median Reward: 14.64 | Max Reward: 40.22
Iteration: 49 | Episodes: 2000 | Median Reward: 16.93 | Max Reward: 40.22
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -80.6      |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 50         |
|    time_elapsed         | 996        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.03643041 |
|    clip_fraction        | 0.0534     |
|    clip_range           | 0.4        |
|    entropy_loss         | -128       |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0005     |
|    loss                 | -4.58      |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0162    |
|    std                  | 1          |
|    value_loss           | 10.3       |
----------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 20.40 | Max Reward: 44.70
Iteration: 54 | Episodes: 2200 | Median Reward: 23.54 | Max Reward: 44.70
Iteration: 56 | Episodes: 2300 | Median Reward: 25.89 | Max Reward: 44.70
Iteration: 59 | Episodes: 2400 | Median Reward: 24.65 | Max Reward: 44.70
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.5        |
| time/                   |              |
|    fps                  | 206          |
|    iterations           | 60           |
|    time_elapsed         | 1191         |
|    total_timesteps      | 245760       |
| train/                  |              |
|    approx_kl            | 0.0026210216 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.4          |
|    entropy_loss         | -138         |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0005       |
|    loss                 | -3.61        |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00394     |
|    std                  | 1.01         |
|    value_loss           | 14.5         |
------------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 26.46 | Max Reward: 44.70
Iteration: 64 | Episodes: 2600 | Median Reward: 23.84 | Max Reward: 44.70
Iteration: 66 | Episodes: 2700 | Median Reward: 23.70 | Max Reward: 44.70
Iteration: 69 | Episodes: 2800 | Median Reward: 22.25 | Max Reward: 44.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76         |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 70          |
|    time_elapsed         | 1393        |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.015736185 |
|    clip_fraction        | 0.00269     |
|    clip_range           | 0.4         |
|    entropy_loss         | -146        |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.28       |
|    n_updates            | 690         |
|    policy_gradient_loss | -0.0156     |
|    std                  | 1.01        |
|    value_loss           | 11          |
-----------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 30.64 | Max Reward: 44.70
Iteration: 73 | Episodes: 3000 | Median Reward: 22.22 | Max Reward: 44.70
Iteration: 76 | Episodes: 3100 | Median Reward: 29.98 | Max Reward: 44.70
Iteration: 78 | Episodes: 3200 | Median Reward: 28.49 | Max Reward: 44.70
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72          |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 80           |
|    time_elapsed         | 1596         |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0010645083 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.4          |
|    entropy_loss         | -152         |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.04        |
|    n_updates            | 790          |
|    policy_gradient_loss | 0.00162      |
|    std                  | 1.01         |
|    value_loss           | 7.26         |
------------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 25.51 | Max Reward: 44.70
Iteration: 83 | Episodes: 3400 | Median Reward: 24.03 | Max Reward: 44.70
Iteration: 86 | Episodes: 3500 | Median Reward: 25.33 | Max Reward: 44.70
Iteration: 88 | Episodes: 3600 | Median Reward: 20.68 | Max Reward: 44.70
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -72.2      |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 90         |
|    time_elapsed         | 1789       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.16835588 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.4        |
|    entropy_loss         | -160       |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0005     |
|    loss                 | -4.99      |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0403    |
|    std                  | 1.01       |
|    value_loss           | 5.5        |
----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 27.74 | Max Reward: 44.70
Iteration: 93 | Episodes: 3800 | Median Reward: 29.51 | Max Reward: 44.70
Iteration: 96 | Episodes: 3900 | Median Reward: 26.76 | Max Reward: 44.70
Iteration: 98 | Episodes: 4000 | Median Reward: 28.16 | Max Reward: 44.70
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.4       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 100         |
|    time_elapsed         | 1994        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.013501523 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.4         |
|    entropy_loss         | -168        |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0005      |
|    loss                 | -6.09       |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.011      |
|    std                  | 1.01        |
|    value_loss           | 4.66        |
-----------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 34.45 | Max Reward: 44.70
Iteration: 103 | Episodes: 4200 | Median Reward: 31.03 | Max Reward: 44.70
Iteration: 106 | Episodes: 4300 | Median Reward: 29.36 | Max Reward: 44.70
Iteration: 108 | Episodes: 4400 | Median Reward: 32.09 | Max Reward: 46.75
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.9       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 110         |
|    time_elapsed         | 2188        |
|    total_timesteps      | 450560      |
| train/                  |             |
|    approx_kl            | 0.040484462 |
|    clip_fraction        | 0.00564     |
|    clip_range           | 0.4         |
|    entropy_loss         | -172        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -8.2        |
|    n_updates            | 1090        |
|    policy_gradient_loss | -0.0322     |
|    std                  | 1.01        |
|    value_loss           | 2.28        |
-----------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 33.45 | Max Reward: 46.75
Iteration: 113 | Episodes: 4600 | Median Reward: 32.16 | Max Reward: 46.97
Iteration: 115 | Episodes: 4700 | Median Reward: 34.18 | Max Reward: 46.97
Iteration: 118 | Episodes: 4800 | Median Reward: 35.09 | Max Reward: 47.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.3       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 120         |
|    time_elapsed         | 2389        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.018373642 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.4         |
|    entropy_loss         | -176        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -8.15       |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0146     |
|    std                  | 1.01        |
|    value_loss           | 2.76        |
-----------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 34.61 | Max Reward: 47.92
Iteration: 123 | Episodes: 5000 | Median Reward: 32.69 | Max Reward: 47.92
Iteration: 125 | Episodes: 5100 | Median Reward: 33.76 | Max Reward: 47.92
Iteration: 128 | Episodes: 5200 | Median Reward: 30.78 | Max Reward: 47.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.6       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 130         |
|    time_elapsed         | 2589        |
|    total_timesteps      | 532480      |
| train/                  |             |
|    approx_kl            | 0.017383508 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -178        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.53       |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00997    |
|    std                  | 1.01        |
|    value_loss           | 2.35        |
-----------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 35.15 | Max Reward: 47.92
Iteration: 133 | Episodes: 5400 | Median Reward: 35.67 | Max Reward: 47.92
Iteration: 135 | Episodes: 5500 | Median Reward: 37.83 | Max Reward: 47.92
Iteration: 138 | Episodes: 5600 | Median Reward: 34.00 | Max Reward: 47.92
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -67.6      |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 140        |
|    time_elapsed         | 2787       |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.02315431 |
|    clip_fraction        | 0.0022     |
|    clip_range           | 0.4        |
|    entropy_loss         | -185       |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0005     |
|    loss                 | -6.77      |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.000198  |
|    std                  | 1.01       |
|    value_loss           | 4.67       |
----------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 32.99 | Max Reward: 47.92
Iteration: 143 | Episodes: 5800 | Median Reward: 32.59 | Max Reward: 47.92
Iteration: 145 | Episodes: 5900 | Median Reward: 31.60 | Max Reward: 47.92
Iteration: 147 | Episodes: 6000 | Median Reward: 34.61 | Max Reward: 47.92
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.4         |
| time/                   |               |
|    fps                  | 206           |
|    iterations           | 150           |
|    time_elapsed         | 2976          |
|    total_timesteps      | 614400        |
| train/                  |               |
|    approx_kl            | 0.00034775748 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -186          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.56         |
|    n_updates            | 1490          |
|    policy_gradient_loss | 0.00307       |
|    std                  | 1.01          |
|    value_loss           | 5.75          |
-------------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 37.49 | Max Reward: 47.92
Iteration: 152 | Episodes: 6200 | Median Reward: 38.86 | Max Reward: 47.92
Iteration: 155 | Episodes: 6300 | Median Reward: 35.77 | Max Reward: 47.92
Iteration: 157 | Episodes: 6400 | Median Reward: 38.01 | Max Reward: 47.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.8       |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 160         |
|    time_elapsed         | 3177        |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.006894438 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -188        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.69       |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00487    |
|    std                  | 1.01        |
|    value_loss           | 3.37        |
-----------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 38.39 | Max Reward: 47.92
Iteration: 162 | Episodes: 6600 | Median Reward: 36.39 | Max Reward: 47.92
Iteration: 165 | Episodes: 6700 | Median Reward: 36.11 | Max Reward: 47.92
Iteration: 167 | Episodes: 6800 | Median Reward: 38.17 | Max Reward: 47.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.4       |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 170         |
|    time_elapsed         | 3375        |
|    total_timesteps      | 696320      |
| train/                  |             |
|    approx_kl            | 0.015460392 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -193        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -8.76       |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0171     |
|    std                  | 1.01        |
|    value_loss           | 1.29        |
-----------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 39.41 | Max Reward: 47.92
Iteration: 172 | Episodes: 7000 | Median Reward: 39.43 | Max Reward: 47.92
Iteration: 175 | Episodes: 7100 | Median Reward: 40.03 | Max Reward: 47.93
Iteration: 177 | Episodes: 7200 | Median Reward: 37.68 | Max Reward: 47.95
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -64.4     |
| time/                   |           |
|    fps                  | 206       |
|    iterations           | 180       |
|    time_elapsed         | 3573      |
|    total_timesteps      | 737280    |
| train/                  |           |
|    approx_kl            | 0.1048495 |
|    clip_fraction        | 0.125     |
|    clip_range           | 0.4       |
|    entropy_loss         | -194      |
|    explained_variance   | 0.997     |
|    learning_rate        | 0.0005    |
|    loss                 | -9.28     |
|    n_updates            | 1790      |
|    policy_gradient_loss | -0.0871   |
|    std                  | 1.01      |
|    value_loss           | 3.2       |
---------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 37.30 | Max Reward: 47.95
Iteration: 182 | Episodes: 7400 | Median Reward: 39.29 | Max Reward: 47.95
Iteration: 184 | Episodes: 7500 | Median Reward: 40.43 | Max Reward: 47.95
Iteration: 187 | Episodes: 7600 | Median Reward: 39.75 | Max Reward: 47.95
Iteration: 189 | Episodes: 7700 | Median Reward: 39.03 | Max Reward: 47.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.3       |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 190         |
|    time_elapsed         | 3772        |
|    total_timesteps      | 778240      |
| train/                  |             |
|    approx_kl            | 0.005327353 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -197        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -8.78       |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0066     |
|    std                  | 1.02        |
|    value_loss           | 2.62        |
-----------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 38.44 | Max Reward: 47.95
Iteration: 194 | Episodes: 7900 | Median Reward: 36.08 | Max Reward: 47.95
Iteration: 197 | Episodes: 8000 | Median Reward: 39.62 | Max Reward: 47.96
Iteration: 199 | Episodes: 8100 | Median Reward: 41.48 | Max Reward: 47.96
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.8        |
| time/                   |              |
|    fps                  | 206          |
|    iterations           | 200          |
|    time_elapsed         | 3963         |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0048335018 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -199         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.99        |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 1.02         |
|    value_loss           | 1.86         |
------------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 40.54 | Max Reward: 48.22
Iteration: 204 | Episodes: 8300 | Median Reward: 40.01 | Max Reward: 48.22
Iteration: 207 | Episodes: 8400 | Median Reward: 39.26 | Max Reward: 48.22
Iteration: 209 | Episodes: 8500 | Median Reward: 43.04 | Max Reward: 48.22
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.3       |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 210         |
|    time_elapsed         | 4167        |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.004504132 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -200        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.32       |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00812    |
|    std                  | 1.02        |
|    value_loss           | 3.85        |
-----------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 42.82 | Max Reward: 48.22
Iteration: 214 | Episodes: 8700 | Median Reward: 40.43 | Max Reward: 48.22
Iteration: 216 | Episodes: 8800 | Median Reward: 43.46 | Max Reward: 48.23
Iteration: 219 | Episodes: 8900 | Median Reward: 39.64 | Max Reward: 48.23
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -61.4      |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 220        |
|    time_elapsed         | 4365       |
|    total_timesteps      | 901120     |
| train/                  |            |
|    approx_kl            | 0.04574824 |
|    clip_fraction        | 0.00122    |
|    clip_range           | 0.4        |
|    entropy_loss         | -203       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -9.7       |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.0317    |
|    std                  | 1.02       |
|    value_loss           | 2.8        |
----------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 43.01 | Max Reward: 48.23
Iteration: 224 | Episodes: 9100 | Median Reward: 43.56 | Max Reward: 48.53
Iteration: 226 | Episodes: 9200 | Median Reward: 40.19 | Max Reward: 48.53
Iteration: 229 | Episodes: 9300 | Median Reward: 40.56 | Max Reward: 48.53
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -59        |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 230        |
|    time_elapsed         | 4565       |
|    total_timesteps      | 942080     |
| train/                  |            |
|    approx_kl            | 0.09139232 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.4        |
|    entropy_loss         | -204       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -8.01      |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.0572    |
|    std                  | 1.02       |
|    value_loss           | 2.92       |
----------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 39.43 | Max Reward: 48.53
Iteration: 234 | Episodes: 9500 | Median Reward: 42.22 | Max Reward: 48.53
Iteration: 236 | Episodes: 9600 | Median Reward: 42.18 | Max Reward: 48.53
Iteration: 239 | Episodes: 9700 | Median Reward: 43.29 | Max Reward: 48.53
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.3        |
| time/                   |              |
|    fps                  | 206          |
|    iterations           | 240          |
|    time_elapsed         | 4757         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0046891393 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -206         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.64        |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00349     |
|    std                  | 1.02         |
|    value_loss           | 1.77         |
------------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 43.47 | Max Reward: 48.53
Iteration: 244 | Episodes: 9900 | Median Reward: 39.47 | Max Reward: 48.53
Iteration: 246 | Episodes: 10000 | Median Reward: 42.55 | Max Reward: 48.53
Iteration: 249 | Episodes: 10100 | Median Reward: 43.17 | Max Reward: 48.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.6       |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 250         |
|    time_elapsed         | 4958        |
|    total_timesteps      | 1024000     |
| train/                  |             |
|    approx_kl            | 0.009153619 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -207        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.22       |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.0124     |
|    std                  | 1.02        |
|    value_loss           | 2.74        |
-----------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 40.73 | Max Reward: 48.53
Iteration: 253 | Episodes: 10300 | Median Reward: 43.38 | Max Reward: 48.53
Iteration: 256 | Episodes: 10400 | Median Reward: 40.07 | Max Reward: 48.53
Iteration: 258 | Episodes: 10500 | Median Reward: 43.37 | Max Reward: 48.53
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 206          |
|    iterations           | 260          |
|    time_elapsed         | 5166         |
|    total_timesteps      | 1064960      |
| train/                  |              |
|    approx_kl            | 0.0073354635 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -209         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.26        |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.00659     |
|    std                  | 1.02         |
|    value_loss           | 3.58         |
------------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 44.56 | Max Reward: 48.53
Iteration: 263 | Episodes: 10700 | Median Reward: 43.62 | Max Reward: 48.53
Iteration: 266 | Episodes: 10800 | Median Reward: 42.95 | Max Reward: 48.53
Iteration: 268 | Episodes: 10900 | Median Reward: 42.40 | Max Reward: 48.53
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.2       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 270         |
|    time_elapsed         | 5369        |
|    total_timesteps      | 1105920     |
| train/                  |             |
|    approx_kl            | 0.008692045 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -210        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -1.73       |
|    n_updates            | 2690        |
|    policy_gradient_loss | -0.0154     |
|    std                  | 1.02        |
|    value_loss           | 5.46        |
-----------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 39.57 | Max Reward: 48.53
Iteration: 273 | Episodes: 11100 | Median Reward: 43.42 | Max Reward: 48.53
Iteration: 276 | Episodes: 11200 | Median Reward: 43.02 | Max Reward: 48.53
Iteration: 278 | Episodes: 11300 | Median Reward: 43.73 | Max Reward: 49.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.3       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 280         |
|    time_elapsed         | 5569        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.004064317 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -210        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -8.71       |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.00299    |
|    std                  | 1.03        |
|    value_loss           | 1.61        |
-----------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 43.10 | Max Reward: 49.16
Iteration: 283 | Episodes: 11500 | Median Reward: 42.58 | Max Reward: 49.16
Iteration: 286 | Episodes: 11600 | Median Reward: 42.84 | Max Reward: 49.16
Iteration: 288 | Episodes: 11700 | Median Reward: 44.43 | Max Reward: 49.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 290         |
|    time_elapsed         | 5762        |
|    total_timesteps      | 1187840     |
| train/                  |             |
|    approx_kl            | 0.001978028 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -211        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -8.36       |
|    n_updates            | 2890        |
|    policy_gradient_loss | -0.00319    |
|    std                  | 1.03        |
|    value_loss           | 2.52        |
-----------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 43.31 | Max Reward: 49.16
Iteration: 293 | Episodes: 11900 | Median Reward: 43.05 | Max Reward: 49.16
Iteration: 295 | Episodes: 12000 | Median Reward: 42.97 | Max Reward: 49.16
Iteration: 298 | Episodes: 12100 | Median Reward: 40.09 | Max Reward: 49.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 300          |
|    time_elapsed         | 5965         |
|    total_timesteps      | 1228800      |
| train/                  |              |
|    approx_kl            | 0.0055366103 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -213         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.03        |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.002       |
|    std                  | 1.03         |
|    value_loss           | 1.45         |
------------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 43.38 | Max Reward: 49.16
Iteration: 303 | Episodes: 12300 | Median Reward: 44.24 | Max Reward: 49.16
Iteration: 305 | Episodes: 12400 | Median Reward: 43.06 | Max Reward: 49.16
Iteration: 308 | Episodes: 12500 | Median Reward: 44.22 | Max Reward: 49.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.7       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 310         |
|    time_elapsed         | 6170        |
|    total_timesteps      | 1269760     |
| train/                  |             |
|    approx_kl            | 0.000586541 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -214        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.42       |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.000621   |
|    std                  | 1.03        |
|    value_loss           | 3.7         |
-----------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: 43.00 | Max Reward: 49.16
Iteration: 313 | Episodes: 12700 | Median Reward: 43.74 | Max Reward: 49.16
Iteration: 315 | Episodes: 12800 | Median Reward: 43.04 | Max Reward: 49.16
Iteration: 318 | Episodes: 12900 | Median Reward: 45.69 | Max Reward: 49.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.3       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 320         |
|    time_elapsed         | 6368        |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.016686354 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -215        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.62       |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.00793    |
|    std                  | 1.03        |
|    value_loss           | 1.76        |
-----------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: 44.27 | Max Reward: 49.16
Iteration: 323 | Episodes: 13100 | Median Reward: 44.02 | Max Reward: 49.16
Iteration: 325 | Episodes: 13200 | Median Reward: 47.64 | Max Reward: 49.16
Iteration: 327 | Episodes: 13300 | Median Reward: 47.41 | Max Reward: 49.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 330         |
|    time_elapsed         | 6564        |
|    total_timesteps      | 1351680     |
| train/                  |             |
|    approx_kl            | 0.009932863 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -215        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.91       |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.0111     |
|    std                  | 1.03        |
|    value_loss           | 2.99        |
-----------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: 43.43 | Max Reward: 49.16
Iteration: 332 | Episodes: 13500 | Median Reward: 43.69 | Max Reward: 49.16
Iteration: 335 | Episodes: 13600 | Median Reward: 43.54 | Max Reward: 49.16
Iteration: 337 | Episodes: 13700 | Median Reward: 42.90 | Max Reward: 49.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57         |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 340         |
|    time_elapsed         | 6757        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.016977727 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -217        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.6       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.0113     |
|    std                  | 1.03        |
|    value_loss           | 1.31        |
-----------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: 44.04 | Max Reward: 49.16
Iteration: 342 | Episodes: 13900 | Median Reward: 43.69 | Max Reward: 49.16
Iteration: 345 | Episodes: 14000 | Median Reward: 42.36 | Max Reward: 49.16
Iteration: 347 | Episodes: 14100 | Median Reward: 44.28 | Max Reward: 49.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.2       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 350         |
|    time_elapsed         | 6964        |
|    total_timesteps      | 1433600     |
| train/                  |             |
|    approx_kl            | 0.054821514 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.4         |
|    entropy_loss         | -218        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.6       |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.0237     |
|    std                  | 1.03        |
|    value_loss           | 1.39        |
-----------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: 44.26 | Max Reward: 49.16
Iteration: 352 | Episodes: 14300 | Median Reward: 46.61 | Max Reward: 49.16
Iteration: 355 | Episodes: 14400 | Median Reward: 45.77 | Max Reward: 49.16
Iteration: 357 | Episodes: 14500 | Median Reward: 43.65 | Max Reward: 49.16
Iteration: 359 | Episodes: 14600 | Median Reward: 44.32 | Max Reward: 49.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.2       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 360         |
|    time_elapsed         | 7167        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.009394441 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -217        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -8.48       |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.00702    |
|    std                  | 1.04        |
|    value_loss           | 1.9         |
-----------------------------------------
Iteration: 362 | Episodes: 14700 | Median Reward: 45.07 | Max Reward: 49.16
Iteration: 364 | Episodes: 14800 | Median Reward: 43.85 | Max Reward: 49.16
Iteration: 367 | Episodes: 14900 | Median Reward: 46.98 | Max Reward: 49.16
Iteration: 369 | Episodes: 15000 | Median Reward: 47.58 | Max Reward: 49.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.1        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 370          |
|    time_elapsed         | 7366         |
|    total_timesteps      | 1515520      |
| train/                  |              |
|    approx_kl            | 0.0040741637 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -219         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.1        |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00288     |
|    std                  | 1.04         |
|    value_loss           | 1.5          |
------------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: 44.20 | Max Reward: 49.16
Iteration: 374 | Episodes: 15200 | Median Reward: 44.43 | Max Reward: 49.16
Iteration: 377 | Episodes: 15300 | Median Reward: 47.31 | Max Reward: 49.16
Iteration: 379 | Episodes: 15400 | Median Reward: 46.42 | Max Reward: 49.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.5       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 380         |
|    time_elapsed         | 7558        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.004498313 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -219        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.5       |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00404    |
|    std                  | 1.04        |
|    value_loss           | 1.3         |
-----------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: 47.12 | Max Reward: 49.16
Iteration: 384 | Episodes: 15600 | Median Reward: 46.99 | Max Reward: 49.16
Iteration: 387 | Episodes: 15700 | Median Reward: 47.73 | Max Reward: 49.16
Iteration: 389 | Episodes: 15800 | Median Reward: 47.41 | Max Reward: 49.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 390          |
|    time_elapsed         | 7755         |
|    total_timesteps      | 1597440      |
| train/                  |              |
|    approx_kl            | 0.0020402535 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -219         |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0005       |
|    loss                 | -4.51        |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.00127     |
|    std                  | 1.04         |
|    value_loss           | 12.1         |
------------------------------------------
Iteration: 391 | Episodes: 15900 | Median Reward: 47.46 | Max Reward: 49.16
Iteration: 394 | Episodes: 16000 | Median Reward: 46.80 | Max Reward: 49.16
Iteration: 396 | Episodes: 16100 | Median Reward: 47.57 | Max Reward: 49.16
Iteration: 399 | Episodes: 16200 | Median Reward: 47.36 | Max Reward: 49.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.2        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 400          |
|    time_elapsed         | 7963         |
|    total_timesteps      | 1638400      |
| train/                  |              |
|    approx_kl            | 0.0025728252 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -220         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -10          |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.00345     |
|    std                  | 1.04         |
|    value_loss           | 5.93         |
------------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: 47.22 | Max Reward: 49.16
Iteration: 404 | Episodes: 16400 | Median Reward: 46.41 | Max Reward: 49.16
Iteration: 406 | Episodes: 16500 | Median Reward: 46.94 | Max Reward: 49.16
Iteration: 409 | Episodes: 16600 | Median Reward: 45.65 | Max Reward: 49.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.6       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 410         |
|    time_elapsed         | 8164        |
|    total_timesteps      | 1679360     |
| train/                  |             |
|    approx_kl            | 0.003975778 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -221        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.2       |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.0047     |
|    std                  | 1.04        |
|    value_loss           | 7.58        |
-----------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: 45.33 | Max Reward: 49.16
Iteration: 414 | Episodes: 16800 | Median Reward: 47.41 | Max Reward: 49.16
Iteration: 416 | Episodes: 16900 | Median Reward: 47.22 | Max Reward: 49.16
Iteration: 419 | Episodes: 17000 | Median Reward: 47.58 | Max Reward: 49.16
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53           |
| time/                   |               |
|    fps                  | 205           |
|    iterations           | 420           |
|    time_elapsed         | 8364          |
|    total_timesteps      | 1720320       |
| train/                  |               |
|    approx_kl            | 0.00046824274 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -221          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.6         |
|    n_updates            | 4190          |
|    policy_gradient_loss | 6.36e-05      |
|    std                  | 1.04          |
|    value_loss           | 1.08          |
-------------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: 47.01 | Max Reward: 49.16
Iteration: 423 | Episodes: 17200 | Median Reward: 46.56 | Max Reward: 49.16
Iteration: 426 | Episodes: 17300 | Median Reward: 43.68 | Max Reward: 49.16
Iteration: 428 | Episodes: 17400 | Median Reward: 46.29 | Max Reward: 49.16
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -53.6      |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 430        |
|    time_elapsed         | 8554       |
|    total_timesteps      | 1761280    |
| train/                  |            |
|    approx_kl            | 0.01872948 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -222       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -10.8      |
|    n_updates            | 4290       |
|    policy_gradient_loss | -0.021     |
|    std                  | 1.05       |
|    value_loss           | 0.662      |
----------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: 47.22 | Max Reward: 49.16
Iteration: 433 | Episodes: 17600 | Median Reward: 47.22 | Max Reward: 49.16
Iteration: 436 | Episodes: 17700 | Median Reward: 44.36 | Max Reward: 49.16
Iteration: 438 | Episodes: 17800 | Median Reward: 46.80 | Max Reward: 49.16
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.5       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 440         |
|    time_elapsed         | 8756        |
|    total_timesteps      | 1802240     |
| train/                  |             |
|    approx_kl            | 0.033960715 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -223        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.1       |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0122     |
|    std                  | 1.05        |
|    value_loss           | 1.98        |
-----------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 47.14 | Max Reward: 49.16
Iteration: 443 | Episodes: 18000 | Median Reward: 47.64 | Max Reward: 49.16
Iteration: 446 | Episodes: 18100 | Median Reward: 47.28 | Max Reward: 49.16
Iteration: 448 | Episodes: 18200 | Median Reward: 47.57 | Max Reward: 49.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.2        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 450          |
|    time_elapsed         | 8959         |
|    total_timesteps      | 1843200      |
| train/                  |              |
|    approx_kl            | 0.0036319303 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -224         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.06        |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 1.05         |
|    value_loss           | 3.05         |
------------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 47.32 | Max Reward: 49.16
Iteration: 453 | Episodes: 18400 | Median Reward: 46.56 | Max Reward: 49.16
Iteration: 455 | Episodes: 18500 | Median Reward: 47.08 | Max Reward: 49.16
Iteration: 458 | Episodes: 18600 | Median Reward: 47.64 | Max Reward: 49.16
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.2        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 460          |
|    time_elapsed         | 9159         |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 0.0049488917 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -225         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -9.7         |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.00238     |
|    std                  | 1.05         |
|    value_loss           | 2.1          |
------------------------------------------
Iteration: 460 | Episodes: 18700 | Median Reward: 47.55 | Max Reward: 49.24
Iteration: 463 | Episodes: 18800 | Median Reward: 47.40 | Max Reward: 49.24
Iteration: 465 | Episodes: 18900 | Median Reward: 47.22 | Max Reward: 49.24
Iteration: 468 | Episodes: 19000 | Median Reward: 47.64 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 205           |
|    iterations           | 470           |
|    time_elapsed         | 9351          |
|    total_timesteps      | 1925120       |
| train/                  |               |
|    approx_kl            | 0.00035845715 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -225          |
|    explained_variance   | 0.989         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.26         |
|    n_updates            | 4690          |
|    policy_gradient_loss | -0.000658     |
|    std                  | 1.05          |
|    value_loss           | 9.67          |
-------------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 47.33 | Max Reward: 49.24
Iteration: 473 | Episodes: 19200 | Median Reward: 47.76 | Max Reward: 49.24
Iteration: 475 | Episodes: 19300 | Median Reward: 47.49 | Max Reward: 49.24
Iteration: 478 | Episodes: 19400 | Median Reward: 44.43 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54.9       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 480         |
|    time_elapsed         | 9549        |
|    total_timesteps      | 1966080     |
| train/                  |             |
|    approx_kl            | 0.061614167 |
|    clip_fraction        | 0.0748      |
|    clip_range           | 0.4         |
|    entropy_loss         | -226        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.6       |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.0391     |
|    std                  | 1.06        |
|    value_loss           | 1.67        |
-----------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 47.21 | Max Reward: 49.24
Iteration: 483 | Episodes: 19600 | Median Reward: 47.21 | Max Reward: 49.24
Iteration: 485 | Episodes: 19700 | Median Reward: 47.15 | Max Reward: 49.24
Iteration: 488 | Episodes: 19800 | Median Reward: 47.23 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.8         |
| time/                   |               |
|    fps                  | 205           |
|    iterations           | 490           |
|    time_elapsed         | 9749          |
|    total_timesteps      | 2007040       |
| train/                  |               |
|    approx_kl            | 0.00022444903 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -228          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.7         |
|    n_updates            | 4890          |
|    policy_gradient_loss | 8.86e-05      |
|    std                  | 1.06          |
|    value_loss           | 1.83          |
-------------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 47.35 | Max Reward: 49.24
Iteration: 492 | Episodes: 20000 | Median Reward: 47.33 | Max Reward: 49.24
Iteration: 495 | Episodes: 20100 | Median Reward: 47.53 | Max Reward: 49.24
Iteration: 497 | Episodes: 20200 | Median Reward: 46.90 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | -52.6       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 500         |
|    time_elapsed         | 9955        |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.008292062 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -228        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -11.1       |
|    n_updates            | 4990        |
|    policy_gradient_loss | -0.0044     |
|    std                  | 1.06        |
|    value_loss           | 1.47        |
-----------------------------------------
Iteration: 500 | Episodes: 20300 | Median Reward: 47.53 | Max Reward: 49.24
Iteration: 502 | Episodes: 20400 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 505 | Episodes: 20500 | Median Reward: 47.68 | Max Reward: 49.24
Iteration: 507 | Episodes: 20600 | Median Reward: 47.29 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 510          |
|    time_elapsed         | 10156        |
|    total_timesteps      | 2088960      |
| train/                  |              |
|    approx_kl            | 0.0015251286 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -229         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.2        |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.000178    |
|    std                  | 1.06         |
|    value_loss           | 1.26         |
------------------------------------------
Iteration: 510 | Episodes: 20700 | Median Reward: 47.60 | Max Reward: 49.24
Iteration: 512 | Episodes: 20800 | Median Reward: 47.64 | Max Reward: 49.24
Iteration: 515 | Episodes: 20900 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 517 | Episodes: 21000 | Median Reward: 47.00 | Max Reward: 49.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -54.5      |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 520        |
|    time_elapsed         | 10352      |
|    total_timesteps      | 2129920    |
| train/                  |            |
|    approx_kl            | 0.00531905 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -230       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -11.2      |
|    n_updates            | 5190       |
|    policy_gradient_loss | -0.00164   |
|    std                  | 1.06       |
|    value_loss           | 1.89       |
----------------------------------------
Iteration: 520 | Episodes: 21100 | Median Reward: 46.11 | Max Reward: 49.24
Iteration: 522 | Episodes: 21200 | Median Reward: 47.57 | Max Reward: 49.24
Iteration: 524 | Episodes: 21300 | Median Reward: 47.08 | Max Reward: 49.24
Iteration: 527 | Episodes: 21400 | Median Reward: 47.52 | Max Reward: 49.24
Iteration: 529 | Episodes: 21500 | Median Reward: 47.61 | Max Reward: 49.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -53.9      |
| time/                   |            |
|    fps                  | 205        |
|    iterations           | 530        |
|    time_elapsed         | 10555      |
|    total_timesteps      | 2170880    |
| train/                  |            |
|    approx_kl            | 0.18663765 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.4        |
|    entropy_loss         | -231       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -10.5      |
|    n_updates            | 5290       |
|    policy_gradient_loss | -0.052     |
|    std                  | 1.06       |
|    value_loss           | 3.54       |
----------------------------------------
Iteration: 532 | Episodes: 21600 | Median Reward: 47.60 | Max Reward: 49.24
Iteration: 534 | Episodes: 21700 | Median Reward: 47.49 | Max Reward: 49.24
Iteration: 537 | Episodes: 21800 | Median Reward: 47.46 | Max Reward: 49.24
Iteration: 539 | Episodes: 21900 | Median Reward: 47.52 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.8       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 540         |
|    time_elapsed         | 10753       |
|    total_timesteps      | 2211840     |
| train/                  |             |
|    approx_kl            | 0.008400673 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -231        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.77       |
|    n_updates            | 5390        |
|    policy_gradient_loss | -0.00571    |
|    std                  | 1.07        |
|    value_loss           | 1.84        |
-----------------------------------------
Iteration: 542 | Episodes: 22000 | Median Reward: 47.31 | Max Reward: 49.24
Iteration: 544 | Episodes: 22100 | Median Reward: 47.54 | Max Reward: 49.24
Iteration: 547 | Episodes: 22200 | Median Reward: 47.22 | Max Reward: 49.24
Iteration: 549 | Episodes: 22300 | Median Reward: 47.55 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.3       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 550         |
|    time_elapsed         | 10948       |
|    total_timesteps      | 2252800     |
| train/                  |             |
|    approx_kl            | 0.011324333 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -232        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.5       |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00708    |
|    std                  | 1.07        |
|    value_loss           | 0.412       |
-----------------------------------------
Iteration: 552 | Episodes: 22400 | Median Reward: 47.47 | Max Reward: 49.24
Iteration: 554 | Episodes: 22500 | Median Reward: 47.51 | Max Reward: 49.24
Iteration: 557 | Episodes: 22600 | Median Reward: 47.56 | Max Reward: 49.24
Iteration: 559 | Episodes: 22700 | Median Reward: 47.80 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53          |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 560          |
|    time_elapsed         | 11144        |
|    total_timesteps      | 2293760      |
| train/                  |              |
|    approx_kl            | 8.505539e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -233         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.6        |
|    n_updates            | 5590         |
|    policy_gradient_loss | 0.000107     |
|    std                  | 1.07         |
|    value_loss           | 1.22         |
------------------------------------------
Iteration: 561 | Episodes: 22800 | Median Reward: 47.38 | Max Reward: 49.24
Iteration: 564 | Episodes: 22900 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 566 | Episodes: 23000 | Median Reward: 47.77 | Max Reward: 49.24
Iteration: 569 | Episodes: 23100 | Median Reward: 47.80 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.3         |
| time/                   |               |
|    fps                  | 205           |
|    iterations           | 570           |
|    time_elapsed         | 11344         |
|    total_timesteps      | 2334720       |
| train/                  |               |
|    approx_kl            | 5.7203462e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -233          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.2         |
|    n_updates            | 5690          |
|    policy_gradient_loss | 2.15e-05      |
|    std                  | 1.07          |
|    value_loss           | 1.51          |
-------------------------------------------
Iteration: 571 | Episodes: 23200 | Median Reward: 47.61 | Max Reward: 49.24
Iteration: 574 | Episodes: 23300 | Median Reward: 47.76 | Max Reward: 49.24
Iteration: 576 | Episodes: 23400 | Median Reward: 47.21 | Max Reward: 49.24
Iteration: 579 | Episodes: 23500 | Median Reward: 47.51 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.6       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 580         |
|    time_elapsed         | 11548       |
|    total_timesteps      | 2375680     |
| train/                  |             |
|    approx_kl            | 0.005326856 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -233        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.3       |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.00541    |
|    std                  | 1.07        |
|    value_loss           | 2.53        |
-----------------------------------------
Iteration: 581 | Episodes: 23600 | Median Reward: 47.61 | Max Reward: 49.24
Iteration: 584 | Episodes: 23700 | Median Reward: 44.38 | Max Reward: 49.24
Iteration: 586 | Episodes: 23800 | Median Reward: 47.18 | Max Reward: 49.24
Iteration: 589 | Episodes: 23900 | Median Reward: 47.76 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 590          |
|    time_elapsed         | 11745        |
|    total_timesteps      | 2416640      |
| train/                  |              |
|    approx_kl            | 0.0037019628 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -234         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -10.3        |
|    n_updates            | 5890         |
|    policy_gradient_loss | -0.00304     |
|    std                  | 1.07         |
|    value_loss           | 1.25         |
------------------------------------------
Iteration: 591 | Episodes: 24000 | Median Reward: 47.62 | Max Reward: 49.24
Iteration: 593 | Episodes: 24100 | Median Reward: 47.57 | Max Reward: 49.24
Iteration: 596 | Episodes: 24200 | Median Reward: 47.52 | Max Reward: 49.24
Iteration: 598 | Episodes: 24300 | Median Reward: 47.70 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.4       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 600         |
|    time_elapsed         | 11934       |
|    total_timesteps      | 2457600     |
| train/                  |             |
|    approx_kl            | 0.008992791 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -234        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -9.29       |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.0084     |
|    std                  | 1.08        |
|    value_loss           | 2.3         |
-----------------------------------------
Iteration: 601 | Episodes: 24400 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 603 | Episodes: 24500 | Median Reward: 46.89 | Max Reward: 49.24
Iteration: 606 | Episodes: 24600 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 608 | Episodes: 24700 | Median Reward: 47.57 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.9        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 610          |
|    time_elapsed         | 12133        |
|    total_timesteps      | 2498560      |
| train/                  |              |
|    approx_kl            | 0.0041689444 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -235         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -9.65        |
|    n_updates            | 6090         |
|    policy_gradient_loss | -0.0047      |
|    std                  | 1.08         |
|    value_loss           | 2.13         |
------------------------------------------
Iteration: 611 | Episodes: 24800 | Median Reward: 47.59 | Max Reward: 49.24
Iteration: 613 | Episodes: 24900 | Median Reward: 47.57 | Max Reward: 49.24
Iteration: 616 | Episodes: 25000 | Median Reward: 47.60 | Max Reward: 49.24
Iteration: 618 | Episodes: 25100 | Median Reward: 47.44 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -51.8        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 620          |
|    time_elapsed         | 12338        |
|    total_timesteps      | 2539520      |
| train/                  |              |
|    approx_kl            | 0.0048093926 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -236         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0005       |
|    loss                 | -3.3         |
|    n_updates            | 6190         |
|    policy_gradient_loss | -0.0038      |
|    std                  | 1.08         |
|    value_loss           | 16.3         |
------------------------------------------
Iteration: 621 | Episodes: 25200 | Median Reward: 47.59 | Max Reward: 49.24
Iteration: 623 | Episodes: 25300 | Median Reward: 47.77 | Max Reward: 49.24
Iteration: 625 | Episodes: 25400 | Median Reward: 47.46 | Max Reward: 49.24
Iteration: 628 | Episodes: 25500 | Median Reward: 47.78 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.4         |
| time/                   |               |
|    fps                  | 205           |
|    iterations           | 630           |
|    time_elapsed         | 12538         |
|    total_timesteps      | 2580480       |
| train/                  |               |
|    approx_kl            | 0.00040933234 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -236          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -11.5         |
|    n_updates            | 6290          |
|    policy_gradient_loss | 0.000101      |
|    std                  | 1.08          |
|    value_loss           | 1.29          |
-------------------------------------------
Iteration: 630 | Episodes: 25600 | Median Reward: 47.77 | Max Reward: 49.24
Iteration: 633 | Episodes: 25700 | Median Reward: 47.64 | Max Reward: 49.24
Iteration: 635 | Episodes: 25800 | Median Reward: 47.36 | Max Reward: 49.24
Iteration: 638 | Episodes: 25900 | Median Reward: 47.64 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 640          |
|    time_elapsed         | 12743        |
|    total_timesteps      | 2621440      |
| train/                  |              |
|    approx_kl            | 0.0008809508 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -237         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -11.3        |
|    n_updates            | 6390         |
|    policy_gradient_loss | -0.000975    |
|    std                  | 1.09         |
|    value_loss           | 1.78         |
------------------------------------------
Iteration: 640 | Episodes: 26000 | Median Reward: 47.57 | Max Reward: 49.24
Iteration: 643 | Episodes: 26100 | Median Reward: 47.79 | Max Reward: 49.24
Iteration: 645 | Episodes: 26200 | Median Reward: 47.57 | Max Reward: 49.24
Iteration: 648 | Episodes: 26300 | Median Reward: 47.80 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.9        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 650          |
|    time_elapsed         | 12937        |
|    total_timesteps      | 2662400      |
| train/                  |              |
|    approx_kl            | 0.0026510032 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -238         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.2        |
|    n_updates            | 6490         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 1.09         |
|    value_loss           | 1.7          |
------------------------------------------
Iteration: 650 | Episodes: 26400 | Median Reward: 47.55 | Max Reward: 49.24
Iteration: 653 | Episodes: 26500 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 655 | Episodes: 26600 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 658 | Episodes: 26700 | Median Reward: 47.22 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.6        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 660          |
|    time_elapsed         | 13135        |
|    total_timesteps      | 2703360      |
| train/                  |              |
|    approx_kl            | 0.0007058244 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -238         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.17        |
|    n_updates            | 6590         |
|    policy_gradient_loss | -0.000481    |
|    std                  | 1.09         |
|    value_loss           | 3.38         |
------------------------------------------
Iteration: 660 | Episodes: 26800 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 662 | Episodes: 26900 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 665 | Episodes: 27000 | Median Reward: 47.61 | Max Reward: 49.24
Iteration: 667 | Episodes: 27100 | Median Reward: 47.83 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 670          |
|    time_elapsed         | 13329        |
|    total_timesteps      | 2744320      |
| train/                  |              |
|    approx_kl            | 0.0022561068 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -239         |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.53        |
|    n_updates            | 6690         |
|    policy_gradient_loss | -0.00139     |
|    std                  | 1.09         |
|    value_loss           | 6.68         |
------------------------------------------
Iteration: 670 | Episodes: 27200 | Median Reward: 47.71 | Max Reward: 49.24
Iteration: 672 | Episodes: 27300 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 675 | Episodes: 27400 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 677 | Episodes: 27500 | Median Reward: 47.36 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.7       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 680         |
|    time_elapsed         | 13528       |
|    total_timesteps      | 2785280     |
| train/                  |             |
|    approx_kl            | 0.026850954 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -240        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.9       |
|    n_updates            | 6790        |
|    policy_gradient_loss | -0.015      |
|    std                  | 1.1         |
|    value_loss           | 0.389       |
-----------------------------------------
Iteration: 680 | Episodes: 27600 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 682 | Episodes: 27700 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 685 | Episodes: 27800 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 687 | Episodes: 27900 | Median Reward: 47.24 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 206          |
|    iterations           | 690          |
|    time_elapsed         | 13717        |
|    total_timesteps      | 2826240      |
| train/                  |              |
|    approx_kl            | 0.0038199346 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -240         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.7        |
|    n_updates            | 6890         |
|    policy_gradient_loss | -0.00347     |
|    std                  | 1.1          |
|    value_loss           | 1.25         |
------------------------------------------
Iteration: 690 | Episodes: 28000 | Median Reward: 47.71 | Max Reward: 49.24
Iteration: 692 | Episodes: 28100 | Median Reward: 47.61 | Max Reward: 49.24
Iteration: 694 | Episodes: 28200 | Median Reward: 47.35 | Max Reward: 49.24
Iteration: 697 | Episodes: 28300 | Median Reward: 47.01 | Max Reward: 49.24
Iteration: 699 | Episodes: 28400 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 206           |
|    iterations           | 700           |
|    time_elapsed         | 13915         |
|    total_timesteps      | 2867200       |
| train/                  |               |
|    approx_kl            | 4.3467327e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -241          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -7.8          |
|    n_updates            | 6990          |
|    policy_gradient_loss | 3.78e-05      |
|    std                  | 1.1           |
|    value_loss           | 3.14          |
-------------------------------------------
Iteration: 702 | Episodes: 28500 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 704 | Episodes: 28600 | Median Reward: 47.64 | Max Reward: 49.24
Iteration: 707 | Episodes: 28700 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 709 | Episodes: 28800 | Median Reward: 47.78 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.3       |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 710         |
|    time_elapsed         | 14113       |
|    total_timesteps      | 2908160     |
| train/                  |             |
|    approx_kl            | 0.038582437 |
|    clip_fraction        | 0.0248      |
|    clip_range           | 0.4         |
|    entropy_loss         | -242        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -11.9       |
|    n_updates            | 7090        |
|    policy_gradient_loss | -0.0391     |
|    std                  | 1.1         |
|    value_loss           | 0.79        |
-----------------------------------------
Iteration: 712 | Episodes: 28900 | Median Reward: 47.63 | Max Reward: 49.24
Iteration: 714 | Episodes: 29000 | Median Reward: 47.79 | Max Reward: 49.24
Iteration: 717 | Episodes: 29100 | Median Reward: 47.19 | Max Reward: 49.24
Iteration: 719 | Episodes: 29200 | Median Reward: 47.73 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.1         |
|    ep_rew_mean          | -50.8        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 720          |
|    time_elapsed         | 14319        |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0007690226 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -242         |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0005       |
|    loss                 | -7.8         |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.000408    |
|    std                  | 1.1          |
|    value_loss           | 7.23         |
------------------------------------------
Iteration: 722 | Episodes: 29300 | Median Reward: 47.65 | Max Reward: 49.24
Iteration: 724 | Episodes: 29400 | Median Reward: 47.84 | Max Reward: 49.24
Iteration: 726 | Episodes: 29500 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 729 | Episodes: 29600 | Median Reward: 47.72 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53          |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 730          |
|    time_elapsed         | 14520        |
|    total_timesteps      | 2990080      |
| train/                  |              |
|    approx_kl            | 0.0057292655 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -243         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12          |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.00904     |
|    std                  | 1.1          |
|    value_loss           | 0.852        |
------------------------------------------
Iteration: 731 | Episodes: 29700 | Median Reward: 47.61 | Max Reward: 49.24
Iteration: 734 | Episodes: 29800 | Median Reward: 47.79 | Max Reward: 49.24
Iteration: 736 | Episodes: 29900 | Median Reward: 47.81 | Max Reward: 49.24
Iteration: 739 | Episodes: 30000 | Median Reward: 47.83 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -51.4         |
| time/                   |               |
|    fps                  | 205           |
|    iterations           | 740           |
|    time_elapsed         | 14717         |
|    total_timesteps      | 3031040       |
| train/                  |               |
|    approx_kl            | 0.00017499509 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -244          |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.0005        |
|    loss                 | -6.35         |
|    n_updates            | 7390          |
|    policy_gradient_loss | -0.000171     |
|    std                  | 1.11          |
|    value_loss           | 11.1          |
-------------------------------------------
Iteration: 741 | Episodes: 30100 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 744 | Episodes: 30200 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 746 | Episodes: 30300 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 748 | Episodes: 30400 | Median Reward: 47.70 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 750          |
|    time_elapsed         | 14919        |
|    total_timesteps      | 3072000      |
| train/                  |              |
|    approx_kl            | 0.0006054407 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -244         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.4        |
|    n_updates            | 7490         |
|    policy_gradient_loss | -0.000535    |
|    std                  | 1.11         |
|    value_loss           | 1.01         |
------------------------------------------
Iteration: 751 | Episodes: 30500 | Median Reward: 47.62 | Max Reward: 49.24
Iteration: 753 | Episodes: 30600 | Median Reward: 46.63 | Max Reward: 49.24
Iteration: 756 | Episodes: 30700 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 758 | Episodes: 30800 | Median Reward: 47.88 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 760          |
|    time_elapsed         | 15118        |
|    total_timesteps      | 3112960      |
| train/                  |              |
|    approx_kl            | 0.0012689693 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -244         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -3.45        |
|    n_updates            | 7590         |
|    policy_gradient_loss | -0.00221     |
|    std                  | 1.11         |
|    value_loss           | 11.3         |
------------------------------------------
Iteration: 761 | Episodes: 30900 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 763 | Episodes: 31000 | Median Reward: 47.57 | Max Reward: 49.24
Iteration: 766 | Episodes: 31100 | Median Reward: 47.64 | Max Reward: 49.24
Iteration: 768 | Episodes: 31200 | Median Reward: 47.76 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 770          |
|    time_elapsed         | 15322        |
|    total_timesteps      | 3153920      |
| train/                  |              |
|    approx_kl            | 0.0023029926 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -244         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -7.42        |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.0039      |
|    std                  | 1.11         |
|    value_loss           | 4.36         |
------------------------------------------
Iteration: 771 | Episodes: 31300 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 773 | Episodes: 31400 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 776 | Episodes: 31500 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 778 | Episodes: 31600 | Median Reward: 47.82 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 205           |
|    iterations           | 780           |
|    time_elapsed         | 15516         |
|    total_timesteps      | 3194880       |
| train/                  |               |
|    approx_kl            | 0.00013562337 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -245          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.1         |
|    n_updates            | 7790          |
|    policy_gradient_loss | -0.000161     |
|    std                  | 1.12          |
|    value_loss           | 1.26          |
-------------------------------------------
Iteration: 780 | Episodes: 31700 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 783 | Episodes: 31800 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 785 | Episodes: 31900 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 788 | Episodes: 32000 | Median Reward: 47.83 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.5         |
| time/                   |               |
|    fps                  | 205           |
|    iterations           | 790           |
|    time_elapsed         | 15716         |
|    total_timesteps      | 3235840       |
| train/                  |               |
|    approx_kl            | 0.00032234332 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -245          |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0005        |
|    loss                 | -10           |
|    n_updates            | 7890          |
|    policy_gradient_loss | -0.000307     |
|    std                  | 1.12          |
|    value_loss           | 2.9           |
-------------------------------------------
Iteration: 790 | Episodes: 32100 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 793 | Episodes: 32200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 795 | Episodes: 32300 | Median Reward: 47.74 | Max Reward: 49.24
Iteration: 798 | Episodes: 32400 | Median Reward: 47.82 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.7         |
| time/                   |               |
|    fps                  | 205           |
|    iterations           | 800           |
|    time_elapsed         | 15917         |
|    total_timesteps      | 3276800       |
| train/                  |               |
|    approx_kl            | 0.00014814851 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -246          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.9         |
|    n_updates            | 7990          |
|    policy_gradient_loss | -0.000462     |
|    std                  | 1.12          |
|    value_loss           | 4.36          |
-------------------------------------------
Iteration: 800 | Episodes: 32500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 803 | Episodes: 32600 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 805 | Episodes: 32700 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 808 | Episodes: 32800 | Median Reward: 47.36 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.4       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 810         |
|    time_elapsed         | 16121       |
|    total_timesteps      | 3317760     |
| train/                  |             |
|    approx_kl            | 0.005535366 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -247        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.1       |
|    n_updates            | 8090        |
|    policy_gradient_loss | -0.000582   |
|    std                  | 1.12        |
|    value_loss           | 1.41        |
-----------------------------------------
Iteration: 810 | Episodes: 32900 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 812 | Episodes: 33000 | Median Reward: 47.58 | Max Reward: 49.24
Iteration: 815 | Episodes: 33100 | Median Reward: 47.52 | Max Reward: 49.24
Iteration: 817 | Episodes: 33200 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -51.4         |
| time/                   |               |
|    fps                  | 205           |
|    iterations           | 820           |
|    time_elapsed         | 16320         |
|    total_timesteps      | 3358720       |
| train/                  |               |
|    approx_kl            | 0.00025517895 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -248          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -11.5         |
|    n_updates            | 8190          |
|    policy_gradient_loss | 0.000137      |
|    std                  | 1.12          |
|    value_loss           | 1.11          |
-------------------------------------------
Iteration: 820 | Episodes: 33300 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 822 | Episodes: 33400 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 825 | Episodes: 33500 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 827 | Episodes: 33600 | Median Reward: 47.87 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.2       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 830         |
|    time_elapsed         | 16521       |
|    total_timesteps      | 3399680     |
| train/                  |             |
|    approx_kl            | 0.010216755 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -248        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -11.8       |
|    n_updates            | 8290        |
|    policy_gradient_loss | -0.00433    |
|    std                  | 1.13        |
|    value_loss           | 1.21        |
-----------------------------------------
Iteration: 830 | Episodes: 33700 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 832 | Episodes: 33800 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 835 | Episodes: 33900 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 837 | Episodes: 34000 | Median Reward: 47.86 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.9        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 840          |
|    time_elapsed         | 16720        |
|    total_timesteps      | 3440640      |
| train/                  |              |
|    approx_kl            | 0.0058836835 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -248         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -11.5        |
|    n_updates            | 8390         |
|    policy_gradient_loss | -0.0145      |
|    std                  | 1.13         |
|    value_loss           | 4.03         |
------------------------------------------
Iteration: 840 | Episodes: 34100 | Median Reward: 47.46 | Max Reward: 49.24
Iteration: 842 | Episodes: 34200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 845 | Episodes: 34300 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 847 | Episodes: 34400 | Median Reward: 47.74 | Max Reward: 49.24
Iteration: 849 | Episodes: 34500 | Median Reward: 47.82 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.3       |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 850         |
|    time_elapsed         | 16915       |
|    total_timesteps      | 3481600     |
| train/                  |             |
|    approx_kl            | 0.004577875 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -249        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -11.8       |
|    n_updates            | 8490        |
|    policy_gradient_loss | -0.00148    |
|    std                  | 1.13        |
|    value_loss           | 0.648       |
-----------------------------------------
Iteration: 852 | Episodes: 34600 | Median Reward: 47.57 | Max Reward: 49.24
Iteration: 854 | Episodes: 34700 | Median Reward: 47.40 | Max Reward: 49.24
Iteration: 857 | Episodes: 34800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 859 | Episodes: 34900 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 860          |
|    time_elapsed         | 17115        |
|    total_timesteps      | 3522560      |
| train/                  |              |
|    approx_kl            | 0.0011871762 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -12          |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.000458    |
|    std                  | 1.14         |
|    value_loss           | 1.23         |
------------------------------------------
Iteration: 862 | Episodes: 35000 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 864 | Episodes: 35100 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 867 | Episodes: 35200 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 869 | Episodes: 35300 | Median Reward: 47.61 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 870          |
|    time_elapsed         | 17302        |
|    total_timesteps      | 3563520      |
| train/                  |              |
|    approx_kl            | 0.0073336735 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.2        |
|    n_updates            | 8690         |
|    policy_gradient_loss | -0.00679     |
|    std                  | 1.14         |
|    value_loss           | 1.3          |
------------------------------------------
Iteration: 872 | Episodes: 35400 | Median Reward: 47.74 | Max Reward: 49.24
Iteration: 874 | Episodes: 35500 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 876 | Episodes: 35600 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 879 | Episodes: 35700 | Median Reward: 47.73 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.6         |
| time/                   |               |
|    fps                  | 205           |
|    iterations           | 880           |
|    time_elapsed         | 17501         |
|    total_timesteps      | 3604480       |
| train/                  |               |
|    approx_kl            | 0.00010456421 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -250          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -12.3         |
|    n_updates            | 8790          |
|    policy_gradient_loss | 2.76e-05      |
|    std                  | 1.14          |
|    value_loss           | 0.495         |
-------------------------------------------
Iteration: 881 | Episodes: 35800 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 884 | Episodes: 35900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 886 | Episodes: 36000 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 889 | Episodes: 36100 | Median Reward: 47.86 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 890          |
|    time_elapsed         | 17698        |
|    total_timesteps      | 3645440      |
| train/                  |              |
|    approx_kl            | 0.0017098015 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -250         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -12.4        |
|    n_updates            | 8890         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 1.14         |
|    value_loss           | 0.405        |
------------------------------------------
Iteration: 891 | Episodes: 36200 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 894 | Episodes: 36300 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 896 | Episodes: 36400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 899 | Episodes: 36500 | Median Reward: 47.58 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 900          |
|    time_elapsed         | 17896        |
|    total_timesteps      | 3686400      |
| train/                  |              |
|    approx_kl            | 0.0010524971 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -251         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.2        |
|    n_updates            | 8990         |
|    policy_gradient_loss | -0.000935    |
|    std                  | 1.15         |
|    value_loss           | 1.31         |
------------------------------------------
Iteration: 901 | Episodes: 36600 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 904 | Episodes: 36700 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 906 | Episodes: 36800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 909 | Episodes: 36900 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 206          |
|    iterations           | 910          |
|    time_elapsed         | 18087        |
|    total_timesteps      | 3727360      |
| train/                  |              |
|    approx_kl            | 0.0010512233 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -251         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.4        |
|    n_updates            | 9090         |
|    policy_gradient_loss | -0.000782    |
|    std                  | 1.15         |
|    value_loss           | 0.46         |
------------------------------------------
Iteration: 911 | Episodes: 37000 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 913 | Episodes: 37100 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 916 | Episodes: 37200 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 918 | Episodes: 37300 | Median Reward: 47.07 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.8       |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 920         |
|    time_elapsed         | 18272       |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.016406693 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -251        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -10         |
|    n_updates            | 9190        |
|    policy_gradient_loss | -0.0269     |
|    std                  | 1.15        |
|    value_loss           | 2           |
-----------------------------------------
Iteration: 921 | Episodes: 37400 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 923 | Episodes: 37500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 926 | Episodes: 37600 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 928 | Episodes: 37700 | Median Reward: 47.77 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.8       |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 930         |
|    time_elapsed         | 18467       |
|    total_timesteps      | 3809280     |
| train/                  |             |
|    approx_kl            | 0.003497783 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -252        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.4       |
|    n_updates            | 9290        |
|    policy_gradient_loss | -0.00125    |
|    std                  | 1.15        |
|    value_loss           | 1.36        |
-----------------------------------------
Iteration: 931 | Episodes: 37800 | Median Reward: 47.74 | Max Reward: 49.24
Iteration: 933 | Episodes: 37900 | Median Reward: 47.84 | Max Reward: 49.24
Iteration: 936 | Episodes: 38000 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 938 | Episodes: 38100 | Median Reward: 47.86 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 206          |
|    iterations           | 940          |
|    time_elapsed         | 18658        |
|    total_timesteps      | 3850240      |
| train/                  |              |
|    approx_kl            | 0.0014249859 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -253         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.4        |
|    n_updates            | 9390         |
|    policy_gradient_loss | -0.00291     |
|    std                  | 1.16         |
|    value_loss           | 1.42         |
------------------------------------------
Iteration: 941 | Episodes: 38200 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 943 | Episodes: 38300 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 945 | Episodes: 38400 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 948 | Episodes: 38500 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 206           |
|    iterations           | 950           |
|    time_elapsed         | 18853         |
|    total_timesteps      | 3891200       |
| train/                  |               |
|    approx_kl            | 0.00052578386 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -253          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -10.9         |
|    n_updates            | 9490          |
|    policy_gradient_loss | -0.000309     |
|    std                  | 1.16          |
|    value_loss           | 1.3           |
-------------------------------------------
Iteration: 950 | Episodes: 38600 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 953 | Episodes: 38700 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 955 | Episodes: 38800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 958 | Episodes: 38900 | Median Reward: 47.92 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.2       |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 960         |
|    time_elapsed         | 19048       |
|    total_timesteps      | 3932160     |
| train/                  |             |
|    approx_kl            | 0.014521139 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -254        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.97       |
|    n_updates            | 9590        |
|    policy_gradient_loss | -0.021      |
|    std                  | 1.16        |
|    value_loss           | 4.57        |
-----------------------------------------
Iteration: 960 | Episodes: 39000 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 963 | Episodes: 39100 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 965 | Episodes: 39200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 968 | Episodes: 39300 | Median Reward: 47.74 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 206           |
|    iterations           | 970           |
|    time_elapsed         | 19234         |
|    total_timesteps      | 3973120       |
| train/                  |               |
|    approx_kl            | 5.8129444e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -254          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -10.5         |
|    n_updates            | 9690          |
|    policy_gradient_loss | -6.97e-05     |
|    std                  | 1.16          |
|    value_loss           | 4.09          |
-------------------------------------------
Iteration: 970 | Episodes: 39400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 973 | Episodes: 39500 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 975 | Episodes: 39600 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 978 | Episodes: 39700 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.6        |
| time/                   |              |
|    fps                  | 206          |
|    iterations           | 980          |
|    time_elapsed         | 19425        |
|    total_timesteps      | 4014080      |
| train/                  |              |
|    approx_kl            | 0.0027577034 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -254         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -10.3        |
|    n_updates            | 9790         |
|    policy_gradient_loss | -0.00612     |
|    std                  | 1.16         |
|    value_loss           | 3.79         |
------------------------------------------
Iteration: 980 | Episodes: 39800 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 982 | Episodes: 39900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 985 | Episodes: 40000 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 987 | Episodes: 40100 | Median Reward: 47.86 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.9         |
| time/                   |               |
|    fps                  | 206           |
|    iterations           | 990           |
|    time_elapsed         | 19615         |
|    total_timesteps      | 4055040       |
| train/                  |               |
|    approx_kl            | 2.3695102e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -254          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -11           |
|    n_updates            | 9890          |
|    policy_gradient_loss | 9.57e-06      |
|    std                  | 1.16          |
|    value_loss           | 3.62          |
-------------------------------------------
Iteration: 990 | Episodes: 40200 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 992 | Episodes: 40300 | Median Reward: 47.67 | Max Reward: 49.24
Iteration: 995 | Episodes: 40400 | Median Reward: 47.76 | Max Reward: 49.24
Iteration: 997 | Episodes: 40500 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.5         |
| time/                   |               |
|    fps                  | 206           |
|    iterations           | 1000          |
|    time_elapsed         | 19807         |
|    total_timesteps      | 4096000       |
| train/                  |               |
|    approx_kl            | 5.7145997e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -254          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -11.2         |
|    n_updates            | 9990          |
|    policy_gradient_loss | -0.000129     |
|    std                  | 1.16          |
|    value_loss           | 3.53          |
-------------------------------------------
Iteration: 1000 | Episodes: 40600 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1002 | Episodes: 40700 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1005 | Episodes: 40800 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 1007 | Episodes: 40900 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 1009 | Episodes: 41000 | Median Reward: 47.83 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.2         |
| time/                   |               |
|    fps                  | 206           |
|    iterations           | 1010          |
|    time_elapsed         | 20002         |
|    total_timesteps      | 4136960       |
| train/                  |               |
|    approx_kl            | 1.0649921e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -254          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -10.7         |
|    n_updates            | 10090         |
|    policy_gradient_loss | -2.31e-05     |
|    std                  | 1.17          |
|    value_loss           | 3.49          |
-------------------------------------------
Iteration: 1012 | Episodes: 41100 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 1014 | Episodes: 41200 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1017 | Episodes: 41300 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 1019 | Episodes: 41400 | Median Reward: 47.70 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 207          |
|    iterations           | 1020         |
|    time_elapsed         | 20178        |
|    total_timesteps      | 4177920      |
| train/                  |              |
|    approx_kl            | 0.0002039345 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -255         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.5        |
|    n_updates            | 10190        |
|    policy_gradient_loss | 7.32e-05     |
|    std                  | 1.17         |
|    value_loss           | 2.86         |
------------------------------------------
Iteration: 1022 | Episodes: 41500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1024 | Episodes: 41600 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1027 | Episodes: 41700 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1029 | Episodes: 41800 | Median Reward: 47.85 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.3       |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 1030        |
|    time_elapsed         | 20372       |
|    total_timesteps      | 4218880     |
| train/                  |             |
|    approx_kl            | 0.002482577 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -255        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.4       |
|    n_updates            | 10290       |
|    policy_gradient_loss | -0.00037    |
|    std                  | 1.17        |
|    value_loss           | 0.387       |
-----------------------------------------
Iteration: 1032 | Episodes: 41900 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1034 | Episodes: 42000 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1037 | Episodes: 42100 | Median Reward: 47.75 | Max Reward: 49.24
Iteration: 1039 | Episodes: 42200 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -51.4        |
| time/                   |              |
|    fps                  | 207          |
|    iterations           | 1040         |
|    time_elapsed         | 20565        |
|    total_timesteps      | 4259840      |
| train/                  |              |
|    approx_kl            | 0.0011015674 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -255         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.7        |
|    n_updates            | 10390        |
|    policy_gradient_loss | -0.000105    |
|    std                  | 1.17         |
|    value_loss           | 1.14         |
------------------------------------------
Iteration: 1042 | Episodes: 42300 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1044 | Episodes: 42400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1046 | Episodes: 42500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1049 | Episodes: 42600 | Median Reward: 47.92 | Max Reward: 49.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -52.5      |
| time/                   |            |
|    fps                  | 207        |
|    iterations           | 1050       |
|    time_elapsed         | 20757      |
|    total_timesteps      | 4300800    |
| train/                  |            |
|    approx_kl            | 0.20527557 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.4        |
|    entropy_loss         | -256       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -12.3      |
|    n_updates            | 10490      |
|    policy_gradient_loss | -0.042     |
|    std                  | 1.18       |
|    value_loss           | 0.603      |
----------------------------------------
Iteration: 1051 | Episodes: 42700 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1054 | Episodes: 42800 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1056 | Episodes: 42900 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 1059 | Episodes: 43000 | Median Reward: 47.57 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53         |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 1060        |
|    time_elapsed         | 20952       |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.008141267 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -256        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -11.9       |
|    n_updates            | 10590       |
|    policy_gradient_loss | -0.00811    |
|    std                  | 1.18        |
|    value_loss           | 0.827       |
-----------------------------------------
Iteration: 1061 | Episodes: 43100 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 1064 | Episodes: 43200 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1066 | Episodes: 43300 | Median Reward: 47.64 | Max Reward: 49.24
Iteration: 1069 | Episodes: 43400 | Median Reward: 47.80 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54         |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 1070        |
|    time_elapsed         | 21137       |
|    total_timesteps      | 4382720     |
| train/                  |             |
|    approx_kl            | 0.014267463 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -256        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.3       |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.00948    |
|    std                  | 1.18        |
|    value_loss           | 1.15        |
-----------------------------------------
Iteration: 1071 | Episodes: 43500 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1074 | Episodes: 43600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1076 | Episodes: 43700 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1078 | Episodes: 43800 | Median Reward: 47.92 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | -52.2       |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 1080        |
|    time_elapsed         | 21330       |
|    total_timesteps      | 4423680     |
| train/                  |             |
|    approx_kl            | 0.025055131 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.4         |
|    entropy_loss         | -255        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.33       |
|    n_updates            | 10790       |
|    policy_gradient_loss | -0.01       |
|    std                  | 1.18        |
|    value_loss           | 5.18        |
-----------------------------------------
Iteration: 1081 | Episodes: 43900 | Median Reward: 47.66 | Max Reward: 49.24
Iteration: 1083 | Episodes: 44000 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1086 | Episodes: 44100 | Median Reward: 47.57 | Max Reward: 49.24
Iteration: 1088 | Episodes: 44200 | Median Reward: 47.46 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53          |
| time/                   |              |
|    fps                  | 207          |
|    iterations           | 1090         |
|    time_elapsed         | 21520        |
|    total_timesteps      | 4464640      |
| train/                  |              |
|    approx_kl            | 0.0009085792 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -256         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -12.1        |
|    n_updates            | 10890        |
|    policy_gradient_loss | -0.00112     |
|    std                  | 1.19         |
|    value_loss           | 1.94         |
------------------------------------------
Iteration: 1091 | Episodes: 44300 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1093 | Episodes: 44400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1096 | Episodes: 44500 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1098 | Episodes: 44600 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 207          |
|    iterations           | 1100         |
|    time_elapsed         | 21710        |
|    total_timesteps      | 4505600      |
| train/                  |              |
|    approx_kl            | 0.0005242024 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -257         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -12.7        |
|    n_updates            | 10990        |
|    policy_gradient_loss | -0.000554    |
|    std                  | 1.19         |
|    value_loss           | 0.436        |
------------------------------------------
Iteration: 1101 | Episodes: 44700 | Median Reward: 47.46 | Max Reward: 49.24
Iteration: 1103 | Episodes: 44800 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1106 | Episodes: 44900 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 1108 | Episodes: 45000 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.2         |
|    ep_rew_mean          | -50.6        |
| time/                   |              |
|    fps                  | 207          |
|    iterations           | 1110         |
|    time_elapsed         | 21903        |
|    total_timesteps      | 4546560      |
| train/                  |              |
|    approx_kl            | 0.0024501216 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -257         |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0005       |
|    loss                 | -9.01        |
|    n_updates            | 11090        |
|    policy_gradient_loss | -0.00174     |
|    std                  | 1.19         |
|    value_loss           | 7.58         |
------------------------------------------
Iteration: 1110 | Episodes: 45100 | Median Reward: 47.57 | Max Reward: 49.24
Iteration: 1113 | Episodes: 45200 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 1115 | Episodes: 45300 | Median Reward: 47.61 | Max Reward: 49.24
Iteration: 1118 | Episodes: 45400 | Median Reward: 47.92 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 99.2        |
|    ep_rew_mean          | -50.3       |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 1120        |
|    time_elapsed         | 22087       |
|    total_timesteps      | 4587520     |
| train/                  |             |
|    approx_kl            | 0.007709588 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -258        |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0005      |
|    loss                 | -7.92       |
|    n_updates            | 11190       |
|    policy_gradient_loss | -0.00287    |
|    std                  | 1.19        |
|    value_loss           | 9.63        |
-----------------------------------------
Iteration: 1120 | Episodes: 45500 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1123 | Episodes: 45600 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 1125 | Episodes: 45700 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1128 | Episodes: 45800 | Median Reward: 47.92 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.1       |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 1130        |
|    time_elapsed         | 22282       |
|    total_timesteps      | 4628480     |
| train/                  |             |
|    approx_kl            | 6.01692e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -258        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.8       |
|    n_updates            | 11290       |
|    policy_gradient_loss | -1.88e-05   |
|    std                  | 1.2         |
|    value_loss           | 0.928       |
-----------------------------------------
Iteration: 1130 | Episodes: 45900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1133 | Episodes: 46000 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1135 | Episodes: 46100 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1137 | Episodes: 46200 | Median Reward: 47.86 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53         |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 1140        |
|    time_elapsed         | 22476       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.010698928 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -258        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.6       |
|    n_updates            | 11390       |
|    policy_gradient_loss | -0.00623    |
|    std                  | 1.2         |
|    value_loss           | 1.15        |
-----------------------------------------
Iteration: 1140 | Episodes: 46300 | Median Reward: 47.55 | Max Reward: 49.24
Iteration: 1142 | Episodes: 46400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1145 | Episodes: 46500 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1147 | Episodes: 46600 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.8        |
| time/                   |              |
|    fps                  | 207          |
|    iterations           | 1150         |
|    time_elapsed         | 22670        |
|    total_timesteps      | 4710400      |
| train/                  |              |
|    approx_kl            | 7.746172e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -259         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.4        |
|    n_updates            | 11490        |
|    policy_gradient_loss | -0.00037     |
|    std                  | 1.2          |
|    value_loss           | 2.27         |
------------------------------------------
Iteration: 1150 | Episodes: 46700 | Median Reward: 47.81 | Max Reward: 49.24
Iteration: 1152 | Episodes: 46800 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 1155 | Episodes: 46900 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1157 | Episodes: 47000 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 207           |
|    iterations           | 1160          |
|    time_elapsed         | 22864         |
|    total_timesteps      | 4751360       |
| train/                  |               |
|    approx_kl            | 0.00049184076 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -259          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.7         |
|    n_updates            | 11590         |
|    policy_gradient_loss | -0.00141      |
|    std                  | 1.21          |
|    value_loss           | 2.27          |
-------------------------------------------
Iteration: 1160 | Episodes: 47100 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 1162 | Episodes: 47200 | Median Reward: 47.81 | Max Reward: 49.24
Iteration: 1165 | Episodes: 47300 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1167 | Episodes: 47400 | Median Reward: 47.68 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.2       |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 1170        |
|    time_elapsed         | 23043       |
|    total_timesteps      | 4792320     |
| train/                  |             |
|    approx_kl            | 0.020015605 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -260        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -12.5       |
|    n_updates            | 11690       |
|    policy_gradient_loss | -0.00857    |
|    std                  | 1.21        |
|    value_loss           | 0.364       |
-----------------------------------------
Iteration: 1170 | Episodes: 47500 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1172 | Episodes: 47600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1174 | Episodes: 47700 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 1177 | Episodes: 47800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1179 | Episodes: 47900 | Median Reward: 47.86 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 208           |
|    iterations           | 1180          |
|    time_elapsed         | 23236         |
|    total_timesteps      | 4833280       |
| train/                  |               |
|    approx_kl            | 0.00045207632 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -261          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.8         |
|    n_updates            | 11790         |
|    policy_gradient_loss | -0.000389     |
|    std                  | 1.21          |
|    value_loss           | 0.709         |
-------------------------------------------
Iteration: 1182 | Episodes: 48000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1184 | Episodes: 48100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1187 | Episodes: 48200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1189 | Episodes: 48300 | Median Reward: 47.89 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.4        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 1190         |
|    time_elapsed         | 23428        |
|    total_timesteps      | 4874240      |
| train/                  |              |
|    approx_kl            | 0.0063420823 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -261         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13          |
|    n_updates            | 11890        |
|    policy_gradient_loss | -0.00799     |
|    std                  | 1.22         |
|    value_loss           | 0.71         |
------------------------------------------
Iteration: 1192 | Episodes: 48400 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 1194 | Episodes: 48500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1197 | Episodes: 48600 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1199 | Episodes: 48700 | Median Reward: 47.90 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -51.9         |
| time/                   |               |
|    fps                  | 208           |
|    iterations           | 1200          |
|    time_elapsed         | 23624         |
|    total_timesteps      | 4915200       |
| train/                  |               |
|    approx_kl            | 0.00034994987 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -261          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.5         |
|    n_updates            | 11990         |
|    policy_gradient_loss | -0.000162     |
|    std                  | 1.22          |
|    value_loss           | 0.404         |
-------------------------------------------
Iteration: 1202 | Episodes: 48800 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1204 | Episodes: 48900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1206 | Episodes: 49000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1209 | Episodes: 49100 | Median Reward: 47.92 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.3       |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 1210        |
|    time_elapsed         | 23820       |
|    total_timesteps      | 4956160     |
| train/                  |             |
|    approx_kl            | 0.006196416 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -261        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -12.5       |
|    n_updates            | 12090       |
|    policy_gradient_loss | -0.000759   |
|    std                  | 1.22        |
|    value_loss           | 0.371       |
-----------------------------------------
Iteration: 1211 | Episodes: 49200 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1214 | Episodes: 49300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1216 | Episodes: 49400 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1219 | Episodes: 49500 | Median Reward: 47.90 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -51.7        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 1220         |
|    time_elapsed         | 24001        |
|    total_timesteps      | 4997120      |
| train/                  |              |
|    approx_kl            | 0.0010032279 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -261         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0005       |
|    loss                 | -8           |
|    n_updates            | 12190        |
|    policy_gradient_loss | 0.000942     |
|    std                  | 1.23         |
|    value_loss           | 7.43         |
------------------------------------------
Iteration: 1221 | Episodes: 49600 | Median Reward: 47.64 | Max Reward: 49.24
Iteration: 1224 | Episodes: 49700 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1226 | Episodes: 49800 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1229 | Episodes: 49900 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 1230         |
|    time_elapsed         | 24193        |
|    total_timesteps      | 5038080      |
| train/                  |              |
|    approx_kl            | 0.0001969104 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -261         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.6        |
|    n_updates            | 12290        |
|    policy_gradient_loss | 2.57e-05     |
|    std                  | 1.23         |
|    value_loss           | 1.17         |
------------------------------------------
Iteration: 1231 | Episodes: 50000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1234 | Episodes: 50100 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1236 | Episodes: 50200 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1238 | Episodes: 50300 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 1240         |
|    time_elapsed         | 24385        |
|    total_timesteps      | 5079040      |
| train/                  |              |
|    approx_kl            | 0.0024233167 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -262         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -11.9        |
|    n_updates            | 12390        |
|    policy_gradient_loss | -0.000383    |
|    std                  | 1.24         |
|    value_loss           | 1.4          |
------------------------------------------
Iteration: 1241 | Episodes: 50400 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1243 | Episodes: 50500 | Median Reward: 47.81 | Max Reward: 49.24
Iteration: 1246 | Episodes: 50600 | Median Reward: 47.76 | Max Reward: 49.24
Iteration: 1248 | Episodes: 50700 | Median Reward: 47.95 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.7       |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 1250        |
|    time_elapsed         | 24576       |
|    total_timesteps      | 5120000     |
| train/                  |             |
|    approx_kl            | 0.029260498 |
|    clip_fraction        | 0.000269    |
|    clip_range           | 0.4         |
|    entropy_loss         | -262        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -12.9       |
|    n_updates            | 12490       |
|    policy_gradient_loss | -0.0191     |
|    std                  | 1.24        |
|    value_loss           | 0.416       |
-----------------------------------------
Iteration: 1251 | Episodes: 50800 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1253 | Episodes: 50900 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1256 | Episodes: 51000 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 1258 | Episodes: 51100 | Median Reward: 47.89 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 1260         |
|    time_elapsed         | 24767        |
|    total_timesteps      | 5160960      |
| train/                  |              |
|    approx_kl            | 0.0071480153 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -263         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13          |
|    n_updates            | 12590        |
|    policy_gradient_loss | -0.00126     |
|    std                  | 1.25         |
|    value_loss           | 0.159        |
------------------------------------------
Iteration: 1261 | Episodes: 51200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1263 | Episodes: 51300 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1266 | Episodes: 51400 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1268 | Episodes: 51500 | Median Reward: 47.92 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.7       |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 1270        |
|    time_elapsed         | 24952       |
|    total_timesteps      | 5201920     |
| train/                  |             |
|    approx_kl            | 0.033838645 |
|    clip_fraction        | 0.00107     |
|    clip_range           | 0.4         |
|    entropy_loss         | -263        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -13         |
|    n_updates            | 12690       |
|    policy_gradient_loss | -0.025      |
|    std                  | 1.25        |
|    value_loss           | 0.936       |
-----------------------------------------
Iteration: 1271 | Episodes: 51600 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1273 | Episodes: 51700 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1275 | Episodes: 51800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1278 | Episodes: 51900 | Median Reward: 47.94 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 1280         |
|    time_elapsed         | 25145        |
|    total_timesteps      | 5242880      |
| train/                  |              |
|    approx_kl            | 0.0016055646 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -264         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.2        |
|    n_updates            | 12790        |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.25         |
|    value_loss           | 0.332        |
------------------------------------------
Iteration: 1280 | Episodes: 52000 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1283 | Episodes: 52100 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1285 | Episodes: 52200 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 1288 | Episodes: 52300 | Median Reward: 47.92 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.2       |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 1290        |
|    time_elapsed         | 25337       |
|    total_timesteps      | 5283840     |
| train/                  |             |
|    approx_kl            | 0.013803927 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -264        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -13.2       |
|    n_updates            | 12890       |
|    policy_gradient_loss | -0.00961    |
|    std                  | 1.26        |
|    value_loss           | 0.232       |
-----------------------------------------
Iteration: 1290 | Episodes: 52400 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1293 | Episodes: 52500 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 1295 | Episodes: 52600 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 1298 | Episodes: 52700 | Median Reward: 47.89 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.6        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 1300         |
|    time_elapsed         | 25528        |
|    total_timesteps      | 5324800      |
| train/                  |              |
|    approx_kl            | 0.0045900373 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -265         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.9        |
|    n_updates            | 12990        |
|    policy_gradient_loss | -0.0107      |
|    std                  | 1.26         |
|    value_loss           | 2.66         |
------------------------------------------
Iteration: 1300 | Episodes: 52800 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 1303 | Episodes: 52900 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1305 | Episodes: 53000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1307 | Episodes: 53100 | Median Reward: 47.90 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -51.6        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 1310         |
|    time_elapsed         | 25717        |
|    total_timesteps      | 5365760      |
| train/                  |              |
|    approx_kl            | 0.0010459222 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -266         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.7        |
|    n_updates            | 13090        |
|    policy_gradient_loss | -0.000728    |
|    std                  | 1.26         |
|    value_loss           | 3.73         |
------------------------------------------
Iteration: 1310 | Episodes: 53200 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1312 | Episodes: 53300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1315 | Episodes: 53400 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1317 | Episodes: 53500 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 208           |
|    iterations           | 1320          |
|    time_elapsed         | 25904         |
|    total_timesteps      | 5406720       |
| train/                  |               |
|    approx_kl            | 0.00040306928 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.4           |
|    entropy_loss         | -266          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -13.2         |
|    n_updates            | 13190         |
|    policy_gradient_loss | -0.000823     |
|    std                  | 1.27          |
|    value_loss           | 0.697         |
-------------------------------------------
Iteration: 1320 | Episodes: 53600 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1322 | Episodes: 53700 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 1325 | Episodes: 53800 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 1327 | Episodes: 53900 | Median Reward: 47.81 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.8       |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 1330        |
|    time_elapsed         | 26096       |
|    total_timesteps      | 5447680     |
| train/                  |             |
|    approx_kl            | 0.021892246 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -266        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -13.2       |
|    n_updates            | 13290       |
|    policy_gradient_loss | -0.00397    |
|    std                  | 1.27        |
|    value_loss           | 0.303       |
-----------------------------------------
Iteration: 1330 | Episodes: 54000 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 1332 | Episodes: 54100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1335 | Episodes: 54200 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1337 | Episodes: 54300 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 1339 | Episodes: 54400 | Median Reward: 47.80 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.6        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 1340         |
|    time_elapsed         | 26289        |
|    total_timesteps      | 5488640      |
| train/                  |              |
|    approx_kl            | 0.0013706324 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.4          |
|    entropy_loss         | -267         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -12.8        |
|    n_updates            | 13390        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 1.28         |
|    value_loss           | 0.542        |
------------------------------------------
Iteration: 1342 | Episodes: 54500 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1344 | Episodes: 54600 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 1347 | Episodes: 54700 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 1349 | Episodes: 54800 | Median Reward: 47.92 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.2       |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 1350        |
|    time_elapsed         | 26481       |
|    total_timesteps      | 5529600     |
| train/                  |             |
|    approx_kl            | 0.014885267 |
|    clip_fraction        | 0.0041      |
|    clip_range           | 0.4         |
|    entropy_loss         | -267        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -13.4       |
|    n_updates            | 13490       |
|    policy_gradient_loss | -0.00925    |
|    std                  | 1.28        |
|    value_loss           | 0.211       |
-----------------------------------------
Iteration: 1352 | Episodes: 54900 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1354 | Episodes: 55000 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1357 | Episodes: 55100 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 1359 | Episodes: 55200 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 1360         |
|    time_elapsed         | 26677        |
|    total_timesteps      | 5570560      |
| train/                  |              |
|    approx_kl            | 0.0028157646 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.4          |
|    entropy_loss         | -268         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.3        |
|    n_updates            | 13590        |
|    policy_gradient_loss | -0.00374     |
|    std                  | 1.29         |
|    value_loss           | 0.668        |
------------------------------------------
Iteration: 1362 | Episodes: 55300 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 1364 | Episodes: 55400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1367 | Episodes: 55500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1369 | Episodes: 55600 | Median Reward: 47.91 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.6       |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 1370        |
|    time_elapsed         | 26860       |
|    total_timesteps      | 5611520     |
| train/                  |             |
|    approx_kl            | 0.019728737 |
|    clip_fraction        | 0.00474     |
|    clip_range           | 0.4         |
|    entropy_loss         | -268        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -13.3       |
|    n_updates            | 13690       |
|    policy_gradient_loss | -0.0136     |
|    std                  | 1.29        |
|    value_loss           | 0.784       |
-----------------------------------------
Iteration: 1371 | Episodes: 55700 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 1374 | Episodes: 55800 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1376 | Episodes: 55900 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1379 | Episodes: 56000 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.4        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 1380         |
|    time_elapsed         | 27049        |
|    total_timesteps      | 5652480      |
| train/                  |              |
|    approx_kl            | 0.0008750651 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -268         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13          |
|    n_updates            | 13790        |
|    policy_gradient_loss | -3.35e-05    |
|    std                  | 1.29         |
|    value_loss           | 0.554        |
------------------------------------------
Iteration: 1381 | Episodes: 56100 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1384 | Episodes: 56200 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1386 | Episodes: 56300 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1389 | Episodes: 56400 | Median Reward: 47.95 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1390         |
|    time_elapsed         | 27240        |
|    total_timesteps      | 5693440      |
| train/                  |              |
|    approx_kl            | 0.0014375009 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.4          |
|    entropy_loss         | -269         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -13          |
|    n_updates            | 13890        |
|    policy_gradient_loss | -0.000824    |
|    std                  | 1.3          |
|    value_loss           | 0.582        |
------------------------------------------
Iteration: 1391 | Episodes: 56500 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1394 | Episodes: 56600 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1396 | Episodes: 56700 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 1399 | Episodes: 56800 | Median Reward: 47.91 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.1       |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 1400        |
|    time_elapsed         | 27432       |
|    total_timesteps      | 5734400     |
| train/                  |             |
|    approx_kl            | 0.045763656 |
|    clip_fraction        | 0.00969     |
|    clip_range           | 0.4         |
|    entropy_loss         | -269        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -13.1       |
|    n_updates            | 13990       |
|    policy_gradient_loss | -0.00169    |
|    std                  | 1.3         |
|    value_loss           | 0.614       |
-----------------------------------------
Iteration: 1401 | Episodes: 56900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1403 | Episodes: 57000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1406 | Episodes: 57100 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 1408 | Episodes: 57200 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1410         |
|    time_elapsed         | 27627        |
|    total_timesteps      | 5775360      |
| train/                  |              |
|    approx_kl            | 0.0018921641 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.4          |
|    entropy_loss         | -270         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.4        |
|    n_updates            | 14090        |
|    policy_gradient_loss | 0.000122     |
|    std                  | 1.3          |
|    value_loss           | 0.742        |
------------------------------------------
Iteration: 1411 | Episodes: 57300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1413 | Episodes: 57400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1416 | Episodes: 57500 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1418 | Episodes: 57600 | Median Reward: 47.89 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -51.4        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1420         |
|    time_elapsed         | 27809        |
|    total_timesteps      | 5816320      |
| train/                  |              |
|    approx_kl            | 0.0009360176 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -270         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -12.7        |
|    n_updates            | 14190        |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.31         |
|    value_loss           | 0.793        |
------------------------------------------
Iteration: 1421 | Episodes: 57700 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 1423 | Episodes: 57800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1426 | Episodes: 57900 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1428 | Episodes: 58000 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1430          |
|    time_elapsed         | 28001         |
|    total_timesteps      | 5857280       |
| train/                  |               |
|    approx_kl            | 0.00018195467 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -270          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -8.03         |
|    n_updates            | 14290         |
|    policy_gradient_loss | -0.000608     |
|    std                  | 1.31          |
|    value_loss           | 6.03          |
-------------------------------------------
Iteration: 1431 | Episodes: 58100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1433 | Episodes: 58200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1435 | Episodes: 58300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1438 | Episodes: 58400 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1440         |
|    time_elapsed         | 28189        |
|    total_timesteps      | 5898240      |
| train/                  |              |
|    approx_kl            | 0.0022240719 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -270         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.4        |
|    n_updates            | 14390        |
|    policy_gradient_loss | -0.000414    |
|    std                  | 1.31         |
|    value_loss           | 0.561        |
------------------------------------------
Iteration: 1440 | Episodes: 58500 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1443 | Episodes: 58600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1445 | Episodes: 58700 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1448 | Episodes: 58800 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1450         |
|    time_elapsed         | 28379        |
|    total_timesteps      | 5939200      |
| train/                  |              |
|    approx_kl            | 8.846191e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -270         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.4        |
|    n_updates            | 14490        |
|    policy_gradient_loss | 6.85e-05     |
|    std                  | 1.32         |
|    value_loss           | 0.513        |
------------------------------------------
Iteration: 1450 | Episodes: 58900 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1453 | Episodes: 59000 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1455 | Episodes: 59100 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1458 | Episodes: 59200 | Median Reward: 47.87 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53          |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1460         |
|    time_elapsed         | 28569        |
|    total_timesteps      | 5980160      |
| train/                  |              |
|    approx_kl            | 0.0019345835 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.4          |
|    entropy_loss         | -270         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.2        |
|    n_updates            | 14590        |
|    policy_gradient_loss | -0.00114     |
|    std                  | 1.32         |
|    value_loss           | 1.1          |
------------------------------------------
Iteration: 1460 | Episodes: 59300 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1463 | Episodes: 59400 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 1465 | Episodes: 59500 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1468 | Episodes: 59600 | Median Reward: 47.88 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1470         |
|    time_elapsed         | 28755        |
|    total_timesteps      | 6021120      |
| train/                  |              |
|    approx_kl            | 0.0013725446 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -270         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.2        |
|    n_updates            | 14690        |
|    policy_gradient_loss | -0.000585    |
|    std                  | 1.33         |
|    value_loss           | 0.617        |
------------------------------------------
Iteration: 1470 | Episodes: 59700 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1472 | Episodes: 59800 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 1475 | Episodes: 59900 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 1477 | Episodes: 60000 | Median Reward: 47.87 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1480          |
|    time_elapsed         | 28946         |
|    total_timesteps      | 6062080       |
| train/                  |               |
|    approx_kl            | 0.00053874153 |
|    clip_fraction        | 0.000269      |
|    clip_range           | 0.4           |
|    entropy_loss         | -270          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -13.2         |
|    n_updates            | 14790         |
|    policy_gradient_loss | -0.000784     |
|    std                  | 1.33          |
|    value_loss           | 1.08          |
-------------------------------------------
Iteration: 1480 | Episodes: 60100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1482 | Episodes: 60200 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1485 | Episodes: 60300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1487 | Episodes: 60400 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1490          |
|    time_elapsed         | 29142         |
|    total_timesteps      | 6103040       |
| train/                  |               |
|    approx_kl            | 0.00011021935 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -271          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -13.2         |
|    n_updates            | 14890         |
|    policy_gradient_loss | -8.99e-05     |
|    std                  | 1.33          |
|    value_loss           | 0.5           |
-------------------------------------------
Iteration: 1490 | Episodes: 60500 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1492 | Episodes: 60600 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1495 | Episodes: 60700 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1497 | Episodes: 60800 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1500         |
|    time_elapsed         | 29336        |
|    total_timesteps      | 6144000      |
| train/                  |              |
|    approx_kl            | 0.0028834036 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -271         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.3        |
|    n_updates            | 14990        |
|    policy_gradient_loss | 0.000394     |
|    std                  | 1.34         |
|    value_loss           | 0.684        |
------------------------------------------
Iteration: 1500 | Episodes: 60900 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1502 | Episodes: 61000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1504 | Episodes: 61100 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1507 | Episodes: 61200 | Median Reward: 47.79 | Max Reward: 49.24
Iteration: 1509 | Episodes: 61300 | Median Reward: 47.82 | Max Reward: 49.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -53.5      |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 1510       |
|    time_elapsed         | 29521      |
|    total_timesteps      | 6184960    |
| train/                  |            |
|    approx_kl            | 0.17794722 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.4        |
|    entropy_loss         | -271       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -13.6      |
|    n_updates            | 15090      |
|    policy_gradient_loss | -0.0814    |
|    std                  | 1.34       |
|    value_loss           | 0.51       |
----------------------------------------
Iteration: 1512 | Episodes: 61400 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 1514 | Episodes: 61500 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1517 | Episodes: 61600 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1519 | Episodes: 61700 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -51.3         |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1520          |
|    time_elapsed         | 29709         |
|    total_timesteps      | 6225920       |
| train/                  |               |
|    approx_kl            | 0.00021289267 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -271          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -13.5         |
|    n_updates            | 15190         |
|    policy_gradient_loss | -0.000278     |
|    std                  | 1.35          |
|    value_loss           | 0.572         |
-------------------------------------------
Iteration: 1522 | Episodes: 61800 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1524 | Episodes: 61900 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1527 | Episodes: 62000 | Median Reward: 47.76 | Max Reward: 49.24
Iteration: 1529 | Episodes: 62100 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1530         |
|    time_elapsed         | 29906        |
|    total_timesteps      | 6266880      |
| train/                  |              |
|    approx_kl            | 0.0025918828 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -271         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -12.6        |
|    n_updates            | 15290        |
|    policy_gradient_loss | -0.000312    |
|    std                  | 1.35         |
|    value_loss           | 0.712        |
------------------------------------------
Iteration: 1532 | Episodes: 62200 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1534 | Episodes: 62300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1536 | Episodes: 62400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1539 | Episodes: 62500 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1540         |
|    time_elapsed         | 30098        |
|    total_timesteps      | 6307840      |
| train/                  |              |
|    approx_kl            | 0.0033806926 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -271         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.1        |
|    n_updates            | 15390        |
|    policy_gradient_loss | -0.000388    |
|    std                  | 1.36         |
|    value_loss           | 0.504        |
------------------------------------------
Iteration: 1541 | Episodes: 62600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1544 | Episodes: 62700 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1546 | Episodes: 62800 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1549 | Episodes: 62900 | Median Reward: 47.80 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.6       |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 1550        |
|    time_elapsed         | 30291       |
|    total_timesteps      | 6348800     |
| train/                  |             |
|    approx_kl            | 0.040101722 |
|    clip_fraction        | 0.0498      |
|    clip_range           | 0.4         |
|    entropy_loss         | -272        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -13.3       |
|    n_updates            | 15490       |
|    policy_gradient_loss | -0.0506     |
|    std                  | 1.36        |
|    value_loss           | 0.607       |
-----------------------------------------
Iteration: 1551 | Episodes: 63000 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1554 | Episodes: 63100 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1556 | Episodes: 63200 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1559 | Episodes: 63300 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.6        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1560         |
|    time_elapsed         | 30478        |
|    total_timesteps      | 6389760      |
| train/                  |              |
|    approx_kl            | 0.0077781137 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -272         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.5        |
|    n_updates            | 15590        |
|    policy_gradient_loss | -0.0151      |
|    std                  | 1.37         |
|    value_loss           | 0.854        |
------------------------------------------
Iteration: 1561 | Episodes: 63400 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1564 | Episodes: 63500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1566 | Episodes: 63600 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1569 | Episodes: 63700 | Median Reward: 47.93 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.4       |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 1570        |
|    time_elapsed         | 30667       |
|    total_timesteps      | 6430720     |
| train/                  |             |
|    approx_kl            | 0.003295043 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -273        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -12         |
|    n_updates            | 15690       |
|    policy_gradient_loss | -0.00128    |
|    std                  | 1.37        |
|    value_loss           | 1.21        |
-----------------------------------------
Iteration: 1571 | Episodes: 63800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1573 | Episodes: 63900 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1576 | Episodes: 64000 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1578 | Episodes: 64100 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1580         |
|    time_elapsed         | 30857        |
|    total_timesteps      | 6471680      |
| train/                  |              |
|    approx_kl            | 0.0053527625 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -273         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.5        |
|    n_updates            | 15790        |
|    policy_gradient_loss | -0.00622     |
|    std                  | 1.37         |
|    value_loss           | 0.586        |
------------------------------------------
Iteration: 1581 | Episodes: 64200 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1583 | Episodes: 64300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1586 | Episodes: 64400 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1588 | Episodes: 64500 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1590          |
|    time_elapsed         | 31046         |
|    total_timesteps      | 6512640       |
| train/                  |               |
|    approx_kl            | 0.00013546925 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -274          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -12.6         |
|    n_updates            | 15890         |
|    policy_gradient_loss | -0.000558     |
|    std                  | 1.38          |
|    value_loss           | 0.684         |
-------------------------------------------
Iteration: 1591 | Episodes: 64600 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1593 | Episodes: 64700 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1596 | Episodes: 64800 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1598 | Episodes: 64900 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1600         |
|    time_elapsed         | 31233        |
|    total_timesteps      | 6553600      |
| train/                  |              |
|    approx_kl            | 0.0002550144 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -274         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.1        |
|    n_updates            | 15990        |
|    policy_gradient_loss | -0.000715    |
|    std                  | 1.38         |
|    value_loss           | 3.32         |
------------------------------------------
Iteration: 1601 | Episodes: 65000 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 1603 | Episodes: 65100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1605 | Episodes: 65200 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1608 | Episodes: 65300 | Median Reward: 47.94 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1610         |
|    time_elapsed         | 31427        |
|    total_timesteps      | 6594560      |
| train/                  |              |
|    approx_kl            | 9.810479e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -274         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.4        |
|    n_updates            | 16090        |
|    policy_gradient_loss | 1.44e-05     |
|    std                  | 1.38         |
|    value_loss           | 0.844        |
------------------------------------------
Iteration: 1610 | Episodes: 65400 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1613 | Episodes: 65500 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 1615 | Episodes: 65600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1618 | Episodes: 65700 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1620          |
|    time_elapsed         | 31630         |
|    total_timesteps      | 6635520       |
| train/                  |               |
|    approx_kl            | 0.00020297772 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -275          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -13.3         |
|    n_updates            | 16190         |
|    policy_gradient_loss | -6.03e-05     |
|    std                  | 1.39          |
|    value_loss           | 0.362         |
-------------------------------------------
Iteration: 1620 | Episodes: 65800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1623 | Episodes: 65900 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1625 | Episodes: 66000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1628 | Episodes: 66100 | Median Reward: 47.90 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -51.3        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1630         |
|    time_elapsed         | 31828        |
|    total_timesteps      | 6676480      |
| train/                  |              |
|    approx_kl            | 0.0001164204 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -276         |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0005       |
|    loss                 | -8.2         |
|    n_updates            | 16290        |
|    policy_gradient_loss | -0.000192    |
|    std                  | 1.39         |
|    value_loss           | 12.5         |
------------------------------------------
Iteration: 1630 | Episodes: 66200 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 1633 | Episodes: 66300 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 1635 | Episodes: 66400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1637 | Episodes: 66500 | Median Reward: 47.91 | Max Reward: 49.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -52.5      |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 1640       |
|    time_elapsed         | 32009      |
|    total_timesteps      | 6717440    |
| train/                  |            |
|    approx_kl            | 0.19385087 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.4        |
|    entropy_loss         | -276       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -13.8      |
|    n_updates            | 16390      |
|    policy_gradient_loss | -0.1       |
|    std                  | 1.4        |
|    value_loss           | 0.231      |
----------------------------------------
Iteration: 1640 | Episodes: 66600 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1642 | Episodes: 66700 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1645 | Episodes: 66800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1647 | Episodes: 66900 | Median Reward: 47.88 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.9       |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 1650        |
|    time_elapsed         | 32210       |
|    total_timesteps      | 6758400     |
| train/                  |             |
|    approx_kl            | 0.033154223 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.4         |
|    entropy_loss         | -276        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -13.5       |
|    n_updates            | 16490       |
|    policy_gradient_loss | -0.0627     |
|    std                  | 1.4         |
|    value_loss           | 0.876       |
-----------------------------------------
Iteration: 1650 | Episodes: 67000 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1652 | Episodes: 67100 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1655 | Episodes: 67200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1657 | Episodes: 67300 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.3         |
|    ep_rew_mean          | -50.5        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1660         |
|    time_elapsed         | 32402        |
|    total_timesteps      | 6799360      |
| train/                  |              |
|    approx_kl            | 0.0006590409 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -277         |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0005       |
|    loss                 | -5.54        |
|    n_updates            | 16590        |
|    policy_gradient_loss | -0.000524    |
|    std                  | 1.4          |
|    value_loss           | 16.8         |
------------------------------------------
Iteration: 1660 | Episodes: 67400 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1662 | Episodes: 67500 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1665 | Episodes: 67600 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1667 | Episodes: 67700 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.8         |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1670          |
|    time_elapsed         | 32593         |
|    total_timesteps      | 6840320       |
| train/                  |               |
|    approx_kl            | 0.00055357395 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -277          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -13.8         |
|    n_updates            | 16690         |
|    policy_gradient_loss | -6.05e-06     |
|    std                  | 1.41          |
|    value_loss           | 0.555         |
-------------------------------------------
Iteration: 1670 | Episodes: 67800 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1672 | Episodes: 67900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1674 | Episodes: 68000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1677 | Episodes: 68100 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1679 | Episodes: 68200 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1680         |
|    time_elapsed         | 32780        |
|    total_timesteps      | 6881280      |
| train/                  |              |
|    approx_kl            | 4.744342e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -278         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.6        |
|    n_updates            | 16790        |
|    policy_gradient_loss | -0.00023     |
|    std                  | 1.41         |
|    value_loss           | 0.857        |
------------------------------------------
Iteration: 1682 | Episodes: 68300 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1684 | Episodes: 68400 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 1687 | Episodes: 68500 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1689 | Episodes: 68600 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1690          |
|    time_elapsed         | 32978         |
|    total_timesteps      | 6922240       |
| train/                  |               |
|    approx_kl            | 2.5926565e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -278          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -13.2         |
|    n_updates            | 16890         |
|    policy_gradient_loss | -1.16e-05     |
|    std                  | 1.42          |
|    value_loss           | 1.76          |
-------------------------------------------
Iteration: 1692 | Episodes: 68700 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 1694 | Episodes: 68800 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1697 | Episodes: 68900 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 1699 | Episodes: 69000 | Median Reward: 47.94 | Max Reward: 49.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -52.7      |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 1700       |
|    time_elapsed         | 33168      |
|    total_timesteps      | 6963200    |
| train/                  |            |
|    approx_kl            | 0.01927621 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -279       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -13.6      |
|    n_updates            | 16990      |
|    policy_gradient_loss | -0.0257    |
|    std                  | 1.42       |
|    value_loss           | 0.462      |
----------------------------------------
Iteration: 1702 | Episodes: 69100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1704 | Episodes: 69200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1706 | Episodes: 69300 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1709 | Episodes: 69400 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1710         |
|    time_elapsed         | 33362        |
|    total_timesteps      | 7004160      |
| train/                  |              |
|    approx_kl            | 0.0002165401 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -279         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.9        |
|    n_updates            | 17090        |
|    policy_gradient_loss | -0.000122    |
|    std                  | 1.42         |
|    value_loss           | 0.483        |
------------------------------------------
Iteration: 1711 | Episodes: 69500 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1714 | Episodes: 69600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1716 | Episodes: 69700 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1719 | Episodes: 69800 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -51.7        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 1720         |
|    time_elapsed         | 33545        |
|    total_timesteps      | 7045120      |
| train/                  |              |
|    approx_kl            | 0.0048049167 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -279         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.7        |
|    n_updates            | 17190        |
|    policy_gradient_loss | -0.00933     |
|    std                  | 1.43         |
|    value_loss           | 0.271        |
------------------------------------------
Iteration: 1721 | Episodes: 69900 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1724 | Episodes: 70000 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1726 | Episodes: 70100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1729 | Episodes: 70200 | Median Reward: 47.91 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.9       |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 1730        |
|    time_elapsed         | 33741       |
|    total_timesteps      | 7086080     |
| train/                  |             |
|    approx_kl            | 0.024903769 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -279        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.3       |
|    n_updates            | 17290       |
|    policy_gradient_loss | -0.0425     |
|    std                  | 1.43        |
|    value_loss           | 1.32        |
-----------------------------------------
Iteration: 1731 | Episodes: 70300 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 1734 | Episodes: 70400 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1736 | Episodes: 70500 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1738 | Episodes: 70600 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 1740          |
|    time_elapsed         | 33936         |
|    total_timesteps      | 7127040       |
| train/                  |               |
|    approx_kl            | 8.3408086e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -280          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -13.8         |
|    n_updates            | 17390         |
|    policy_gradient_loss | -0.000284     |
|    std                  | 1.44          |
|    value_loss           | 0.694         |
-------------------------------------------
Iteration: 1741 | Episodes: 70700 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1743 | Episodes: 70800 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1746 | Episodes: 70900 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1748 | Episodes: 71000 | Median Reward: 47.82 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.4        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1750         |
|    time_elapsed         | 34138        |
|    total_timesteps      | 7168000      |
| train/                  |              |
|    approx_kl            | 0.0040322114 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -280         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.6        |
|    n_updates            | 17490        |
|    policy_gradient_loss | -0.00106     |
|    std                  | 1.44         |
|    value_loss           | 3.3          |
------------------------------------------
Iteration: 1751 | Episodes: 71100 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1753 | Episodes: 71200 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1756 | Episodes: 71300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1758 | Episodes: 71400 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1760         |
|    time_elapsed         | 34335        |
|    total_timesteps      | 7208960      |
| train/                  |              |
|    approx_kl            | 8.651512e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -281         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.9        |
|    n_updates            | 17590        |
|    policy_gradient_loss | -0.000163    |
|    std                  | 1.45         |
|    value_loss           | 0.586        |
------------------------------------------
Iteration: 1761 | Episodes: 71500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1763 | Episodes: 71600 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1766 | Episodes: 71700 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1768 | Episodes: 71800 | Median Reward: 47.86 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.9        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1770         |
|    time_elapsed         | 34525        |
|    total_timesteps      | 7249920      |
| train/                  |              |
|    approx_kl            | 5.361192e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -281         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.6        |
|    n_updates            | 17690        |
|    policy_gradient_loss | -4.9e-05     |
|    std                  | 1.45         |
|    value_loss           | 1.11         |
------------------------------------------
Iteration: 1770 | Episodes: 71900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1773 | Episodes: 72000 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 1775 | Episodes: 72100 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 1778 | Episodes: 72200 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1780         |
|    time_elapsed         | 34723        |
|    total_timesteps      | 7290880      |
| train/                  |              |
|    approx_kl            | 0.0021173467 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -281         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14          |
|    n_updates            | 17790        |
|    policy_gradient_loss | 1.9e-05      |
|    std                  | 1.46         |
|    value_loss           | 0.75         |
------------------------------------------
Iteration: 1780 | Episodes: 72300 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1783 | Episodes: 72400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1785 | Episodes: 72500 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 1788 | Episodes: 72600 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1790         |
|    time_elapsed         | 34923        |
|    total_timesteps      | 7331840      |
| train/                  |              |
|    approx_kl            | 8.525017e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -282         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14          |
|    n_updates            | 17890        |
|    policy_gradient_loss | -0.000135    |
|    std                  | 1.46         |
|    value_loss           | 0.598        |
------------------------------------------
Iteration: 1790 | Episodes: 72700 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1793 | Episodes: 72800 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1795 | Episodes: 72900 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1798 | Episodes: 73000 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1800          |
|    time_elapsed         | 35119         |
|    total_timesteps      | 7372800       |
| train/                  |               |
|    approx_kl            | 2.6618465e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -282          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -14           |
|    n_updates            | 17990         |
|    policy_gradient_loss | -1.09e-06     |
|    std                  | 1.47          |
|    value_loss           | 0.369         |
-------------------------------------------
Iteration: 1800 | Episodes: 73100 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1803 | Episodes: 73200 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1805 | Episodes: 73300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1807 | Episodes: 73400 | Median Reward: 47.94 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1810         |
|    time_elapsed         | 35304        |
|    total_timesteps      | 7413760      |
| train/                  |              |
|    approx_kl            | 7.460333e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -283         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.9        |
|    n_updates            | 18090        |
|    policy_gradient_loss | -0.000249    |
|    std                  | 1.47         |
|    value_loss           | 1.45         |
------------------------------------------
Iteration: 1810 | Episodes: 73500 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1812 | Episodes: 73600 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1815 | Episodes: 73700 | Median Reward: 47.75 | Max Reward: 49.24
Iteration: 1817 | Episodes: 73800 | Median Reward: 47.88 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1820          |
|    time_elapsed         | 35501         |
|    total_timesteps      | 7454720       |
| train/                  |               |
|    approx_kl            | 3.2215845e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -283          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -13.9         |
|    n_updates            | 18190         |
|    policy_gradient_loss | -1.6e-05      |
|    std                  | 1.47          |
|    value_loss           | 0.638         |
-------------------------------------------
Iteration: 1820 | Episodes: 73900 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1822 | Episodes: 74000 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 1825 | Episodes: 74100 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1827 | Episodes: 74200 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1830          |
|    time_elapsed         | 35697         |
|    total_timesteps      | 7495680       |
| train/                  |               |
|    approx_kl            | 7.6675526e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -284          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.1         |
|    n_updates            | 18290         |
|    policy_gradient_loss | -0.000164     |
|    std                  | 1.48          |
|    value_loss           | 0.732         |
-------------------------------------------
Iteration: 1830 | Episodes: 74300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1832 | Episodes: 74400 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1835 | Episodes: 74500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1837 | Episodes: 74600 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 1840         |
|    time_elapsed         | 35890        |
|    total_timesteps      | 7536640      |
| train/                  |              |
|    approx_kl            | 0.0012903524 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -284         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14          |
|    n_updates            | 18390        |
|    policy_gradient_loss | -0.000756    |
|    std                  | 1.48         |
|    value_loss           | 0.376        |
------------------------------------------
Iteration: 1840 | Episodes: 74700 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1842 | Episodes: 74800 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1844 | Episodes: 74900 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1847 | Episodes: 75000 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 1849 | Episodes: 75100 | Median Reward: 47.72 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53           |
| time/                   |               |
|    fps                  | 209           |
|    iterations           | 1850          |
|    time_elapsed         | 36086         |
|    total_timesteps      | 7577600       |
| train/                  |               |
|    approx_kl            | 0.00026536005 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -284          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -12           |
|    n_updates            | 18490         |
|    policy_gradient_loss | -4.89e-05     |
|    std                  | 1.49          |
|    value_loss           | 1.51          |
-------------------------------------------
Iteration: 1852 | Episodes: 75200 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1854 | Episodes: 75300 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 1857 | Episodes: 75400 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1859 | Episodes: 75500 | Median Reward: 47.91 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.6       |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 1860        |
|    time_elapsed         | 36269       |
|    total_timesteps      | 7618560     |
| train/                  |             |
|    approx_kl            | 0.023827706 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -285        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -14.2       |
|    n_updates            | 18590       |
|    policy_gradient_loss | -0.0334     |
|    std                  | 1.5         |
|    value_loss           | 0.175       |
-----------------------------------------
Iteration: 1862 | Episodes: 75600 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1864 | Episodes: 75700 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 1867 | Episodes: 75800 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1869 | Episodes: 75900 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 1870         |
|    time_elapsed         | 36468        |
|    total_timesteps      | 7659520      |
| train/                  |              |
|    approx_kl            | 0.0002643051 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -285         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.8        |
|    n_updates            | 18690        |
|    policy_gradient_loss | -0.000241    |
|    std                  | 1.5          |
|    value_loss           | 0.502        |
------------------------------------------
Iteration: 1872 | Episodes: 76000 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 1874 | Episodes: 76100 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1876 | Episodes: 76200 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1879 | Episodes: 76300 | Median Reward: 47.86 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -51.7         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 1880          |
|    time_elapsed         | 36666         |
|    total_timesteps      | 7700480       |
| train/                  |               |
|    approx_kl            | 0.00045429694 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -285          |
|    explained_variance   | 0.988         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.4          |
|    n_updates            | 18790         |
|    policy_gradient_loss | -0.000404     |
|    std                  | 1.51          |
|    value_loss           | 9.25          |
-------------------------------------------
Iteration: 1881 | Episodes: 76400 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1884 | Episodes: 76500 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1886 | Episodes: 76600 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1889 | Episodes: 76700 | Median Reward: 47.94 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 1890         |
|    time_elapsed         | 36858        |
|    total_timesteps      | 7741440      |
| train/                  |              |
|    approx_kl            | 0.0009919853 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -286         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.7        |
|    n_updates            | 18890        |
|    policy_gradient_loss | -0.00121     |
|    std                  | 1.51         |
|    value_loss           | 0.548        |
------------------------------------------
Iteration: 1891 | Episodes: 76800 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1894 | Episodes: 76900 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1896 | Episodes: 77000 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1899 | Episodes: 77100 | Median Reward: 47.86 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.4       |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 1900        |
|    time_elapsed         | 37049       |
|    total_timesteps      | 7782400     |
| train/                  |             |
|    approx_kl            | 0.015583351 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -286        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -14.2       |
|    n_updates            | 18990       |
|    policy_gradient_loss | -0.0243     |
|    std                  | 1.52        |
|    value_loss           | 0.557       |
-----------------------------------------
Iteration: 1901 | Episodes: 77200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1904 | Episodes: 77300 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1906 | Episodes: 77400 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 1909 | Episodes: 77500 | Median Reward: 47.70 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.5       |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 1910        |
|    time_elapsed         | 37247       |
|    total_timesteps      | 7823360     |
| train/                  |             |
|    approx_kl            | 0.031212214 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -287        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -14.4       |
|    n_updates            | 19090       |
|    policy_gradient_loss | -0.0466     |
|    std                  | 1.52        |
|    value_loss           | 0.286       |
-----------------------------------------
Iteration: 1911 | Episodes: 77600 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 1913 | Episodes: 77700 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1916 | Episodes: 77800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1918 | Episodes: 77900 | Median Reward: 47.94 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 1920          |
|    time_elapsed         | 37436         |
|    total_timesteps      | 7864320       |
| train/                  |               |
|    approx_kl            | 4.7104666e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -287          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -13.6         |
|    n_updates            | 19190         |
|    policy_gradient_loss | -9.36e-05     |
|    std                  | 1.52          |
|    value_loss           | 0.331         |
-------------------------------------------
Iteration: 1921 | Episodes: 78000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1923 | Episodes: 78100 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1926 | Episodes: 78200 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1928 | Episodes: 78300 | Median Reward: 47.94 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 1930         |
|    time_elapsed         | 37626        |
|    total_timesteps      | 7905280      |
| train/                  |              |
|    approx_kl            | 0.0010894614 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -287         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.9        |
|    n_updates            | 19290        |
|    policy_gradient_loss | -0.00205     |
|    std                  | 1.53         |
|    value_loss           | 0.308        |
------------------------------------------
Iteration: 1931 | Episodes: 78400 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1933 | Episodes: 78500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1936 | Episodes: 78600 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 1938 | Episodes: 78700 | Median Reward: 47.73 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 1940         |
|    time_elapsed         | 37816        |
|    total_timesteps      | 7946240      |
| train/                  |              |
|    approx_kl            | 0.0002249315 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -288         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.1        |
|    n_updates            | 19390        |
|    policy_gradient_loss | -0.000412    |
|    std                  | 1.53         |
|    value_loss           | 2.58         |
------------------------------------------
Iteration: 1941 | Episodes: 78800 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 1943 | Episodes: 78900 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 1945 | Episodes: 79000 | Median Reward: 47.66 | Max Reward: 49.24
Iteration: 1948 | Episodes: 79100 | Median Reward: 47.87 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 1950         |
|    time_elapsed         | 38007        |
|    total_timesteps      | 7987200      |
| train/                  |              |
|    approx_kl            | 0.0008166202 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -288         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.4        |
|    n_updates            | 19490        |
|    policy_gradient_loss | -0.000797    |
|    std                  | 1.54         |
|    value_loss           | 0.236        |
------------------------------------------
Iteration: 1950 | Episodes: 79200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1953 | Episodes: 79300 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1955 | Episodes: 79400 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1958 | Episodes: 79500 | Median Reward: 47.94 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -51.3         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 1960          |
|    time_elapsed         | 38197         |
|    total_timesteps      | 8028160       |
| train/                  |               |
|    approx_kl            | 0.00015613782 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -288          |
|    explained_variance   | 0.987         |
|    learning_rate        | 0.0005        |
|    loss                 | -9.59         |
|    n_updates            | 19590         |
|    policy_gradient_loss | -0.00046      |
|    std                  | 1.54          |
|    value_loss           | 9.87          |
-------------------------------------------
Iteration: 1960 | Episodes: 79600 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1963 | Episodes: 79700 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 1965 | Episodes: 79800 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 1968 | Episodes: 79900 | Median Reward: 47.88 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 1970         |
|    time_elapsed         | 38387        |
|    total_timesteps      | 8069120      |
| train/                  |              |
|    approx_kl            | 0.0053579034 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -289         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.1        |
|    n_updates            | 19690        |
|    policy_gradient_loss | 3.68e-05     |
|    std                  | 1.55         |
|    value_loss           | 0.772        |
------------------------------------------
Iteration: 1970 | Episodes: 80000 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 1973 | Episodes: 80100 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 1975 | Episodes: 80200 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1978 | Episodes: 80300 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 1980          |
|    time_elapsed         | 38578         |
|    total_timesteps      | 8110080       |
| train/                  |               |
|    approx_kl            | 1.4920341e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -289          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.3         |
|    n_updates            | 19790         |
|    policy_gradient_loss | -5.46e-05     |
|    std                  | 1.55          |
|    value_loss           | 0.7           |
-------------------------------------------
Iteration: 1980 | Episodes: 80400 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 1982 | Episodes: 80500 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 1985 | Episodes: 80600 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 1987 | Episodes: 80700 | Median Reward: 47.85 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.5         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 1990          |
|    time_elapsed         | 38766         |
|    total_timesteps      | 8151040       |
| train/                  |               |
|    approx_kl            | 0.00049828715 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -289          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.2         |
|    n_updates            | 19890         |
|    policy_gradient_loss | 0.000259      |
|    std                  | 1.56          |
|    value_loss           | 1             |
-------------------------------------------
Iteration: 1990 | Episodes: 80800 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 1992 | Episodes: 80900 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 1995 | Episodes: 81000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 1997 | Episodes: 81100 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2000          |
|    time_elapsed         | 38957         |
|    total_timesteps      | 8192000       |
| train/                  |               |
|    approx_kl            | 3.3719713e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -289          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.4         |
|    n_updates            | 19990         |
|    policy_gradient_loss | -7.65e-05     |
|    std                  | 1.56          |
|    value_loss           | 1.81          |
-------------------------------------------
Iteration: 2000 | Episodes: 81200 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2002 | Episodes: 81300 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2005 | Episodes: 81400 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2007 | Episodes: 81500 | Median Reward: 47.89 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2010          |
|    time_elapsed         | 39148         |
|    total_timesteps      | 8232960       |
| train/                  |               |
|    approx_kl            | 0.00043211895 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -289          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -13.9         |
|    n_updates            | 20090         |
|    policy_gradient_loss | -0.000414     |
|    std                  | 1.56          |
|    value_loss           | 2.91          |
-------------------------------------------
Iteration: 2010 | Episodes: 81600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2012 | Episodes: 81700 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2015 | Episodes: 81800 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2017 | Episodes: 81900 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2019 | Episodes: 82000 | Median Reward: 47.82 | Max Reward: 49.24
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -52.5     |
| time/                   |           |
|    fps                  | 210       |
|    iterations           | 2020      |
|    time_elapsed         | 39340     |
|    total_timesteps      | 8273920   |
| train/                  |           |
|    approx_kl            | 0.3491683 |
|    clip_fraction        | 0.35      |
|    clip_range           | 0.4       |
|    entropy_loss         | -290      |
|    explained_variance   | 1         |
|    learning_rate        | 0.0005    |
|    loss                 | -14.5     |
|    n_updates            | 20190     |
|    policy_gradient_loss | -0.0668   |
|    std                  | 1.56      |
|    value_loss           | 0.105     |
---------------------------------------
Iteration: 2022 | Episodes: 82100 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2024 | Episodes: 82200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2027 | Episodes: 82300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2029 | Episodes: 82400 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2030          |
|    time_elapsed         | 39531         |
|    total_timesteps      | 8314880       |
| train/                  |               |
|    approx_kl            | 0.00048627018 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -290          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -14.3         |
|    n_updates            | 20290         |
|    policy_gradient_loss | -2.47e-06     |
|    std                  | 1.57          |
|    value_loss           | 0.66          |
-------------------------------------------
Iteration: 2032 | Episodes: 82500 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2034 | Episodes: 82600 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2037 | Episodes: 82700 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2039 | Episodes: 82800 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.4         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2040          |
|    time_elapsed         | 39719         |
|    total_timesteps      | 8355840       |
| train/                  |               |
|    approx_kl            | 0.00063693326 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -290          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.5         |
|    n_updates            | 20390         |
|    policy_gradient_loss | -0.000329     |
|    std                  | 1.57          |
|    value_loss           | 0.192         |
-------------------------------------------
Iteration: 2042 | Episodes: 82900 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2044 | Episodes: 83000 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2047 | Episodes: 83100 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 2049 | Episodes: 83200 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.7         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2050          |
|    time_elapsed         | 39910         |
|    total_timesteps      | 8396800       |
| train/                  |               |
|    approx_kl            | 2.3838962e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -291          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.4         |
|    n_updates            | 20490         |
|    policy_gradient_loss | -9.98e-05     |
|    std                  | 1.58          |
|    value_loss           | 0.442         |
-------------------------------------------
Iteration: 2052 | Episodes: 83300 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2054 | Episodes: 83400 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2056 | Episodes: 83500 | Median Reward: 47.79 | Max Reward: 49.24
Iteration: 2059 | Episodes: 83600 | Median Reward: 47.94 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 2060         |
|    time_elapsed         | 40099        |
|    total_timesteps      | 8437760      |
| train/                  |              |
|    approx_kl            | 9.577167e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -291         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.8        |
|    n_updates            | 20590        |
|    policy_gradient_loss | -2.06e-05    |
|    std                  | 1.58         |
|    value_loss           | 0.933        |
------------------------------------------
Iteration: 2061 | Episodes: 83700 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2064 | Episodes: 83800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2066 | Episodes: 83900 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2069 | Episodes: 84000 | Median Reward: 47.90 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.8       |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 2070        |
|    time_elapsed         | 40284       |
|    total_timesteps      | 8478720     |
| train/                  |             |
|    approx_kl            | 0.006847473 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -291        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -12.5       |
|    n_updates            | 20690       |
|    policy_gradient_loss | -0.00524    |
|    std                  | 1.59        |
|    value_loss           | 1.14        |
-----------------------------------------
Iteration: 2071 | Episodes: 84100 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2074 | Episodes: 84200 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2076 | Episodes: 84300 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 2079 | Episodes: 84400 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2080          |
|    time_elapsed         | 40475         |
|    total_timesteps      | 8519680       |
| train/                  |               |
|    approx_kl            | 0.00010107619 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -292          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.6         |
|    n_updates            | 20790         |
|    policy_gradient_loss | -0.000119     |
|    std                  | 1.6           |
|    value_loss           | 0.159         |
-------------------------------------------
Iteration: 2081 | Episodes: 84500 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2084 | Episodes: 84600 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2086 | Episodes: 84700 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2088 | Episodes: 84800 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.8         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2090          |
|    time_elapsed         | 40665         |
|    total_timesteps      | 8560640       |
| train/                  |               |
|    approx_kl            | 2.2072432e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -292          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.5         |
|    n_updates            | 20890         |
|    policy_gradient_loss | -8.43e-05     |
|    std                  | 1.6           |
|    value_loss           | 1.44          |
-------------------------------------------
Iteration: 2091 | Episodes: 84900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2093 | Episodes: 85000 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2096 | Episodes: 85100 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2098 | Episodes: 85200 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 2100         |
|    time_elapsed         | 40854        |
|    total_timesteps      | 8601600      |
| train/                  |              |
|    approx_kl            | 6.994033e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -292         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.5        |
|    n_updates            | 20990        |
|    policy_gradient_loss | 0.000141     |
|    std                  | 1.61         |
|    value_loss           | 0.296        |
------------------------------------------
Iteration: 2101 | Episodes: 85300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2103 | Episodes: 85400 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2106 | Episodes: 85500 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2108 | Episodes: 85600 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52           |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2110          |
|    time_elapsed         | 41045         |
|    total_timesteps      | 8642560       |
| train/                  |               |
|    approx_kl            | 0.00012845977 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -293          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -14.3         |
|    n_updates            | 21090         |
|    policy_gradient_loss | -9.48e-05     |
|    std                  | 1.61          |
|    value_loss           | 0.645         |
-------------------------------------------
Iteration: 2111 | Episodes: 85700 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2113 | Episodes: 85800 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2116 | Episodes: 85900 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2118 | Episodes: 86000 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2120          |
|    time_elapsed         | 41233         |
|    total_timesteps      | 8683520       |
| train/                  |               |
|    approx_kl            | 4.8734117e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -293          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.6         |
|    n_updates            | 21190         |
|    policy_gradient_loss | -9.95e-05     |
|    std                  | 1.62          |
|    value_loss           | 0.279         |
-------------------------------------------
Iteration: 2120 | Episodes: 86100 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2123 | Episodes: 86200 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2125 | Episodes: 86300 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2128 | Episodes: 86400 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -51.8         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2130          |
|    time_elapsed         | 41424         |
|    total_timesteps      | 8724480       |
| train/                  |               |
|    approx_kl            | 5.4753153e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -293          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.1         |
|    n_updates            | 21290         |
|    policy_gradient_loss | -0.000216     |
|    std                  | 1.62          |
|    value_loss           | 0.661         |
-------------------------------------------
Iteration: 2130 | Episodes: 86500 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2133 | Episodes: 86600 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2135 | Episodes: 86700 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2138 | Episodes: 86800 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2140          |
|    time_elapsed         | 41614         |
|    total_timesteps      | 8765440       |
| train/                  |               |
|    approx_kl            | 2.4688692e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -293          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.5         |
|    n_updates            | 21390         |
|    policy_gradient_loss | -8.92e-05     |
|    std                  | 1.62          |
|    value_loss           | 0.516         |
-------------------------------------------
Iteration: 2140 | Episodes: 86900 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2143 | Episodes: 87000 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2145 | Episodes: 87100 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2148 | Episodes: 87200 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2150          |
|    time_elapsed         | 41801         |
|    total_timesteps      | 8806400       |
| train/                  |               |
|    approx_kl            | 0.00017322558 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -293          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -13.3         |
|    n_updates            | 21490         |
|    policy_gradient_loss | -0.000266     |
|    std                  | 1.63          |
|    value_loss           | 2.88          |
-------------------------------------------
Iteration: 2150 | Episodes: 87300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2153 | Episodes: 87400 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2155 | Episodes: 87500 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2157 | Episodes: 87600 | Median Reward: 47.90 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.4       |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 2160        |
|    time_elapsed         | 41991       |
|    total_timesteps      | 8847360     |
| train/                  |             |
|    approx_kl            | 0.009365899 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -293        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -14.6       |
|    n_updates            | 21590       |
|    policy_gradient_loss | -0.00372    |
|    std                  | 1.63        |
|    value_loss           | 0.156       |
-----------------------------------------
Iteration: 2160 | Episodes: 87700 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2162 | Episodes: 87800 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 2165 | Episodes: 87900 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2167 | Episodes: 88000 | Median Reward: 47.90 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.7         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2170          |
|    time_elapsed         | 42181         |
|    total_timesteps      | 8888320       |
| train/                  |               |
|    approx_kl            | 0.00072938245 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -294          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.5         |
|    n_updates            | 21690         |
|    policy_gradient_loss | -0.00112      |
|    std                  | 1.64          |
|    value_loss           | 0.22          |
-------------------------------------------
Iteration: 2170 | Episodes: 88100 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 2172 | Episodes: 88200 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2175 | Episodes: 88300 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2177 | Episodes: 88400 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 2180         |
|    time_elapsed         | 42371        |
|    total_timesteps      | 8929280      |
| train/                  |              |
|    approx_kl            | 9.810741e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -294         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.4        |
|    n_updates            | 21790        |
|    policy_gradient_loss | -4.28e-05    |
|    std                  | 1.65         |
|    value_loss           | 0.605        |
------------------------------------------
Iteration: 2180 | Episodes: 88500 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2182 | Episodes: 88600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2185 | Episodes: 88700 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2187 | Episodes: 88800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2189 | Episodes: 88900 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2190          |
|    time_elapsed         | 42562         |
|    total_timesteps      | 8970240       |
| train/                  |               |
|    approx_kl            | 0.00060886005 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -294          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.7         |
|    n_updates            | 21890         |
|    policy_gradient_loss | -0.00164      |
|    std                  | 1.65          |
|    value_loss           | 0.428         |
-------------------------------------------
Iteration: 2192 | Episodes: 89000 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2194 | Episodes: 89100 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2197 | Episodes: 89200 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2199 | Episodes: 89300 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.8        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 2200         |
|    time_elapsed         | 42750        |
|    total_timesteps      | 9011200      |
| train/                  |              |
|    approx_kl            | 0.0068981806 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -295         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.6        |
|    n_updates            | 21990        |
|    policy_gradient_loss | -0.00671     |
|    std                  | 1.65         |
|    value_loss           | 0.569        |
------------------------------------------
Iteration: 2202 | Episodes: 89400 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2204 | Episodes: 89500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2207 | Episodes: 89600 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2209 | Episodes: 89700 | Median Reward: 47.89 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2210          |
|    time_elapsed         | 42941         |
|    total_timesteps      | 9052160       |
| train/                  |               |
|    approx_kl            | 1.7012368e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -295          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.2         |
|    n_updates            | 22090         |
|    policy_gradient_loss | -0.000101     |
|    std                  | 1.66          |
|    value_loss           | 2.7           |
-------------------------------------------
Iteration: 2212 | Episodes: 89800 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 2214 | Episodes: 89900 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2217 | Episodes: 90000 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2219 | Episodes: 90100 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 2220         |
|    time_elapsed         | 43133        |
|    total_timesteps      | 9093120      |
| train/                  |              |
|    approx_kl            | 0.0032780753 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -295         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.4        |
|    n_updates            | 22190        |
|    policy_gradient_loss | -0.00164     |
|    std                  | 1.66         |
|    value_loss           | 0.385        |
------------------------------------------
Iteration: 2222 | Episodes: 90200 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2224 | Episodes: 90300 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2226 | Episodes: 90400 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2229 | Episodes: 90500 | Median Reward: 47.85 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 2230         |
|    time_elapsed         | 43320        |
|    total_timesteps      | 9134080      |
| train/                  |              |
|    approx_kl            | 6.294351e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -295         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.8        |
|    n_updates            | 22290        |
|    policy_gradient_loss | -0.000105    |
|    std                  | 1.67         |
|    value_loss           | 2.27         |
------------------------------------------
Iteration: 2231 | Episodes: 90600 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2234 | Episodes: 90700 | Median Reward: 47.79 | Max Reward: 49.24
Iteration: 2236 | Episodes: 90800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2239 | Episodes: 90900 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2240          |
|    time_elapsed         | 43505         |
|    total_timesteps      | 9175040       |
| train/                  |               |
|    approx_kl            | 1.1083437e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -295          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -13.8         |
|    n_updates            | 22390         |
|    policy_gradient_loss | -3.69e-05     |
|    std                  | 1.67          |
|    value_loss           | 2.17          |
-------------------------------------------
Iteration: 2241 | Episodes: 91000 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2244 | Episodes: 91100 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2246 | Episodes: 91200 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2249 | Episodes: 91300 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 2250         |
|    time_elapsed         | 43698        |
|    total_timesteps      | 9216000      |
| train/                  |              |
|    approx_kl            | 0.0037334491 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -296         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.7        |
|    n_updates            | 22490        |
|    policy_gradient_loss | -0.0158      |
|    std                  | 1.67         |
|    value_loss           | 2.16         |
------------------------------------------
Iteration: 2251 | Episodes: 91400 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2254 | Episodes: 91500 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2256 | Episodes: 91600 | Median Reward: 47.62 | Max Reward: 49.24
Iteration: 2259 | Episodes: 91700 | Median Reward: 47.86 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 2260         |
|    time_elapsed         | 43888        |
|    total_timesteps      | 9256960      |
| train/                  |              |
|    approx_kl            | 0.0004940892 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -296         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.7        |
|    n_updates            | 22590        |
|    policy_gradient_loss | -0.000198    |
|    std                  | 1.67         |
|    value_loss           | 0.239        |
------------------------------------------
Iteration: 2261 | Episodes: 91800 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2263 | Episodes: 91900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2266 | Episodes: 92000 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 2268 | Episodes: 92100 | Median Reward: 47.95 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.5         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2270          |
|    time_elapsed         | 44077         |
|    total_timesteps      | 9297920       |
| train/                  |               |
|    approx_kl            | 0.00020219419 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -296          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -14.5         |
|    n_updates            | 22690         |
|    policy_gradient_loss | -0.00101      |
|    std                  | 1.68          |
|    value_loss           | 2.23          |
-------------------------------------------
Iteration: 2271 | Episodes: 92200 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2273 | Episodes: 92300 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2276 | Episodes: 92400 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2278 | Episodes: 92500 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.3         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2280          |
|    time_elapsed         | 44268         |
|    total_timesteps      | 9338880       |
| train/                  |               |
|    approx_kl            | 0.00066309795 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -296          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.8         |
|    n_updates            | 22790         |
|    policy_gradient_loss | -0.000268     |
|    std                  | 1.68          |
|    value_loss           | 0.451         |
-------------------------------------------
Iteration: 2281 | Episodes: 92600 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2283 | Episodes: 92700 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2286 | Episodes: 92800 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 2288 | Episodes: 92900 | Median Reward: 47.84 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.3         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2290          |
|    time_elapsed         | 44458         |
|    total_timesteps      | 9379840       |
| train/                  |               |
|    approx_kl            | 0.00067349675 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -297          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -14.5         |
|    n_updates            | 22890         |
|    policy_gradient_loss | -0.00101      |
|    std                  | 1.69          |
|    value_loss           | 0.563         |
-------------------------------------------
Iteration: 2291 | Episodes: 93000 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2293 | Episodes: 93100 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2296 | Episodes: 93200 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2298 | Episodes: 93300 | Median Reward: 47.89 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 210          |
|    iterations           | 2300         |
|    time_elapsed         | 44651        |
|    total_timesteps      | 9420800      |
| train/                  |              |
|    approx_kl            | 8.661901e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -297         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.7        |
|    n_updates            | 22990        |
|    policy_gradient_loss | -0.000114    |
|    std                  | 1.69         |
|    value_loss           | 0.953        |
------------------------------------------
Iteration: 2300 | Episodes: 93400 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2303 | Episodes: 93500 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2305 | Episodes: 93600 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2308 | Episodes: 93700 | Median Reward: 47.93 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.7       |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 2310        |
|    time_elapsed         | 44843       |
|    total_timesteps      | 9461760     |
| train/                  |             |
|    approx_kl            | 0.011981323 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -297        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -14.7       |
|    n_updates            | 23090       |
|    policy_gradient_loss | -0.0208     |
|    std                  | 1.69        |
|    value_loss           | 0.519       |
-----------------------------------------
Iteration: 2310 | Episodes: 93800 | Median Reward: 47.79 | Max Reward: 49.24
Iteration: 2313 | Episodes: 93900 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2315 | Episodes: 94000 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 2318 | Episodes: 94100 | Median Reward: 47.88 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2320          |
|    time_elapsed         | 45033         |
|    total_timesteps      | 9502720       |
| train/                  |               |
|    approx_kl            | 0.00077521603 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -297          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -14.7         |
|    n_updates            | 23190         |
|    policy_gradient_loss | -0.000383     |
|    std                  | 1.7           |
|    value_loss           | 0.235         |
-------------------------------------------
Iteration: 2320 | Episodes: 94200 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2323 | Episodes: 94300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2325 | Episodes: 94400 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2328 | Episodes: 94500 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2330         |
|    time_elapsed         | 45226        |
|    total_timesteps      | 9543680      |
| train/                  |              |
|    approx_kl            | 0.0008829057 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -297         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.5        |
|    n_updates            | 23290        |
|    policy_gradient_loss | -0.00107     |
|    std                  | 1.71         |
|    value_loss           | 0.395        |
------------------------------------------
Iteration: 2330 | Episodes: 94600 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2332 | Episodes: 94700 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 2335 | Episodes: 94800 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2337 | Episodes: 94900 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2340         |
|    time_elapsed         | 45418        |
|    total_timesteps      | 9584640      |
| train/                  |              |
|    approx_kl            | 0.0005265652 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -298         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -13.2        |
|    n_updates            | 23390        |
|    policy_gradient_loss | -0.000491    |
|    std                  | 1.72         |
|    value_loss           | 1.02         |
------------------------------------------
Iteration: 2340 | Episodes: 95000 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2342 | Episodes: 95100 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2345 | Episodes: 95200 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2347 | Episodes: 95300 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2350         |
|    time_elapsed         | 45610        |
|    total_timesteps      | 9625600      |
| train/                  |              |
|    approx_kl            | 0.0014852928 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -298         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.4        |
|    n_updates            | 23490        |
|    policy_gradient_loss | -0.000773    |
|    std                  | 1.72         |
|    value_loss           | 0.669        |
------------------------------------------
Iteration: 2350 | Episodes: 95400 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2352 | Episodes: 95500 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2355 | Episodes: 95600 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2357 | Episodes: 95700 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53          |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2360         |
|    time_elapsed         | 45802        |
|    total_timesteps      | 9666560      |
| train/                  |              |
|    approx_kl            | 0.0031766044 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -298         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.9        |
|    n_updates            | 23590        |
|    policy_gradient_loss | -0.00217     |
|    std                  | 1.73         |
|    value_loss           | 0.228        |
------------------------------------------
Iteration: 2360 | Episodes: 95800 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2362 | Episodes: 95900 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2365 | Episodes: 96000 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2367 | Episodes: 96100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2369 | Episodes: 96200 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2370         |
|    time_elapsed         | 45995        |
|    total_timesteps      | 9707520      |
| train/                  |              |
|    approx_kl            | 0.0005904309 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -299         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.9        |
|    n_updates            | 23690        |
|    policy_gradient_loss | -0.00102     |
|    std                  | 1.74         |
|    value_loss           | 0.547        |
------------------------------------------
Iteration: 2372 | Episodes: 96300 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 2374 | Episodes: 96400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2377 | Episodes: 96500 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2379 | Episodes: 96600 | Median Reward: 47.88 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2380         |
|    time_elapsed         | 46186        |
|    total_timesteps      | 9748480      |
| train/                  |              |
|    approx_kl            | 0.0012491221 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -300         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.6        |
|    n_updates            | 23790        |
|    policy_gradient_loss | -0.000692    |
|    std                  | 1.74         |
|    value_loss           | 0.349        |
------------------------------------------
Iteration: 2382 | Episodes: 96700 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2384 | Episodes: 96800 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2387 | Episodes: 96900 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2389 | Episodes: 97000 | Median Reward: 47.89 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2390          |
|    time_elapsed         | 46377         |
|    total_timesteps      | 9789440       |
| train/                  |               |
|    approx_kl            | 0.00029944238 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -300          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.9         |
|    n_updates            | 23890         |
|    policy_gradient_loss | -2.4e-05      |
|    std                  | 1.75          |
|    value_loss           | 0.248         |
-------------------------------------------
Iteration: 2392 | Episodes: 97100 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2394 | Episodes: 97200 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2397 | Episodes: 97300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2399 | Episodes: 97400 | Median Reward: 47.90 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.8       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 2400        |
|    time_elapsed         | 46571       |
|    total_timesteps      | 9830400     |
| train/                  |             |
|    approx_kl            | 0.000898169 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -300        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -15         |
|    n_updates            | 23990       |
|    policy_gradient_loss | -0.00283    |
|    std                  | 1.76        |
|    value_loss           | 0.505       |
-----------------------------------------
Iteration: 2401 | Episodes: 97500 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2404 | Episodes: 97600 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2406 | Episodes: 97700 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2409 | Episodes: 97800 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2410         |
|    time_elapsed         | 46760        |
|    total_timesteps      | 9871360      |
| train/                  |              |
|    approx_kl            | 7.992747e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -301         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.5        |
|    n_updates            | 24090        |
|    policy_gradient_loss | -0.000204    |
|    std                  | 1.76         |
|    value_loss           | 0.947        |
------------------------------------------
Iteration: 2411 | Episodes: 97900 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2414 | Episodes: 98000 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 2416 | Episodes: 98100 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2419 | Episodes: 98200 | Median Reward: 47.82 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -51.6         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2420          |
|    time_elapsed         | 46947         |
|    total_timesteps      | 9912320       |
| train/                  |               |
|    approx_kl            | 0.00027535594 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -301          |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.6         |
|    n_updates            | 24190         |
|    policy_gradient_loss | -0.000469     |
|    std                  | 1.77          |
|    value_loss           | 5.4           |
-------------------------------------------
Iteration: 2421 | Episodes: 98300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2424 | Episodes: 98400 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2426 | Episodes: 98500 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2429 | Episodes: 98600 | Median Reward: 47.97 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2430          |
|    time_elapsed         | 47133         |
|    total_timesteps      | 9953280       |
| train/                  |               |
|    approx_kl            | 0.00013433331 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -301          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.7         |
|    n_updates            | 24290         |
|    policy_gradient_loss | 0.000144      |
|    std                  | 1.78          |
|    value_loss           | 0.404         |
-------------------------------------------
Iteration: 2431 | Episodes: 98700 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2434 | Episodes: 98800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2436 | Episodes: 98900 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2438 | Episodes: 99000 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2440          |
|    time_elapsed         | 47328         |
|    total_timesteps      | 9994240       |
| train/                  |               |
|    approx_kl            | 5.9994927e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -301          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -12.4         |
|    n_updates            | 24390         |
|    policy_gradient_loss | -1.01e-05     |
|    std                  | 1.78          |
|    value_loss           | 3.42          |
-------------------------------------------
Iteration: 2441 | Episodes: 99100 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2443 | Episodes: 99200 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2446 | Episodes: 99300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2448 | Episodes: 99400 | Median Reward: 47.94 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.3       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 2450        |
|    time_elapsed         | 47520       |
|    total_timesteps      | 10035200    |
| train/                  |             |
|    approx_kl            | 0.008512642 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -302        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -15         |
|    n_updates            | 24490       |
|    policy_gradient_loss | -0.00413    |
|    std                  | 1.79        |
|    value_loss           | 0.194       |
-----------------------------------------
Iteration: 2451 | Episodes: 99500 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2453 | Episodes: 99600 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2456 | Episodes: 99700 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2458 | Episodes: 99800 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2460         |
|    time_elapsed         | 47714        |
|    total_timesteps      | 10076160     |
| train/                  |              |
|    approx_kl            | 0.0011734872 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -302         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.9        |
|    n_updates            | 24590        |
|    policy_gradient_loss | -0.00728     |
|    std                  | 1.8          |
|    value_loss           | 2.02         |
------------------------------------------
Iteration: 2461 | Episodes: 99900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2463 | Episodes: 100000 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2466 | Episodes: 100100 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2468 | Episodes: 100200 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2470         |
|    time_elapsed         | 47907        |
|    total_timesteps      | 10117120     |
| train/                  |              |
|    approx_kl            | 5.409344e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -303         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14          |
|    n_updates            | 24690        |
|    policy_gradient_loss | -0.000161    |
|    std                  | 1.81         |
|    value_loss           | 0.706        |
------------------------------------------
Iteration: 2470 | Episodes: 100300 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2473 | Episodes: 100400 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2475 | Episodes: 100500 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2478 | Episodes: 100600 | Median Reward: 47.80 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.6        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2480         |
|    time_elapsed         | 48102        |
|    total_timesteps      | 10158080     |
| train/                  |              |
|    approx_kl            | 0.0006599594 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -303         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.9        |
|    n_updates            | 24790        |
|    policy_gradient_loss | -0.000454    |
|    std                  | 1.81         |
|    value_loss           | 0.551        |
------------------------------------------
Iteration: 2480 | Episodes: 100700 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2483 | Episodes: 100800 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2485 | Episodes: 100900 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2488 | Episodes: 101000 | Median Reward: 47.94 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2490         |
|    time_elapsed         | 48296        |
|    total_timesteps      | 10199040     |
| train/                  |              |
|    approx_kl            | 5.348424e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -303         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -15          |
|    n_updates            | 24890        |
|    policy_gradient_loss | -3.81e-05    |
|    std                  | 1.82         |
|    value_loss           | 0.534        |
------------------------------------------
Iteration: 2490 | Episodes: 101100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2493 | Episodes: 101200 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2495 | Episodes: 101300 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2498 | Episodes: 101400 | Median Reward: 47.90 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.3         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2500          |
|    time_elapsed         | 48491         |
|    total_timesteps      | 10240000      |
| train/                  |               |
|    approx_kl            | 0.00039403245 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -303          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -15           |
|    n_updates            | 24990         |
|    policy_gradient_loss | -0.000314     |
|    std                  | 1.82          |
|    value_loss           | 0.609         |
-------------------------------------------
Iteration: 2500 | Episodes: 101500 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2503 | Episodes: 101600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2505 | Episodes: 101700 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2507 | Episodes: 101800 | Median Reward: 47.88 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.4       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 2510        |
|    time_elapsed         | 48687       |
|    total_timesteps      | 10280960    |
| train/                  |             |
|    approx_kl            | 6.45852e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -304        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -13.3       |
|    n_updates            | 25090       |
|    policy_gradient_loss | 5.79e-05    |
|    std                  | 1.83        |
|    value_loss           | 0.964       |
-----------------------------------------
Iteration: 2510 | Episodes: 101900 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2512 | Episodes: 102000 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2515 | Episodes: 102100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2517 | Episodes: 102200 | Median Reward: 47.87 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2520         |
|    time_elapsed         | 48880        |
|    total_timesteps      | 10321920     |
| train/                  |              |
|    approx_kl            | 0.0002073363 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -304         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.8        |
|    n_updates            | 25190        |
|    policy_gradient_loss | -0.000353    |
|    std                  | 1.84         |
|    value_loss           | 0.584        |
------------------------------------------
Iteration: 2520 | Episodes: 102300 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2522 | Episodes: 102400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2525 | Episodes: 102500 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2527 | Episodes: 102600 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2530          |
|    time_elapsed         | 49074         |
|    total_timesteps      | 10362880      |
| train/                  |               |
|    approx_kl            | 4.9452647e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -304          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.1         |
|    n_updates            | 25290         |
|    policy_gradient_loss | -0.00013      |
|    std                  | 1.84          |
|    value_loss           | 0.432         |
-------------------------------------------
Iteration: 2530 | Episodes: 102700 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2532 | Episodes: 102800 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2535 | Episodes: 102900 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2537 | Episodes: 103000 | Median Reward: 47.88 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.7       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 2540        |
|    time_elapsed         | 49268       |
|    total_timesteps      | 10403840    |
| train/                  |             |
|    approx_kl            | 0.014415042 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -304        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -15.2       |
|    n_updates            | 25390       |
|    policy_gradient_loss | -0.0368     |
|    std                  | 1.85        |
|    value_loss           | 0.516       |
-----------------------------------------
Iteration: 2540 | Episodes: 103100 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2542 | Episodes: 103200 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2544 | Episodes: 103300 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2547 | Episodes: 103400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2549 | Episodes: 103500 | Median Reward: 47.93 | Max Reward: 49.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -52.6      |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 2550       |
|    time_elapsed         | 49463      |
|    total_timesteps      | 10444800   |
| train/                  |            |
|    approx_kl            | 0.03314332 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -305       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -15        |
|    n_updates            | 25490      |
|    policy_gradient_loss | -0.0452    |
|    std                  | 1.86       |
|    value_loss           | 0.379      |
----------------------------------------
Iteration: 2552 | Episodes: 103600 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 2554 | Episodes: 103700 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2557 | Episodes: 103800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2559 | Episodes: 103900 | Median Reward: 47.88 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.9        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2560         |
|    time_elapsed         | 49658        |
|    total_timesteps      | 10485760     |
| train/                  |              |
|    approx_kl            | 0.0010844255 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -305         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.7        |
|    n_updates            | 25590        |
|    policy_gradient_loss | -0.00311     |
|    std                  | 1.87         |
|    value_loss           | 0.825        |
------------------------------------------
Iteration: 2562 | Episodes: 104000 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2564 | Episodes: 104100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2567 | Episodes: 104200 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2569 | Episodes: 104300 | Median Reward: 47.94 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2570         |
|    time_elapsed         | 49852        |
|    total_timesteps      | 10526720     |
| train/                  |              |
|    approx_kl            | 2.566955e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -306         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.2        |
|    n_updates            | 25690        |
|    policy_gradient_loss | 2.77e-05     |
|    std                  | 1.87         |
|    value_loss           | 0.461        |
------------------------------------------
Iteration: 2572 | Episodes: 104400 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2574 | Episodes: 104500 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2577 | Episodes: 104600 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2579 | Episodes: 104700 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2580          |
|    time_elapsed         | 50044         |
|    total_timesteps      | 10567680      |
| train/                  |               |
|    approx_kl            | 5.5338518e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -306          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.7         |
|    n_updates            | 25790         |
|    policy_gradient_loss | -6.86e-05     |
|    std                  | 1.88          |
|    value_loss           | 2.2           |
-------------------------------------------
Iteration: 2581 | Episodes: 104800 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2584 | Episodes: 104900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2586 | Episodes: 105000 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2589 | Episodes: 105100 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.3         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2590          |
|    time_elapsed         | 50239         |
|    total_timesteps      | 10608640      |
| train/                  |               |
|    approx_kl            | 0.00018132255 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -306          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.4         |
|    n_updates            | 25890         |
|    policy_gradient_loss | -0.000109     |
|    std                  | 1.88          |
|    value_loss           | 1.72          |
-------------------------------------------
Iteration: 2591 | Episodes: 105200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2594 | Episodes: 105300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2596 | Episodes: 105400 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2599 | Episodes: 105500 | Median Reward: 47.94 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2600          |
|    time_elapsed         | 50432         |
|    total_timesteps      | 10649600      |
| train/                  |               |
|    approx_kl            | 2.7926726e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -306          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.5         |
|    n_updates            | 25990         |
|    policy_gradient_loss | -4.8e-06      |
|    std                  | 1.88          |
|    value_loss           | 1.78          |
-------------------------------------------
Iteration: 2601 | Episodes: 105600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2604 | Episodes: 105700 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2606 | Episodes: 105800 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2609 | Episodes: 105900 | Median Reward: 47.88 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2610          |
|    time_elapsed         | 50627         |
|    total_timesteps      | 10690560      |
| train/                  |               |
|    approx_kl            | 4.3256005e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -306          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.4         |
|    n_updates            | 26090         |
|    policy_gradient_loss | -7.05e-05     |
|    std                  | 1.88          |
|    value_loss           | 1.74          |
-------------------------------------------
Iteration: 2611 | Episodes: 106000 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2613 | Episodes: 106100 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2616 | Episodes: 106200 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2618 | Episodes: 106300 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53          |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2620         |
|    time_elapsed         | 50820        |
|    total_timesteps      | 10731520     |
| train/                  |              |
|    approx_kl            | 0.0026029125 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -306         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.4        |
|    n_updates            | 26190        |
|    policy_gradient_loss | -0.0102      |
|    std                  | 1.89         |
|    value_loss           | 1.8          |
------------------------------------------
Iteration: 2621 | Episodes: 106400 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2623 | Episodes: 106500 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2626 | Episodes: 106600 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2628 | Episodes: 106700 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.1         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2630          |
|    time_elapsed         | 51007         |
|    total_timesteps      | 10772480      |
| train/                  |               |
|    approx_kl            | 9.2608694e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -307          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.4         |
|    n_updates            | 26290         |
|    policy_gradient_loss | 5.14e-05      |
|    std                  | 1.89          |
|    value_loss           | 1.77          |
-------------------------------------------
Iteration: 2631 | Episodes: 106800 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2633 | Episodes: 106900 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2636 | Episodes: 107000 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2638 | Episodes: 107100 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2640          |
|    time_elapsed         | 51203         |
|    total_timesteps      | 10813440      |
| train/                  |               |
|    approx_kl            | 1.9747444e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -307          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.2         |
|    n_updates            | 26390         |
|    policy_gradient_loss | -0.000117     |
|    std                  | 1.89          |
|    value_loss           | 1.72          |
-------------------------------------------
Iteration: 2641 | Episodes: 107200 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2643 | Episodes: 107300 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2645 | Episodes: 107400 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2648 | Episodes: 107500 | Median Reward: 47.93 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.3       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 2650        |
|    time_elapsed         | 51399       |
|    total_timesteps      | 10854400    |
| train/                  |             |
|    approx_kl            | 0.004422377 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -307        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -15.1       |
|    n_updates            | 26490       |
|    policy_gradient_loss | -0.00917    |
|    std                  | 1.89        |
|    value_loss           | 0.779       |
-----------------------------------------
Iteration: 2650 | Episodes: 107600 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2653 | Episodes: 107700 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2655 | Episodes: 107800 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2658 | Episodes: 107900 | Median Reward: 47.89 | Max Reward: 49.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -52.6      |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 2660       |
|    time_elapsed         | 51591      |
|    total_timesteps      | 10895360   |
| train/                  |            |
|    approx_kl            | 0.02282954 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -307       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -15.4      |
|    n_updates            | 26590      |
|    policy_gradient_loss | -0.0204    |
|    std                  | 1.9        |
|    value_loss           | 0.108      |
----------------------------------------
Iteration: 2660 | Episodes: 108000 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2663 | Episodes: 108100 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2665 | Episodes: 108200 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2668 | Episodes: 108300 | Median Reward: 47.93 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.8         |
|    ep_rew_mean          | -50.9        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2670         |
|    time_elapsed         | 51786        |
|    total_timesteps      | 10936320     |
| train/                  |              |
|    approx_kl            | 0.0030492395 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -308         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.3        |
|    n_updates            | 26690        |
|    policy_gradient_loss | -0.000852    |
|    std                  | 1.91         |
|    value_loss           | 0.221        |
------------------------------------------
Iteration: 2670 | Episodes: 108400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2673 | Episodes: 108500 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2675 | Episodes: 108600 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2678 | Episodes: 108700 | Median Reward: 47.95 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2680          |
|    time_elapsed         | 51979         |
|    total_timesteps      | 10977280      |
| train/                  |               |
|    approx_kl            | 3.0925352e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -308          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.3         |
|    n_updates            | 26790         |
|    policy_gradient_loss | -2.23e-05     |
|    std                  | 1.92          |
|    value_loss           | 0.364         |
-------------------------------------------
Iteration: 2680 | Episodes: 108800 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2682 | Episodes: 108900 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2685 | Episodes: 109000 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2687 | Episodes: 109100 | Median Reward: 47.90 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -51.4         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2690          |
|    time_elapsed         | 52173         |
|    total_timesteps      | 11018240      |
| train/                  |               |
|    approx_kl            | 0.00019519767 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -308          |
|    explained_variance   | 0.991         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.1         |
|    n_updates            | 26890         |
|    policy_gradient_loss | -0.000176     |
|    std                  | 1.93          |
|    value_loss           | 6.71          |
-------------------------------------------
Iteration: 2690 | Episodes: 109200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2692 | Episodes: 109300 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2695 | Episodes: 109400 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2697 | Episodes: 109500 | Median Reward: 47.89 | Max Reward: 49.24
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 100            |
|    ep_rew_mean          | -51.4          |
| time/                   |                |
|    fps                  | 211            |
|    iterations           | 2700           |
|    time_elapsed         | 52364          |
|    total_timesteps      | 11059200       |
| train/                  |                |
|    approx_kl            | 1.18596945e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -309           |
|    explained_variance   | 1              |
|    learning_rate        | 0.0005         |
|    loss                 | -15.4          |
|    n_updates            | 26990          |
|    policy_gradient_loss | -3.86e-05      |
|    std                  | 1.93           |
|    value_loss           | 1.61           |
--------------------------------------------
Iteration: 2700 | Episodes: 109600 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2702 | Episodes: 109700 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2705 | Episodes: 109800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2707 | Episodes: 109900 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2710          |
|    time_elapsed         | 52558         |
|    total_timesteps      | 11100160      |
| train/                  |               |
|    approx_kl            | 0.00015245848 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -309          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.4         |
|    n_updates            | 27090         |
|    policy_gradient_loss | -0.000357     |
|    std                  | 1.94          |
|    value_loss           | 0.317         |
-------------------------------------------
Iteration: 2710 | Episodes: 110000 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2712 | Episodes: 110100 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2714 | Episodes: 110200 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2717 | Episodes: 110300 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2719 | Episodes: 110400 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2720         |
|    time_elapsed         | 52750        |
|    total_timesteps      | 11141120     |
| train/                  |              |
|    approx_kl            | 6.493078e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -309         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.3        |
|    n_updates            | 27190        |
|    policy_gradient_loss | -8.64e-05    |
|    std                  | 1.95         |
|    value_loss           | 0.5          |
------------------------------------------
Iteration: 2722 | Episodes: 110500 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2724 | Episodes: 110600 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2727 | Episodes: 110700 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2729 | Episodes: 110800 | Median Reward: 47.88 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.5         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2730          |
|    time_elapsed         | 52946         |
|    total_timesteps      | 11182080      |
| train/                  |               |
|    approx_kl            | 0.00016179334 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -310          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.4         |
|    n_updates            | 27290         |
|    policy_gradient_loss | -0.000165     |
|    std                  | 1.96          |
|    value_loss           | 0.253         |
-------------------------------------------
Iteration: 2732 | Episodes: 110900 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2734 | Episodes: 111000 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2737 | Episodes: 111100 | Median Reward: 47.76 | Max Reward: 49.24
Iteration: 2739 | Episodes: 111200 | Median Reward: 47.94 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2740         |
|    time_elapsed         | 53137        |
|    total_timesteps      | 11223040     |
| train/                  |              |
|    approx_kl            | 0.0005146386 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -310         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.4        |
|    n_updates            | 27390        |
|    policy_gradient_loss | -9.75e-06    |
|    std                  | 1.97         |
|    value_loss           | 0.159        |
------------------------------------------
Iteration: 2742 | Episodes: 111300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2744 | Episodes: 111400 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2746 | Episodes: 111500 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2749 | Episodes: 111600 | Median Reward: 47.91 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.5       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 2750        |
|    time_elapsed         | 53332       |
|    total_timesteps      | 11264000    |
| train/                  |             |
|    approx_kl            | 0.004017406 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -310        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -15.5       |
|    n_updates            | 27490       |
|    policy_gradient_loss | -0.0067     |
|    std                  | 1.98        |
|    value_loss           | 0.158       |
-----------------------------------------
Iteration: 2751 | Episodes: 111700 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2754 | Episodes: 111800 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2756 | Episodes: 111900 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2759 | Episodes: 112000 | Median Reward: 47.90 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2760          |
|    time_elapsed         | 53524         |
|    total_timesteps      | 11304960      |
| train/                  |               |
|    approx_kl            | 0.00012150247 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -310          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -14.5         |
|    n_updates            | 27590         |
|    policy_gradient_loss | 4.76e-05      |
|    std                  | 1.98          |
|    value_loss           | 1.15          |
-------------------------------------------
Iteration: 2761 | Episodes: 112100 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2764 | Episodes: 112200 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2766 | Episodes: 112300 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2769 | Episodes: 112400 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2770          |
|    time_elapsed         | 53717         |
|    total_timesteps      | 11345920      |
| train/                  |               |
|    approx_kl            | 0.00076200755 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -311          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.5         |
|    n_updates            | 27690         |
|    policy_gradient_loss | -0.000847     |
|    std                  | 1.99          |
|    value_loss           | 0.0973        |
-------------------------------------------
Iteration: 2771 | Episodes: 112500 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2774 | Episodes: 112600 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2776 | Episodes: 112700 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2778 | Episodes: 112800 | Median Reward: 47.94 | Max Reward: 49.24
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -52.2          |
| time/                   |                |
|    fps                  | 211            |
|    iterations           | 2780           |
|    time_elapsed         | 53910          |
|    total_timesteps      | 11386880       |
| train/                  |                |
|    approx_kl            | 1.36096205e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -311           |
|    explained_variance   | 0.999          |
|    learning_rate        | 0.0005         |
|    loss                 | -15.5          |
|    n_updates            | 27790          |
|    policy_gradient_loss | -4.54e-06      |
|    std                  | 2              |
|    value_loss           | 0.28           |
--------------------------------------------
Iteration: 2781 | Episodes: 112900 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2783 | Episodes: 113000 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2786 | Episodes: 113100 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2788 | Episodes: 113200 | Median Reward: 47.89 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2790          |
|    time_elapsed         | 54106         |
|    total_timesteps      | 11427840      |
| train/                  |               |
|    approx_kl            | 2.1219457e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -311          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.2         |
|    n_updates            | 27890         |
|    policy_gradient_loss | 7.69e-06      |
|    std                  | 2             |
|    value_loss           | 0.688         |
-------------------------------------------
Iteration: 2791 | Episodes: 113300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2793 | Episodes: 113400 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2796 | Episodes: 113500 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2798 | Episodes: 113600 | Median Reward: 47.89 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.3         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2800          |
|    time_elapsed         | 54299         |
|    total_timesteps      | 11468800      |
| train/                  |               |
|    approx_kl            | 5.7399884e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -311          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.5         |
|    n_updates            | 27990         |
|    policy_gradient_loss | -3.04e-05     |
|    std                  | 2.01          |
|    value_loss           | 0.338         |
-------------------------------------------
Iteration: 2801 | Episodes: 113700 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2803 | Episodes: 113800 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2806 | Episodes: 113900 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2808 | Episodes: 114000 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2810         |
|    time_elapsed         | 54493        |
|    total_timesteps      | 11509760     |
| train/                  |              |
|    approx_kl            | 0.0036276167 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -311         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.5        |
|    n_updates            | 28090        |
|    policy_gradient_loss | -0.00553     |
|    std                  | 2.02         |
|    value_loss           | 0.084        |
------------------------------------------
Iteration: 2811 | Episodes: 114100 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2813 | Episodes: 114200 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2815 | Episodes: 114300 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2818 | Episodes: 114400 | Median Reward: 47.86 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.5       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 2820        |
|    time_elapsed         | 54686       |
|    total_timesteps      | 11550720    |
| train/                  |             |
|    approx_kl            | 5.93031e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -312        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -14.9       |
|    n_updates            | 28190       |
|    policy_gradient_loss | 4.55e-05    |
|    std                  | 2.03        |
|    value_loss           | 0.839       |
-----------------------------------------
Iteration: 2820 | Episodes: 114500 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2823 | Episodes: 114600 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2825 | Episodes: 114700 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2828 | Episodes: 114800 | Median Reward: 47.89 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.4        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2830         |
|    time_elapsed         | 54881        |
|    total_timesteps      | 11591680     |
| train/                  |              |
|    approx_kl            | 0.0020732798 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -312         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -15.1        |
|    n_updates            | 28290        |
|    policy_gradient_loss | -0.00652     |
|    std                  | 2.05         |
|    value_loss           | 1.07         |
------------------------------------------
Iteration: 2830 | Episodes: 114900 | Median Reward: 47.67 | Max Reward: 49.24
Iteration: 2833 | Episodes: 115000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2835 | Episodes: 115100 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2838 | Episodes: 115200 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.6        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2840         |
|    time_elapsed         | 55070        |
|    total_timesteps      | 11632640     |
| train/                  |              |
|    approx_kl            | 0.0006857809 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -312         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.4        |
|    n_updates            | 28390        |
|    policy_gradient_loss | -0.00238     |
|    std                  | 2.06         |
|    value_loss           | 0.913        |
------------------------------------------
Iteration: 2840 | Episodes: 115300 | Median Reward: 47.62 | Max Reward: 49.24
Iteration: 2843 | Episodes: 115400 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2845 | Episodes: 115500 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2847 | Episodes: 115600 | Median Reward: 47.80 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2850         |
|    time_elapsed         | 55259        |
|    total_timesteps      | 11673600     |
| train/                  |              |
|    approx_kl            | 0.0003448507 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -313         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.5        |
|    n_updates            | 28490        |
|    policy_gradient_loss | -0.000487    |
|    std                  | 2.06         |
|    value_loss           | 0.244        |
------------------------------------------
Iteration: 2850 | Episodes: 115700 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2852 | Episodes: 115800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 2855 | Episodes: 115900 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2857 | Episodes: 116000 | Median Reward: 47.87 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2860         |
|    time_elapsed         | 55456        |
|    total_timesteps      | 11714560     |
| train/                  |              |
|    approx_kl            | 4.659203e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -313         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.6        |
|    n_updates            | 28590        |
|    policy_gradient_loss | -7.94e-05    |
|    std                  | 2.07         |
|    value_loss           | 0.182        |
------------------------------------------
Iteration: 2860 | Episodes: 116100 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2862 | Episodes: 116200 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 2865 | Episodes: 116300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2867 | Episodes: 116400 | Median Reward: 47.87 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.3         |
|    ep_rew_mean          | -50.4        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2870         |
|    time_elapsed         | 55650        |
|    total_timesteps      | 11755520     |
| train/                  |              |
|    approx_kl            | 0.0005651651 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -313         |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0005       |
|    loss                 | -11.9        |
|    n_updates            | 28690        |
|    policy_gradient_loss | -0.00064     |
|    std                  | 2.08         |
|    value_loss           | 7.52         |
------------------------------------------
Iteration: 2870 | Episodes: 116500 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2872 | Episodes: 116600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2875 | Episodes: 116700 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2877 | Episodes: 116800 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2879 | Episodes: 116900 | Median Reward: 47.87 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.2        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2880         |
|    time_elapsed         | 55844        |
|    total_timesteps      | 11796480     |
| train/                  |              |
|    approx_kl            | 0.0002710382 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -313         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.5        |
|    n_updates            | 28790        |
|    policy_gradient_loss | 2.11e-05     |
|    std                  | 2.09         |
|    value_loss           | 0.338        |
------------------------------------------
Iteration: 2882 | Episodes: 117000 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2884 | Episodes: 117100 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2887 | Episodes: 117200 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2889 | Episodes: 117300 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.6        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2890         |
|    time_elapsed         | 56037        |
|    total_timesteps      | 11837440     |
| train/                  |              |
|    approx_kl            | 0.0014653981 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -313         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -15.6        |
|    n_updates            | 28890        |
|    policy_gradient_loss | -0.00428     |
|    std                  | 2.09         |
|    value_loss           | 0.697        |
------------------------------------------
Iteration: 2892 | Episodes: 117400 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 2894 | Episodes: 117500 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2897 | Episodes: 117600 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2899 | Episodes: 117700 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.7         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2900          |
|    time_elapsed         | 56231         |
|    total_timesteps      | 11878400      |
| train/                  |               |
|    approx_kl            | 1.0676085e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -313          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -15.3         |
|    n_updates            | 28990         |
|    policy_gradient_loss | -2.23e-05     |
|    std                  | 2.1           |
|    value_loss           | 0.664         |
-------------------------------------------
Iteration: 2902 | Episodes: 117800 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2904 | Episodes: 117900 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2907 | Episodes: 118000 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 2909 | Episodes: 118100 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2910         |
|    time_elapsed         | 56423        |
|    total_timesteps      | 11919360     |
| train/                  |              |
|    approx_kl            | 0.0023211595 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -313         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.4        |
|    n_updates            | 29090        |
|    policy_gradient_loss | -0.00185     |
|    std                  | 2.1          |
|    value_loss           | 0.432        |
------------------------------------------
Iteration: 2911 | Episodes: 118200 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 2914 | Episodes: 118300 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 2916 | Episodes: 118400 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 2919 | Episodes: 118500 | Median Reward: 47.94 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.5       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 2920        |
|    time_elapsed         | 56618       |
|    total_timesteps      | 11960320    |
| train/                  |             |
|    approx_kl            | 0.008407218 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -313        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -15.5       |
|    n_updates            | 29190       |
|    policy_gradient_loss | -0.0112     |
|    std                  | 2.11        |
|    value_loss           | 0.337       |
-----------------------------------------
Iteration: 2921 | Episodes: 118600 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2924 | Episodes: 118700 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2926 | Episodes: 118800 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2929 | Episodes: 118900 | Median Reward: 47.93 | Max Reward: 49.24
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -52.6          |
| time/                   |                |
|    fps                  | 211            |
|    iterations           | 2930           |
|    time_elapsed         | 56814          |
|    total_timesteps      | 12001280       |
| train/                  |                |
|    approx_kl            | 0.000100314515 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -314           |
|    explained_variance   | 1              |
|    learning_rate        | 0.0005         |
|    loss                 | -15.6          |
|    n_updates            | 29290          |
|    policy_gradient_loss | 4.21e-05       |
|    std                  | 2.12           |
|    value_loss           | 0.116          |
--------------------------------------------
Iteration: 2931 | Episodes: 119000 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 2934 | Episodes: 119100 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 2936 | Episodes: 119200 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2939 | Episodes: 119300 | Median Reward: 47.90 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2940         |
|    time_elapsed         | 57013        |
|    total_timesteps      | 12042240     |
| train/                  |              |
|    approx_kl            | 0.0017624708 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -314         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.8        |
|    n_updates            | 29390        |
|    policy_gradient_loss | -0.00235     |
|    std                  | 2.13         |
|    value_loss           | 0.481        |
------------------------------------------
Iteration: 2941 | Episodes: 119400 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2944 | Episodes: 119500 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2946 | Episodes: 119600 | Median Reward: 47.74 | Max Reward: 49.24
Iteration: 2948 | Episodes: 119700 | Median Reward: 47.87 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 99.2          |
|    ep_rew_mean          | -50.6         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2950          |
|    time_elapsed         | 57205         |
|    total_timesteps      | 12083200      |
| train/                  |               |
|    approx_kl            | 0.00015869632 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -314          |
|    explained_variance   | 0.991         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.3         |
|    n_updates            | 29490         |
|    policy_gradient_loss | -0.000258     |
|    std                  | 2.14          |
|    value_loss           | 7.07          |
-------------------------------------------
Iteration: 2951 | Episodes: 119800 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 2953 | Episodes: 119900 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 2956 | Episodes: 120000 | Median Reward: 47.71 | Max Reward: 49.24
Iteration: 2958 | Episodes: 120100 | Median Reward: 47.86 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2960         |
|    time_elapsed         | 57400        |
|    total_timesteps      | 12124160     |
| train/                  |              |
|    approx_kl            | 9.389805e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -315         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.7        |
|    n_updates            | 29590        |
|    policy_gradient_loss | -0.000393    |
|    std                  | 2.15         |
|    value_loss           | 0.164        |
------------------------------------------
Iteration: 2961 | Episodes: 120200 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 2963 | Episodes: 120300 | Median Reward: 47.77 | Max Reward: 49.24
Iteration: 2966 | Episodes: 120400 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2968 | Episodes: 120500 | Median Reward: 47.91 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2970         |
|    time_elapsed         | 57594        |
|    total_timesteps      | 12165120     |
| train/                  |              |
|    approx_kl            | 0.0056078034 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -316         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.8        |
|    n_updates            | 29690        |
|    policy_gradient_loss | -0.00919     |
|    std                  | 2.16         |
|    value_loss           | 0.174        |
------------------------------------------
Iteration: 2971 | Episodes: 120600 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2973 | Episodes: 120700 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 2976 | Episodes: 120800 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 2978 | Episodes: 120900 | Median Reward: 47.88 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2980         |
|    time_elapsed         | 57789        |
|    total_timesteps      | 12206080     |
| train/                  |              |
|    approx_kl            | 4.130024e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -316         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.4        |
|    n_updates            | 29790        |
|    policy_gradient_loss | -4.82e-05    |
|    std                  | 2.17         |
|    value_loss           | 0.717        |
------------------------------------------
Iteration: 2980 | Episodes: 121000 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2983 | Episodes: 121100 | Median Reward: 47.77 | Max Reward: 49.24
Iteration: 2985 | Episodes: 121200 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 2988 | Episodes: 121300 | Median Reward: 47.86 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.7         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 2990          |
|    time_elapsed         | 57980         |
|    total_timesteps      | 12247040      |
| train/                  |               |
|    approx_kl            | 1.7308499e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -316          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.7         |
|    n_updates            | 29890         |
|    policy_gradient_loss | -3.88e-05     |
|    std                  | 2.18          |
|    value_loss           | 0.262         |
-------------------------------------------
Iteration: 2990 | Episodes: 121400 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2993 | Episodes: 121500 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 2995 | Episodes: 121600 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 2998 | Episodes: 121700 | Median Reward: 47.86 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3000          |
|    time_elapsed         | 58175         |
|    total_timesteps      | 12288000      |
| train/                  |               |
|    approx_kl            | 9.3857176e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -316          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -15.7         |
|    n_updates            | 29990         |
|    policy_gradient_loss | 8.36e-05      |
|    std                  | 2.19          |
|    value_loss           | 1.17          |
-------------------------------------------
Iteration: 3000 | Episodes: 121800 | Median Reward: 47.84 | Max Reward: 49.24
Iteration: 3003 | Episodes: 121900 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3005 | Episodes: 122000 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 3008 | Episodes: 122100 | Median Reward: 47.87 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.4        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3010         |
|    time_elapsed         | 58365        |
|    total_timesteps      | 12328960     |
| train/                  |              |
|    approx_kl            | 0.0016849678 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -316         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.5        |
|    n_updates            | 30090        |
|    policy_gradient_loss | -0.00124     |
|    std                  | 2.19         |
|    value_loss           | 0.167        |
------------------------------------------
Iteration: 3010 | Episodes: 122200 | Median Reward: 47.94 | Max Reward: 49.24
Iteration: 3012 | Episodes: 122300 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 3015 | Episodes: 122400 | Median Reward: 47.79 | Max Reward: 49.24
Iteration: 3017 | Episodes: 122500 | Median Reward: 47.88 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.5         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3020          |
|    time_elapsed         | 58560         |
|    total_timesteps      | 12369920      |
| train/                  |               |
|    approx_kl            | 0.00040862022 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -316          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.6         |
|    n_updates            | 30190         |
|    policy_gradient_loss | -0.000837     |
|    std                  | 2.2           |
|    value_loss           | 0.579         |
-------------------------------------------
Iteration: 3020 | Episodes: 122600 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3022 | Episodes: 122700 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3025 | Episodes: 122800 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3027 | Episodes: 122900 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.8         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3030          |
|    time_elapsed         | 58752         |
|    total_timesteps      | 12410880      |
| train/                  |               |
|    approx_kl            | 0.00014969415 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -317          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -15.7         |
|    n_updates            | 30290         |
|    policy_gradient_loss | -8.87e-05     |
|    std                  | 2.21          |
|    value_loss           | 0.585         |
-------------------------------------------
Iteration: 3030 | Episodes: 123000 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 3032 | Episodes: 123100 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 3035 | Episodes: 123200 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 3037 | Episodes: 123300 | Median Reward: 47.88 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3040          |
|    time_elapsed         | 58947         |
|    total_timesteps      | 12451840      |
| train/                  |               |
|    approx_kl            | 0.00010124358 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -317          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.1         |
|    n_updates            | 30390         |
|    policy_gradient_loss | 4.48e-05      |
|    std                  | 2.22          |
|    value_loss           | 0.777         |
-------------------------------------------
Iteration: 3040 | Episodes: 123400 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3042 | Episodes: 123500 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 3044 | Episodes: 123600 | Median Reward: 47.57 | Max Reward: 49.24
Iteration: 3047 | Episodes: 123700 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3049 | Episodes: 123800 | Median Reward: 47.88 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3050         |
|    time_elapsed         | 59138        |
|    total_timesteps      | 12492800     |
| train/                  |              |
|    approx_kl            | 0.0001959517 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -317         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15          |
|    n_updates            | 30490        |
|    policy_gradient_loss | -0.000237    |
|    std                  | 2.23         |
|    value_loss           | 0.702        |
------------------------------------------
Iteration: 3052 | Episodes: 123900 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3054 | Episodes: 124000 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 3057 | Episodes: 124100 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 3059 | Episodes: 124200 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.3         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3060          |
|    time_elapsed         | 59326         |
|    total_timesteps      | 12533760      |
| train/                  |               |
|    approx_kl            | 0.00029345247 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -317          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.8         |
|    n_updates            | 30590         |
|    policy_gradient_loss | -3.44e-05     |
|    std                  | 2.24          |
|    value_loss           | 0.273         |
-------------------------------------------
Iteration: 3062 | Episodes: 124300 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 3064 | Episodes: 124400 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 3067 | Episodes: 124500 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 3069 | Episodes: 124600 | Median Reward: 47.92 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3070          |
|    time_elapsed         | 59522         |
|    total_timesteps      | 12574720      |
| train/                  |               |
|    approx_kl            | 0.00032115058 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -318          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.9         |
|    n_updates            | 30690         |
|    policy_gradient_loss | -0.000486     |
|    std                  | 2.25          |
|    value_loss           | 4.29          |
-------------------------------------------
Iteration: 3072 | Episodes: 124700 | Median Reward: 47.80 | Max Reward: 49.24
Iteration: 3074 | Episodes: 124800 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 3076 | Episodes: 124900 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 3079 | Episodes: 125000 | Median Reward: 47.80 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.7         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3080          |
|    time_elapsed         | 59713         |
|    total_timesteps      | 12615680      |
| train/                  |               |
|    approx_kl            | 4.5671288e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -318          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.8         |
|    n_updates            | 30790         |
|    policy_gradient_loss | -0.00012      |
|    std                  | 2.25          |
|    value_loss           | 0.617         |
-------------------------------------------
Iteration: 3081 | Episodes: 125100 | Median Reward: 47.60 | Max Reward: 49.24
Iteration: 3084 | Episodes: 125200 | Median Reward: 47.74 | Max Reward: 49.24
Iteration: 3086 | Episodes: 125300 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 3089 | Episodes: 125400 | Median Reward: 47.88 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3090          |
|    time_elapsed         | 59907         |
|    total_timesteps      | 12656640      |
| train/                  |               |
|    approx_kl            | 4.7160676e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -318          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.9         |
|    n_updates            | 30890         |
|    policy_gradient_loss | -0.000239     |
|    std                  | 2.26          |
|    value_loss           | 0.282         |
-------------------------------------------
Iteration: 3091 | Episodes: 125500 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3094 | Episodes: 125600 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3096 | Episodes: 125700 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 3099 | Episodes: 125800 | Median Reward: 47.92 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.1        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3100         |
|    time_elapsed         | 60102        |
|    total_timesteps      | 12697600     |
| train/                  |              |
|    approx_kl            | 4.740825e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -318         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.8        |
|    n_updates            | 30990        |
|    policy_gradient_loss | 7.22e-06     |
|    std                  | 2.27         |
|    value_loss           | 0.354        |
------------------------------------------
Iteration: 3101 | Episodes: 125900 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 3104 | Episodes: 126000 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3106 | Episodes: 126100 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 3108 | Episodes: 126200 | Median Reward: 47.91 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.7       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3110        |
|    time_elapsed         | 60292       |
|    total_timesteps      | 12738560    |
| train/                  |             |
|    approx_kl            | 0.001983895 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -318        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -15.8       |
|    n_updates            | 31090       |
|    policy_gradient_loss | -0.00487    |
|    std                  | 2.28        |
|    value_loss           | 0.102       |
-----------------------------------------
Iteration: 3111 | Episodes: 126300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 3113 | Episodes: 126400 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 3116 | Episodes: 126500 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 3118 | Episodes: 126600 | Median Reward: 47.85 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.3        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3120         |
|    time_elapsed         | 60487        |
|    total_timesteps      | 12779520     |
| train/                  |              |
|    approx_kl            | 6.822028e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -319         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -15.9        |
|    n_updates            | 31190        |
|    policy_gradient_loss | -0.000173    |
|    std                  | 2.29         |
|    value_loss           | 0.323        |
------------------------------------------
Iteration: 3121 | Episodes: 126700 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 3123 | Episodes: 126800 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 3126 | Episodes: 126900 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 3128 | Episodes: 127000 | Median Reward: 47.88 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.5       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3130        |
|    time_elapsed         | 60685       |
|    total_timesteps      | 12820480    |
| train/                  |             |
|    approx_kl            | 0.000607671 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -319        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -14.9       |
|    n_updates            | 31290       |
|    policy_gradient_loss | -0.00143    |
|    std                  | 2.3         |
|    value_loss           | 1.08        |
-----------------------------------------
Iteration: 3131 | Episodes: 127100 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3133 | Episodes: 127200 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3136 | Episodes: 127300 | Median Reward: 47.95 | Max Reward: 49.24
Iteration: 3138 | Episodes: 127400 | Median Reward: 47.77 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.3         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3140          |
|    time_elapsed         | 60881         |
|    total_timesteps      | 12861440      |
| train/                  |               |
|    approx_kl            | 5.0165487e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -319          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -15.7         |
|    n_updates            | 31390         |
|    policy_gradient_loss | -4.94e-05     |
|    std                  | 2.31          |
|    value_loss           | 0.4           |
-------------------------------------------
Iteration: 3140 | Episodes: 127500 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 3143 | Episodes: 127600 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 3145 | Episodes: 127700 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 3148 | Episodes: 127800 | Median Reward: 47.79 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3150         |
|    time_elapsed         | 61076        |
|    total_timesteps      | 12902400     |
| train/                  |              |
|    approx_kl            | 0.0006094916 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -320         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.4        |
|    n_updates            | 31490        |
|    policy_gradient_loss | 0.000359     |
|    std                  | 2.31         |
|    value_loss           | 0.348        |
------------------------------------------
Iteration: 3150 | Episodes: 127900 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 3153 | Episodes: 128000 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3155 | Episodes: 128100 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 3158 | Episodes: 128200 | Median Reward: 47.88 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53          |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3160         |
|    time_elapsed         | 61270        |
|    total_timesteps      | 12943360     |
| train/                  |              |
|    approx_kl            | 0.0007583025 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -320         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.9        |
|    n_updates            | 31590        |
|    policy_gradient_loss | 0.00281      |
|    std                  | 2.32         |
|    value_loss           | 0.821        |
------------------------------------------
Iteration: 3160 | Episodes: 128300 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3163 | Episodes: 128400 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3165 | Episodes: 128500 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3168 | Episodes: 128600 | Median Reward: 47.82 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3170         |
|    time_elapsed         | 61465        |
|    total_timesteps      | 12984320     |
| train/                  |              |
|    approx_kl            | 4.825536e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -320         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.8        |
|    n_updates            | 31690        |
|    policy_gradient_loss | 3.12e-06     |
|    std                  | 2.33         |
|    value_loss           | 0.169        |
------------------------------------------
Iteration: 3170 | Episodes: 128700 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3173 | Episodes: 128800 | Median Reward: 47.84 | Max Reward: 49.24
Iteration: 3175 | Episodes: 128900 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3177 | Episodes: 129000 | Median Reward: 47.94 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.9         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3180          |
|    time_elapsed         | 61658         |
|    total_timesteps      | 13025280      |
| train/                  |               |
|    approx_kl            | 0.00043756183 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -320          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -15.9         |
|    n_updates            | 31790         |
|    policy_gradient_loss | -0.00192      |
|    std                  | 2.34          |
|    value_loss           | 0.691         |
-------------------------------------------
Iteration: 3180 | Episodes: 129100 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3182 | Episodes: 129200 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 3185 | Episodes: 129300 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3187 | Episodes: 129400 | Median Reward: 47.93 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.1         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3190          |
|    time_elapsed         | 61857         |
|    total_timesteps      | 13066240      |
| train/                  |               |
|    approx_kl            | 3.8850776e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -321          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -15.8         |
|    n_updates            | 31890         |
|    policy_gradient_loss | -9.04e-05     |
|    std                  | 2.35          |
|    value_loss           | 0.335         |
-------------------------------------------
Iteration: 3190 | Episodes: 129500 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3192 | Episodes: 129600 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3195 | Episodes: 129700 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 3197 | Episodes: 129800 | Median Reward: 47.74 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3200         |
|    time_elapsed         | 62049        |
|    total_timesteps      | 13107200     |
| train/                  |              |
|    approx_kl            | 0.0057382546 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -321         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.8        |
|    n_updates            | 31990        |
|    policy_gradient_loss | -0.0178      |
|    std                  | 2.37         |
|    value_loss           | 0.476        |
------------------------------------------
Iteration: 3200 | Episodes: 129900 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3202 | Episodes: 130000 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 3205 | Episodes: 130100 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 3207 | Episodes: 130200 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 3209 | Episodes: 130300 | Median Reward: 47.78 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.9         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3210          |
|    time_elapsed         | 62246         |
|    total_timesteps      | 13148160      |
| train/                  |               |
|    approx_kl            | 0.00057838694 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -321          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16           |
|    n_updates            | 32090         |
|    policy_gradient_loss | -0.000111     |
|    std                  | 2.38          |
|    value_loss           | 0.22          |
-------------------------------------------
Iteration: 3212 | Episodes: 130400 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 3214 | Episodes: 130500 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 3217 | Episodes: 130600 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3219 | Episodes: 130700 | Median Reward: 47.87 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.6        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3220         |
|    time_elapsed         | 62438        |
|    total_timesteps      | 13189120     |
| train/                  |              |
|    approx_kl            | 8.222021e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -322         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -14.9        |
|    n_updates            | 32190        |
|    policy_gradient_loss | 6.04e-05     |
|    std                  | 2.39         |
|    value_loss           | 0.806        |
------------------------------------------
Iteration: 3222 | Episodes: 130800 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 3224 | Episodes: 130900 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 3227 | Episodes: 131000 | Median Reward: 47.84 | Max Reward: 49.24
Iteration: 3229 | Episodes: 131100 | Median Reward: 47.83 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53           |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3230          |
|    time_elapsed         | 62633         |
|    total_timesteps      | 13230080      |
| train/                  |               |
|    approx_kl            | 6.6020075e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -322          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.8         |
|    n_updates            | 32290         |
|    policy_gradient_loss | -0.000188     |
|    std                  | 2.4           |
|    value_loss           | 0.628         |
-------------------------------------------
Iteration: 3232 | Episodes: 131200 | Median Reward: 47.74 | Max Reward: 49.24
Iteration: 3234 | Episodes: 131300 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 3237 | Episodes: 131400 | Median Reward: 47.66 | Max Reward: 49.24
Iteration: 3239 | Episodes: 131500 | Median Reward: 47.87 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3240          |
|    time_elapsed         | 62827         |
|    total_timesteps      | 13271040      |
| train/                  |               |
|    approx_kl            | 7.0524664e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -323          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -16.1         |
|    n_updates            | 32390         |
|    policy_gradient_loss | -8.47e-05     |
|    std                  | 2.42          |
|    value_loss           | 0.527         |
-------------------------------------------
Iteration: 3242 | Episodes: 131600 | Median Reward: 47.60 | Max Reward: 49.24
Iteration: 3244 | Episodes: 131700 | Median Reward: 47.74 | Max Reward: 49.24
Iteration: 3246 | Episodes: 131800 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 3249 | Episodes: 131900 | Median Reward: 47.88 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.7       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3250        |
|    time_elapsed         | 63022       |
|    total_timesteps      | 13312000    |
| train/                  |             |
|    approx_kl            | 0.001102926 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -323        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -15.6       |
|    n_updates            | 32490       |
|    policy_gradient_loss | -0.00328    |
|    std                  | 2.42        |
|    value_loss           | 0.555       |
-----------------------------------------
Iteration: 3251 | Episodes: 132000 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 3254 | Episodes: 132100 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 3256 | Episodes: 132200 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 3259 | Episodes: 132300 | Median Reward: 47.77 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.7       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3260        |
|    time_elapsed         | 63215       |
|    total_timesteps      | 13352960    |
| train/                  |             |
|    approx_kl            | 0.010759391 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -323        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -16         |
|    n_updates            | 32590       |
|    policy_gradient_loss | -0.0169     |
|    std                  | 2.43        |
|    value_loss           | 0.24        |
-----------------------------------------
Iteration: 3261 | Episodes: 132400 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3264 | Episodes: 132500 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3266 | Episodes: 132600 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 3269 | Episodes: 132700 | Median Reward: 47.88 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.1         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3270          |
|    time_elapsed         | 63403         |
|    total_timesteps      | 13393920      |
| train/                  |               |
|    approx_kl            | 0.00052307575 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -324          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.5         |
|    n_updates            | 32690         |
|    policy_gradient_loss | -0.00564      |
|    std                  | 2.44          |
|    value_loss           | 1.82          |
-------------------------------------------
Iteration: 3271 | Episodes: 132800 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 3274 | Episodes: 132900 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 3276 | Episodes: 133000 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 3278 | Episodes: 133100 | Median Reward: 47.86 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -51.8        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3280         |
|    time_elapsed         | 63600        |
|    total_timesteps      | 13434880     |
| train/                  |              |
|    approx_kl            | 0.0012120976 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -324         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.7        |
|    n_updates            | 32790        |
|    policy_gradient_loss | -0.0011      |
|    std                  | 2.45         |
|    value_loss           | 2.96         |
------------------------------------------
Iteration: 3281 | Episodes: 133200 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 3283 | Episodes: 133300 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 3286 | Episodes: 133400 | Median Reward: 47.93 | Max Reward: 49.24
Iteration: 3288 | Episodes: 133500 | Median Reward: 47.81 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.2       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3290        |
|    time_elapsed         | 63793       |
|    total_timesteps      | 13475840    |
| train/                  |             |
|    approx_kl            | 0.004367169 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -324        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -16         |
|    n_updates            | 32890       |
|    policy_gradient_loss | -0.0216     |
|    std                  | 2.46        |
|    value_loss           | 0.592       |
-----------------------------------------
Iteration: 3291 | Episodes: 133600 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3293 | Episodes: 133700 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3296 | Episodes: 133800 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3298 | Episodes: 133900 | Median Reward: 47.88 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3300         |
|    time_elapsed         | 63988        |
|    total_timesteps      | 13516800     |
| train/                  |              |
|    approx_kl            | 5.967215e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -324         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.2        |
|    n_updates            | 32990        |
|    policy_gradient_loss | -6.58e-05    |
|    std                  | 2.48         |
|    value_loss           | 0.636        |
------------------------------------------
Iteration: 3301 | Episodes: 134000 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3303 | Episodes: 134100 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 3306 | Episodes: 134200 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 3308 | Episodes: 134300 | Median Reward: 47.88 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.9         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3310          |
|    time_elapsed         | 64182         |
|    total_timesteps      | 13557760      |
| train/                  |               |
|    approx_kl            | 0.00010559734 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -324          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -15.9         |
|    n_updates            | 33090         |
|    policy_gradient_loss | -0.000111     |
|    std                  | 2.48          |
|    value_loss           | 0.785         |
-------------------------------------------
Iteration: 3311 | Episodes: 134400 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3313 | Episodes: 134500 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 3315 | Episodes: 134600 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3318 | Episodes: 134700 | Median Reward: 47.94 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3320         |
|    time_elapsed         | 64376        |
|    total_timesteps      | 13598720     |
| train/                  |              |
|    approx_kl            | 3.883429e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -324         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -15.6        |
|    n_updates            | 33190        |
|    policy_gradient_loss | -0.000149    |
|    std                  | 2.49         |
|    value_loss           | 0.747        |
------------------------------------------
Iteration: 3320 | Episodes: 134800 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 3323 | Episodes: 134900 | Median Reward: 47.76 | Max Reward: 49.24
Iteration: 3325 | Episodes: 135000 | Median Reward: 47.90 | Max Reward: 49.24
Iteration: 3328 | Episodes: 135100 | Median Reward: 47.91 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3330          |
|    time_elapsed         | 64570         |
|    total_timesteps      | 13639680      |
| train/                  |               |
|    approx_kl            | 0.00048075832 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -325          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.7         |
|    n_updates            | 33290         |
|    policy_gradient_loss | 4.05e-05      |
|    std                  | 2.5           |
|    value_loss           | 0.587         |
-------------------------------------------
Iteration: 3330 | Episodes: 135200 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3333 | Episodes: 135300 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 3335 | Episodes: 135400 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 3338 | Episodes: 135500 | Median Reward: 47.88 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53         |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3340        |
|    time_elapsed         | 64748       |
|    total_timesteps      | 13680640    |
| train/                  |             |
|    approx_kl            | 0.017672844 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -325        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -15.6       |
|    n_updates            | 33390       |
|    policy_gradient_loss | -0.0246     |
|    std                  | 2.51        |
|    value_loss           | 0.564       |
-----------------------------------------
Iteration: 3340 | Episodes: 135600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3343 | Episodes: 135700 | Median Reward: 47.74 | Max Reward: 49.24
Iteration: 3345 | Episodes: 135800 | Median Reward: 47.64 | Max Reward: 49.24
Iteration: 3348 | Episodes: 135900 | Median Reward: 47.86 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.3       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3350        |
|    time_elapsed         | 64931       |
|    total_timesteps      | 13721600    |
| train/                  |             |
|    approx_kl            | 0.014810588 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -325        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -16.2       |
|    n_updates            | 33490       |
|    policy_gradient_loss | -0.0359     |
|    std                  | 2.52        |
|    value_loss           | 0.581       |
-----------------------------------------
Iteration: 3350 | Episodes: 136000 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3352 | Episodes: 136100 | Median Reward: 47.67 | Max Reward: 49.24
Iteration: 3355 | Episodes: 136200 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 3357 | Episodes: 136300 | Median Reward: 47.76 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -51.6        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3360         |
|    time_elapsed         | 65116        |
|    total_timesteps      | 13762560     |
| train/                  |              |
|    approx_kl            | 7.428847e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -326         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.1        |
|    n_updates            | 33590        |
|    policy_gradient_loss | -0.000196    |
|    std                  | 2.53         |
|    value_loss           | 4.61         |
------------------------------------------
Iteration: 3360 | Episodes: 136400 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 3362 | Episodes: 136500 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3365 | Episodes: 136600 | Median Reward: 47.72 | Max Reward: 49.24
Iteration: 3367 | Episodes: 136700 | Median Reward: 47.83 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.2       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3370        |
|    time_elapsed         | 65305       |
|    total_timesteps      | 13803520    |
| train/                  |             |
|    approx_kl            | 0.000551437 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -326        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -15.9       |
|    n_updates            | 33690       |
|    policy_gradient_loss | -0.00334    |
|    std                  | 2.54        |
|    value_loss           | 1.24        |
-----------------------------------------
Iteration: 3370 | Episodes: 136800 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3372 | Episodes: 136900 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3375 | Episodes: 137000 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3377 | Episodes: 137100 | Median Reward: 47.70 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.7         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3380          |
|    time_elapsed         | 65497         |
|    total_timesteps      | 13844480      |
| train/                  |               |
|    approx_kl            | 0.00033761433 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -326          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.9         |
|    n_updates            | 33790         |
|    policy_gradient_loss | -0.000655     |
|    std                  | 2.54          |
|    value_loss           | 1.45          |
-------------------------------------------
Iteration: 3380 | Episodes: 137200 | Median Reward: 47.78 | Max Reward: 49.24
Iteration: 3382 | Episodes: 137300 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3384 | Episodes: 137400 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 3387 | Episodes: 137500 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 3389 | Episodes: 137600 | Median Reward: 47.61 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3390         |
|    time_elapsed         | 65689        |
|    total_timesteps      | 13885440     |
| train/                  |              |
|    approx_kl            | 6.018761e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -326         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.2        |
|    n_updates            | 33890        |
|    policy_gradient_loss | -0.00054     |
|    std                  | 2.55         |
|    value_loss           | 0.379        |
------------------------------------------
Iteration: 3392 | Episodes: 137700 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 3394 | Episodes: 137800 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 3397 | Episodes: 137900 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3399 | Episodes: 138000 | Median Reward: 47.78 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53           |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3400          |
|    time_elapsed         | 65871         |
|    total_timesteps      | 13926400      |
| train/                  |               |
|    approx_kl            | 2.9385934e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -326          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.3         |
|    n_updates            | 33990         |
|    policy_gradient_loss | -6.62e-05     |
|    std                  | 2.57          |
|    value_loss           | 0.276         |
-------------------------------------------
Iteration: 3402 | Episodes: 138100 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 3404 | Episodes: 138200 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 3407 | Episodes: 138300 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 3409 | Episodes: 138400 | Median Reward: 47.84 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3410         |
|    time_elapsed         | 66061        |
|    total_timesteps      | 13967360     |
| train/                  |              |
|    approx_kl            | 6.946543e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -327         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.3        |
|    n_updates            | 34090        |
|    policy_gradient_loss | -0.000103    |
|    std                  | 2.58         |
|    value_loss           | 0.165        |
------------------------------------------
Iteration: 3411 | Episodes: 138500 | Median Reward: 47.84 | Max Reward: 49.24
Iteration: 3414 | Episodes: 138600 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 3416 | Episodes: 138700 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3419 | Episodes: 138800 | Median Reward: 47.85 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 99.2         |
|    ep_rew_mean          | -50.5        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3420         |
|    time_elapsed         | 66248        |
|    total_timesteps      | 14008320     |
| train/                  |              |
|    approx_kl            | 0.0001296165 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -327         |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0005       |
|    loss                 | -12.6        |
|    n_updates            | 34190        |
|    policy_gradient_loss | -0.000393    |
|    std                  | 2.59         |
|    value_loss           | 7.77         |
------------------------------------------
Iteration: 3421 | Episodes: 138900 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 3424 | Episodes: 139000 | Median Reward: 47.66 | Max Reward: 49.24
Iteration: 3426 | Episodes: 139100 | Median Reward: 47.69 | Max Reward: 49.24
Iteration: 3429 | Episodes: 139200 | Median Reward: 47.86 | Max Reward: 49.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -52.9      |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 3430       |
|    time_elapsed         | 66433      |
|    total_timesteps      | 14049280   |
| train/                  |            |
|    approx_kl            | 0.00880542 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -327       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -16.3      |
|    n_updates            | 34290      |
|    policy_gradient_loss | -0.0274    |
|    std                  | 2.6        |
|    value_loss           | 0.459      |
----------------------------------------
Iteration: 3431 | Episodes: 139300 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 3434 | Episodes: 139400 | Median Reward: 47.74 | Max Reward: 49.24
Iteration: 3436 | Episodes: 139500 | Median Reward: 47.62 | Max Reward: 49.24
Iteration: 3439 | Episodes: 139600 | Median Reward: 47.83 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.9        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3440         |
|    time_elapsed         | 66617        |
|    total_timesteps      | 14090240     |
| train/                  |              |
|    approx_kl            | 7.522489e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -327         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -16.2        |
|    n_updates            | 34390        |
|    policy_gradient_loss | -0.000318    |
|    std                  | 2.61         |
|    value_loss           | 0.379        |
------------------------------------------
Iteration: 3441 | Episodes: 139700 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 3443 | Episodes: 139800 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3446 | Episodes: 139900 | Median Reward: 47.86 | Max Reward: 49.24
Iteration: 3448 | Episodes: 140000 | Median Reward: 47.88 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -52           |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3450          |
|    time_elapsed         | 66797         |
|    total_timesteps      | 14131200      |
| train/                  |               |
|    approx_kl            | 0.00035611645 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -328          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -14.7         |
|    n_updates            | 34490         |
|    policy_gradient_loss | -0.000456     |
|    std                  | 2.62          |
|    value_loss           | 3.26          |
-------------------------------------------
Iteration: 3451 | Episodes: 140100 | Median Reward: 47.87 | Max Reward: 49.24
Iteration: 3453 | Episodes: 140200 | Median Reward: 47.83 | Max Reward: 49.24
Iteration: 3456 | Episodes: 140300 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 3458 | Episodes: 140400 | Median Reward: 47.84 | Max Reward: 49.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -51.6         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3460          |
|    time_elapsed         | 66984         |
|    total_timesteps      | 14172160      |
| train/                  |               |
|    approx_kl            | 0.00016218616 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -328          |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.0005        |
|    loss                 | -13.4         |
|    n_updates            | 34590         |
|    policy_gradient_loss | -0.000258     |
|    std                  | 2.63          |
|    value_loss           | 5.97          |
-------------------------------------------
Iteration: 3461 | Episodes: 140500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 3463 | Episodes: 140600 | Median Reward: 47.91 | Max Reward: 49.24
Iteration: 3466 | Episodes: 140700 | Median Reward: 47.84 | Max Reward: 49.24
Iteration: 3468 | Episodes: 140800 | Median Reward: 47.86 | Max Reward: 49.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3470         |
|    time_elapsed         | 67171        |
|    total_timesteps      | 14213120     |
| train/                  |              |
|    approx_kl            | 0.0010753896 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -328         |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.9        |
|    n_updates            | 34690        |
|    policy_gradient_loss | -0.00113     |
|    std                  | 2.65         |
|    value_loss           | 9.04         |
------------------------------------------
Iteration: 3471 | Episodes: 140900 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 3473 | Episodes: 141000 | Median Reward: 47.88 | Max Reward: 49.24
Iteration: 3475 | Episodes: 141100 | Median Reward: 47.82 | Max Reward: 49.24
Iteration: 3478 | Episodes: 141200 | Median Reward: 47.83 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -54         |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3480        |
|    time_elapsed         | 67355       |
|    total_timesteps      | 14254080    |
| train/                  |             |
|    approx_kl            | 0.011951262 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -328        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -16.4       |
|    n_updates            | 34790       |
|    policy_gradient_loss | -0.0241     |
|    std                  | 2.66        |
|    value_loss           | 0.456       |
-----------------------------------------
Iteration: 3480 | Episodes: 141300 | Median Reward: 47.73 | Max Reward: 49.24
Iteration: 3483 | Episodes: 141400 | Median Reward: 47.85 | Max Reward: 49.24
Iteration: 3485 | Episodes: 141500 | Median Reward: 47.92 | Max Reward: 49.24
Iteration: 3488 | Episodes: 141600 | Median Reward: 47.88 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.4       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3490        |
|    time_elapsed         | 67535       |
|    total_timesteps      | 14295040    |
| train/                  |             |
|    approx_kl            | 0.003476529 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -329        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -16.2       |
|    n_updates            | 34890       |
|    policy_gradient_loss | -0.00556    |
|    std                  | 2.66        |
|    value_loss           | 0.219       |
-----------------------------------------
Iteration: 3490 | Episodes: 141700 | Median Reward: 47.22 | Max Reward: 49.24
Iteration: 3493 | Episodes: 141800 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 3495 | Episodes: 141900 | Median Reward: 47.89 | Max Reward: 49.24
Iteration: 3498 | Episodes: 142000 | Median Reward: 47.50 | Max Reward: 49.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.5       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3500        |
|    time_elapsed         | 67722       |
|    total_timesteps      | 14336000    |
| train/                  |             |
|    approx_kl            | 0.000165166 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -329        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -14.2       |
|    n_updates            | 34990       |
|    policy_gradient_loss | -0.000443   |
|    std                  | 2.67        |
|    value_loss           | 3.99        |
-----------------------------------------
Iteration: 3500 | Episodes: 142100 | Median Reward: 47.70 | Max Reward: 49.24
Iteration: 3503 | Episodes: 142200 | Median Reward: 47.76 | Max Reward: 49.24
Iteration: 3505 | Episodes: 142300 | Median Reward: 47.70 | Max Reward: 49.25
Iteration: 3507 | Episodes: 142400 | Median Reward: 47.80 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3510          |
|    time_elapsed         | 67910         |
|    total_timesteps      | 14376960      |
| train/                  |               |
|    approx_kl            | 1.1096126e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -329          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.2         |
|    n_updates            | 35090         |
|    policy_gradient_loss | 9.81e-06      |
|    std                  | 2.67          |
|    value_loss           | 0.368         |
-------------------------------------------
Iteration: 3510 | Episodes: 142500 | Median Reward: 47.92 | Max Reward: 49.25
Iteration: 3512 | Episodes: 142600 | Median Reward: 47.80 | Max Reward: 49.25
Iteration: 3515 | Episodes: 142700 | Median Reward: 47.88 | Max Reward: 49.25
Iteration: 3517 | Episodes: 142800 | Median Reward: 47.64 | Max Reward: 49.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.2       |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 3520        |
|    time_elapsed         | 68094       |
|    total_timesteps      | 14417920    |
| train/                  |             |
|    approx_kl            | 1.12134e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -330        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -16.2       |
|    n_updates            | 35190       |
|    policy_gradient_loss | 5.32e-05    |
|    std                  | 2.69        |
|    value_loss           | 0.992       |
-----------------------------------------
Iteration: 3520 | Episodes: 142900 | Median Reward: 47.77 | Max Reward: 49.25
Iteration: 3522 | Episodes: 143000 | Median Reward: 47.90 | Max Reward: 49.25
Iteration: 3525 | Episodes: 143100 | Median Reward: 47.73 | Max Reward: 49.25
Iteration: 3527 | Episodes: 143200 | Median Reward: 47.90 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3530         |
|    time_elapsed         | 68274        |
|    total_timesteps      | 14458880     |
| train/                  |              |
|    approx_kl            | 8.085946e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -330         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.4        |
|    n_updates            | 35290        |
|    policy_gradient_loss | -3.68e-05    |
|    std                  | 2.7          |
|    value_loss           | 0.38         |
------------------------------------------
Iteration: 3530 | Episodes: 143300 | Median Reward: 47.91 | Max Reward: 49.25
Iteration: 3532 | Episodes: 143400 | Median Reward: 47.70 | Max Reward: 49.25
Iteration: 3535 | Episodes: 143500 | Median Reward: 47.87 | Max Reward: 49.25
Iteration: 3537 | Episodes: 143600 | Median Reward: 47.82 | Max Reward: 49.25
Iteration: 3539 | Episodes: 143700 | Median Reward: 47.86 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.2         |
| time/                   |               |
|    fps                  | 211           |
|    iterations           | 3540          |
|    time_elapsed         | 68456         |
|    total_timesteps      | 14499840      |
| train/                  |               |
|    approx_kl            | 1.6878737e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -330          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -15.1         |
|    n_updates            | 35390         |
|    policy_gradient_loss | -2.11e-05     |
|    std                  | 2.7           |
|    value_loss           | 2.3           |
-------------------------------------------
Iteration: 3542 | Episodes: 143800 | Median Reward: 47.82 | Max Reward: 49.25
Iteration: 3544 | Episodes: 143900 | Median Reward: 47.82 | Max Reward: 49.25
Iteration: 3547 | Episodes: 144000 | Median Reward: 47.85 | Max Reward: 49.25
Iteration: 3549 | Episodes: 144100 | Median Reward: 47.88 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.6        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3550         |
|    time_elapsed         | 68639        |
|    total_timesteps      | 14540800     |
| train/                  |              |
|    approx_kl            | 0.0011896591 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -330         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.5        |
|    n_updates            | 35490        |
|    policy_gradient_loss | -0.00512     |
|    std                  | 2.71         |
|    value_loss           | 0.328        |
------------------------------------------
Iteration: 3552 | Episodes: 144200 | Median Reward: 47.90 | Max Reward: 49.25
Iteration: 3554 | Episodes: 144300 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3557 | Episodes: 144400 | Median Reward: 47.62 | Max Reward: 49.25
Iteration: 3559 | Episodes: 144500 | Median Reward: 47.82 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.9        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3560         |
|    time_elapsed         | 68822        |
|    total_timesteps      | 14581760     |
| train/                  |              |
|    approx_kl            | 3.167776e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -330         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.4        |
|    n_updates            | 35590        |
|    policy_gradient_loss | -4.71e-05    |
|    std                  | 2.72         |
|    value_loss           | 0.36         |
------------------------------------------
Iteration: 3562 | Episodes: 144600 | Median Reward: 47.64 | Max Reward: 49.25
Iteration: 3564 | Episodes: 144700 | Median Reward: 47.88 | Max Reward: 49.25
Iteration: 3567 | Episodes: 144800 | Median Reward: 47.65 | Max Reward: 49.25
Iteration: 3569 | Episodes: 144900 | Median Reward: 47.74 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53          |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3570         |
|    time_elapsed         | 68997        |
|    total_timesteps      | 14622720     |
| train/                  |              |
|    approx_kl            | 0.0005452691 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -331         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.4        |
|    n_updates            | 35690        |
|    policy_gradient_loss | -0.0007      |
|    std                  | 2.73         |
|    value_loss           | 0.419        |
------------------------------------------
Iteration: 3571 | Episodes: 145000 | Median Reward: 47.92 | Max Reward: 49.25
Iteration: 3574 | Episodes: 145100 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3576 | Episodes: 145200 | Median Reward: 47.84 | Max Reward: 49.25
Iteration: 3579 | Episodes: 145300 | Median Reward: 47.88 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.8        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 3580         |
|    time_elapsed         | 69179        |
|    total_timesteps      | 14663680     |
| train/                  |              |
|    approx_kl            | 0.0008751751 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -331         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.5        |
|    n_updates            | 35790        |
|    policy_gradient_loss | 0.000128     |
|    std                  | 2.74         |
|    value_loss           | 0.431        |
------------------------------------------
Iteration: 3581 | Episodes: 145400 | Median Reward: 47.77 | Max Reward: 49.25
Iteration: 3584 | Episodes: 145500 | Median Reward: 47.88 | Max Reward: 49.25
Iteration: 3586 | Episodes: 145600 | Median Reward: 47.77 | Max Reward: 49.25
Iteration: 3589 | Episodes: 145700 | Median Reward: 47.70 | Max Reward: 49.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.6       |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 3590        |
|    time_elapsed         | 69360       |
|    total_timesteps      | 14704640    |
| train/                  |             |
|    approx_kl            | 0.018197693 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -331        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -16.5       |
|    n_updates            | 35890       |
|    policy_gradient_loss | -0.0367     |
|    std                  | 2.74        |
|    value_loss           | 0.176       |
-----------------------------------------
Iteration: 3591 | Episodes: 145800 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3594 | Episodes: 145900 | Median Reward: 47.89 | Max Reward: 49.25
Iteration: 3596 | Episodes: 146000 | Median Reward: 47.88 | Max Reward: 49.25
Iteration: 3599 | Episodes: 146100 | Median Reward: 47.76 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.2        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3600         |
|    time_elapsed         | 69542        |
|    total_timesteps      | 14745600     |
| train/                  |              |
|    approx_kl            | 0.0025809377 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -331         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.5        |
|    n_updates            | 35990        |
|    policy_gradient_loss | -0.00547     |
|    std                  | 2.75         |
|    value_loss           | 0.206        |
------------------------------------------
Iteration: 3601 | Episodes: 146200 | Median Reward: 47.68 | Max Reward: 49.25
Iteration: 3604 | Episodes: 146300 | Median Reward: 47.88 | Max Reward: 49.25
Iteration: 3606 | Episodes: 146400 | Median Reward: 47.88 | Max Reward: 49.25
Iteration: 3608 | Episodes: 146500 | Median Reward: 47.82 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.2        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3610         |
|    time_elapsed         | 69726        |
|    total_timesteps      | 14786560     |
| train/                  |              |
|    approx_kl            | 0.0014516814 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -331         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.3        |
|    n_updates            | 36090        |
|    policy_gradient_loss | -0.00389     |
|    std                  | 2.76         |
|    value_loss           | 0.235        |
------------------------------------------
Iteration: 3611 | Episodes: 146600 | Median Reward: 47.81 | Max Reward: 49.25
Iteration: 3613 | Episodes: 146700 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3616 | Episodes: 146800 | Median Reward: 47.85 | Max Reward: 49.25
Iteration: 3618 | Episodes: 146900 | Median Reward: 47.42 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.2        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3620         |
|    time_elapsed         | 69911        |
|    total_timesteps      | 14827520     |
| train/                  |              |
|    approx_kl            | 0.0030062827 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -332         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.5        |
|    n_updates            | 36190        |
|    policy_gradient_loss | -0.0051      |
|    std                  | 2.77         |
|    value_loss           | 0.234        |
------------------------------------------
Iteration: 3621 | Episodes: 147000 | Median Reward: 47.78 | Max Reward: 49.25
Iteration: 3623 | Episodes: 147100 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3626 | Episodes: 147200 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3628 | Episodes: 147300 | Median Reward: 47.84 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -51.4         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3630          |
|    time_elapsed         | 70094         |
|    total_timesteps      | 14868480      |
| train/                  |               |
|    approx_kl            | 0.00040747828 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -332          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.6         |
|    n_updates            | 36290         |
|    policy_gradient_loss | 0.000365      |
|    std                  | 2.78          |
|    value_loss           | 0.236         |
-------------------------------------------
Iteration: 3631 | Episodes: 147400 | Median Reward: 47.91 | Max Reward: 49.25
Iteration: 3633 | Episodes: 147500 | Median Reward: 47.54 | Max Reward: 49.25
Iteration: 3636 | Episodes: 147600 | Median Reward: 47.94 | Max Reward: 49.25
Iteration: 3638 | Episodes: 147700 | Median Reward: 47.72 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.7         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3640          |
|    time_elapsed         | 70275         |
|    total_timesteps      | 14909440      |
| train/                  |               |
|    approx_kl            | 0.00011393205 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -332          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.5         |
|    n_updates            | 36390         |
|    policy_gradient_loss | -0.000441     |
|    std                  | 2.8           |
|    value_loss           | 1.24          |
-------------------------------------------
Iteration: 3640 | Episodes: 147800 | Median Reward: 47.70 | Max Reward: 49.25
Iteration: 3643 | Episodes: 147900 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3645 | Episodes: 148000 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3648 | Episodes: 148100 | Median Reward: 47.86 | Max Reward: 49.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.6       |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 3650        |
|    time_elapsed         | 70451       |
|    total_timesteps      | 14950400    |
| train/                  |             |
|    approx_kl            | 0.000719596 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -333        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -16.5       |
|    n_updates            | 36490       |
|    policy_gradient_loss | -6.57e-05   |
|    std                  | 2.82        |
|    value_loss           | 0.23        |
-----------------------------------------
Iteration: 3650 | Episodes: 148200 | Median Reward: 47.79 | Max Reward: 49.25
Iteration: 3653 | Episodes: 148300 | Median Reward: 47.80 | Max Reward: 49.25
Iteration: 3655 | Episodes: 148400 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3658 | Episodes: 148500 | Median Reward: 47.57 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.5         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3660          |
|    time_elapsed         | 70635         |
|    total_timesteps      | 14991360      |
| train/                  |               |
|    approx_kl            | 3.7114063e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -333          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.5         |
|    n_updates            | 36590         |
|    policy_gradient_loss | 2.71e-05      |
|    std                  | 2.83          |
|    value_loss           | 0.301         |
-------------------------------------------
Iteration: 3660 | Episodes: 148600 | Median Reward: 47.88 | Max Reward: 49.25
Iteration: 3663 | Episodes: 148700 | Median Reward: 47.88 | Max Reward: 49.25
Iteration: 3665 | Episodes: 148800 | Median Reward: 47.91 | Max Reward: 49.25
Iteration: 3668 | Episodes: 148900 | Median Reward: 47.85 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -51.8        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3670         |
|    time_elapsed         | 70818        |
|    total_timesteps      | 15032320     |
| train/                  |              |
|    approx_kl            | 7.571347e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -333         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.8        |
|    n_updates            | 36690        |
|    policy_gradient_loss | -2.9e-05     |
|    std                  | 2.84         |
|    value_loss           | 1.86         |
------------------------------------------
Iteration: 3670 | Episodes: 149000 | Median Reward: 47.73 | Max Reward: 49.25
Iteration: 3672 | Episodes: 149100 | Median Reward: 47.82 | Max Reward: 49.25
Iteration: 3675 | Episodes: 149200 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3677 | Episodes: 149300 | Median Reward: 47.88 | Max Reward: 49.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.4       |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 3680        |
|    time_elapsed         | 71001       |
|    total_timesteps      | 15073280    |
| train/                  |             |
|    approx_kl            | 0.004092554 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -333        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -16.6       |
|    n_updates            | 36790       |
|    policy_gradient_loss | -0.0234     |
|    std                  | 2.85        |
|    value_loss           | 0.51        |
-----------------------------------------
Iteration: 3680 | Episodes: 149400 | Median Reward: 47.89 | Max Reward: 49.25
Iteration: 3682 | Episodes: 149500 | Median Reward: 47.87 | Max Reward: 49.25
Iteration: 3685 | Episodes: 149600 | Median Reward: 47.65 | Max Reward: 49.25
Iteration: 3687 | Episodes: 149700 | Median Reward: 47.87 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.7        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3690         |
|    time_elapsed         | 71185        |
|    total_timesteps      | 15114240     |
| train/                  |              |
|    approx_kl            | 9.923708e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -333         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.6        |
|    n_updates            | 36890        |
|    policy_gradient_loss | -0.000392    |
|    std                  | 2.86         |
|    value_loss           | 0.206        |
------------------------------------------
Iteration: 3690 | Episodes: 149800 | Median Reward: 47.62 | Max Reward: 49.25
Iteration: 3692 | Episodes: 149900 | Median Reward: 47.77 | Max Reward: 49.25
Iteration: 3695 | Episodes: 150000 | Median Reward: 47.85 | Max Reward: 49.25
Iteration: 3697 | Episodes: 150100 | Median Reward: 47.82 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3700         |
|    time_elapsed         | 71370        |
|    total_timesteps      | 15155200     |
| train/                  |              |
|    approx_kl            | 0.0002197324 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -334         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.7        |
|    n_updates            | 36990        |
|    policy_gradient_loss | -0.000508    |
|    std                  | 2.88         |
|    value_loss           | 0.235        |
------------------------------------------
Iteration: 3700 | Episodes: 150200 | Median Reward: 47.85 | Max Reward: 49.25
Iteration: 3702 | Episodes: 150300 | Median Reward: 47.81 | Max Reward: 49.25
Iteration: 3705 | Episodes: 150400 | Median Reward: 47.56 | Max Reward: 49.25
Iteration: 3707 | Episodes: 150500 | Median Reward: 47.76 | Max Reward: 49.25
Iteration: 3709 | Episodes: 150600 | Median Reward: 47.42 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.2        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3710         |
|    time_elapsed         | 71553        |
|    total_timesteps      | 15196160     |
| train/                  |              |
|    approx_kl            | 0.0015466656 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -334         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.4        |
|    n_updates            | 37090        |
|    policy_gradient_loss | -0.00155     |
|    std                  | 2.89         |
|    value_loss           | 0.182        |
------------------------------------------
Iteration: 3712 | Episodes: 150700 | Median Reward: 47.59 | Max Reward: 49.25
Iteration: 3714 | Episodes: 150800 | Median Reward: 47.85 | Max Reward: 49.25
Iteration: 3717 | Episodes: 150900 | Median Reward: 47.88 | Max Reward: 49.25
Iteration: 3719 | Episodes: 151000 | Median Reward: 47.75 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.9        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3720         |
|    time_elapsed         | 71737        |
|    total_timesteps      | 15237120     |
| train/                  |              |
|    approx_kl            | 0.0059360876 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -334         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.5        |
|    n_updates            | 37190        |
|    policy_gradient_loss | -0.0271      |
|    std                  | 2.91         |
|    value_loss           | 0.7          |
------------------------------------------
Iteration: 3722 | Episodes: 151100 | Median Reward: 47.73 | Max Reward: 49.25
Iteration: 3724 | Episodes: 151200 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3727 | Episodes: 151300 | Median Reward: 47.74 | Max Reward: 49.25
Iteration: 3729 | Episodes: 151400 | Median Reward: 47.86 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52           |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3730          |
|    time_elapsed         | 71919         |
|    total_timesteps      | 15278080      |
| train/                  |               |
|    approx_kl            | 0.00016779607 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -334          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -15.5         |
|    n_updates            | 37290         |
|    policy_gradient_loss | -0.000389     |
|    std                  | 2.91          |
|    value_loss           | 2.71          |
-------------------------------------------
Iteration: 3732 | Episodes: 151500 | Median Reward: 47.68 | Max Reward: 49.25
Iteration: 3734 | Episodes: 151600 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3737 | Episodes: 151700 | Median Reward: 47.73 | Max Reward: 49.25
Iteration: 3739 | Episodes: 151800 | Median Reward: 47.66 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.4         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3740          |
|    time_elapsed         | 72093         |
|    total_timesteps      | 15319040      |
| train/                  |               |
|    approx_kl            | 0.00023404419 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -334          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.6         |
|    n_updates            | 37390         |
|    policy_gradient_loss | -0.000589     |
|    std                  | 2.92          |
|    value_loss           | 0.15          |
-------------------------------------------
Iteration: 3742 | Episodes: 151900 | Median Reward: 47.82 | Max Reward: 49.25
Iteration: 3744 | Episodes: 152000 | Median Reward: 47.57 | Max Reward: 49.25
Iteration: 3746 | Episodes: 152100 | Median Reward: 47.88 | Max Reward: 49.25
Iteration: 3749 | Episodes: 152200 | Median Reward: 47.90 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -52.2        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3750         |
|    time_elapsed         | 72278        |
|    total_timesteps      | 15360000     |
| train/                  |              |
|    approx_kl            | 0.0004741063 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -335         |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.3        |
|    n_updates            | 37490        |
|    policy_gradient_loss | -0.000789    |
|    std                  | 2.94         |
|    value_loss           | 6.97         |
------------------------------------------
Iteration: 3751 | Episodes: 152300 | Median Reward: 47.78 | Max Reward: 49.25
Iteration: 3754 | Episodes: 152400 | Median Reward: 47.68 | Max Reward: 49.25
Iteration: 3756 | Episodes: 152500 | Median Reward: 47.59 | Max Reward: 49.25
Iteration: 3759 | Episodes: 152600 | Median Reward: 47.89 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.6         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3760          |
|    time_elapsed         | 72464         |
|    total_timesteps      | 15400960      |
| train/                  |               |
|    approx_kl            | 9.9279554e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -335          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -15.2         |
|    n_updates            | 37590         |
|    policy_gradient_loss | -0.000416     |
|    std                  | 2.95          |
|    value_loss           | 0.826         |
-------------------------------------------
Iteration: 3761 | Episodes: 152700 | Median Reward: 47.73 | Max Reward: 49.25
Iteration: 3764 | Episodes: 152800 | Median Reward: 47.83 | Max Reward: 49.25
Iteration: 3766 | Episodes: 152900 | Median Reward: 47.82 | Max Reward: 49.25
Iteration: 3769 | Episodes: 153000 | Median Reward: 47.89 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.4        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3770         |
|    time_elapsed         | 72648        |
|    total_timesteps      | 15441920     |
| train/                  |              |
|    approx_kl            | 0.0011685842 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -335         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.6        |
|    n_updates            | 37690        |
|    policy_gradient_loss | 2.28e-05     |
|    std                  | 2.96         |
|    value_loss           | 0.239        |
------------------------------------------
Iteration: 3771 | Episodes: 153100 | Median Reward: 47.89 | Max Reward: 49.25
Iteration: 3773 | Episodes: 153200 | Median Reward: 47.89 | Max Reward: 49.25
Iteration: 3776 | Episodes: 153300 | Median Reward: 47.87 | Max Reward: 49.25
Iteration: 3778 | Episodes: 153400 | Median Reward: 47.90 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.9         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3780          |
|    time_elapsed         | 72833         |
|    total_timesteps      | 15482880      |
| train/                  |               |
|    approx_kl            | 7.9180056e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -335          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -16.7         |
|    n_updates            | 37790         |
|    policy_gradient_loss | -3.02e-05     |
|    std                  | 2.98          |
|    value_loss           | 0.263         |
-------------------------------------------
Iteration: 3781 | Episodes: 153500 | Median Reward: 47.89 | Max Reward: 49.25
Iteration: 3783 | Episodes: 153600 | Median Reward: 47.68 | Max Reward: 49.25
Iteration: 3786 | Episodes: 153700 | Median Reward: 47.66 | Max Reward: 49.25
Iteration: 3788 | Episodes: 153800 | Median Reward: 46.38 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.9        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3790         |
|    time_elapsed         | 73019        |
|    total_timesteps      | 15523840     |
| train/                  |              |
|    approx_kl            | 0.0001141181 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -336         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -15.2        |
|    n_updates            | 37890        |
|    policy_gradient_loss | -0.000417    |
|    std                  | 2.99         |
|    value_loss           | 3.46         |
------------------------------------------
Iteration: 3791 | Episodes: 153900 | Median Reward: 47.23 | Max Reward: 49.25
Iteration: 3793 | Episodes: 154000 | Median Reward: 47.87 | Max Reward: 49.25
Iteration: 3796 | Episodes: 154100 | Median Reward: 47.79 | Max Reward: 49.25
Iteration: 3798 | Episodes: 154200 | Median Reward: 47.94 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.8        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3800         |
|    time_elapsed         | 73204        |
|    total_timesteps      | 15564800     |
| train/                  |              |
|    approx_kl            | 0.0013229745 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -336         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.7        |
|    n_updates            | 37990        |
|    policy_gradient_loss | -0.00297     |
|    std                  | 3            |
|    value_loss           | 0.196        |
------------------------------------------
Iteration: 3801 | Episodes: 154300 | Median Reward: 47.64 | Max Reward: 49.25
Iteration: 3803 | Episodes: 154400 | Median Reward: 47.84 | Max Reward: 49.25
Iteration: 3805 | Episodes: 154500 | Median Reward: 47.74 | Max Reward: 49.25
Iteration: 3808 | Episodes: 154600 | Median Reward: 47.86 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.5         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3810          |
|    time_elapsed         | 73387         |
|    total_timesteps      | 15605760      |
| train/                  |               |
|    approx_kl            | 0.00025403546 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -336          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -16.7         |
|    n_updates            | 38090         |
|    policy_gradient_loss | -0.000462     |
|    std                  | 3.02          |
|    value_loss           | 0.394         |
-------------------------------------------
Iteration: 3810 | Episodes: 154700 | Median Reward: 47.86 | Max Reward: 49.25
Iteration: 3813 | Episodes: 154800 | Median Reward: 47.91 | Max Reward: 49.25
Iteration: 3815 | Episodes: 154900 | Median Reward: 47.28 | Max Reward: 49.25
Iteration: 3818 | Episodes: 155000 | Median Reward: 47.76 | Max Reward: 49.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -52.6       |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 3820        |
|    time_elapsed         | 73569       |
|    total_timesteps      | 15646720    |
| train/                  |             |
|    approx_kl            | 0.001970984 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -336        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -16.6       |
|    n_updates            | 38190       |
|    policy_gradient_loss | -0.00804    |
|    std                  | 3.03        |
|    value_loss           | 0.625       |
-----------------------------------------
Iteration: 3820 | Episodes: 155100 | Median Reward: 47.71 | Max Reward: 49.25
Iteration: 3823 | Episodes: 155200 | Median Reward: 47.66 | Max Reward: 49.25
Iteration: 3825 | Episodes: 155300 | Median Reward: 47.83 | Max Reward: 49.25
Iteration: 3828 | Episodes: 155400 | Median Reward: 47.89 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.1        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3830         |
|    time_elapsed         | 73748        |
|    total_timesteps      | 15687680     |
| train/                  |              |
|    approx_kl            | 0.0006932784 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -336         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -15.6        |
|    n_updates            | 38290        |
|    policy_gradient_loss | -0.00348     |
|    std                  | 3.04         |
|    value_loss           | 0.76         |
------------------------------------------
Iteration: 3830 | Episodes: 155500 | Median Reward: 47.85 | Max Reward: 49.25
Iteration: 3832 | Episodes: 155600 | Median Reward: 47.85 | Max Reward: 49.25
Iteration: 3835 | Episodes: 155700 | Median Reward: 47.92 | Max Reward: 49.25
Iteration: 3837 | Episodes: 155800 | Median Reward: 47.60 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.5        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3840         |
|    time_elapsed         | 73931        |
|    total_timesteps      | 15728640     |
| train/                  |              |
|    approx_kl            | 5.611025e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -337         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -16.8        |
|    n_updates            | 38390        |
|    policy_gradient_loss | -1.96e-05    |
|    std                  | 3.06         |
|    value_loss           | 0.208        |
------------------------------------------
Iteration: 3840 | Episodes: 155900 | Median Reward: 47.76 | Max Reward: 49.25
Iteration: 3842 | Episodes: 156000 | Median Reward: 47.73 | Max Reward: 49.25
Iteration: 3845 | Episodes: 156100 | Median Reward: 47.80 | Max Reward: 49.25
Iteration: 3847 | Episodes: 156200 | Median Reward: 47.79 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.4         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3850          |
|    time_elapsed         | 74119         |
|    total_timesteps      | 15769600      |
| train/                  |               |
|    approx_kl            | 1.2185759e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -337          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.5         |
|    n_updates            | 38490         |
|    policy_gradient_loss | -4.67e-05     |
|    std                  | 3.07          |
|    value_loss           | 0.295         |
-------------------------------------------
Iteration: 3850 | Episodes: 156300 | Median Reward: 47.80 | Max Reward: 49.25
Iteration: 3852 | Episodes: 156400 | Median Reward: 47.85 | Max Reward: 49.25
Iteration: 3855 | Episodes: 156500 | Median Reward: 47.70 | Max Reward: 49.25
Iteration: 3857 | Episodes: 156600 | Median Reward: 47.35 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -52.7        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3860         |
|    time_elapsed         | 74302        |
|    total_timesteps      | 15810560     |
| train/                  |              |
|    approx_kl            | 8.407397e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -337         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.8        |
|    n_updates            | 38590        |
|    policy_gradient_loss | -0.000157    |
|    std                  | 3.08         |
|    value_loss           | 0.695        |
------------------------------------------
Iteration: 3860 | Episodes: 156700 | Median Reward: 47.63 | Max Reward: 49.25
Iteration: 3862 | Episodes: 156800 | Median Reward: 47.72 | Max Reward: 49.25
Iteration: 3864 | Episodes: 156900 | Median Reward: 47.82 | Max Reward: 49.25
Iteration: 3867 | Episodes: 157000 | Median Reward: 47.65 | Max Reward: 49.25
Iteration: 3869 | Episodes: 157100 | Median Reward: 47.61 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.1         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3870          |
|    time_elapsed         | 74485         |
|    total_timesteps      | 15851520      |
| train/                  |               |
|    approx_kl            | 0.00028553282 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -337          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.8         |
|    n_updates            | 38690         |
|    policy_gradient_loss | -0.00102      |
|    std                  | 3.09          |
|    value_loss           | 0.174         |
-------------------------------------------
Iteration: 3872 | Episodes: 157200 | Median Reward: 47.73 | Max Reward: 49.25
Iteration: 3874 | Episodes: 157300 | Median Reward: 47.85 | Max Reward: 49.25
Iteration: 3877 | Episodes: 157400 | Median Reward: 47.73 | Max Reward: 49.25
Iteration: 3879 | Episodes: 157500 | Median Reward: 47.51 | Max Reward: 49.25
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.7       |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 3880        |
|    time_elapsed         | 74666       |
|    total_timesteps      | 15892480    |
| train/                  |             |
|    approx_kl            | 0.001151762 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -337        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -16.8       |
|    n_updates            | 38790       |
|    policy_gradient_loss | -0.00556    |
|    std                  | 3.1         |
|    value_loss           | 0.376       |
-----------------------------------------
Iteration: 3882 | Episodes: 157600 | Median Reward: 47.65 | Max Reward: 49.25
Iteration: 3884 | Episodes: 157700 | Median Reward: 47.66 | Max Reward: 49.25
Iteration: 3887 | Episodes: 157800 | Median Reward: 47.71 | Max Reward: 49.25
Iteration: 3889 | Episodes: 157900 | Median Reward: 47.70 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.2        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3890         |
|    time_elapsed         | 74845        |
|    total_timesteps      | 15933440     |
| train/                  |              |
|    approx_kl            | 0.0011896013 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -337         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -16.5        |
|    n_updates            | 38890        |
|    policy_gradient_loss | -0.00386     |
|    std                  | 3.11         |
|    value_loss           | 0.308        |
------------------------------------------
Iteration: 3892 | Episodes: 158000 | Median Reward: 47.74 | Max Reward: 49.25
Iteration: 3894 | Episodes: 158100 | Median Reward: 46.33 | Max Reward: 49.25
Iteration: 3896 | Episodes: 158200 | Median Reward: 47.85 | Max Reward: 49.25
Iteration: 3899 | Episodes: 158300 | Median Reward: 47.60 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53           |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3900          |
|    time_elapsed         | 75025         |
|    total_timesteps      | 15974400      |
| train/                  |               |
|    approx_kl            | 0.00011283427 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -337          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.8         |
|    n_updates            | 38990         |
|    policy_gradient_loss | -0.000976     |
|    std                  | 3.13          |
|    value_loss           | 0.236         |
-------------------------------------------
Iteration: 3901 | Episodes: 158400 | Median Reward: 47.82 | Max Reward: 49.25
Iteration: 3904 | Episodes: 158500 | Median Reward: 47.36 | Max Reward: 49.25
Iteration: 3906 | Episodes: 158600 | Median Reward: 46.86 | Max Reward: 49.25
Iteration: 3909 | Episodes: 158700 | Median Reward: 47.66 | Max Reward: 49.25
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -52.4        |
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 3910         |
|    time_elapsed         | 75207        |
|    total_timesteps      | 16015360     |
| train/                  |              |
|    approx_kl            | 4.466118e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -338         |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0005       |
|    loss                 | -12.4        |
|    n_updates            | 39090        |
|    policy_gradient_loss | -0.000217    |
|    std                  | 3.13         |
|    value_loss           | 9.24         |
------------------------------------------
Iteration: 3911 | Episodes: 158800 | Median Reward: 47.88 | Max Reward: 49.25
Iteration: 3914 | Episodes: 158900 | Median Reward: 47.64 | Max Reward: 49.25
Iteration: 3916 | Episodes: 159000 | Median Reward: 47.67 | Max Reward: 49.25
Iteration: 3919 | Episodes: 159100 | Median Reward: 47.77 | Max Reward: 49.25
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.1         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 3920          |
|    time_elapsed         | 75390         |
|    total_timesteps      | 16056320      |
| train/                  |               |
|    approx_kl            | 0.00082607334 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -338          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.9         |
|    n_updates            | 39190         |
|    policy_gradient_loss | -0.00255      |
|    std                  | 3.15          |
|    value_loss           | 0.223         |
-------------------------------------------
Iteration: 3921 | Episodes: 159200 | Median Reward: 47.77 | Max Reward: 49.25
Iteration: 3924 | Episodes: 159300 | Median Reward: 47.80 | Max Reward: 49.25
Iteration: 3926 | Episodes: 159400 | Median Reward: 47.91 | Max Reward: 49.25
Iteration: 3928 | Episodes: 159500 | Median Reward: 47.58 | Max Reward: 49.35
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 100          |
|    ep_rew_mean          | -53          |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 3930         |
|    time_elapsed         | 75564        |
|    total_timesteps      | 16097280     |
| train/                  |              |
|    approx_kl            | 0.0008889665 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -338         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -15          |
|    n_updates            | 39290        |
|    policy_gradient_loss | -2.79e-05    |
|    std                  | 3.17         |
|    value_loss           | 4.01         |
------------------------------------------
Iteration: 3931 | Episodes: 159600 | Median Reward: 47.77 | Max Reward: 49.35
Iteration: 3933 | Episodes: 159700 | Median Reward: 47.74 | Max Reward: 49.35
Iteration: 3936 | Episodes: 159800 | Median Reward: 47.51 | Max Reward: 49.35
Iteration: 3938 | Episodes: 159900 | Median Reward: 47.79 | Max Reward: 49.35
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.1         |
| time/                   |               |
|    fps                  | 213           |
|    iterations           | 3940          |
|    time_elapsed         | 75746         |
|    total_timesteps      | 16138240      |
| train/                  |               |
|    approx_kl            | 4.2705084e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -339          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.9         |
|    n_updates            | 39390         |
|    policy_gradient_loss | -8.8e-05      |
|    std                  | 3.18          |
|    value_loss           | 0.176         |
-------------------------------------------
Iteration: 3941 | Episodes: 160000 | Median Reward: 46.94 | Max Reward: 49.35
Iteration: 3943 | Episodes: 160100 | Median Reward: 47.87 | Max Reward: 49.35
Iteration: 3946 | Episodes: 160200 | Median Reward: 47.55 | Max Reward: 49.35
Iteration: 3948 | Episodes: 160300 | Median Reward: 47.90 | Max Reward: 49.35
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 100           |
|    ep_rew_mean          | -52           |
| time/                   |               |
|    fps                  | 213           |
|    iterations           | 3950          |
|    time_elapsed         | 75929         |
|    total_timesteps      | 16179200      |
| train/                  |               |
|    approx_kl            | 1.1355907e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -339          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -16.9         |
|    n_updates            | 39490         |
|    policy_gradient_loss | -1.72e-05     |
|    std                  | 3.19          |
|    value_loss           | 0.418         |
-------------------------------------------
Iteration: 3951 | Episodes: 160400 | Median Reward: 47.78 | Max Reward: 49.35
Iteration: 3953 | Episodes: 160500 | Median Reward: 47.53 | Max Reward: 49.35
Iteration: 3956 | Episodes: 160600 | Median Reward: 47.74 | Max Reward: 49.35
Iteration: 3958 | Episodes: 160700 | Median Reward: 47.64 | Max Reward: 49.35
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54          |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 3960         |
|    time_elapsed         | 76111        |
|    total_timesteps      | 16220160     |
| train/                  |              |
|    approx_kl            | 3.390509e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -339         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -16.8        |
|    n_updates            | 39590        |
|    policy_gradient_loss | 1.23e-06     |
|    std                  | 3.2          |
|    value_loss           | 0.555        |
------------------------------------------
Iteration: 3960 | Episodes: 160800 | Median Reward: 47.70 | Max Reward: 49.35
Iteration: 3963 | Episodes: 160900 | Median Reward: 47.72 | Max Reward: 49.35
Iteration: 3965 | Episodes: 161000 | Median Reward: 47.62 | Max Reward: 49.35
Iteration: 3968 | Episodes: 161100 | Median Reward: 47.78 | Max Reward: 49.35
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -52.8         |
| time/                   |               |
|    fps                  | 213           |
|    iterations           | 3970          |
|    time_elapsed         | 76295         |
|    total_timesteps      | 16261120      |
| train/                  |               |
|    approx_kl            | 2.6076857e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -339          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16.7         |
|    n_updates            | 39690         |
|    policy_gradient_loss | -9.8e-05      |
|    std                  | 3.21          |
|    value_loss           | 0.338         |
-------------------------------------------
Iteration: 3970 | Episodes: 161200 | Median Reward: 47.64 | Max Reward: 49.35
Iteration: 3973 | Episodes: 161300 | Median Reward: 47.70 | Max Reward: 49.35
Iteration: 3975 | Episodes: 161400 | Median Reward: 47.64 | Max Reward: 49.35
Iteration: 3978 | Episodes: 161500 | Median Reward: 47.89 | Max Reward: 49.35
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -53.8         |
| time/                   |               |
|    fps                  | 213           |
|    iterations           | 3980          |
|    time_elapsed         | 76478         |
|    total_timesteps      | 16302080      |
| train/                  |               |
|    approx_kl            | 0.00019499553 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -339          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -16           |
|    n_updates            | 39790         |
|    policy_gradient_loss | -0.000293     |
|    std                  | 3.22          |
|    value_loss           | 0.463         |
-------------------------------------------
Iteration: 3980 | Episodes: 161600 | Median Reward: 47.57 | Max Reward: 49.35
Iteration: 3983 | Episodes: 161700 | Median Reward: 47.88 | Max Reward: 49.35
Iteration: 3985 | Episodes: 161800 | Median Reward: 47.47 | Max Reward: 49.35
Iteration: 3988 | Episodes: 161900 | Median Reward: 47.37 | Max Reward: 49.35
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -53.7       |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 3990        |
|    time_elapsed         | 76659       |
|    total_timesteps      | 16343040    |
| train/                  |             |
|    approx_kl            | 0.011627669 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -340        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -17         |
|    n_updates            | 39890       |
|    policy_gradient_loss | -0.0281     |
|    std                  | 3.24        |
|    value_loss           | 0.121       |
-----------------------------------------
Iteration: 3990 | Episodes: 162000 | Median Reward: 47.30 | Max Reward: 49.35
Iteration: 3992 | Episodes: 162100 | Median Reward: 47.32 | Max Reward: 49.35
Iteration: 3995 | Episodes: 162200 | Median Reward: 47.72 | Max Reward: 49.35
Iteration: 3997 | Episodes: 162300 | Median Reward: 47.48 | Max Reward: 49.35
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.3        |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 4000         |
|    time_elapsed         | 76842        |
|    total_timesteps      | 16384000     |
| train/                  |              |
|    approx_kl            | 0.0024865365 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -340         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -17          |
|    n_updates            | 39990        |
|    policy_gradient_loss | -0.0117      |
|    std                  | 3.26         |
|    value_loss           | 0.451        |
------------------------------------------
Training End | Episodes: 162385 | Median Reward: 47.74 | Max Reward: 49.35
Plot saved as fig_code1_1_basic.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> ^Dexit

Script done on 2024-10-24 01:13:45-04:00 [COMMAND_EXIT_CODE="0"]
