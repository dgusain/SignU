Script started on 2024-10-24 21:03:57-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/46" COLUMNS="203" LINES="53"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> conda activate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> pyton[K[Khon cd[Kd[Kode1_1_residual_b_005.py
GPU 2: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
Iteration: 2 | Episodes: 100 | Median Reward: 6.94 | Max Reward: 19.52
Iteration: 4 | Episodes: 200 | Median Reward: 8.95 | Max Reward: 19.52
Iteration: 7 | Episodes: 300 | Median Reward: 4.85 | Max Reward: 19.52
Iteration: 9 | Episodes: 400 | Median Reward: 0.03 | Max Reward: 19.52
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -95.8        |
| time/                   |              |
|    fps                  | 186          |
|    iterations           | 10           |
|    time_elapsed         | 219          |
|    total_timesteps      | 40960        |
| train/                  |              |
|    approx_kl            | 0.0074870614 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -89.5        |
|    explained_variance   | -0.0145      |
|    learning_rate        | 0.0005       |
|    loss                 | 67.4         |
|    n_updates            | 90           |
|    policy_gradient_loss | 0.000723     |
|    std                  | 1            |
|    value_loss           | 156          |
------------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 5.53 | Max Reward: 19.52
Iteration: 14 | Episodes: 600 | Median Reward: 4.84 | Max Reward: 25.42
Iteration: 17 | Episodes: 700 | Median Reward: 6.73 | Max Reward: 25.42
Iteration: 19 | Episodes: 800 | Median Reward: 3.50 | Max Reward: 25.42
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -97.9        |
| time/                   |              |
|    fps                  | 183          |
|    iterations           | 20           |
|    time_elapsed         | 445          |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0025186045 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -90.9        |
|    explained_variance   | 0.92         |
|    learning_rate        | 0.0005       |
|    loss                 | 0.339        |
|    n_updates            | 190          |
|    policy_gradient_loss | 0.00607      |
|    std                  | 1.01         |
|    value_loss           | 20.3         |
------------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 1.52 | Max Reward: 25.42
Iteration: 24 | Episodes: 1000 | Median Reward: 11.11 | Max Reward: 25.42
Iteration: 27 | Episodes: 1100 | Median Reward: 5.40 | Max Reward: 25.42
Iteration: 29 | Episodes: 1200 | Median Reward: 0.11 | Max Reward: 25.42
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -99          |
| time/                   |              |
|    fps                  | 181          |
|    iterations           | 30           |
|    time_elapsed         | 675          |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.0052518984 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -95.7        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0005       |
|    loss                 | -6.2         |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00841     |
|    std                  | 1.01         |
|    value_loss           | 11.5         |
------------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 8.21 | Max Reward: 25.42
Iteration: 34 | Episodes: 1400 | Median Reward: 13.33 | Max Reward: 25.42
Iteration: 36 | Episodes: 1500 | Median Reward: 6.67 | Max Reward: 25.42
Iteration: 39 | Episodes: 1600 | Median Reward: 0.14 | Max Reward: 25.42
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -97.3     |
| time/                   |           |
|    fps                  | 180       |
|    iterations           | 40        |
|    time_elapsed         | 906       |
|    total_timesteps      | 163840    |
| train/                  |           |
|    approx_kl            | 0.2106282 |
|    clip_fraction        | 0.606     |
|    clip_range           | 0.3       |
|    entropy_loss         | -105      |
|    explained_variance   | 0.973     |
|    learning_rate        | 0.0005    |
|    loss                 | -7.48     |
|    n_updates            | 390       |
|    policy_gradient_loss | -0.0286   |
|    std                  | 1.01      |
|    value_loss           | 7.05      |
---------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 6.42 | Max Reward: 25.42
Iteration: 44 | Episodes: 1800 | Median Reward: 6.32 | Max Reward: 25.42
Iteration: 46 | Episodes: 1900 | Median Reward: -2.06 | Max Reward: 25.42
Iteration: 49 | Episodes: 2000 | Median Reward: 8.27 | Max Reward: 25.42
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -91.4       |
| time/                   |             |
|    fps                  | 180         |
|    iterations           | 50          |
|    time_elapsed         | 1136        |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.007568069 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.3         |
|    entropy_loss         | -119        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0005      |
|    loss                 | -10.7       |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00309    |
|    std                  | 1.01        |
|    value_loss           | 6.7         |
-----------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 1.64 | Max Reward: 25.42
Iteration: 54 | Episodes: 2200 | Median Reward: 13.36 | Max Reward: 25.42
Iteration: 56 | Episodes: 2300 | Median Reward: 3.78 | Max Reward: 25.42
Iteration: 59 | Episodes: 2400 | Median Reward: 6.24 | Max Reward: 25.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -92.2       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 60          |
|    time_elapsed         | 1366        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.027498279 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.3         |
|    entropy_loss         | -130        |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0005      |
|    loss                 | -11         |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.00981    |
|    std                  | 1.01        |
|    value_loss           | 5.6         |
-----------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 7.87 | Max Reward: 25.77
Iteration: 64 | Episodes: 2600 | Median Reward: 8.69 | Max Reward: 26.50
Iteration: 66 | Episodes: 2700 | Median Reward: 10.23 | Max Reward: 27.86
Iteration: 69 | Episodes: 2800 | Median Reward: 8.46 | Max Reward: 27.86
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -92.6     |
| time/                   |           |
|    fps                  | 179       |
|    iterations           | 70        |
|    time_elapsed         | 1598      |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.0855248 |
|    clip_fraction        | 0.2       |
|    clip_range           | 0.3       |
|    entropy_loss         | -144      |
|    explained_variance   | 0.987     |
|    learning_rate        | 0.0005    |
|    loss                 | -13.3     |
|    n_updates            | 690       |
|    policy_gradient_loss | -0.0357   |
|    std                  | 1.01      |
|    value_loss           | 3.93      |
---------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 11.63 | Max Reward: 27.86
Iteration: 73 | Episodes: 3000 | Median Reward: 17.15 | Max Reward: 27.86
Iteration: 76 | Episodes: 3100 | Median Reward: 18.57 | Max Reward: 27.86
Iteration: 78 | Episodes: 3200 | Median Reward: 10.77 | Max Reward: 27.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -89.1       |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 80          |
|    time_elapsed         | 1831        |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.016480235 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -148        |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.32       |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.0226     |
|    std                  | 1.01        |
|    value_loss           | 12.5        |
-----------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 11.47 | Max Reward: 27.86
Iteration: 83 | Episodes: 3400 | Median Reward: 14.14 | Max Reward: 29.04
Iteration: 86 | Episodes: 3500 | Median Reward: 14.35 | Max Reward: 33.23
Iteration: 88 | Episodes: 3600 | Median Reward: 14.32 | Max Reward: 33.23
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -84.7      |
| time/                   |            |
|    fps                  | 178        |
|    iterations           | 90         |
|    time_elapsed         | 2064       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.06819092 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.3        |
|    entropy_loss         | -158       |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0005     |
|    loss                 | -12.2      |
|    n_updates            | 890        |
|    policy_gradient_loss | 0.000128   |
|    std                  | 1.02       |
|    value_loss           | 4.28       |
----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 16.73 | Max Reward: 33.23
Iteration: 93 | Episodes: 3800 | Median Reward: 11.40 | Max Reward: 33.23
Iteration: 96 | Episodes: 3900 | Median Reward: 12.72 | Max Reward: 33.23
Iteration: 98 | Episodes: 4000 | Median Reward: 16.51 | Max Reward: 33.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -85         |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 100         |
|    time_elapsed         | 2295        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.012258173 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -163        |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0005      |
|    loss                 | -14.5       |
|    n_updates            | 990         |
|    policy_gradient_loss | 0.00743     |
|    std                  | 1.02        |
|    value_loss           | 3.19        |
-----------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 15.26 | Max Reward: 33.23
Iteration: 103 | Episodes: 4200 | Median Reward: 12.57 | Max Reward: 33.23
Iteration: 106 | Episodes: 4300 | Median Reward: 18.56 | Max Reward: 33.23
Iteration: 108 | Episodes: 4400 | Median Reward: 17.33 | Max Reward: 33.23
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -81.8      |
| time/                   |            |
|    fps                  | 178        |
|    iterations           | 110        |
|    time_elapsed         | 2523       |
|    total_timesteps      | 450560     |
| train/                  |            |
|    approx_kl            | 0.00378985 |
|    clip_fraction        | 0.00188    |
|    clip_range           | 0.3        |
|    entropy_loss         | -169       |
|    explained_variance   | 0.975      |
|    learning_rate        | 0.0005     |
|    loss                 | -15.3      |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.00377   |
|    std                  | 1.02       |
|    value_loss           | 6.21       |
----------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 21.84 | Max Reward: 33.23
Iteration: 113 | Episodes: 4600 | Median Reward: 18.48 | Max Reward: 33.23
Iteration: 115 | Episodes: 4700 | Median Reward: 15.35 | Max Reward: 33.23
Iteration: 118 | Episodes: 4800 | Median Reward: 18.93 | Max Reward: 33.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -83.7       |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 120         |
|    time_elapsed         | 2747        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.039554566 |
|    clip_fraction        | 0.0992      |
|    clip_range           | 0.3         |
|    entropy_loss         | -172        |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0005      |
|    loss                 | -15.6       |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0337     |
|    std                  | 1.02        |
|    value_loss           | 5.62        |
-----------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 17.34 | Max Reward: 33.23
Iteration: 123 | Episodes: 5000 | Median Reward: 18.40 | Max Reward: 33.23
Iteration: 125 | Episodes: 5100 | Median Reward: 17.68 | Max Reward: 33.23
Iteration: 128 | Episodes: 5200 | Median Reward: 22.50 | Max Reward: 33.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75.5        |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 130          |
|    time_elapsed         | 2985         |
|    total_timesteps      | 532480       |
| train/                  |              |
|    approx_kl            | 0.0025951578 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -178         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -16.8        |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 1.03         |
|    value_loss           | 3.92         |
------------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 24.86 | Max Reward: 33.23
Iteration: 133 | Episodes: 5400 | Median Reward: 20.03 | Max Reward: 33.23
Iteration: 135 | Episodes: 5500 | Median Reward: 13.90 | Max Reward: 33.23
Iteration: 138 | Episodes: 5600 | Median Reward: 24.71 | Max Reward: 33.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.5        |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 140          |
|    time_elapsed         | 3217         |
|    total_timesteps      | 573440       |
| train/                  |              |
|    approx_kl            | 0.0108436905 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -183         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -15.9        |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00385     |
|    std                  | 1.03         |
|    value_loss           | 2            |
------------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 18.63 | Max Reward: 33.23
Iteration: 143 | Episodes: 5800 | Median Reward: 21.71 | Max Reward: 33.23
Iteration: 145 | Episodes: 5900 | Median Reward: 26.18 | Max Reward: 33.23
Iteration: 147 | Episodes: 6000 | Median Reward: 23.91 | Max Reward: 33.23
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -78.2      |
| time/                   |            |
|    fps                  | 178        |
|    iterations           | 150        |
|    time_elapsed         | 3448       |
|    total_timesteps      | 614400     |
| train/                  |            |
|    approx_kl            | 0.06012601 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.3        |
|    entropy_loss         | -186       |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0005     |
|    loss                 | -17.8      |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0333    |
|    std                  | 1.03       |
|    value_loss           | 2.72       |
----------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 21.35 | Max Reward: 33.23
Iteration: 152 | Episodes: 6200 | Median Reward: 20.74 | Max Reward: 33.23
Iteration: 155 | Episodes: 6300 | Median Reward: 22.53 | Max Reward: 33.23
Iteration: 157 | Episodes: 6400 | Median Reward: 22.65 | Max Reward: 33.23
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -82.1       |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 160         |
|    time_elapsed         | 3677        |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.010592554 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -190        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0005      |
|    loss                 | -18.2       |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.0207     |
|    std                  | 1.03        |
|    value_loss           | 3.71        |
-----------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 15.68 | Max Reward: 33.23
Iteration: 162 | Episodes: 6600 | Median Reward: 23.15 | Max Reward: 33.23
Iteration: 165 | Episodes: 6700 | Median Reward: 22.09 | Max Reward: 33.23
Iteration: 167 | Episodes: 6800 | Median Reward: 22.89 | Max Reward: 33.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.6        |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 170          |
|    time_elapsed         | 3906         |
|    total_timesteps      | 696320       |
| train/                  |              |
|    approx_kl            | 0.0069830487 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -195         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.2        |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.0193      |
|    std                  | 1.04         |
|    value_loss           | 12.4         |
------------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 21.94 | Max Reward: 33.23
Iteration: 172 | Episodes: 7000 | Median Reward: 25.35 | Max Reward: 33.23
Iteration: 175 | Episodes: 7100 | Median Reward: 25.45 | Max Reward: 33.23
Iteration: 177 | Episodes: 7200 | Median Reward: 26.77 | Max Reward: 33.23
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72.8        |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 180          |
|    time_elapsed         | 4137         |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.0026584635 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -196         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -17.5        |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.0038      |
|    std                  | 1.04         |
|    value_loss           | 12.8         |
------------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 29.35 | Max Reward: 33.23
Iteration: 182 | Episodes: 7400 | Median Reward: 21.89 | Max Reward: 33.23
Iteration: 184 | Episodes: 7500 | Median Reward: 23.44 | Max Reward: 33.23
Iteration: 187 | Episodes: 7600 | Median Reward: 23.33 | Max Reward: 33.51
Iteration: 189 | Episodes: 7700 | Median Reward: 23.65 | Max Reward: 33.51
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -77.8      |
| time/                   |            |
|    fps                  | 178        |
|    iterations           | 190        |
|    time_elapsed         | 4367       |
|    total_timesteps      | 778240     |
| train/                  |            |
|    approx_kl            | 0.01238962 |
|    clip_fraction        | 0.000977   |
|    clip_range           | 0.3        |
|    entropy_loss         | -199       |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0005     |
|    loss                 | -19.6      |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.021     |
|    std                  | 1.04       |
|    value_loss           | 1.91       |
----------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 25.29 | Max Reward: 33.51
Iteration: 194 | Episodes: 7900 | Median Reward: 22.96 | Max Reward: 33.51
Iteration: 197 | Episodes: 8000 | Median Reward: 23.78 | Max Reward: 33.51
Iteration: 199 | Episodes: 8100 | Median Reward: 24.80 | Max Reward: 33.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.4       |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 200         |
|    time_elapsed         | 4601        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.006138661 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -202        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0005      |
|    loss                 | -15.8       |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00822    |
|    std                  | 1.05        |
|    value_loss           | 4.74        |
-----------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 28.31 | Max Reward: 33.51
Iteration: 204 | Episodes: 8300 | Median Reward: 26.50 | Max Reward: 33.51
Iteration: 207 | Episodes: 8400 | Median Reward: 29.17 | Max Reward: 33.51
Iteration: 209 | Episodes: 8500 | Median Reward: 25.89 | Max Reward: 33.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74         |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 210         |
|    time_elapsed         | 4822        |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.004596687 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -205        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -19.8       |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00619    |
|    std                  | 1.05        |
|    value_loss           | 0.921       |
-----------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 25.37 | Max Reward: 33.51
Iteration: 214 | Episodes: 8700 | Median Reward: 28.39 | Max Reward: 33.51
Iteration: 216 | Episodes: 8800 | Median Reward: 29.02 | Max Reward: 33.51
Iteration: 219 | Episodes: 8900 | Median Reward: 29.08 | Max Reward: 33.51
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -71.2     |
| time/                   |           |
|    fps                  | 178       |
|    iterations           | 220       |
|    time_elapsed         | 5040      |
|    total_timesteps      | 901120    |
| train/                  |           |
|    approx_kl            | 0.0351968 |
|    clip_fraction        | 0.0743    |
|    clip_range           | 0.3       |
|    entropy_loss         | -209      |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.0005    |
|    loss                 | -15.5     |
|    n_updates            | 2190      |
|    policy_gradient_loss | -0.019    |
|    std                  | 1.05      |
|    value_loss           | 2.6       |
---------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 25.49 | Max Reward: 33.51
Iteration: 224 | Episodes: 9100 | Median Reward: 26.97 | Max Reward: 33.51
Iteration: 226 | Episodes: 9200 | Median Reward: 26.10 | Max Reward: 33.51
Iteration: 229 | Episodes: 9300 | Median Reward: 29.92 | Max Reward: 33.51
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.7        |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 230          |
|    time_elapsed         | 5265         |
|    total_timesteps      | 942080       |
| train/                  |              |
|    approx_kl            | 0.0024948826 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -211         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -19.6        |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00367     |
|    std                  | 1.06         |
|    value_loss           | 2.8          |
------------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 28.95 | Max Reward: 33.51
Iteration: 234 | Episodes: 9500 | Median Reward: 28.99 | Max Reward: 33.51
Iteration: 236 | Episodes: 9600 | Median Reward: 27.88 | Max Reward: 33.51
Iteration: 239 | Episodes: 9700 | Median Reward: 24.25 | Max Reward: 33.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76.8       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 240         |
|    time_elapsed         | 5487        |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.011204073 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -213        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -20.7       |
|    n_updates            | 2390        |
|    policy_gradient_loss | -0.00964    |
|    std                  | 1.06        |
|    value_loss           | 2.31        |
-----------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 28.91 | Max Reward: 33.51
Iteration: 244 | Episodes: 9900 | Median Reward: 26.73 | Max Reward: 33.51
Iteration: 246 | Episodes: 10000 | Median Reward: 26.04 | Max Reward: 33.51
Iteration: 249 | Episodes: 10100 | Median Reward: 29.94 | Max Reward: 33.51
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.6       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 250         |
|    time_elapsed         | 5710        |
|    total_timesteps      | 1024000     |
| train/                  |             |
|    approx_kl            | 0.019855212 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -216        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.2       |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.0471     |
|    std                  | 1.06        |
|    value_loss           | 7.27        |
-----------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 28.93 | Max Reward: 33.51
Iteration: 253 | Episodes: 10300 | Median Reward: 28.94 | Max Reward: 33.51
Iteration: 256 | Episodes: 10400 | Median Reward: 29.03 | Max Reward: 33.51
Iteration: 258 | Episodes: 10500 | Median Reward: 28.94 | Max Reward: 33.51
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.5        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 260          |
|    time_elapsed         | 5941         |
|    total_timesteps      | 1064960      |
| train/                  |              |
|    approx_kl            | 0.0052299676 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -218         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.8        |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 1.07         |
|    value_loss           | 3.6          |
------------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 29.23 | Max Reward: 33.51
Iteration: 263 | Episodes: 10700 | Median Reward: 27.32 | Max Reward: 33.51
Iteration: 266 | Episodes: 10800 | Median Reward: 29.43 | Max Reward: 33.51
Iteration: 268 | Episodes: 10900 | Median Reward: 28.49 | Max Reward: 33.51
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71          |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 270          |
|    time_elapsed         | 6173         |
|    total_timesteps      | 1105920      |
| train/                  |              |
|    approx_kl            | 0.0011324811 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -218         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.4        |
|    n_updates            | 2690         |
|    policy_gradient_loss | 0.00155      |
|    std                  | 1.07         |
|    value_loss           | 3.52         |
------------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 29.11 | Max Reward: 33.51
Iteration: 273 | Episodes: 11100 | Median Reward: 28.45 | Max Reward: 33.51
Iteration: 276 | Episodes: 11200 | Median Reward: 27.46 | Max Reward: 33.55
Iteration: 278 | Episodes: 11300 | Median Reward: 28.73 | Max Reward: 33.55
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -72.2       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 280         |
|    time_elapsed         | 6400        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.034577988 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.3         |
|    entropy_loss         | -219        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -21.3       |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.0261     |
|    std                  | 1.07        |
|    value_loss           | 1.16        |
-----------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 31.41 | Max Reward: 33.58
Iteration: 283 | Episodes: 11500 | Median Reward: 29.62 | Max Reward: 33.58
Iteration: 286 | Episodes: 11600 | Median Reward: 28.99 | Max Reward: 33.58
Iteration: 288 | Episodes: 11700 | Median Reward: 29.61 | Max Reward: 33.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71          |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 290          |
|    time_elapsed         | 6639         |
|    total_timesteps      | 1187840      |
| train/                  |              |
|    approx_kl            | 0.0062463544 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -220         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -21          |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.00407     |
|    std                  | 1.08         |
|    value_loss           | 2.02         |
------------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 29.60 | Max Reward: 33.58
Iteration: 293 | Episodes: 11900 | Median Reward: 31.15 | Max Reward: 33.58
Iteration: 295 | Episodes: 12000 | Median Reward: 30.76 | Max Reward: 33.58
Iteration: 298 | Episodes: 12100 | Median Reward: 32.22 | Max Reward: 33.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.1       |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 300         |
|    time_elapsed         | 6883        |
|    total_timesteps      | 1228800     |
| train/                  |             |
|    approx_kl            | 0.014254872 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -221        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -20.8       |
|    n_updates            | 2990        |
|    policy_gradient_loss | -0.0224     |
|    std                  | 1.08        |
|    value_loss           | 3.94        |
-----------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 29.01 | Max Reward: 33.58
Iteration: 303 | Episodes: 12300 | Median Reward: 29.57 | Max Reward: 33.58
Iteration: 305 | Episodes: 12400 | Median Reward: 30.36 | Max Reward: 33.58
Iteration: 308 | Episodes: 12500 | Median Reward: 31.93 | Max Reward: 33.58
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.9       |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 310         |
|    time_elapsed         | 7114        |
|    total_timesteps      | 1269760     |
| train/                  |             |
|    approx_kl            | 0.005310215 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -222        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -20.3       |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00416    |
|    std                  | 1.08        |
|    value_loss           | 3.91        |
-----------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: 31.63 | Max Reward: 33.58
Iteration: 313 | Episodes: 12700 | Median Reward: 29.35 | Max Reward: 33.58
Iteration: 315 | Episodes: 12800 | Median Reward: 29.64 | Max Reward: 33.58
Iteration: 318 | Episodes: 12900 | Median Reward: 29.04 | Max Reward: 33.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.7        |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 320          |
|    time_elapsed         | 7341         |
|    total_timesteps      | 1310720      |
| train/                  |              |
|    approx_kl            | 5.264778e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -223         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0005       |
|    loss                 | -21.4        |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.000216    |
|    std                  | 1.09         |
|    value_loss           | 5.62         |
------------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: 30.07 | Max Reward: 33.58
Iteration: 323 | Episodes: 13100 | Median Reward: 30.35 | Max Reward: 33.58
Iteration: 325 | Episodes: 13200 | Median Reward: 32.22 | Max Reward: 33.58
Iteration: 327 | Episodes: 13300 | Median Reward: 29.96 | Max Reward: 33.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.9        |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 330          |
|    time_elapsed         | 7575         |
|    total_timesteps      | 1351680      |
| train/                  |              |
|    approx_kl            | 0.0005605549 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -224         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -19.7        |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 1.09         |
|    value_loss           | 3.71         |
------------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: 29.93 | Max Reward: 33.58
Iteration: 332 | Episodes: 13500 | Median Reward: 31.87 | Max Reward: 33.58
Iteration: 335 | Episodes: 13600 | Median Reward: 31.84 | Max Reward: 33.58
Iteration: 337 | Episodes: 13700 | Median Reward: 31.86 | Max Reward: 33.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70           |
| time/                   |               |
|    fps                  | 178           |
|    iterations           | 340           |
|    time_elapsed         | 7803          |
|    total_timesteps      | 1392640       |
| train/                  |               |
|    approx_kl            | 0.00032963976 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -224          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -22.1         |
|    n_updates            | 3390          |
|    policy_gradient_loss | -0.000733     |
|    std                  | 1.1           |
|    value_loss           | 1.74          |
-------------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: 29.98 | Max Reward: 33.58
Iteration: 342 | Episodes: 13900 | Median Reward: 29.97 | Max Reward: 33.58
Iteration: 345 | Episodes: 14000 | Median Reward: 28.93 | Max Reward: 33.58
Iteration: 347 | Episodes: 14100 | Median Reward: 29.98 | Max Reward: 33.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.7        |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 350          |
|    time_elapsed         | 8032         |
|    total_timesteps      | 1433600      |
| train/                  |              |
|    approx_kl            | 0.0040642256 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -225         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.2        |
|    n_updates            | 3490         |
|    policy_gradient_loss | -0.00855     |
|    std                  | 1.1          |
|    value_loss           | 1.98         |
------------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: 29.32 | Max Reward: 33.58
Iteration: 352 | Episodes: 14300 | Median Reward: 29.18 | Max Reward: 33.58
Iteration: 355 | Episodes: 14400 | Median Reward: 29.07 | Max Reward: 33.58
Iteration: 357 | Episodes: 14500 | Median Reward: 29.43 | Max Reward: 33.58
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -72.6      |
| time/                   |            |
|    fps                  | 178        |
|    iterations           | 360        |
|    time_elapsed         | 8254       |
|    total_timesteps      | 1474560    |
| train/                  |            |
|    approx_kl            | 0.01535314 |
|    clip_fraction        | 0.000244   |
|    clip_range           | 0.3        |
|    entropy_loss         | -226       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0005     |
|    loss                 | -21        |
|    n_updates            | 3590       |
|    policy_gradient_loss | -0.0101    |
|    std                  | 1.1        |
|    value_loss           | 2.63       |
----------------------------------------
Iteration: 360 | Episodes: 14600 | Median Reward: 28.03 | Max Reward: 33.58
Iteration: 362 | Episodes: 14700 | Median Reward: 31.69 | Max Reward: 33.58
Iteration: 364 | Episodes: 14800 | Median Reward: 29.98 | Max Reward: 33.58
Iteration: 367 | Episodes: 14900 | Median Reward: 29.74 | Max Reward: 33.58
Iteration: 369 | Episodes: 15000 | Median Reward: 30.92 | Max Reward: 33.58
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.3        |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 370          |
|    time_elapsed         | 8477         |
|    total_timesteps      | 1515520      |
| train/                  |              |
|    approx_kl            | 0.0038516773 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -227         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.4        |
|    n_updates            | 3690         |
|    policy_gradient_loss | -0.00608     |
|    std                  | 1.11         |
|    value_loss           | 1.13         |
------------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: 30.84 | Max Reward: 33.58
Iteration: 374 | Episodes: 15200 | Median Reward: 29.12 | Max Reward: 33.58
Iteration: 377 | Episodes: 15300 | Median Reward: 29.56 | Max Reward: 33.58
Iteration: 379 | Episodes: 15400 | Median Reward: 29.66 | Max Reward: 33.58
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.3         |
| time/                   |               |
|    fps                  | 178           |
|    iterations           | 380           |
|    time_elapsed         | 8704          |
|    total_timesteps      | 1556480       |
| train/                  |               |
|    approx_kl            | 9.2843344e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -228          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -22.1         |
|    n_updates            | 3790          |
|    policy_gradient_loss | -0.000201     |
|    std                  | 1.11          |
|    value_loss           | 7.84          |
-------------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: 32.78 | Max Reward: 33.95
Iteration: 384 | Episodes: 15600 | Median Reward: 28.97 | Max Reward: 33.95
Iteration: 387 | Episodes: 15700 | Median Reward: 32.41 | Max Reward: 33.95
Iteration: 389 | Episodes: 15800 | Median Reward: 32.66 | Max Reward: 33.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.7       |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 390         |
|    time_elapsed         | 8930        |
|    total_timesteps      | 1597440     |
| train/                  |             |
|    approx_kl            | 0.008582395 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -229        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.7       |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.00643    |
|    std                  | 1.11        |
|    value_loss           | 0.734       |
-----------------------------------------
Iteration: 392 | Episodes: 15900 | Median Reward: 29.60 | Max Reward: 33.95
Iteration: 394 | Episodes: 16000 | Median Reward: 30.27 | Max Reward: 33.95
Iteration: 396 | Episodes: 16100 | Median Reward: 30.22 | Max Reward: 33.95
Iteration: 399 | Episodes: 16200 | Median Reward: 32.37 | Max Reward: 33.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70         |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 400         |
|    time_elapsed         | 9155        |
|    total_timesteps      | 1638400     |
| train/                  |             |
|    approx_kl            | 0.001705193 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -229        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.3       |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.00204    |
|    std                  | 1.12        |
|    value_loss           | 1.16        |
-----------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: 29.88 | Max Reward: 33.95
Iteration: 404 | Episodes: 16400 | Median Reward: 30.17 | Max Reward: 33.95
Iteration: 406 | Episodes: 16500 | Median Reward: 29.88 | Max Reward: 33.95
Iteration: 409 | Episodes: 16600 | Median Reward: 29.94 | Max Reward: 33.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.8        |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 410          |
|    time_elapsed         | 9386         |
|    total_timesteps      | 1679360      |
| train/                  |              |
|    approx_kl            | 0.0034686413 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -230         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.3        |
|    n_updates            | 4090         |
|    policy_gradient_loss | -0.00909     |
|    std                  | 1.12         |
|    value_loss           | 1.2          |
------------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: 31.62 | Max Reward: 33.95
Iteration: 414 | Episodes: 16800 | Median Reward: 30.28 | Max Reward: 33.95
Iteration: 416 | Episodes: 16900 | Median Reward: 30.76 | Max Reward: 33.95
Iteration: 419 | Episodes: 17000 | Median Reward: 29.42 | Max Reward: 33.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70          |
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 420          |
|    time_elapsed         | 9612         |
|    total_timesteps      | 1720320      |
| train/                  |              |
|    approx_kl            | 0.0023863162 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -230         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.8        |
|    n_updates            | 4190         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 1.13         |
|    value_loss           | 1.16         |
------------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: 28.95 | Max Reward: 33.95
Iteration: 424 | Episodes: 17200 | Median Reward: 26.22 | Max Reward: 33.95
Iteration: 426 | Episodes: 17300 | Median Reward: 31.76 | Max Reward: 33.95
Iteration: 429 | Episodes: 17400 | Median Reward: 32.08 | Max Reward: 33.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.3        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 430          |
|    time_elapsed         | 9831         |
|    total_timesteps      | 1761280      |
| train/                  |              |
|    approx_kl            | 0.0021410217 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -232         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.3        |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 1.13         |
|    value_loss           | 2.34         |
------------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: 31.87 | Max Reward: 33.95
Iteration: 433 | Episodes: 17600 | Median Reward: 32.71 | Max Reward: 33.95
Iteration: 436 | Episodes: 17700 | Median Reward: 29.86 | Max Reward: 33.95
Iteration: 438 | Episodes: 17800 | Median Reward: 29.94 | Max Reward: 33.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -72.3       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 440         |
|    time_elapsed         | 10063       |
|    total_timesteps      | 1802240     |
| train/                  |             |
|    approx_kl            | 0.008910413 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -232        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.7       |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0117     |
|    std                  | 1.13        |
|    value_loss           | 2.58        |
-----------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 28.98 | Max Reward: 33.95
Iteration: 443 | Episodes: 18000 | Median Reward: 28.74 | Max Reward: 33.95
Iteration: 446 | Episodes: 18100 | Median Reward: 32.40 | Max Reward: 33.95
Iteration: 448 | Episodes: 18200 | Median Reward: 29.89 | Max Reward: 33.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.6         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 450           |
|    time_elapsed         | 10287         |
|    total_timesteps      | 1843200       |
| train/                  |               |
|    approx_kl            | 0.00059375167 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -233          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -23.1         |
|    n_updates            | 4490          |
|    policy_gradient_loss | -0.000289     |
|    std                  | 1.14          |
|    value_loss           | 0.456         |
-------------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 30.94 | Max Reward: 33.95
Iteration: 453 | Episodes: 18400 | Median Reward: 29.80 | Max Reward: 33.95
Iteration: 456 | Episodes: 18500 | Median Reward: 30.10 | Max Reward: 33.95
Iteration: 458 | Episodes: 18600 | Median Reward: 32.43 | Max Reward: 33.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.3       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 460         |
|    time_elapsed         | 10509       |
|    total_timesteps      | 1884160     |
| train/                  |             |
|    approx_kl            | 0.037197337 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -234        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.7       |
|    n_updates            | 4590        |
|    policy_gradient_loss | -0.0179     |
|    std                  | 1.14        |
|    value_loss           | 1           |
-----------------------------------------
Iteration: 461 | Episodes: 18700 | Median Reward: 30.21 | Max Reward: 33.95
Iteration: 463 | Episodes: 18800 | Median Reward: 29.39 | Max Reward: 33.95
Iteration: 466 | Episodes: 18900 | Median Reward: 31.82 | Max Reward: 33.95
Iteration: 468 | Episodes: 19000 | Median Reward: 30.07 | Max Reward: 33.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69          |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 470          |
|    time_elapsed         | 10734        |
|    total_timesteps      | 1925120      |
| train/                  |              |
|    approx_kl            | 0.0023399435 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -235         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.1        |
|    n_updates            | 4690         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 1.15         |
|    value_loss           | 1.68         |
------------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 31.82 | Max Reward: 33.95
Iteration: 473 | Episodes: 19200 | Median Reward: 31.35 | Max Reward: 33.95
Iteration: 475 | Episodes: 19300 | Median Reward: 29.92 | Max Reward: 33.95
Iteration: 478 | Episodes: 19400 | Median Reward: 31.37 | Max Reward: 33.95
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -71.6      |
| time/                   |            |
|    fps                  | 179        |
|    iterations           | 480        |
|    time_elapsed         | 10960      |
|    total_timesteps      | 1966080    |
| train/                  |            |
|    approx_kl            | 0.02344535 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -236       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -23.1      |
|    n_updates            | 4790       |
|    policy_gradient_loss | -0.0209    |
|    std                  | 1.15       |
|    value_loss           | 1.44       |
----------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 28.85 | Max Reward: 33.95
Iteration: 483 | Episodes: 19600 | Median Reward: 31.56 | Max Reward: 33.95
Iteration: 485 | Episodes: 19700 | Median Reward: 31.56 | Max Reward: 33.95
Iteration: 488 | Episodes: 19800 | Median Reward: 30.24 | Max Reward: 33.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.9        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 490          |
|    time_elapsed         | 11183        |
|    total_timesteps      | 2007040      |
| train/                  |              |
|    approx_kl            | 0.0022406355 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -237         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -23.5        |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 1.16         |
|    value_loss           | 1.25         |
------------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 30.19 | Max Reward: 33.95
Iteration: 493 | Episodes: 20000 | Median Reward: 29.04 | Max Reward: 33.95
Iteration: 495 | Episodes: 20100 | Median Reward: 31.85 | Max Reward: 33.95
Iteration: 498 | Episodes: 20200 | Median Reward: 31.94 | Max Reward: 33.95
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.3        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 500          |
|    time_elapsed         | 11410        |
|    total_timesteps      | 2048000      |
| train/                  |              |
|    approx_kl            | 0.0009528999 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -237         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -22          |
|    n_updates            | 4990         |
|    policy_gradient_loss | -0.00252     |
|    std                  | 1.16         |
|    value_loss           | 2.3          |
------------------------------------------
Iteration: 500 | Episodes: 20300 | Median Reward: 31.96 | Max Reward: 33.95
Iteration: 503 | Episodes: 20400 | Median Reward: 31.90 | Max Reward: 33.95
Iteration: 505 | Episodes: 20500 | Median Reward: 29.26 | Max Reward: 33.95
Iteration: 507 | Episodes: 20600 | Median Reward: 31.36 | Max Reward: 33.95
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.9       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 510         |
|    time_elapsed         | 11634       |
|    total_timesteps      | 2088960     |
| train/                  |             |
|    approx_kl            | 0.004588049 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -238        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -22         |
|    n_updates            | 5090        |
|    policy_gradient_loss | -0.00127    |
|    std                  | 1.16        |
|    value_loss           | 2.07        |
-----------------------------------------
Iteration: 510 | Episodes: 20700 | Median Reward: 32.96 | Max Reward: 33.95
Iteration: 512 | Episodes: 20800 | Median Reward: 32.07 | Max Reward: 33.95
Iteration: 515 | Episodes: 20900 | Median Reward: 30.80 | Max Reward: 33.95
Iteration: 517 | Episodes: 21000 | Median Reward: 32.76 | Max Reward: 33.95
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.3         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 520           |
|    time_elapsed         | 11859         |
|    total_timesteps      | 2129920       |
| train/                  |               |
|    approx_kl            | 0.00088429457 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -239          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -23.7         |
|    n_updates            | 5190          |
|    policy_gradient_loss | -0.00119      |
|    std                  | 1.17          |
|    value_loss           | 1.34          |
-------------------------------------------
Iteration: 520 | Episodes: 21100 | Median Reward: 32.85 | Max Reward: 34.24
Iteration: 522 | Episodes: 21200 | Median Reward: 32.58 | Max Reward: 34.24
Iteration: 525 | Episodes: 21300 | Median Reward: 31.87 | Max Reward: 34.24
Iteration: 527 | Episodes: 21400 | Median Reward: 30.95 | Max Reward: 34.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -70.4      |
| time/                   |            |
|    fps                  | 179        |
|    iterations           | 530        |
|    time_elapsed         | 12087      |
|    total_timesteps      | 2170880    |
| train/                  |            |
|    approx_kl            | 0.11443137 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.3        |
|    entropy_loss         | -239       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -23.4      |
|    n_updates            | 5290       |
|    policy_gradient_loss | -0.0743    |
|    std                  | 1.17       |
|    value_loss           | 1.29       |
----------------------------------------
Iteration: 530 | Episodes: 21500 | Median Reward: 29.56 | Max Reward: 34.24
Iteration: 532 | Episodes: 21600 | Median Reward: 31.30 | Max Reward: 34.24
Iteration: 535 | Episodes: 21700 | Median Reward: 32.24 | Max Reward: 34.24
Iteration: 537 | Episodes: 21800 | Median Reward: 31.90 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70          |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 540          |
|    time_elapsed         | 12312        |
|    total_timesteps      | 2211840      |
| train/                  |              |
|    approx_kl            | 0.0033344603 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -240         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24          |
|    n_updates            | 5390         |
|    policy_gradient_loss | -0.00533     |
|    std                  | 1.18         |
|    value_loss           | 1.1          |
------------------------------------------
Iteration: 540 | Episodes: 21900 | Median Reward: 29.38 | Max Reward: 34.24
Iteration: 542 | Episodes: 22000 | Median Reward: 31.35 | Max Reward: 34.24
Iteration: 544 | Episodes: 22100 | Median Reward: 32.01 | Max Reward: 34.24
Iteration: 547 | Episodes: 22200 | Median Reward: 31.84 | Max Reward: 34.24
Iteration: 549 | Episodes: 22300 | Median Reward: 30.28 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.5         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 550           |
|    time_elapsed         | 12536         |
|    total_timesteps      | 2252800       |
| train/                  |               |
|    approx_kl            | 0.00017219763 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -241          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -23.6         |
|    n_updates            | 5490          |
|    policy_gradient_loss | -8.07e-05     |
|    std                  | 1.18          |
|    value_loss           | 1             |
-------------------------------------------
Iteration: 552 | Episodes: 22400 | Median Reward: 31.75 | Max Reward: 34.24
Iteration: 554 | Episodes: 22500 | Median Reward: 31.41 | Max Reward: 34.24
Iteration: 557 | Episodes: 22600 | Median Reward: 32.16 | Max Reward: 34.24
Iteration: 559 | Episodes: 22700 | Median Reward: 31.96 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.7       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 560         |
|    time_elapsed         | 12760       |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.005668645 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -241        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -23.8       |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.00905    |
|    std                  | 1.19        |
|    value_loss           | 0.843       |
-----------------------------------------
Iteration: 562 | Episodes: 22800 | Median Reward: 29.92 | Max Reward: 34.24
Iteration: 564 | Episodes: 22900 | Median Reward: 31.85 | Max Reward: 34.24
Iteration: 567 | Episodes: 23000 | Median Reward: 32.63 | Max Reward: 34.24
Iteration: 569 | Episodes: 23100 | Median Reward: 31.86 | Max Reward: 34.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -69        |
| time/                   |            |
|    fps                  | 179        |
|    iterations           | 570        |
|    time_elapsed         | 12991      |
|    total_timesteps      | 2334720    |
| train/                  |            |
|    approx_kl            | 0.02303962 |
|    clip_fraction        | 0.025      |
|    clip_range           | 0.3        |
|    entropy_loss         | -242       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -24        |
|    n_updates            | 5690       |
|    policy_gradient_loss | -0.0181    |
|    std                  | 1.19       |
|    value_loss           | 0.554      |
----------------------------------------
Iteration: 572 | Episodes: 23200 | Median Reward: 29.99 | Max Reward: 34.24
Iteration: 574 | Episodes: 23300 | Median Reward: 31.70 | Max Reward: 34.24
Iteration: 577 | Episodes: 23400 | Median Reward: 29.96 | Max Reward: 34.24
Iteration: 579 | Episodes: 23500 | Median Reward: 32.12 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.1       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 580         |
|    time_elapsed         | 13217       |
|    total_timesteps      | 2375680     |
| train/                  |             |
|    approx_kl            | 0.007215584 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -242        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -23         |
|    n_updates            | 5790        |
|    policy_gradient_loss | -0.0173     |
|    std                  | 1.2         |
|    value_loss           | 1.04        |
-----------------------------------------
Iteration: 581 | Episodes: 23600 | Median Reward: 32.17 | Max Reward: 34.24
Iteration: 584 | Episodes: 23700 | Median Reward: 32.53 | Max Reward: 34.24
Iteration: 586 | Episodes: 23800 | Median Reward: 32.01 | Max Reward: 34.24
Iteration: 589 | Episodes: 23900 | Median Reward: 31.56 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69           |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 590           |
|    time_elapsed         | 13442         |
|    total_timesteps      | 2416640       |
| train/                  |               |
|    approx_kl            | 0.00061177765 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -243          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -24.1         |
|    n_updates            | 5890          |
|    policy_gradient_loss | -0.00262      |
|    std                  | 1.2           |
|    value_loss           | 3.32          |
-------------------------------------------
Iteration: 591 | Episodes: 24000 | Median Reward: 30.14 | Max Reward: 34.24
Iteration: 594 | Episodes: 24100 | Median Reward: 32.45 | Max Reward: 34.24
Iteration: 596 | Episodes: 24200 | Median Reward: 31.87 | Max Reward: 34.24
Iteration: 599 | Episodes: 24300 | Median Reward: 32.26 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.2       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 600         |
|    time_elapsed         | 13669       |
|    total_timesteps      | 2457600     |
| train/                  |             |
|    approx_kl            | 0.044696327 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.3         |
|    entropy_loss         | -244        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.6       |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.0299     |
|    std                  | 1.2         |
|    value_loss           | 2.22        |
-----------------------------------------
Iteration: 601 | Episodes: 24400 | Median Reward: 32.43 | Max Reward: 34.24
Iteration: 604 | Episodes: 24500 | Median Reward: 31.37 | Max Reward: 34.24
Iteration: 606 | Episodes: 24600 | Median Reward: 31.86 | Max Reward: 34.24
Iteration: 609 | Episodes: 24700 | Median Reward: 31.86 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.9        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 610          |
|    time_elapsed         | 13888        |
|    total_timesteps      | 2498560      |
| train/                  |              |
|    approx_kl            | 0.0035010255 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -245         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.5        |
|    n_updates            | 6090         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 1.21         |
|    value_loss           | 4.02         |
------------------------------------------
Iteration: 611 | Episodes: 24800 | Median Reward: 32.64 | Max Reward: 34.24
Iteration: 613 | Episodes: 24900 | Median Reward: 31.75 | Max Reward: 34.24
Iteration: 616 | Episodes: 25000 | Median Reward: 31.99 | Max Reward: 34.24
Iteration: 618 | Episodes: 25100 | Median Reward: 32.87 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.5       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 620         |
|    time_elapsed         | 14118       |
|    total_timesteps      | 2539520     |
| train/                  |             |
|    approx_kl            | 0.011677475 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -246        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -24.5       |
|    n_updates            | 6190        |
|    policy_gradient_loss | -0.0205     |
|    std                  | 1.21        |
|    value_loss           | 0.653       |
-----------------------------------------
Iteration: 621 | Episodes: 25200 | Median Reward: 31.59 | Max Reward: 34.24
Iteration: 623 | Episodes: 25300 | Median Reward: 30.19 | Max Reward: 34.24
Iteration: 626 | Episodes: 25400 | Median Reward: 30.94 | Max Reward: 34.24
Iteration: 628 | Episodes: 25500 | Median Reward: 32.96 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.5         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 630           |
|    time_elapsed         | 14345         |
|    total_timesteps      | 2580480       |
| train/                  |               |
|    approx_kl            | 0.00012361932 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -246          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -23.6         |
|    n_updates            | 6290          |
|    policy_gradient_loss | 9.02e-05      |
|    std                  | 1.22          |
|    value_loss           | 1.07          |
-------------------------------------------
Iteration: 631 | Episodes: 25600 | Median Reward: 32.67 | Max Reward: 34.24
Iteration: 633 | Episodes: 25700 | Median Reward: 30.43 | Max Reward: 34.24
Iteration: 636 | Episodes: 25800 | Median Reward: 32.70 | Max Reward: 34.24
Iteration: 638 | Episodes: 25900 | Median Reward: 32.42 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.8       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 640         |
|    time_elapsed         | 14569       |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.000610622 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -247        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -22.8       |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.00334    |
|    std                  | 1.22        |
|    value_loss           | 5           |
-----------------------------------------
Iteration: 641 | Episodes: 26000 | Median Reward: 29.97 | Max Reward: 34.24
Iteration: 643 | Episodes: 26100 | Median Reward: 32.73 | Max Reward: 34.24
Iteration: 646 | Episodes: 26200 | Median Reward: 32.72 | Max Reward: 34.24
Iteration: 648 | Episodes: 26300 | Median Reward: 31.65 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.7       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 650         |
|    time_elapsed         | 14795       |
|    total_timesteps      | 2662400     |
| train/                  |             |
|    approx_kl            | 0.003667105 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -248        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.6       |
|    n_updates            | 6490        |
|    policy_gradient_loss | -0.000328   |
|    std                  | 1.23        |
|    value_loss           | 2.17        |
-----------------------------------------
Iteration: 650 | Episodes: 26400 | Median Reward: 32.21 | Max Reward: 34.24
Iteration: 653 | Episodes: 26500 | Median Reward: 32.45 | Max Reward: 34.24
Iteration: 655 | Episodes: 26600 | Median Reward: 32.62 | Max Reward: 34.24
Iteration: 658 | Episodes: 26700 | Median Reward: 32.87 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.8        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 660          |
|    time_elapsed         | 15023        |
|    total_timesteps      | 2703360      |
| train/                  |              |
|    approx_kl            | 0.0018316305 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -249         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.9        |
|    n_updates            | 6590         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 1.23         |
|    value_loss           | 0.597        |
------------------------------------------
Iteration: 660 | Episodes: 26800 | Median Reward: 31.85 | Max Reward: 34.24
Iteration: 663 | Episodes: 26900 | Median Reward: 31.87 | Max Reward: 34.24
Iteration: 665 | Episodes: 27000 | Median Reward: 32.77 | Max Reward: 34.24
Iteration: 668 | Episodes: 27100 | Median Reward: 32.93 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68          |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 670          |
|    time_elapsed         | 15247        |
|    total_timesteps      | 2744320      |
| train/                  |              |
|    approx_kl            | 0.0031187502 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.3        |
|    n_updates            | 6690         |
|    policy_gradient_loss | -0.00653     |
|    std                  | 1.24         |
|    value_loss           | 2.31         |
------------------------------------------
Iteration: 670 | Episodes: 27200 | Median Reward: 32.51 | Max Reward: 34.24
Iteration: 673 | Episodes: 27300 | Median Reward: 32.66 | Max Reward: 34.24
Iteration: 675 | Episodes: 27400 | Median Reward: 32.63 | Max Reward: 34.24
Iteration: 678 | Episodes: 27500 | Median Reward: 32.72 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.6         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 680           |
|    time_elapsed         | 15476         |
|    total_timesteps      | 2785280       |
| train/                  |               |
|    approx_kl            | 0.00017788114 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -251          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -22.9         |
|    n_updates            | 6790          |
|    policy_gradient_loss | -0.000552     |
|    std                  | 1.25          |
|    value_loss           | 4.45          |
-------------------------------------------
Iteration: 680 | Episodes: 27600 | Median Reward: 32.56 | Max Reward: 34.24
Iteration: 683 | Episodes: 27700 | Median Reward: 32.91 | Max Reward: 34.24
Iteration: 685 | Episodes: 27800 | Median Reward: 32.85 | Max Reward: 34.24
Iteration: 687 | Episodes: 27900 | Median Reward: 29.91 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.2       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 690         |
|    time_elapsed         | 15705       |
|    total_timesteps      | 2826240     |
| train/                  |             |
|    approx_kl            | 0.002762883 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -252        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.3       |
|    n_updates            | 6890        |
|    policy_gradient_loss | -0.000767   |
|    std                  | 1.25        |
|    value_loss           | 1.74        |
-----------------------------------------
Iteration: 690 | Episodes: 28000 | Median Reward: 32.56 | Max Reward: 34.24
Iteration: 692 | Episodes: 28100 | Median Reward: 32.83 | Max Reward: 34.24
Iteration: 695 | Episodes: 28200 | Median Reward: 32.66 | Max Reward: 34.24
Iteration: 697 | Episodes: 28300 | Median Reward: 32.95 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.7       |
| time/                   |             |
|    fps                  | 180         |
|    iterations           | 700         |
|    time_elapsed         | 15926       |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.008264382 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -253        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -25.2       |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.0109     |
|    std                  | 1.25        |
|    value_loss           | 0.513       |
-----------------------------------------
Iteration: 700 | Episodes: 28400 | Median Reward: 32.13 | Max Reward: 34.24
Iteration: 702 | Episodes: 28500 | Median Reward: 32.89 | Max Reward: 34.24
Iteration: 705 | Episodes: 28600 | Median Reward: 32.19 | Max Reward: 34.24
Iteration: 707 | Episodes: 28700 | Median Reward: 32.93 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.4        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 710          |
|    time_elapsed         | 16146        |
|    total_timesteps      | 2908160      |
| train/                  |              |
|    approx_kl            | 0.0022975462 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -253         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -23.7        |
|    n_updates            | 7090         |
|    policy_gradient_loss | -0.0125      |
|    std                  | 1.26         |
|    value_loss           | 4.34         |
------------------------------------------
Iteration: 710 | Episodes: 28800 | Median Reward: 32.76 | Max Reward: 34.24
Iteration: 712 | Episodes: 28900 | Median Reward: 32.74 | Max Reward: 34.24
Iteration: 715 | Episodes: 29000 | Median Reward: 33.07 | Max Reward: 34.24
Iteration: 717 | Episodes: 29100 | Median Reward: 33.03 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.9        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 720          |
|    time_elapsed         | 16369        |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0035435162 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -254         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.8        |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.00269     |
|    std                  | 1.26         |
|    value_loss           | 2.1          |
------------------------------------------
Iteration: 720 | Episodes: 29200 | Median Reward: 32.49 | Max Reward: 34.24
Iteration: 722 | Episodes: 29300 | Median Reward: 32.42 | Max Reward: 34.24
Iteration: 724 | Episodes: 29400 | Median Reward: 32.70 | Max Reward: 34.24
Iteration: 727 | Episodes: 29500 | Median Reward: 31.59 | Max Reward: 34.24
Iteration: 729 | Episodes: 29600 | Median Reward: 31.89 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.7        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 730          |
|    time_elapsed         | 16594        |
|    total_timesteps      | 2990080      |
| train/                  |              |
|    approx_kl            | 0.0003797352 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -254         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.3        |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.000839    |
|    std                  | 1.27         |
|    value_loss           | 1.01         |
------------------------------------------
Iteration: 732 | Episodes: 29700 | Median Reward: 32.98 | Max Reward: 34.24
Iteration: 734 | Episodes: 29800 | Median Reward: 32.51 | Max Reward: 34.24
Iteration: 737 | Episodes: 29900 | Median Reward: 32.44 | Max Reward: 34.24
Iteration: 739 | Episodes: 30000 | Median Reward: 32.45 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.5        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 740          |
|    time_elapsed         | 16822        |
|    total_timesteps      | 3031040      |
| train/                  |              |
|    approx_kl            | 0.0005502183 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -255         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -25          |
|    n_updates            | 7390         |
|    policy_gradient_loss | 0.000319     |
|    std                  | 1.27         |
|    value_loss           | 1.47         |
------------------------------------------
Iteration: 742 | Episodes: 30100 | Median Reward: 32.47 | Max Reward: 34.24
Iteration: 744 | Episodes: 30200 | Median Reward: 32.90 | Max Reward: 34.24
Iteration: 747 | Episodes: 30300 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 749 | Episodes: 30400 | Median Reward: 32.42 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.4        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 750          |
|    time_elapsed         | 17045        |
|    total_timesteps      | 3072000      |
| train/                  |              |
|    approx_kl            | 0.0008883575 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -255         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.4        |
|    n_updates            | 7490         |
|    policy_gradient_loss | -0.00172     |
|    std                  | 1.28         |
|    value_loss           | 0.599        |
------------------------------------------
Iteration: 752 | Episodes: 30500 | Median Reward: 32.87 | Max Reward: 34.24
Iteration: 754 | Episodes: 30600 | Median Reward: 32.87 | Max Reward: 34.24
Iteration: 757 | Episodes: 30700 | Median Reward: 32.72 | Max Reward: 34.24
Iteration: 759 | Episodes: 30800 | Median Reward: 33.02 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.3        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 760          |
|    time_elapsed         | 17265        |
|    total_timesteps      | 3112960      |
| train/                  |              |
|    approx_kl            | 0.0012057193 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -256         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.8        |
|    n_updates            | 7590         |
|    policy_gradient_loss | -0.00203     |
|    std                  | 1.28         |
|    value_loss           | 0.978        |
------------------------------------------
Iteration: 761 | Episodes: 30900 | Median Reward: 32.83 | Max Reward: 34.24
Iteration: 764 | Episodes: 31000 | Median Reward: 30.19 | Max Reward: 34.24
Iteration: 766 | Episodes: 31100 | Median Reward: 31.88 | Max Reward: 34.24
Iteration: 769 | Episodes: 31200 | Median Reward: 32.95 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.5         |
| time/                   |               |
|    fps                  | 180           |
|    iterations           | 770           |
|    time_elapsed         | 17485         |
|    total_timesteps      | 3153920       |
| train/                  |               |
|    approx_kl            | 3.3575838e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -257          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -25.6         |
|    n_updates            | 7690          |
|    policy_gradient_loss | -1.67e-05     |
|    std                  | 1.29          |
|    value_loss           | 0.437         |
-------------------------------------------
Iteration: 771 | Episodes: 31300 | Median Reward: 32.67 | Max Reward: 34.24
Iteration: 774 | Episodes: 31400 | Median Reward: 32.99 | Max Reward: 34.24
Iteration: 776 | Episodes: 31500 | Median Reward: 32.74 | Max Reward: 34.24
Iteration: 779 | Episodes: 31600 | Median Reward: 30.21 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.7        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 780          |
|    time_elapsed         | 17709        |
|    total_timesteps      | 3194880      |
| train/                  |              |
|    approx_kl            | 0.0014067132 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -258         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.5        |
|    n_updates            | 7790         |
|    policy_gradient_loss | -0.00266     |
|    std                  | 1.29         |
|    value_loss           | 0.949        |
------------------------------------------
Iteration: 781 | Episodes: 31700 | Median Reward: 31.93 | Max Reward: 34.24
Iteration: 784 | Episodes: 31800 | Median Reward: 32.78 | Max Reward: 34.24
Iteration: 786 | Episodes: 31900 | Median Reward: 31.92 | Max Reward: 34.24
Iteration: 789 | Episodes: 32000 | Median Reward: 32.87 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.7         |
| time/                   |               |
|    fps                  | 180           |
|    iterations           | 790           |
|    time_elapsed         | 17929         |
|    total_timesteps      | 3235840       |
| train/                  |               |
|    approx_kl            | 0.00082891964 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -259          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -25.7         |
|    n_updates            | 7890          |
|    policy_gradient_loss | -0.000449     |
|    std                  | 1.3           |
|    value_loss           | 1.04          |
-------------------------------------------
Iteration: 791 | Episodes: 32100 | Median Reward: 32.76 | Max Reward: 34.24
Iteration: 793 | Episodes: 32200 | Median Reward: 33.07 | Max Reward: 34.24
Iteration: 796 | Episodes: 32300 | Median Reward: 32.46 | Max Reward: 34.24
Iteration: 798 | Episodes: 32400 | Median Reward: 32.40 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.8         |
| time/                   |               |
|    fps                  | 180           |
|    iterations           | 800           |
|    time_elapsed         | 18149         |
|    total_timesteps      | 3276800       |
| train/                  |               |
|    approx_kl            | 0.00059536635 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -259          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -24.1         |
|    n_updates            | 7990          |
|    policy_gradient_loss | -0.000859     |
|    std                  | 1.3           |
|    value_loss           | 1.63          |
-------------------------------------------
Iteration: 801 | Episodes: 32500 | Median Reward: 32.46 | Max Reward: 34.24
Iteration: 803 | Episodes: 32600 | Median Reward: 32.77 | Max Reward: 34.24
Iteration: 806 | Episodes: 32700 | Median Reward: 32.89 | Max Reward: 34.24
Iteration: 808 | Episodes: 32800 | Median Reward: 32.98 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.3         |
| time/                   |               |
|    fps                  | 180           |
|    iterations           | 810           |
|    time_elapsed         | 18366         |
|    total_timesteps      | 3317760       |
| train/                  |               |
|    approx_kl            | 0.00043147156 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -260          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -25.3         |
|    n_updates            | 8090          |
|    policy_gradient_loss | 0.000292      |
|    std                  | 1.31          |
|    value_loss           | 1.36          |
-------------------------------------------
Iteration: 811 | Episodes: 32900 | Median Reward: 29.89 | Max Reward: 34.24
Iteration: 813 | Episodes: 33000 | Median Reward: 32.78 | Max Reward: 34.24
Iteration: 816 | Episodes: 33100 | Median Reward: 32.76 | Max Reward: 34.24
Iteration: 818 | Episodes: 33200 | Median Reward: 33.07 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.3        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 820          |
|    time_elapsed         | 18596        |
|    total_timesteps      | 3358720      |
| train/                  |              |
|    approx_kl            | 9.376103e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -260         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.8        |
|    n_updates            | 8190         |
|    policy_gradient_loss | -0.000221    |
|    std                  | 1.32         |
|    value_loss           | 0.855        |
------------------------------------------
Iteration: 821 | Episodes: 33300 | Median Reward: 31.92 | Max Reward: 34.24
Iteration: 823 | Episodes: 33400 | Median Reward: 32.67 | Max Reward: 34.24
Iteration: 826 | Episodes: 33500 | Median Reward: 32.46 | Max Reward: 34.24
Iteration: 828 | Episodes: 33600 | Median Reward: 33.07 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.1       |
| time/                   |             |
|    fps                  | 180         |
|    iterations           | 830         |
|    time_elapsed         | 18828       |
|    total_timesteps      | 3399680     |
| train/                  |             |
|    approx_kl            | 0.021145493 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -261        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -26.1       |
|    n_updates            | 8290        |
|    policy_gradient_loss | -0.0172     |
|    std                  | 1.32        |
|    value_loss           | 0.3         |
-----------------------------------------
Iteration: 830 | Episodes: 33700 | Median Reward: 32.94 | Max Reward: 34.24
Iteration: 833 | Episodes: 33800 | Median Reward: 32.39 | Max Reward: 34.24
Iteration: 835 | Episodes: 33900 | Median Reward: 33.07 | Max Reward: 34.24
Iteration: 838 | Episodes: 34000 | Median Reward: 32.89 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.8        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 840          |
|    time_elapsed         | 19058        |
|    total_timesteps      | 3440640      |
| train/                  |              |
|    approx_kl            | 0.0020445741 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -262         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.7        |
|    n_updates            | 8390         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 1.33         |
|    value_loss           | 0.681        |
------------------------------------------
Iteration: 840 | Episodes: 34100 | Median Reward: 32.74 | Max Reward: 34.24
Iteration: 843 | Episodes: 34200 | Median Reward: 33.01 | Max Reward: 34.24
Iteration: 845 | Episodes: 34300 | Median Reward: 33.13 | Max Reward: 34.24
Iteration: 848 | Episodes: 34400 | Median Reward: 32.78 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.3        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 850          |
|    time_elapsed         | 19292        |
|    total_timesteps      | 3481600      |
| train/                  |              |
|    approx_kl            | 0.0030857837 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -263         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -22.4        |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.00415     |
|    std                  | 1.34         |
|    value_loss           | 2.31         |
------------------------------------------
Iteration: 850 | Episodes: 34500 | Median Reward: 32.58 | Max Reward: 34.24
Iteration: 853 | Episodes: 34600 | Median Reward: 32.44 | Max Reward: 34.24
Iteration: 855 | Episodes: 34700 | Median Reward: 32.72 | Max Reward: 34.24
Iteration: 858 | Episodes: 34800 | Median Reward: 32.64 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.6        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 860          |
|    time_elapsed         | 19522        |
|    total_timesteps      | 3522560      |
| train/                  |              |
|    approx_kl            | 0.0034153026 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -264         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26          |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.00561     |
|    std                  | 1.34         |
|    value_loss           | 0.746        |
------------------------------------------
Iteration: 860 | Episodes: 34900 | Median Reward: 32.78 | Max Reward: 34.24
Iteration: 863 | Episodes: 35000 | Median Reward: 32.73 | Max Reward: 34.24
Iteration: 865 | Episodes: 35100 | Median Reward: 32.32 | Max Reward: 34.24
Iteration: 867 | Episodes: 35200 | Median Reward: 32.93 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.1        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 870          |
|    time_elapsed         | 19756        |
|    total_timesteps      | 3563520      |
| train/                  |              |
|    approx_kl            | 0.0010972231 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -264         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.1        |
|    n_updates            | 8690         |
|    policy_gradient_loss | -0.000857    |
|    std                  | 1.35         |
|    value_loss           | 0.873        |
------------------------------------------
Iteration: 870 | Episodes: 35300 | Median Reward: 32.91 | Max Reward: 34.24
Iteration: 872 | Episodes: 35400 | Median Reward: 32.95 | Max Reward: 34.24
Iteration: 875 | Episodes: 35500 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 877 | Episodes: 35600 | Median Reward: 32.90 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68         |
| time/                   |             |
|    fps                  | 180         |
|    iterations           | 880         |
|    time_elapsed         | 19983       |
|    total_timesteps      | 3604480     |
| train/                  |             |
|    approx_kl            | 0.004898312 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -265        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -26.4       |
|    n_updates            | 8790        |
|    policy_gradient_loss | -0.00364    |
|    std                  | 1.36        |
|    value_loss           | 0.419       |
-----------------------------------------
Iteration: 880 | Episodes: 35700 | Median Reward: 32.87 | Max Reward: 34.24
Iteration: 882 | Episodes: 35800 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 885 | Episodes: 35900 | Median Reward: 33.12 | Max Reward: 34.24
Iteration: 887 | Episodes: 36000 | Median Reward: 32.89 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.2       |
| time/                   |             |
|    fps                  | 180         |
|    iterations           | 890         |
|    time_elapsed         | 20213       |
|    total_timesteps      | 3645440     |
| train/                  |             |
|    approx_kl            | 0.011196241 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -266        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -26.5       |
|    n_updates            | 8890        |
|    policy_gradient_loss | -0.0108     |
|    std                  | 1.37        |
|    value_loss           | 0.384       |
-----------------------------------------
Iteration: 890 | Episodes: 36100 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 892 | Episodes: 36200 | Median Reward: 32.88 | Max Reward: 34.24
Iteration: 895 | Episodes: 36300 | Median Reward: 32.83 | Max Reward: 34.24
Iteration: 897 | Episodes: 36400 | Median Reward: 33.10 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.7       |
| time/                   |             |
|    fps                  | 180         |
|    iterations           | 900         |
|    time_elapsed         | 20441       |
|    total_timesteps      | 3686400     |
| train/                  |             |
|    approx_kl            | 0.004893264 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -267        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -25.8       |
|    n_updates            | 8990        |
|    policy_gradient_loss | -0.00646    |
|    std                  | 1.37        |
|    value_loss           | 0.542       |
-----------------------------------------
Iteration: 900 | Episodes: 36500 | Median Reward: 33.03 | Max Reward: 34.24
Iteration: 902 | Episodes: 36600 | Median Reward: 32.37 | Max Reward: 34.24
Iteration: 904 | Episodes: 36700 | Median Reward: 32.42 | Max Reward: 34.24
Iteration: 907 | Episodes: 36800 | Median Reward: 32.26 | Max Reward: 34.24
Iteration: 909 | Episodes: 36900 | Median Reward: 32.96 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.6         |
| time/                   |               |
|    fps                  | 180           |
|    iterations           | 910           |
|    time_elapsed         | 20670         |
|    total_timesteps      | 3727360       |
| train/                  |               |
|    approx_kl            | 0.00026792032 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -267          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -26.6         |
|    n_updates            | 9090          |
|    policy_gradient_loss | 0.000165      |
|    std                  | 1.38          |
|    value_loss           | 0.66          |
-------------------------------------------
Iteration: 912 | Episodes: 37000 | Median Reward: 32.91 | Max Reward: 34.24
Iteration: 914 | Episodes: 37100 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 917 | Episodes: 37200 | Median Reward: 32.69 | Max Reward: 34.24
Iteration: 919 | Episodes: 37300 | Median Reward: 32.78 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.9         |
| time/                   |               |
|    fps                  | 180           |
|    iterations           | 920           |
|    time_elapsed         | 20901         |
|    total_timesteps      | 3768320       |
| train/                  |               |
|    approx_kl            | 0.00032569957 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -267          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -26.2         |
|    n_updates            | 9190          |
|    policy_gradient_loss | -8.37e-05     |
|    std                  | 1.38          |
|    value_loss           | 0.726         |
-------------------------------------------
Iteration: 922 | Episodes: 37400 | Median Reward: 32.76 | Max Reward: 34.24
Iteration: 924 | Episodes: 37500 | Median Reward: 32.87 | Max Reward: 34.24
Iteration: 927 | Episodes: 37600 | Median Reward: 32.43 | Max Reward: 34.24
Iteration: 929 | Episodes: 37700 | Median Reward: 32.04 | Max Reward: 34.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -68        |
| time/                   |            |
|    fps                  | 180        |
|    iterations           | 930        |
|    time_elapsed         | 21129      |
|    total_timesteps      | 3809280    |
| train/                  |            |
|    approx_kl            | 0.02803001 |
|    clip_fraction        | 0.05       |
|    clip_range           | 0.3        |
|    entropy_loss         | -267       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -26.4      |
|    n_updates            | 9290       |
|    policy_gradient_loss | -0.0392    |
|    std                  | 1.39       |
|    value_loss           | 0.331      |
----------------------------------------
Iteration: 932 | Episodes: 37800 | Median Reward: 32.46 | Max Reward: 34.24
Iteration: 934 | Episodes: 37900 | Median Reward: 31.90 | Max Reward: 34.24
Iteration: 937 | Episodes: 38000 | Median Reward: 32.29 | Max Reward: 34.24
Iteration: 939 | Episodes: 38100 | Median Reward: 32.72 | Max Reward: 34.24
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -68.4      |
| time/                   |            |
|    fps                  | 180        |
|    iterations           | 940        |
|    time_elapsed         | 21358      |
|    total_timesteps      | 3850240    |
| train/                  |            |
|    approx_kl            | 0.06852595 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.3        |
|    entropy_loss         | -268       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -25.6      |
|    n_updates            | 9390       |
|    policy_gradient_loss | -0.0225    |
|    std                  | 1.4        |
|    value_loss           | 0.918      |
----------------------------------------
Iteration: 941 | Episodes: 38200 | Median Reward: 32.18 | Max Reward: 34.24
Iteration: 944 | Episodes: 38300 | Median Reward: 32.49 | Max Reward: 34.24
Iteration: 946 | Episodes: 38400 | Median Reward: 32.81 | Max Reward: 34.24
Iteration: 949 | Episodes: 38500 | Median Reward: 32.84 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.7        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 950          |
|    time_elapsed         | 21587        |
|    total_timesteps      | 3891200      |
| train/                  |              |
|    approx_kl            | 0.0029474648 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -269         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26.7        |
|    n_updates            | 9490         |
|    policy_gradient_loss | -0.00171     |
|    std                  | 1.4          |
|    value_loss           | 0.6          |
------------------------------------------
Iteration: 951 | Episodes: 38600 | Median Reward: 32.63 | Max Reward: 34.24
Iteration: 954 | Episodes: 38700 | Median Reward: 32.99 | Max Reward: 34.24
Iteration: 956 | Episodes: 38800 | Median Reward: 33.01 | Max Reward: 34.24
Iteration: 959 | Episodes: 38900 | Median Reward: 32.84 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.8        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 960          |
|    time_elapsed         | 21814        |
|    total_timesteps      | 3932160      |
| train/                  |              |
|    approx_kl            | 0.0073042456 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -270         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26.9        |
|    n_updates            | 9590         |
|    policy_gradient_loss | -0.00545     |
|    std                  | 1.41         |
|    value_loss           | 0.548        |
------------------------------------------
Iteration: 961 | Episodes: 39000 | Median Reward: 32.61 | Max Reward: 34.24
Iteration: 964 | Episodes: 39100 | Median Reward: 32.89 | Max Reward: 34.24
Iteration: 966 | Episodes: 39200 | Median Reward: 32.89 | Max Reward: 34.24
Iteration: 969 | Episodes: 39300 | Median Reward: 32.81 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.8        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 970          |
|    time_elapsed         | 22048        |
|    total_timesteps      | 3973120      |
| train/                  |              |
|    approx_kl            | 0.0017938669 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -271         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.7        |
|    n_updates            | 9690         |
|    policy_gradient_loss | -0.000807    |
|    std                  | 1.42         |
|    value_loss           | 1.16         |
------------------------------------------
Iteration: 971 | Episodes: 39400 | Median Reward: 32.95 | Max Reward: 34.24
Iteration: 973 | Episodes: 39500 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 976 | Episodes: 39600 | Median Reward: 32.44 | Max Reward: 34.24
Iteration: 978 | Episodes: 39700 | Median Reward: 32.79 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.6        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 980          |
|    time_elapsed         | 22275        |
|    total_timesteps      | 4014080      |
| train/                  |              |
|    approx_kl            | 6.513327e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -272         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -27          |
|    n_updates            | 9790         |
|    policy_gradient_loss | -8.46e-05    |
|    std                  | 1.43         |
|    value_loss           | 0.551        |
------------------------------------------
Iteration: 981 | Episodes: 39800 | Median Reward: 32.73 | Max Reward: 34.24
Iteration: 983 | Episodes: 39900 | Median Reward: 32.75 | Max Reward: 34.24
Iteration: 986 | Episodes: 40000 | Median Reward: 32.49 | Max Reward: 34.24
Iteration: 988 | Episodes: 40100 | Median Reward: 32.92 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.9        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 990          |
|    time_elapsed         | 22507        |
|    total_timesteps      | 4055040      |
| train/                  |              |
|    approx_kl            | 0.0019018374 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -272         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27          |
|    n_updates            | 9890         |
|    policy_gradient_loss | -0.0026      |
|    std                  | 1.43         |
|    value_loss           | 1.94         |
------------------------------------------
Iteration: 991 | Episodes: 40200 | Median Reward: 32.64 | Max Reward: 34.24
Iteration: 993 | Episodes: 40300 | Median Reward: 32.99 | Max Reward: 34.24
Iteration: 996 | Episodes: 40400 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 998 | Episodes: 40500 | Median Reward: 32.44 | Max Reward: 34.24
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -68.5     |
| time/                   |           |
|    fps                  | 180       |
|    iterations           | 1000      |
|    time_elapsed         | 22734     |
|    total_timesteps      | 4096000   |
| train/                  |           |
|    approx_kl            | 0.0064992 |
|    clip_fraction        | 0         |
|    clip_range           | 0.3       |
|    entropy_loss         | -273      |
|    explained_variance   | 1         |
|    learning_rate        | 0.0005    |
|    loss                 | -27       |
|    n_updates            | 9990      |
|    policy_gradient_loss | -0.0108   |
|    std                  | 1.44      |
|    value_loss           | 1.42      |
---------------------------------------
Iteration: 1001 | Episodes: 40600 | Median Reward: 31.88 | Max Reward: 34.24
Iteration: 1003 | Episodes: 40700 | Median Reward: 32.46 | Max Reward: 34.24
Iteration: 1006 | Episodes: 40800 | Median Reward: 32.94 | Max Reward: 34.24
Iteration: 1008 | Episodes: 40900 | Median Reward: 32.87 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.7        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 1010         |
|    time_elapsed         | 22961        |
|    total_timesteps      | 4136960      |
| train/                  |              |
|    approx_kl            | 0.0040765973 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -273         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27.1        |
|    n_updates            | 10090        |
|    policy_gradient_loss | -0.00596     |
|    std                  | 1.45         |
|    value_loss           | 2.54         |
------------------------------------------
Iteration: 1010 | Episodes: 41000 | Median Reward: 32.80 | Max Reward: 34.24
Iteration: 1013 | Episodes: 41100 | Median Reward: 32.98 | Max Reward: 34.24
Iteration: 1015 | Episodes: 41200 | Median Reward: 32.95 | Max Reward: 34.24
Iteration: 1018 | Episodes: 41300 | Median Reward: 33.22 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.2        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 1020         |
|    time_elapsed         | 23194        |
|    total_timesteps      | 4177920      |
| train/                  |              |
|    approx_kl            | 0.0011782462 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -274         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26.9        |
|    n_updates            | 10190        |
|    policy_gradient_loss | -0.00179     |
|    std                  | 1.46         |
|    value_loss           | 0.558        |
------------------------------------------
Iteration: 1020 | Episodes: 41400 | Median Reward: 32.72 | Max Reward: 34.24
Iteration: 1023 | Episodes: 41500 | Median Reward: 32.85 | Max Reward: 34.24
Iteration: 1025 | Episodes: 41600 | Median Reward: 32.65 | Max Reward: 34.24
Iteration: 1028 | Episodes: 41700 | Median Reward: 33.14 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.4        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 1030         |
|    time_elapsed         | 23427        |
|    total_timesteps      | 4218880      |
| train/                  |              |
|    approx_kl            | 0.0004427752 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -274         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.1        |
|    n_updates            | 10290        |
|    policy_gradient_loss | 9.22e-05     |
|    std                  | 1.46         |
|    value_loss           | 1.03         |
------------------------------------------
Iteration: 1030 | Episodes: 41800 | Median Reward: 33.03 | Max Reward: 34.24
Iteration: 1033 | Episodes: 41900 | Median Reward: 32.60 | Max Reward: 34.24
Iteration: 1035 | Episodes: 42000 | Median Reward: 32.44 | Max Reward: 34.24
Iteration: 1038 | Episodes: 42100 | Median Reward: 32.45 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.8        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 1040         |
|    time_elapsed         | 23656        |
|    total_timesteps      | 4259840      |
| train/                  |              |
|    approx_kl            | 0.0027761494 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -274         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.7        |
|    n_updates            | 10390        |
|    policy_gradient_loss | -0.00433     |
|    std                  | 1.47         |
|    value_loss           | 1.29         |
------------------------------------------
Iteration: 1040 | Episodes: 42200 | Median Reward: 32.66 | Max Reward: 34.24
Iteration: 1043 | Episodes: 42300 | Median Reward: 33.07 | Max Reward: 34.24
Iteration: 1045 | Episodes: 42400 | Median Reward: 32.47 | Max Reward: 34.24
Iteration: 1047 | Episodes: 42500 | Median Reward: 32.93 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.4         |
| time/                   |               |
|    fps                  | 180           |
|    iterations           | 1050          |
|    time_elapsed         | 23888         |
|    total_timesteps      | 4300800       |
| train/                  |               |
|    approx_kl            | 0.00020273661 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -274          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -26.6         |
|    n_updates            | 10490         |
|    policy_gradient_loss | -9.6e-05      |
|    std                  | 1.48          |
|    value_loss           | 1.37          |
-------------------------------------------
Iteration: 1050 | Episodes: 42600 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1052 | Episodes: 42700 | Median Reward: 32.89 | Max Reward: 34.24
Iteration: 1055 | Episodes: 42800 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1057 | Episodes: 42900 | Median Reward: 32.77 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.7         |
| time/                   |               |
|    fps                  | 180           |
|    iterations           | 1060          |
|    time_elapsed         | 24119         |
|    total_timesteps      | 4341760       |
| train/                  |               |
|    approx_kl            | 0.00023509028 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -274          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -27.2         |
|    n_updates            | 10590         |
|    policy_gradient_loss | -0.000282     |
|    std                  | 1.49          |
|    value_loss           | 0.488         |
-------------------------------------------
Iteration: 1060 | Episodes: 43000 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 1062 | Episodes: 43100 | Median Reward: 32.70 | Max Reward: 34.24
Iteration: 1065 | Episodes: 43200 | Median Reward: 32.83 | Max Reward: 34.24
Iteration: 1067 | Episodes: 43300 | Median Reward: 32.95 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.3        |
| time/                   |              |
|    fps                  | 180          |
|    iterations           | 1070         |
|    time_elapsed         | 24345        |
|    total_timesteps      | 4382720      |
| train/                  |              |
|    approx_kl            | 0.0038006394 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -274         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.1        |
|    n_updates            | 10690        |
|    policy_gradient_loss | -0.00232     |
|    std                  | 1.5          |
|    value_loss           | 0.49         |
------------------------------------------
Iteration: 1070 | Episodes: 43400 | Median Reward: 32.95 | Max Reward: 34.24
Iteration: 1072 | Episodes: 43500 | Median Reward: 33.12 | Max Reward: 34.24
Iteration: 1075 | Episodes: 43600 | Median Reward: 32.65 | Max Reward: 34.24
Iteration: 1077 | Episodes: 43700 | Median Reward: 32.67 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.6         |
| time/                   |               |
|    fps                  | 180           |
|    iterations           | 1080          |
|    time_elapsed         | 24575         |
|    total_timesteps      | 4423680       |
| train/                  |               |
|    approx_kl            | 0.00089006854 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -275          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -27.3         |
|    n_updates            | 10790         |
|    policy_gradient_loss | -0.00161      |
|    std                  | 1.51          |
|    value_loss           | 0.317         |
-------------------------------------------
Iteration: 1080 | Episodes: 43800 | Median Reward: 33.01 | Max Reward: 34.24
Iteration: 1082 | Episodes: 43900 | Median Reward: 32.97 | Max Reward: 34.24
Iteration: 1084 | Episodes: 44000 | Median Reward: 33.06 | Max Reward: 34.24
Iteration: 1087 | Episodes: 44100 | Median Reward: 32.68 | Max Reward: 34.24
Iteration: 1089 | Episodes: 44200 | Median Reward: 32.88 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.8         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1090          |
|    time_elapsed         | 24807         |
|    total_timesteps      | 4464640       |
| train/                  |               |
|    approx_kl            | 0.00062173256 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -276          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -27.4         |
|    n_updates            | 10890         |
|    policy_gradient_loss | -0.000685     |
|    std                  | 1.52          |
|    value_loss           | 0.446         |
-------------------------------------------
Iteration: 1092 | Episodes: 44300 | Median Reward: 32.54 | Max Reward: 34.24
Iteration: 1094 | Episodes: 44400 | Median Reward: 32.91 | Max Reward: 34.24
Iteration: 1097 | Episodes: 44500 | Median Reward: 31.85 | Max Reward: 34.24
Iteration: 1099 | Episodes: 44600 | Median Reward: 32.86 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.1        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1100         |
|    time_elapsed         | 25039        |
|    total_timesteps      | 4505600      |
| train/                  |              |
|    approx_kl            | 0.0006871574 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -276         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.9        |
|    n_updates            | 10990        |
|    policy_gradient_loss | -5.77e-06    |
|    std                  | 1.52         |
|    value_loss           | 0.938        |
------------------------------------------
Iteration: 1102 | Episodes: 44700 | Median Reward: 32.05 | Max Reward: 34.24
Iteration: 1104 | Episodes: 44800 | Median Reward: 32.22 | Max Reward: 34.24
Iteration: 1107 | Episodes: 44900 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 1109 | Episodes: 45000 | Median Reward: 32.93 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.2         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1110          |
|    time_elapsed         | 25267         |
|    total_timesteps      | 4546560       |
| train/                  |               |
|    approx_kl            | 0.00022377649 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -277          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -25.6         |
|    n_updates            | 11090         |
|    policy_gradient_loss | -0.000685     |
|    std                  | 1.53          |
|    value_loss           | 1.46          |
-------------------------------------------
Iteration: 1112 | Episodes: 45100 | Median Reward: 32.81 | Max Reward: 34.24
Iteration: 1114 | Episodes: 45200 | Median Reward: 31.88 | Max Reward: 34.24
Iteration: 1117 | Episodes: 45300 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 1119 | Episodes: 45400 | Median Reward: 32.45 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.5         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1120          |
|    time_elapsed         | 25494         |
|    total_timesteps      | 4587520       |
| train/                  |               |
|    approx_kl            | 0.00016076755 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -278          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -26.2         |
|    n_updates            | 11190         |
|    policy_gradient_loss | -0.000376     |
|    std                  | 1.54          |
|    value_loss           | 1.38          |
-------------------------------------------
Iteration: 1121 | Episodes: 45500 | Median Reward: 32.91 | Max Reward: 34.24
Iteration: 1124 | Episodes: 45600 | Median Reward: 32.48 | Max Reward: 34.24
Iteration: 1126 | Episodes: 45700 | Median Reward: 32.66 | Max Reward: 34.24
Iteration: 1129 | Episodes: 45800 | Median Reward: 33.00 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.4        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1130         |
|    time_elapsed         | 25722        |
|    total_timesteps      | 4628480      |
| train/                  |              |
|    approx_kl            | 3.856994e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -278         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27.7        |
|    n_updates            | 11290        |
|    policy_gradient_loss | -8.88e-05    |
|    std                  | 1.55         |
|    value_loss           | 0.544        |
------------------------------------------
Iteration: 1131 | Episodes: 45900 | Median Reward: 33.12 | Max Reward: 34.24
Iteration: 1134 | Episodes: 46000 | Median Reward: 32.83 | Max Reward: 34.24
Iteration: 1136 | Episodes: 46100 | Median Reward: 32.48 | Max Reward: 34.24
Iteration: 1139 | Episodes: 46200 | Median Reward: 33.08 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.1        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1140         |
|    time_elapsed         | 25949        |
|    total_timesteps      | 4669440      |
| train/                  |              |
|    approx_kl            | 0.0004134115 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -278         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.5        |
|    n_updates            | 11390        |
|    policy_gradient_loss | -0.000351    |
|    std                  | 1.55         |
|    value_loss           | 0.913        |
------------------------------------------
Iteration: 1141 | Episodes: 46300 | Median Reward: 33.04 | Max Reward: 34.24
Iteration: 1144 | Episodes: 46400 | Median Reward: 32.81 | Max Reward: 34.24
Iteration: 1146 | Episodes: 46500 | Median Reward: 32.87 | Max Reward: 34.24
Iteration: 1149 | Episodes: 46600 | Median Reward: 31.83 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.4        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1150         |
|    time_elapsed         | 26177        |
|    total_timesteps      | 4710400      |
| train/                  |              |
|    approx_kl            | 0.0006133442 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -279         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26.4        |
|    n_updates            | 11490        |
|    policy_gradient_loss | -0.00302     |
|    std                  | 1.56         |
|    value_loss           | 3.28         |
------------------------------------------
Iteration: 1151 | Episodes: 46700 | Median Reward: 32.23 | Max Reward: 34.24
Iteration: 1154 | Episodes: 46800 | Median Reward: 32.91 | Max Reward: 34.24
Iteration: 1156 | Episodes: 46900 | Median Reward: 32.86 | Max Reward: 34.24
Iteration: 1158 | Episodes: 47000 | Median Reward: 32.67 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.2         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1160          |
|    time_elapsed         | 26407         |
|    total_timesteps      | 4751360       |
| train/                  |               |
|    approx_kl            | 0.00035541772 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -279          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -26.3         |
|    n_updates            | 11590         |
|    policy_gradient_loss | -0.00238      |
|    std                  | 1.56          |
|    value_loss           | 3.35          |
-------------------------------------------
Iteration: 1161 | Episodes: 47100 | Median Reward: 32.64 | Max Reward: 34.24
Iteration: 1163 | Episodes: 47200 | Median Reward: 32.51 | Max Reward: 34.24
Iteration: 1166 | Episodes: 47300 | Median Reward: 32.85 | Max Reward: 34.24
Iteration: 1168 | Episodes: 47400 | Median Reward: 32.99 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.9        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1170         |
|    time_elapsed         | 26640        |
|    total_timesteps      | 4792320      |
| train/                  |              |
|    approx_kl            | 0.0008249946 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -280         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27.9        |
|    n_updates            | 11690        |
|    policy_gradient_loss | -0.000648    |
|    std                  | 1.57         |
|    value_loss           | 0.839        |
------------------------------------------
Iteration: 1171 | Episodes: 47500 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 1173 | Episodes: 47600 | Median Reward: 32.87 | Max Reward: 34.24
Iteration: 1176 | Episodes: 47700 | Median Reward: 32.66 | Max Reward: 34.24
Iteration: 1178 | Episodes: 47800 | Median Reward: 33.00 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.6         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1180          |
|    time_elapsed         | 26872         |
|    total_timesteps      | 4833280       |
| train/                  |               |
|    approx_kl            | 4.7857568e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -281          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -27.9         |
|    n_updates            | 11790         |
|    policy_gradient_loss | -6.79e-05     |
|    std                  | 1.57          |
|    value_loss           | 1.13          |
-------------------------------------------
Iteration: 1181 | Episodes: 47900 | Median Reward: 32.69 | Max Reward: 34.24
Iteration: 1183 | Episodes: 48000 | Median Reward: 32.88 | Max Reward: 34.24
Iteration: 1186 | Episodes: 48100 | Median Reward: 32.83 | Max Reward: 34.24
Iteration: 1188 | Episodes: 48200 | Median Reward: 32.57 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68           |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1190          |
|    time_elapsed         | 27110         |
|    total_timesteps      | 4874240       |
| train/                  |               |
|    approx_kl            | 0.00035078195 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -281          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -26.3         |
|    n_updates            | 11890         |
|    policy_gradient_loss | 0.000296      |
|    std                  | 1.58          |
|    value_loss           | 3.11          |
-------------------------------------------
Iteration: 1190 | Episodes: 48300 | Median Reward: 32.91 | Max Reward: 34.24
Iteration: 1193 | Episodes: 48400 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1195 | Episodes: 48500 | Median Reward: 32.72 | Max Reward: 34.24
Iteration: 1198 | Episodes: 48600 | Median Reward: 32.99 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.4       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1200        |
|    time_elapsed         | 27346       |
|    total_timesteps      | 4915200     |
| train/                  |             |
|    approx_kl            | 0.005302192 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -281        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -28.1       |
|    n_updates            | 11990       |
|    policy_gradient_loss | -0.00721    |
|    std                  | 1.59        |
|    value_loss           | 0.507       |
-----------------------------------------
Iteration: 1200 | Episodes: 48700 | Median Reward: 33.03 | Max Reward: 34.24
Iteration: 1203 | Episodes: 48800 | Median Reward: 31.77 | Max Reward: 34.24
Iteration: 1205 | Episodes: 48900 | Median Reward: 32.91 | Max Reward: 34.24
Iteration: 1208 | Episodes: 49000 | Median Reward: 33.08 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.1        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1210         |
|    time_elapsed         | 27577        |
|    total_timesteps      | 4956160      |
| train/                  |              |
|    approx_kl            | 0.0002626137 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -282         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -28.1        |
|    n_updates            | 12090        |
|    policy_gradient_loss | 0.000221     |
|    std                  | 1.59         |
|    value_loss           | 0.691        |
------------------------------------------
Iteration: 1210 | Episodes: 49100 | Median Reward: 33.00 | Max Reward: 34.24
Iteration: 1213 | Episodes: 49200 | Median Reward: 32.33 | Max Reward: 34.24
Iteration: 1215 | Episodes: 49300 | Median Reward: 32.92 | Max Reward: 34.24
Iteration: 1218 | Episodes: 49400 | Median Reward: 32.80 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.6        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1220         |
|    time_elapsed         | 27810        |
|    total_timesteps      | 4997120      |
| train/                  |              |
|    approx_kl            | 0.0006825126 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -282         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.9        |
|    n_updates            | 12190        |
|    policy_gradient_loss | -0.00143     |
|    std                  | 1.6          |
|    value_loss           | 1.24         |
------------------------------------------
Iteration: 1220 | Episodes: 49500 | Median Reward: 32.67 | Max Reward: 34.24
Iteration: 1223 | Episodes: 49600 | Median Reward: 32.98 | Max Reward: 34.24
Iteration: 1225 | Episodes: 49700 | Median Reward: 32.80 | Max Reward: 34.24
Iteration: 1227 | Episodes: 49800 | Median Reward: 33.01 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.4        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1230         |
|    time_elapsed         | 28037        |
|    total_timesteps      | 5038080      |
| train/                  |              |
|    approx_kl            | 0.0006127759 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -283         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -27.2        |
|    n_updates            | 12290        |
|    policy_gradient_loss | -6.86e-05    |
|    std                  | 1.62         |
|    value_loss           | 1.37         |
------------------------------------------
Iteration: 1230 | Episodes: 49900 | Median Reward: 32.87 | Max Reward: 34.24
Iteration: 1232 | Episodes: 50000 | Median Reward: 33.04 | Max Reward: 34.24
Iteration: 1235 | Episodes: 50100 | Median Reward: 32.94 | Max Reward: 34.24
Iteration: 1237 | Episodes: 50200 | Median Reward: 32.98 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.5       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1240        |
|    time_elapsed         | 28265       |
|    total_timesteps      | 5079040     |
| train/                  |             |
|    approx_kl            | 0.015194774 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -283        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -28.3       |
|    n_updates            | 12390       |
|    policy_gradient_loss | -0.0102     |
|    std                  | 1.63        |
|    value_loss           | 0.126       |
-----------------------------------------
Iteration: 1240 | Episodes: 50300 | Median Reward: 32.95 | Max Reward: 34.24
Iteration: 1242 | Episodes: 50400 | Median Reward: 32.73 | Max Reward: 34.24
Iteration: 1245 | Episodes: 50500 | Median Reward: 33.12 | Max Reward: 34.24
Iteration: 1247 | Episodes: 50600 | Median Reward: 32.96 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.1       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1250        |
|    time_elapsed         | 28497       |
|    total_timesteps      | 5120000     |
| train/                  |             |
|    approx_kl            | 0.028599937 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -284        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -28.3       |
|    n_updates            | 12490       |
|    policy_gradient_loss | -0.0275     |
|    std                  | 1.64        |
|    value_loss           | 0.242       |
-----------------------------------------
Iteration: 1250 | Episodes: 50700 | Median Reward: 33.07 | Max Reward: 34.24
Iteration: 1252 | Episodes: 50800 | Median Reward: 33.12 | Max Reward: 34.24
Iteration: 1255 | Episodes: 50900 | Median Reward: 33.04 | Max Reward: 34.24
Iteration: 1257 | Episodes: 51000 | Median Reward: 33.05 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.9         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1260          |
|    time_elapsed         | 28718         |
|    total_timesteps      | 5160960       |
| train/                  |               |
|    approx_kl            | 9.0471265e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -284          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -28.1         |
|    n_updates            | 12590         |
|    policy_gradient_loss | -0.000127     |
|    std                  | 1.65          |
|    value_loss           | 1.8           |
-------------------------------------------
Iteration: 1260 | Episodes: 51100 | Median Reward: 32.92 | Max Reward: 34.24
Iteration: 1262 | Episodes: 51200 | Median Reward: 32.48 | Max Reward: 34.24
Iteration: 1264 | Episodes: 51300 | Median Reward: 32.77 | Max Reward: 34.24
Iteration: 1267 | Episodes: 51400 | Median Reward: 33.01 | Max Reward: 34.24
Iteration: 1269 | Episodes: 51500 | Median Reward: 32.88 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.9        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1270         |
|    time_elapsed         | 28950        |
|    total_timesteps      | 5201920      |
| train/                  |              |
|    approx_kl            | 0.0026378187 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -285         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.1        |
|    n_updates            | 12690        |
|    policy_gradient_loss | -0.00247     |
|    std                  | 1.66         |
|    value_loss           | 0.242        |
------------------------------------------
Iteration: 1272 | Episodes: 51600 | Median Reward: 32.67 | Max Reward: 34.24
Iteration: 1274 | Episodes: 51700 | Median Reward: 32.83 | Max Reward: 34.24
Iteration: 1277 | Episodes: 51800 | Median Reward: 32.83 | Max Reward: 34.24
Iteration: 1279 | Episodes: 51900 | Median Reward: 32.58 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68          |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1280         |
|    time_elapsed         | 29186        |
|    total_timesteps      | 5242880      |
| train/                  |              |
|    approx_kl            | 0.0007488369 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -285         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -28.2        |
|    n_updates            | 12790        |
|    policy_gradient_loss | -0.000501    |
|    std                  | 1.67         |
|    value_loss           | 0.539        |
------------------------------------------
Iteration: 1282 | Episodes: 52000 | Median Reward: 32.98 | Max Reward: 34.24
Iteration: 1284 | Episodes: 52100 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1287 | Episodes: 52200 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1289 | Episodes: 52300 | Median Reward: 32.82 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.1        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1290         |
|    time_elapsed         | 29413        |
|    total_timesteps      | 5283840      |
| train/                  |              |
|    approx_kl            | 0.0070335036 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -286         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.5        |
|    n_updates            | 12890        |
|    policy_gradient_loss | -0.00858     |
|    std                  | 1.68         |
|    value_loss           | 0.278        |
------------------------------------------
Iteration: 1292 | Episodes: 52400 | Median Reward: 32.64 | Max Reward: 34.24
Iteration: 1294 | Episodes: 52500 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1297 | Episodes: 52600 | Median Reward: 32.95 | Max Reward: 34.24
Iteration: 1299 | Episodes: 52700 | Median Reward: 32.99 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.3       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1300        |
|    time_elapsed         | 29647       |
|    total_timesteps      | 5324800     |
| train/                  |             |
|    approx_kl            | 0.006031019 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -286        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -28.4       |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.000278   |
|    std                  | 1.69        |
|    value_loss           | 0.696       |
-----------------------------------------
Iteration: 1301 | Episodes: 52800 | Median Reward: 33.21 | Max Reward: 34.24
Iteration: 1304 | Episodes: 52900 | Median Reward: 31.97 | Max Reward: 34.24
Iteration: 1306 | Episodes: 53000 | Median Reward: 32.97 | Max Reward: 34.24
Iteration: 1309 | Episodes: 53100 | Median Reward: 33.13 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.6        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1310         |
|    time_elapsed         | 29879        |
|    total_timesteps      | 5365760      |
| train/                  |              |
|    approx_kl            | 7.392028e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -286         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.4        |
|    n_updates            | 13090        |
|    policy_gradient_loss | -0.000291    |
|    std                  | 1.7          |
|    value_loss           | 0.522        |
------------------------------------------
Iteration: 1311 | Episodes: 53200 | Median Reward: 32.85 | Max Reward: 34.24
Iteration: 1314 | Episodes: 53300 | Median Reward: 32.80 | Max Reward: 34.24
Iteration: 1316 | Episodes: 53400 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1319 | Episodes: 53500 | Median Reward: 33.10 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.4        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1320         |
|    time_elapsed         | 30105        |
|    total_timesteps      | 5406720      |
| train/                  |              |
|    approx_kl            | 0.0048813666 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -287         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.1        |
|    n_updates            | 13190        |
|    policy_gradient_loss | -0.00792     |
|    std                  | 1.71         |
|    value_loss           | 0.448        |
------------------------------------------
Iteration: 1321 | Episodes: 53600 | Median Reward: 32.80 | Max Reward: 34.24
Iteration: 1324 | Episodes: 53700 | Median Reward: 32.66 | Max Reward: 34.24
Iteration: 1326 | Episodes: 53800 | Median Reward: 31.76 | Max Reward: 34.24
Iteration: 1329 | Episodes: 53900 | Median Reward: 32.93 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.6       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1330        |
|    time_elapsed         | 30335       |
|    total_timesteps      | 5447680     |
| train/                  |             |
|    approx_kl            | 0.023193143 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.3         |
|    entropy_loss         | -287        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -28.5       |
|    n_updates            | 13290       |
|    policy_gradient_loss | -0.0415     |
|    std                  | 1.72        |
|    value_loss           | 0.704       |
-----------------------------------------
Iteration: 1331 | Episodes: 54000 | Median Reward: 33.10 | Max Reward: 34.24
Iteration: 1334 | Episodes: 54100 | Median Reward: 32.85 | Max Reward: 34.24
Iteration: 1336 | Episodes: 54200 | Median Reward: 32.16 | Max Reward: 34.24
Iteration: 1338 | Episodes: 54300 | Median Reward: 32.70 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.3       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1340        |
|    time_elapsed         | 30559       |
|    total_timesteps      | 5488640     |
| train/                  |             |
|    approx_kl            | 0.001170817 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -287        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -28.4       |
|    n_updates            | 13390       |
|    policy_gradient_loss | -0.000394   |
|    std                  | 1.74        |
|    value_loss           | 0.917       |
-----------------------------------------
Iteration: 1341 | Episodes: 54400 | Median Reward: 32.54 | Max Reward: 34.24
Iteration: 1343 | Episodes: 54500 | Median Reward: 32.60 | Max Reward: 34.24
Iteration: 1346 | Episodes: 54600 | Median Reward: 33.04 | Max Reward: 34.24
Iteration: 1348 | Episodes: 54700 | Median Reward: 33.07 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.3       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1350        |
|    time_elapsed         | 30789       |
|    total_timesteps      | 5529600     |
| train/                  |             |
|    approx_kl            | 0.020016333 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -288        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -28.4       |
|    n_updates            | 13490       |
|    policy_gradient_loss | -0.0184     |
|    std                  | 1.74        |
|    value_loss           | 0.391       |
-----------------------------------------
Iteration: 1351 | Episodes: 54800 | Median Reward: 32.74 | Max Reward: 34.24
Iteration: 1353 | Episodes: 54900 | Median Reward: 32.63 | Max Reward: 34.24
Iteration: 1356 | Episodes: 55000 | Median Reward: 32.41 | Max Reward: 34.24
Iteration: 1358 | Episodes: 55100 | Median Reward: 32.97 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.5         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1360          |
|    time_elapsed         | 31016         |
|    total_timesteps      | 5570560       |
| train/                  |               |
|    approx_kl            | 0.00064784545 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -288          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -28.7         |
|    n_updates            | 13590         |
|    policy_gradient_loss | -0.00147      |
|    std                  | 1.75          |
|    value_loss           | 1.92          |
-------------------------------------------
Iteration: 1361 | Episodes: 55200 | Median Reward: 32.49 | Max Reward: 34.24
Iteration: 1363 | Episodes: 55300 | Median Reward: 32.55 | Max Reward: 34.24
Iteration: 1366 | Episodes: 55400 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 1368 | Episodes: 55500 | Median Reward: 32.99 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.6        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1370         |
|    time_elapsed         | 31245        |
|    total_timesteps      | 5611520      |
| train/                  |              |
|    approx_kl            | 0.0032560595 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -289         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.7        |
|    n_updates            | 13690        |
|    policy_gradient_loss | -0.00313     |
|    std                  | 1.76         |
|    value_loss           | 0.644        |
------------------------------------------
Iteration: 1370 | Episodes: 55600 | Median Reward: 33.12 | Max Reward: 34.24
Iteration: 1373 | Episodes: 55700 | Median Reward: 32.10 | Max Reward: 34.24
Iteration: 1375 | Episodes: 55800 | Median Reward: 32.47 | Max Reward: 34.24
Iteration: 1378 | Episodes: 55900 | Median Reward: 32.91 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.2         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1380          |
|    time_elapsed         | 31478         |
|    total_timesteps      | 5652480       |
| train/                  |               |
|    approx_kl            | 0.00022427163 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -289          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -28.5         |
|    n_updates            | 13790         |
|    policy_gradient_loss | -0.000679     |
|    std                  | 1.77          |
|    value_loss           | 0.718         |
-------------------------------------------
Iteration: 1380 | Episodes: 56000 | Median Reward: 32.22 | Max Reward: 34.24
Iteration: 1383 | Episodes: 56100 | Median Reward: 33.22 | Max Reward: 34.24
Iteration: 1385 | Episodes: 56200 | Median Reward: 33.08 | Max Reward: 34.24
Iteration: 1388 | Episodes: 56300 | Median Reward: 33.02 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.1        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1390         |
|    time_elapsed         | 31711        |
|    total_timesteps      | 5693440      |
| train/                  |              |
|    approx_kl            | 0.0012225858 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -290         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.5        |
|    n_updates            | 13890        |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.78         |
|    value_loss           | 0.601        |
------------------------------------------
Iteration: 1390 | Episodes: 56400 | Median Reward: 32.42 | Max Reward: 34.24
Iteration: 1393 | Episodes: 56500 | Median Reward: 33.24 | Max Reward: 34.24
Iteration: 1395 | Episodes: 56600 | Median Reward: 32.87 | Max Reward: 34.24
Iteration: 1398 | Episodes: 56700 | Median Reward: 32.83 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.8        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1400         |
|    time_elapsed         | 31940        |
|    total_timesteps      | 5734400      |
| train/                  |              |
|    approx_kl            | 0.0012217757 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -290         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -28.9        |
|    n_updates            | 13990        |
|    policy_gradient_loss | -0.00302     |
|    std                  | 1.79         |
|    value_loss           | 0.748        |
------------------------------------------
Iteration: 1400 | Episodes: 56800 | Median Reward: 32.50 | Max Reward: 34.24
Iteration: 1403 | Episodes: 56900 | Median Reward: 32.85 | Max Reward: 34.24
Iteration: 1405 | Episodes: 57000 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1407 | Episodes: 57100 | Median Reward: 32.97 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.8        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1410         |
|    time_elapsed         | 32171        |
|    total_timesteps      | 5775360      |
| train/                  |              |
|    approx_kl            | 0.0011751468 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -290         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -28.9        |
|    n_updates            | 14090        |
|    policy_gradient_loss | -0.00429     |
|    std                  | 1.79         |
|    value_loss           | 0.611        |
------------------------------------------
Iteration: 1410 | Episodes: 57200 | Median Reward: 32.79 | Max Reward: 34.24
Iteration: 1412 | Episodes: 57300 | Median Reward: 32.84 | Max Reward: 34.24
Iteration: 1415 | Episodes: 57400 | Median Reward: 31.77 | Max Reward: 34.24
Iteration: 1417 | Episodes: 57500 | Median Reward: 32.55 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.3        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1420         |
|    time_elapsed         | 32405        |
|    total_timesteps      | 5816320      |
| train/                  |              |
|    approx_kl            | 0.0041881893 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -291         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -28.9        |
|    n_updates            | 14190        |
|    policy_gradient_loss | -0.00152     |
|    std                  | 1.81         |
|    value_loss           | 0.287        |
------------------------------------------
Iteration: 1420 | Episodes: 57600 | Median Reward: 33.01 | Max Reward: 34.24
Iteration: 1422 | Episodes: 57700 | Median Reward: 32.77 | Max Reward: 34.24
Iteration: 1425 | Episodes: 57800 | Median Reward: 32.97 | Max Reward: 34.24
Iteration: 1427 | Episodes: 57900 | Median Reward: 32.42 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.7        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1430         |
|    time_elapsed         | 32634        |
|    total_timesteps      | 5857280      |
| train/                  |              |
|    approx_kl            | 0.0009828038 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -292         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -28.9        |
|    n_updates            | 14290        |
|    policy_gradient_loss | 9.92e-05     |
|    std                  | 1.81         |
|    value_loss           | 1.75         |
------------------------------------------
Iteration: 1430 | Episodes: 58000 | Median Reward: 33.07 | Max Reward: 34.24
Iteration: 1432 | Episodes: 58100 | Median Reward: 31.85 | Max Reward: 34.24
Iteration: 1435 | Episodes: 58200 | Median Reward: 32.87 | Max Reward: 34.24
Iteration: 1437 | Episodes: 58300 | Median Reward: 32.99 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.4       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1440        |
|    time_elapsed         | 32868       |
|    total_timesteps      | 5898240     |
| train/                  |             |
|    approx_kl            | 0.001069042 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -292        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -28.9       |
|    n_updates            | 14390       |
|    policy_gradient_loss | -0.00564    |
|    std                  | 1.82        |
|    value_loss           | 0.984       |
-----------------------------------------
Iteration: 1440 | Episodes: 58400 | Median Reward: 32.51 | Max Reward: 34.24
Iteration: 1442 | Episodes: 58500 | Median Reward: 32.81 | Max Reward: 34.24
Iteration: 1444 | Episodes: 58600 | Median Reward: 33.08 | Max Reward: 34.24
Iteration: 1447 | Episodes: 58700 | Median Reward: 33.03 | Max Reward: 34.24
Iteration: 1449 | Episodes: 58800 | Median Reward: 33.08 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.6        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1450         |
|    time_elapsed         | 33098        |
|    total_timesteps      | 5939200      |
| train/                  |              |
|    approx_kl            | 0.0032001033 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -293         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -28.9        |
|    n_updates            | 14490        |
|    policy_gradient_loss | -0.00952     |
|    std                  | 1.83         |
|    value_loss           | 1.07         |
------------------------------------------
Iteration: 1452 | Episodes: 58900 | Median Reward: 33.22 | Max Reward: 34.24
Iteration: 1454 | Episodes: 59000 | Median Reward: 33.07 | Max Reward: 34.24
Iteration: 1457 | Episodes: 59100 | Median Reward: 32.73 | Max Reward: 34.24
Iteration: 1459 | Episodes: 59200 | Median Reward: 32.91 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.1        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1460         |
|    time_elapsed         | 33329        |
|    total_timesteps      | 5980160      |
| train/                  |              |
|    approx_kl            | 0.0001525019 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -293         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27.6        |
|    n_updates            | 14590        |
|    policy_gradient_loss | 7.21e-05     |
|    std                  | 1.84         |
|    value_loss           | 2.9          |
------------------------------------------
Iteration: 1462 | Episodes: 59300 | Median Reward: 32.48 | Max Reward: 34.24
Iteration: 1464 | Episodes: 59400 | Median Reward: 33.04 | Max Reward: 34.24
Iteration: 1467 | Episodes: 59500 | Median Reward: 33.13 | Max Reward: 34.24
Iteration: 1469 | Episodes: 59600 | Median Reward: 32.93 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.4         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1470          |
|    time_elapsed         | 33559         |
|    total_timesteps      | 6021120       |
| train/                  |               |
|    approx_kl            | 6.9680245e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -294          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -29.3         |
|    n_updates            | 14690         |
|    policy_gradient_loss | -2.1e-05      |
|    std                  | 1.85          |
|    value_loss           | 0.469         |
-------------------------------------------
Iteration: 1472 | Episodes: 59700 | Median Reward: 33.04 | Max Reward: 34.24
Iteration: 1474 | Episodes: 59800 | Median Reward: 32.83 | Max Reward: 34.24
Iteration: 1477 | Episodes: 59900 | Median Reward: 32.94 | Max Reward: 34.24
Iteration: 1479 | Episodes: 60000 | Median Reward: 32.94 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.1        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1480         |
|    time_elapsed         | 33787        |
|    total_timesteps      | 6062080      |
| train/                  |              |
|    approx_kl            | 0.0038263234 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -294         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -29.2        |
|    n_updates            | 14790        |
|    policy_gradient_loss | -0.0138      |
|    std                  | 1.86         |
|    value_loss           | 0.403        |
------------------------------------------
Iteration: 1481 | Episodes: 60100 | Median Reward: 32.88 | Max Reward: 34.24
Iteration: 1484 | Episodes: 60200 | Median Reward: 32.66 | Max Reward: 34.24
Iteration: 1486 | Episodes: 60300 | Median Reward: 32.73 | Max Reward: 34.24
Iteration: 1489 | Episodes: 60400 | Median Reward: 32.38 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.1         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1490          |
|    time_elapsed         | 34019         |
|    total_timesteps      | 6103040       |
| train/                  |               |
|    approx_kl            | 0.00035856466 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -295          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -29.1         |
|    n_updates            | 14890         |
|    policy_gradient_loss | -0.000417     |
|    std                  | 1.87          |
|    value_loss           | 0.318         |
-------------------------------------------
Iteration: 1491 | Episodes: 60500 | Median Reward: 32.90 | Max Reward: 34.24
Iteration: 1494 | Episodes: 60600 | Median Reward: 33.01 | Max Reward: 34.24
Iteration: 1496 | Episodes: 60700 | Median Reward: 32.74 | Max Reward: 34.24
Iteration: 1499 | Episodes: 60800 | Median Reward: 33.12 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68         |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1500        |
|    time_elapsed         | 34251       |
|    total_timesteps      | 6144000     |
| train/                  |             |
|    approx_kl            | 0.006760281 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -295        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -29.1       |
|    n_updates            | 14990       |
|    policy_gradient_loss | -0.0131     |
|    std                  | 1.88        |
|    value_loss           | 0.536       |
-----------------------------------------
Iteration: 1501 | Episodes: 60900 | Median Reward: 32.95 | Max Reward: 34.24
Iteration: 1504 | Episodes: 61000 | Median Reward: 33.10 | Max Reward: 34.24
Iteration: 1506 | Episodes: 61100 | Median Reward: 33.07 | Max Reward: 34.24
Iteration: 1509 | Episodes: 61200 | Median Reward: 32.66 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.8         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1510          |
|    time_elapsed         | 34479         |
|    total_timesteps      | 6184960       |
| train/                  |               |
|    approx_kl            | 0.00050527113 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -296          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -29.4         |
|    n_updates            | 15090         |
|    policy_gradient_loss | -0.00116      |
|    std                  | 1.89          |
|    value_loss           | 0.548         |
-------------------------------------------
Iteration: 1511 | Episodes: 61300 | Median Reward: 33.10 | Max Reward: 34.24
Iteration: 1514 | Episodes: 61400 | Median Reward: 31.38 | Max Reward: 34.24
Iteration: 1516 | Episodes: 61500 | Median Reward: 32.79 | Max Reward: 34.24
Iteration: 1518 | Episodes: 61600 | Median Reward: 32.64 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.6        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1520         |
|    time_elapsed         | 34712        |
|    total_timesteps      | 6225920      |
| train/                  |              |
|    approx_kl            | 0.0012747585 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -297         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29.4        |
|    n_updates            | 15190        |
|    policy_gradient_loss | -0.000589    |
|    std                  | 1.91         |
|    value_loss           | 0.234        |
------------------------------------------
Iteration: 1521 | Episodes: 61700 | Median Reward: 32.91 | Max Reward: 34.24
Iteration: 1523 | Episodes: 61800 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1526 | Episodes: 61900 | Median Reward: 32.67 | Max Reward: 34.24
Iteration: 1528 | Episodes: 62000 | Median Reward: 32.93 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.4         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1530          |
|    time_elapsed         | 34941         |
|    total_timesteps      | 6266880       |
| train/                  |               |
|    approx_kl            | 0.00014823415 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -297          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -29.6         |
|    n_updates            | 15290         |
|    policy_gradient_loss | -0.000315     |
|    std                  | 1.92          |
|    value_loss           | 0.327         |
-------------------------------------------
Iteration: 1531 | Episodes: 62100 | Median Reward: 32.20 | Max Reward: 34.24
Iteration: 1533 | Episodes: 62200 | Median Reward: 32.33 | Max Reward: 34.24
Iteration: 1536 | Episodes: 62300 | Median Reward: 33.13 | Max Reward: 34.24
Iteration: 1538 | Episodes: 62400 | Median Reward: 32.39 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.6         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1540          |
|    time_elapsed         | 35170         |
|    total_timesteps      | 6307840       |
| train/                  |               |
|    approx_kl            | 0.00010624678 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -298          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -29.6         |
|    n_updates            | 15390         |
|    policy_gradient_loss | -0.000285     |
|    std                  | 1.93          |
|    value_loss           | 0.288         |
-------------------------------------------
Iteration: 1541 | Episodes: 62500 | Median Reward: 32.60 | Max Reward: 34.24
Iteration: 1543 | Episodes: 62600 | Median Reward: 33.01 | Max Reward: 34.24
Iteration: 1546 | Episodes: 62700 | Median Reward: 33.23 | Max Reward: 34.24
Iteration: 1548 | Episodes: 62800 | Median Reward: 32.92 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68           |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1550          |
|    time_elapsed         | 35400         |
|    total_timesteps      | 6348800       |
| train/                  |               |
|    approx_kl            | 0.00088604155 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -298          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -29.7         |
|    n_updates            | 15490         |
|    policy_gradient_loss | -0.00301      |
|    std                  | 1.94          |
|    value_loss           | 0.459         |
-------------------------------------------
Iteration: 1551 | Episodes: 62900 | Median Reward: 32.97 | Max Reward: 34.24
Iteration: 1553 | Episodes: 63000 | Median Reward: 33.02 | Max Reward: 34.24
Iteration: 1555 | Episodes: 63100 | Median Reward: 32.97 | Max Reward: 34.24
Iteration: 1558 | Episodes: 63200 | Median Reward: 32.97 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.5       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1560        |
|    time_elapsed         | 35631       |
|    total_timesteps      | 6389760     |
| train/                  |             |
|    approx_kl            | 0.017572757 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.3         |
|    entropy_loss         | -299        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -29.9       |
|    n_updates            | 15590       |
|    policy_gradient_loss | -0.0231     |
|    std                  | 1.95        |
|    value_loss           | 0.695       |
-----------------------------------------
Iteration: 1560 | Episodes: 63300 | Median Reward: 32.05 | Max Reward: 34.24
Iteration: 1563 | Episodes: 63400 | Median Reward: 32.79 | Max Reward: 34.24
Iteration: 1565 | Episodes: 63500 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1568 | Episodes: 63600 | Median Reward: 33.12 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.5        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1570         |
|    time_elapsed         | 35865        |
|    total_timesteps      | 6430720      |
| train/                  |              |
|    approx_kl            | 9.720432e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -299         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29.8        |
|    n_updates            | 15690        |
|    policy_gradient_loss | -8.98e-05    |
|    std                  | 1.96         |
|    value_loss           | 1.15         |
------------------------------------------
Iteration: 1570 | Episodes: 63700 | Median Reward: 32.78 | Max Reward: 34.24
Iteration: 1573 | Episodes: 63800 | Median Reward: 32.78 | Max Reward: 34.24
Iteration: 1575 | Episodes: 63900 | Median Reward: 32.64 | Max Reward: 34.24
Iteration: 1578 | Episodes: 64000 | Median Reward: 31.77 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.8       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1580        |
|    time_elapsed         | 36097       |
|    total_timesteps      | 6471680     |
| train/                  |             |
|    approx_kl            | 0.051943533 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.3         |
|    entropy_loss         | -299        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -29.8       |
|    n_updates            | 15790       |
|    policy_gradient_loss | -0.0306     |
|    std                  | 1.98        |
|    value_loss           | 0.168       |
-----------------------------------------
Iteration: 1580 | Episodes: 64100 | Median Reward: 32.94 | Max Reward: 34.24
Iteration: 1583 | Episodes: 64200 | Median Reward: 29.78 | Max Reward: 34.24
Iteration: 1585 | Episodes: 64300 | Median Reward: 33.01 | Max Reward: 34.24
Iteration: 1587 | Episodes: 64400 | Median Reward: 33.13 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.7        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1590         |
|    time_elapsed         | 36324        |
|    total_timesteps      | 6512640      |
| train/                  |              |
|    approx_kl            | 0.0030547811 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -300         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -29.9        |
|    n_updates            | 15890        |
|    policy_gradient_loss | -0.00346     |
|    std                  | 1.99         |
|    value_loss           | 0.24         |
------------------------------------------
Iteration: 1590 | Episodes: 64500 | Median Reward: 31.40 | Max Reward: 34.24
Iteration: 1592 | Episodes: 64600 | Median Reward: 31.40 | Max Reward: 34.24
Iteration: 1595 | Episodes: 64700 | Median Reward: 30.25 | Max Reward: 34.24
Iteration: 1597 | Episodes: 64800 | Median Reward: 32.98 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.2         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1600          |
|    time_elapsed         | 36557         |
|    total_timesteps      | 6553600       |
| train/                  |               |
|    approx_kl            | 1.6984079e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -300          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -28.8         |
|    n_updates            | 15990         |
|    policy_gradient_loss | -6.16e-05     |
|    std                  | 2             |
|    value_loss           | 0.72          |
-------------------------------------------
Iteration: 1600 | Episodes: 64900 | Median Reward: 32.07 | Max Reward: 34.24
Iteration: 1602 | Episodes: 65000 | Median Reward: 33.15 | Max Reward: 34.24
Iteration: 1605 | Episodes: 65100 | Median Reward: 32.74 | Max Reward: 34.24
Iteration: 1607 | Episodes: 65200 | Median Reward: 32.64 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.1        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1610         |
|    time_elapsed         | 36787        |
|    total_timesteps      | 6594560      |
| train/                  |              |
|    approx_kl            | 0.0008260048 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -301         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.6        |
|    n_updates            | 16090        |
|    policy_gradient_loss | -0.00452     |
|    std                  | 2.01         |
|    value_loss           | 0.993        |
------------------------------------------
Iteration: 1610 | Episodes: 65300 | Median Reward: 33.01 | Max Reward: 34.24
Iteration: 1612 | Episodes: 65400 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1615 | Episodes: 65500 | Median Reward: 32.55 | Max Reward: 34.24
Iteration: 1617 | Episodes: 65600 | Median Reward: 32.59 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.7         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1620          |
|    time_elapsed         | 37020         |
|    total_timesteps      | 6635520       |
| train/                  |               |
|    approx_kl            | 0.00041616015 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -301          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -30           |
|    n_updates            | 16190         |
|    policy_gradient_loss | -0.00102      |
|    std                  | 2.02          |
|    value_loss           | 0.418         |
-------------------------------------------
Iteration: 1620 | Episodes: 65700 | Median Reward: 32.11 | Max Reward: 34.24
Iteration: 1622 | Episodes: 65800 | Median Reward: 31.80 | Max Reward: 34.24
Iteration: 1624 | Episodes: 65900 | Median Reward: 32.83 | Max Reward: 34.24
Iteration: 1627 | Episodes: 66000 | Median Reward: 33.15 | Max Reward: 34.24
Iteration: 1629 | Episodes: 66100 | Median Reward: 33.04 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.9         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1630          |
|    time_elapsed         | 37248         |
|    total_timesteps      | 6676480       |
| train/                  |               |
|    approx_kl            | 0.00011276259 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -301          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -29.9         |
|    n_updates            | 16290         |
|    policy_gradient_loss | -0.000283     |
|    std                  | 2.03          |
|    value_loss           | 0.595         |
-------------------------------------------
Iteration: 1632 | Episodes: 66200 | Median Reward: 32.99 | Max Reward: 34.24
Iteration: 1634 | Episodes: 66300 | Median Reward: 32.39 | Max Reward: 34.24
Iteration: 1637 | Episodes: 66400 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 1639 | Episodes: 66500 | Median Reward: 33.04 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.2         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1640          |
|    time_elapsed         | 37472         |
|    total_timesteps      | 6717440       |
| train/                  |               |
|    approx_kl            | 3.8316866e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -301          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -29.4         |
|    n_updates            | 16390         |
|    policy_gradient_loss | -5.84e-05     |
|    std                  | 2.04          |
|    value_loss           | 0.547         |
-------------------------------------------
Iteration: 1642 | Episodes: 66600 | Median Reward: 32.42 | Max Reward: 34.24
Iteration: 1644 | Episodes: 66700 | Median Reward: 32.76 | Max Reward: 34.24
Iteration: 1647 | Episodes: 66800 | Median Reward: 31.75 | Max Reward: 34.24
Iteration: 1649 | Episodes: 66900 | Median Reward: 32.91 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.7         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1650          |
|    time_elapsed         | 37706         |
|    total_timesteps      | 6758400       |
| train/                  |               |
|    approx_kl            | 1.8622843e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -302          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -29.6         |
|    n_updates            | 16490         |
|    policy_gradient_loss | 5.08e-05      |
|    std                  | 2.05          |
|    value_loss           | 2.58          |
-------------------------------------------
Iteration: 1652 | Episodes: 67000 | Median Reward: 30.76 | Max Reward: 34.24
Iteration: 1654 | Episodes: 67100 | Median Reward: 30.13 | Max Reward: 34.24
Iteration: 1657 | Episodes: 67200 | Median Reward: 32.98 | Max Reward: 34.24
Iteration: 1659 | Episodes: 67300 | Median Reward: 33.12 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.9         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1660          |
|    time_elapsed         | 37943         |
|    total_timesteps      | 6799360       |
| train/                  |               |
|    approx_kl            | 4.1820967e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -302          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -30.1         |
|    n_updates            | 16590         |
|    policy_gradient_loss | -0.000116     |
|    std                  | 2.06          |
|    value_loss           | 0.244         |
-------------------------------------------
Iteration: 1661 | Episodes: 67400 | Median Reward: 33.04 | Max Reward: 34.24
Iteration: 1664 | Episodes: 67500 | Median Reward: 31.58 | Max Reward: 34.24
Iteration: 1666 | Episodes: 67600 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 1669 | Episodes: 67700 | Median Reward: 32.29 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.5         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1670          |
|    time_elapsed         | 38166         |
|    total_timesteps      | 6840320       |
| train/                  |               |
|    approx_kl            | 1.7834827e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -302          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -30           |
|    n_updates            | 16690         |
|    policy_gradient_loss | -0.000127     |
|    std                  | 2.08          |
|    value_loss           | 0.705         |
-------------------------------------------
Iteration: 1671 | Episodes: 67800 | Median Reward: 31.44 | Max Reward: 34.24
Iteration: 1674 | Episodes: 67900 | Median Reward: 30.83 | Max Reward: 34.24
Iteration: 1676 | Episodes: 68000 | Median Reward: 32.89 | Max Reward: 34.24
Iteration: 1679 | Episodes: 68100 | Median Reward: 32.89 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.2        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1680         |
|    time_elapsed         | 38397        |
|    total_timesteps      | 6881280      |
| train/                  |              |
|    approx_kl            | 0.0012356995 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -303         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -29.8        |
|    n_updates            | 16790        |
|    policy_gradient_loss | -0.00572     |
|    std                  | 2.09         |
|    value_loss           | 0.563        |
------------------------------------------
Iteration: 1681 | Episodes: 68200 | Median Reward: 32.87 | Max Reward: 34.24
Iteration: 1684 | Episodes: 68300 | Median Reward: 31.84 | Max Reward: 34.24
Iteration: 1686 | Episodes: 68400 | Median Reward: 32.73 | Max Reward: 34.24
Iteration: 1689 | Episodes: 68500 | Median Reward: 32.64 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.4         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1690          |
|    time_elapsed         | 38622         |
|    total_timesteps      | 6922240       |
| train/                  |               |
|    approx_kl            | 0.00015777301 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -303          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -29.7         |
|    n_updates            | 16890         |
|    policy_gradient_loss | -0.000516     |
|    std                  | 2.1           |
|    value_loss           | 0.921         |
-------------------------------------------
Iteration: 1691 | Episodes: 68600 | Median Reward: 33.04 | Max Reward: 34.24
Iteration: 1694 | Episodes: 68700 | Median Reward: 30.93 | Max Reward: 34.24
Iteration: 1696 | Episodes: 68800 | Median Reward: 32.91 | Max Reward: 34.24
Iteration: 1698 | Episodes: 68900 | Median Reward: 32.76 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.3        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1700         |
|    time_elapsed         | 38847        |
|    total_timesteps      | 6963200      |
| train/                  |              |
|    approx_kl            | 0.0005508509 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -303         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -30.3        |
|    n_updates            | 16990        |
|    policy_gradient_loss | -0.00104     |
|    std                  | 2.11         |
|    value_loss           | 0.297        |
------------------------------------------
Iteration: 1701 | Episodes: 69000 | Median Reward: 33.08 | Max Reward: 34.24
Iteration: 1703 | Episodes: 69100 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1706 | Episodes: 69200 | Median Reward: 32.74 | Max Reward: 34.24
Iteration: 1708 | Episodes: 69300 | Median Reward: 32.81 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.2        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1710         |
|    time_elapsed         | 39070        |
|    total_timesteps      | 7004160      |
| train/                  |              |
|    approx_kl            | 0.0010664815 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -304         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -28.7        |
|    n_updates            | 17090        |
|    policy_gradient_loss | -0.00375     |
|    std                  | 2.14         |
|    value_loss           | 1.1          |
------------------------------------------
Iteration: 1711 | Episodes: 69400 | Median Reward: 32.93 | Max Reward: 34.24
Iteration: 1713 | Episodes: 69500 | Median Reward: 32.43 | Max Reward: 34.24
Iteration: 1716 | Episodes: 69600 | Median Reward: 32.42 | Max Reward: 34.24
Iteration: 1718 | Episodes: 69700 | Median Reward: 32.95 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.5         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1720          |
|    time_elapsed         | 39304         |
|    total_timesteps      | 7045120       |
| train/                  |               |
|    approx_kl            | 1.0096148e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -304          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -29.5         |
|    n_updates            | 17190         |
|    policy_gradient_loss | -1.26e-06     |
|    std                  | 2.15          |
|    value_loss           | 0.985         |
-------------------------------------------
Iteration: 1721 | Episodes: 69800 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 1723 | Episodes: 69900 | Median Reward: 32.87 | Max Reward: 34.24
Iteration: 1726 | Episodes: 70000 | Median Reward: 31.85 | Max Reward: 34.24
Iteration: 1728 | Episodes: 70100 | Median Reward: 31.76 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.9        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1730         |
|    time_elapsed         | 39539        |
|    total_timesteps      | 7086080      |
| train/                  |              |
|    approx_kl            | 0.0003575405 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -305         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -30.2        |
|    n_updates            | 17290        |
|    policy_gradient_loss | -0.000926    |
|    std                  | 2.16         |
|    value_loss           | 0.292        |
------------------------------------------
Iteration: 1731 | Episodes: 70200 | Median Reward: 33.00 | Max Reward: 34.24
Iteration: 1733 | Episodes: 70300 | Median Reward: 33.04 | Max Reward: 34.24
Iteration: 1735 | Episodes: 70400 | Median Reward: 32.89 | Max Reward: 34.24
Iteration: 1738 | Episodes: 70500 | Median Reward: 32.94 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.9        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1740         |
|    time_elapsed         | 39777        |
|    total_timesteps      | 7127040      |
| train/                  |              |
|    approx_kl            | 0.0018396141 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -305         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -30.4        |
|    n_updates            | 17390        |
|    policy_gradient_loss | -0.00213     |
|    std                  | 2.18         |
|    value_loss           | 0.574        |
------------------------------------------
Iteration: 1740 | Episodes: 70600 | Median Reward: 30.21 | Max Reward: 34.24
Iteration: 1743 | Episodes: 70700 | Median Reward: 33.14 | Max Reward: 34.24
Iteration: 1745 | Episodes: 70800 | Median Reward: 32.92 | Max Reward: 34.24
Iteration: 1748 | Episodes: 70900 | Median Reward: 32.13 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68           |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1750          |
|    time_elapsed         | 40005         |
|    total_timesteps      | 7168000       |
| train/                  |               |
|    approx_kl            | 5.4754855e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -306          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -30.5         |
|    n_updates            | 17490         |
|    policy_gradient_loss | -0.000271     |
|    std                  | 2.19          |
|    value_loss           | 0.538         |
-------------------------------------------
Iteration: 1750 | Episodes: 71000 | Median Reward: 32.89 | Max Reward: 34.24
Iteration: 1753 | Episodes: 71100 | Median Reward: 31.59 | Max Reward: 34.24
Iteration: 1755 | Episodes: 71200 | Median Reward: 32.63 | Max Reward: 34.24
Iteration: 1758 | Episodes: 71300 | Median Reward: 29.98 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.4        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1760         |
|    time_elapsed         | 40237        |
|    total_timesteps      | 7208960      |
| train/                  |              |
|    approx_kl            | 0.0033494793 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -307         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -30.4        |
|    n_updates            | 17590        |
|    policy_gradient_loss | -0.00659     |
|    std                  | 2.21         |
|    value_loss           | 0.401        |
------------------------------------------
Iteration: 1760 | Episodes: 71400 | Median Reward: 29.93 | Max Reward: 34.24
Iteration: 1763 | Episodes: 71500 | Median Reward: 32.72 | Max Reward: 34.24
Iteration: 1765 | Episodes: 71600 | Median Reward: 32.42 | Max Reward: 34.24
Iteration: 1767 | Episodes: 71700 | Median Reward: 32.70 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68          |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1770         |
|    time_elapsed         | 40467        |
|    total_timesteps      | 7249920      |
| train/                  |              |
|    approx_kl            | 0.0021497826 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -307         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -30          |
|    n_updates            | 17690        |
|    policy_gradient_loss | -0.00513     |
|    std                  | 2.22         |
|    value_loss           | 0.385        |
------------------------------------------
Iteration: 1770 | Episodes: 71800 | Median Reward: 32.91 | Max Reward: 34.24
Iteration: 1772 | Episodes: 71900 | Median Reward: 32.66 | Max Reward: 34.24
Iteration: 1775 | Episodes: 72000 | Median Reward: 32.66 | Max Reward: 34.24
Iteration: 1777 | Episodes: 72100 | Median Reward: 31.80 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.6         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1780          |
|    time_elapsed         | 40699         |
|    total_timesteps      | 7290880       |
| train/                  |               |
|    approx_kl            | 7.6801836e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -307          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -30.4         |
|    n_updates            | 17790         |
|    policy_gradient_loss | 0.000129      |
|    std                  | 2.24          |
|    value_loss           | 0.248         |
-------------------------------------------
Iteration: 1780 | Episodes: 72200 | Median Reward: 32.98 | Max Reward: 34.24
Iteration: 1782 | Episodes: 72300 | Median Reward: 30.09 | Max Reward: 34.24
Iteration: 1785 | Episodes: 72400 | Median Reward: 30.27 | Max Reward: 34.24
Iteration: 1787 | Episodes: 72500 | Median Reward: 29.59 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.8         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1790          |
|    time_elapsed         | 40931         |
|    total_timesteps      | 7331840       |
| train/                  |               |
|    approx_kl            | 0.00052131753 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -308          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -30.5         |
|    n_updates            | 17890         |
|    policy_gradient_loss | 6.66e-05      |
|    std                  | 2.25          |
|    value_loss           | 0.626         |
-------------------------------------------
Iteration: 1790 | Episodes: 72600 | Median Reward: 30.88 | Max Reward: 34.24
Iteration: 1792 | Episodes: 72700 | Median Reward: 32.88 | Max Reward: 34.24
Iteration: 1795 | Episodes: 72800 | Median Reward: 32.96 | Max Reward: 34.24
Iteration: 1797 | Episodes: 72900 | Median Reward: 30.80 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69          |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1800         |
|    time_elapsed         | 41164        |
|    total_timesteps      | 7372800      |
| train/                  |              |
|    approx_kl            | 0.0005732842 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -308         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -30.3        |
|    n_updates            | 17990        |
|    policy_gradient_loss | -0.000626    |
|    std                  | 2.27         |
|    value_loss           | 1.24         |
------------------------------------------
Iteration: 1800 | Episodes: 73000 | Median Reward: 30.80 | Max Reward: 34.24
Iteration: 1802 | Episodes: 73100 | Median Reward: 32.33 | Max Reward: 34.24
Iteration: 1804 | Episodes: 73200 | Median Reward: 32.62 | Max Reward: 34.24
Iteration: 1807 | Episodes: 73300 | Median Reward: 32.35 | Max Reward: 34.24
Iteration: 1809 | Episodes: 73400 | Median Reward: 32.97 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.5        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1810         |
|    time_elapsed         | 41388        |
|    total_timesteps      | 7413760      |
| train/                  |              |
|    approx_kl            | 6.005951e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -308         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -30.7        |
|    n_updates            | 18090        |
|    policy_gradient_loss | -0.000226    |
|    std                  | 2.29         |
|    value_loss           | 0.558        |
------------------------------------------
Iteration: 1812 | Episodes: 73500 | Median Reward: 30.23 | Max Reward: 34.24
Iteration: 1814 | Episodes: 73600 | Median Reward: 32.39 | Max Reward: 34.24
Iteration: 1817 | Episodes: 73700 | Median Reward: 32.84 | Max Reward: 34.24
Iteration: 1819 | Episodes: 73800 | Median Reward: 32.84 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.1         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1820          |
|    time_elapsed         | 41618         |
|    total_timesteps      | 7454720       |
| train/                  |               |
|    approx_kl            | 0.00015963265 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -309          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -30.7         |
|    n_updates            | 18190         |
|    policy_gradient_loss | 0.000675      |
|    std                  | 2.3           |
|    value_loss           | 0.578         |
-------------------------------------------
Iteration: 1822 | Episodes: 73900 | Median Reward: 29.81 | Max Reward: 34.24
Iteration: 1824 | Episodes: 74000 | Median Reward: 32.89 | Max Reward: 34.24
Iteration: 1827 | Episodes: 74100 | Median Reward: 31.76 | Max Reward: 34.24
Iteration: 1829 | Episodes: 74200 | Median Reward: 32.96 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.3         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1830          |
|    time_elapsed         | 41845         |
|    total_timesteps      | 7495680       |
| train/                  |               |
|    approx_kl            | 0.00018773167 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -309          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -30.8         |
|    n_updates            | 18290         |
|    policy_gradient_loss | -0.000325     |
|    std                  | 2.31          |
|    value_loss           | 0.579         |
-------------------------------------------
Iteration: 1832 | Episodes: 74300 | Median Reward: 32.68 | Max Reward: 34.24
Iteration: 1834 | Episodes: 74400 | Median Reward: 33.07 | Max Reward: 34.24
Iteration: 1837 | Episodes: 74500 | Median Reward: 32.64 | Max Reward: 34.24
Iteration: 1839 | Episodes: 74600 | Median Reward: 32.39 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.4         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1840          |
|    time_elapsed         | 42072         |
|    total_timesteps      | 7536640       |
| train/                  |               |
|    approx_kl            | 0.00024154298 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -310          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -30.7         |
|    n_updates            | 18390         |
|    policy_gradient_loss | -0.000554     |
|    std                  | 2.33          |
|    value_loss           | 0.498         |
-------------------------------------------
Iteration: 1841 | Episodes: 74700 | Median Reward: 31.25 | Max Reward: 34.24
Iteration: 1844 | Episodes: 74800 | Median Reward: 31.87 | Max Reward: 34.24
Iteration: 1846 | Episodes: 74900 | Median Reward: 31.04 | Max Reward: 34.24
Iteration: 1849 | Episodes: 75000 | Median Reward: 29.70 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.9         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1850          |
|    time_elapsed         | 42301         |
|    total_timesteps      | 7577600       |
| train/                  |               |
|    approx_kl            | 0.00018999973 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -310          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -30.3         |
|    n_updates            | 18490         |
|    policy_gradient_loss | -0.000292     |
|    std                  | 2.34          |
|    value_loss           | 1.31          |
-------------------------------------------
Iteration: 1851 | Episodes: 75100 | Median Reward: 32.48 | Max Reward: 34.24
Iteration: 1854 | Episodes: 75200 | Median Reward: 32.01 | Max Reward: 34.24
Iteration: 1856 | Episodes: 75300 | Median Reward: 32.85 | Max Reward: 34.24
Iteration: 1859 | Episodes: 75400 | Median Reward: 33.08 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.8        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1860         |
|    time_elapsed         | 42529        |
|    total_timesteps      | 7618560      |
| train/                  |              |
|    approx_kl            | 0.0021806185 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -310         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -30.9        |
|    n_updates            | 18590        |
|    policy_gradient_loss | -0.00365     |
|    std                  | 2.36         |
|    value_loss           | 0.412        |
------------------------------------------
Iteration: 1861 | Episodes: 75500 | Median Reward: 32.14 | Max Reward: 34.24
Iteration: 1864 | Episodes: 75600 | Median Reward: 32.85 | Max Reward: 34.24
Iteration: 1866 | Episodes: 75700 | Median Reward: 32.81 | Max Reward: 34.24
Iteration: 1869 | Episodes: 75800 | Median Reward: 33.01 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.4        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1870         |
|    time_elapsed         | 42756        |
|    total_timesteps      | 7659520      |
| train/                  |              |
|    approx_kl            | 9.185154e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -311         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -30.9        |
|    n_updates            | 18690        |
|    policy_gradient_loss | -2.94e-05    |
|    std                  | 2.37         |
|    value_loss           | 0.42         |
------------------------------------------
Iteration: 1871 | Episodes: 75900 | Median Reward: 31.89 | Max Reward: 34.24
Iteration: 1874 | Episodes: 76000 | Median Reward: 31.33 | Max Reward: 34.24
Iteration: 1876 | Episodes: 76100 | Median Reward: 32.66 | Max Reward: 34.24
Iteration: 1878 | Episodes: 76200 | Median Reward: 31.40 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.2         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1880          |
|    time_elapsed         | 42986         |
|    total_timesteps      | 7700480       |
| train/                  |               |
|    approx_kl            | 0.00020116131 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -311          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -30.5         |
|    n_updates            | 18790         |
|    policy_gradient_loss | -0.000682     |
|    std                  | 2.38          |
|    value_loss           | 0.604         |
-------------------------------------------
Iteration: 1881 | Episodes: 76300 | Median Reward: 31.36 | Max Reward: 34.24
Iteration: 1883 | Episodes: 76400 | Median Reward: 30.19 | Max Reward: 34.24
Iteration: 1886 | Episodes: 76500 | Median Reward: 33.05 | Max Reward: 34.24
Iteration: 1888 | Episodes: 76600 | Median Reward: 32.98 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -69.3         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1890          |
|    time_elapsed         | 43210         |
|    total_timesteps      | 7741440       |
| train/                  |               |
|    approx_kl            | 6.8378184e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -312          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -30.8         |
|    n_updates            | 18890         |
|    policy_gradient_loss | -0.00015      |
|    std                  | 2.4           |
|    value_loss           | 0.65          |
-------------------------------------------
Iteration: 1891 | Episodes: 76700 | Median Reward: 31.37 | Max Reward: 34.24
Iteration: 1893 | Episodes: 76800 | Median Reward: 30.06 | Max Reward: 34.24
Iteration: 1896 | Episodes: 76900 | Median Reward: 31.59 | Max Reward: 34.24
Iteration: 1898 | Episodes: 77000 | Median Reward: 30.20 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.4        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1900         |
|    time_elapsed         | 43436        |
|    total_timesteps      | 7782400      |
| train/                  |              |
|    approx_kl            | 0.0011419142 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -312         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -31.2        |
|    n_updates            | 18990        |
|    policy_gradient_loss | -0.00308     |
|    std                  | 2.41         |
|    value_loss           | 0.564        |
------------------------------------------
Iteration: 1901 | Episodes: 77100 | Median Reward: 30.26 | Max Reward: 34.24
Iteration: 1903 | Episodes: 77200 | Median Reward: 32.88 | Max Reward: 34.24
Iteration: 1906 | Episodes: 77300 | Median Reward: 31.84 | Max Reward: 34.24
Iteration: 1908 | Episodes: 77400 | Median Reward: 33.14 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.3         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1910          |
|    time_elapsed         | 43669         |
|    total_timesteps      | 7823360       |
| train/                  |               |
|    approx_kl            | 0.00097170676 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -313          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -31.1         |
|    n_updates            | 19090         |
|    policy_gradient_loss | -0.00254      |
|    std                  | 2.43          |
|    value_loss           | 0.22          |
-------------------------------------------
Iteration: 1911 | Episodes: 77500 | Median Reward: 32.85 | Max Reward: 34.24
Iteration: 1913 | Episodes: 77600 | Median Reward: 32.43 | Max Reward: 34.24
Iteration: 1915 | Episodes: 77700 | Median Reward: 30.12 | Max Reward: 34.24
Iteration: 1918 | Episodes: 77800 | Median Reward: 32.63 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.5       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1920        |
|    time_elapsed         | 43899       |
|    total_timesteps      | 7864320     |
| train/                  |             |
|    approx_kl            | 0.021898884 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.3         |
|    entropy_loss         | -313        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -31.1       |
|    n_updates            | 19190       |
|    policy_gradient_loss | -0.0359     |
|    std                  | 2.44        |
|    value_loss           | 1           |
-----------------------------------------
Iteration: 1920 | Episodes: 77900 | Median Reward: 31.74 | Max Reward: 34.24
Iteration: 1923 | Episodes: 78000 | Median Reward: 30.13 | Max Reward: 34.24
Iteration: 1925 | Episodes: 78100 | Median Reward: 32.32 | Max Reward: 34.24
Iteration: 1928 | Episodes: 78200 | Median Reward: 33.07 | Max Reward: 34.24
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.2         |
| time/                   |               |
|    fps                  | 179           |
|    iterations           | 1930          |
|    time_elapsed         | 44128         |
|    total_timesteps      | 7905280       |
| train/                  |               |
|    approx_kl            | 0.00022218545 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -314          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -31.2         |
|    n_updates            | 19290         |
|    policy_gradient_loss | -1.74e-05     |
|    std                  | 2.45          |
|    value_loss           | 0.345         |
-------------------------------------------
Iteration: 1930 | Episodes: 78300 | Median Reward: 28.92 | Max Reward: 34.24
Iteration: 1933 | Episodes: 78400 | Median Reward: 29.50 | Max Reward: 34.24
Iteration: 1935 | Episodes: 78500 | Median Reward: 30.27 | Max Reward: 34.24
Iteration: 1938 | Episodes: 78600 | Median Reward: 30.06 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.3        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1940         |
|    time_elapsed         | 44353        |
|    total_timesteps      | 7946240      |
| train/                  |              |
|    approx_kl            | 0.0006398226 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -314         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -30.9        |
|    n_updates            | 19390        |
|    policy_gradient_loss | -0.0017      |
|    std                  | 2.47         |
|    value_loss           | 0.35         |
------------------------------------------
Iteration: 1940 | Episodes: 78700 | Median Reward: 32.90 | Max Reward: 34.24
Iteration: 1943 | Episodes: 78800 | Median Reward: 29.64 | Max Reward: 34.24
Iteration: 1945 | Episodes: 78900 | Median Reward: 32.51 | Max Reward: 34.24
Iteration: 1947 | Episodes: 79000 | Median Reward: 32.20 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.1        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1950         |
|    time_elapsed         | 44582        |
|    total_timesteps      | 7987200      |
| train/                  |              |
|    approx_kl            | 0.0006196647 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -314         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -31.3        |
|    n_updates            | 19490        |
|    policy_gradient_loss | -0.00318     |
|    std                  | 2.48         |
|    value_loss           | 0.367        |
------------------------------------------
Iteration: 1950 | Episodes: 79100 | Median Reward: 32.72 | Max Reward: 34.24
Iteration: 1952 | Episodes: 79200 | Median Reward: 30.10 | Max Reward: 34.24
Iteration: 1955 | Episodes: 79300 | Median Reward: 32.43 | Max Reward: 34.24
Iteration: 1957 | Episodes: 79400 | Median Reward: 31.75 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.5        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1960         |
|    time_elapsed         | 44807        |
|    total_timesteps      | 8028160      |
| train/                  |              |
|    approx_kl            | 0.0001790251 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -315         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -31.4        |
|    n_updates            | 19590        |
|    policy_gradient_loss | -0.000253    |
|    std                  | 2.49         |
|    value_loss           | 0.548        |
------------------------------------------
Iteration: 1960 | Episodes: 79500 | Median Reward: 32.45 | Max Reward: 34.24
Iteration: 1962 | Episodes: 79600 | Median Reward: 32.54 | Max Reward: 34.24
Iteration: 1965 | Episodes: 79700 | Median Reward: 31.69 | Max Reward: 34.24
Iteration: 1967 | Episodes: 79800 | Median Reward: 30.03 | Max Reward: 34.24
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.5        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 1970         |
|    time_elapsed         | 45033        |
|    total_timesteps      | 8069120      |
| train/                  |              |
|    approx_kl            | 0.0006422414 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -315         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -31.3        |
|    n_updates            | 19690        |
|    policy_gradient_loss | 0.000514     |
|    std                  | 2.51         |
|    value_loss           | 0.279        |
------------------------------------------
Iteration: 1970 | Episodes: 79900 | Median Reward: 32.69 | Max Reward: 34.24
Iteration: 1972 | Episodes: 80000 | Median Reward: 32.68 | Max Reward: 34.24
Iteration: 1975 | Episodes: 80100 | Median Reward: 32.63 | Max Reward: 34.24
Iteration: 1977 | Episodes: 80200 | Median Reward: 30.26 | Max Reward: 34.24
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.4       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 1980        |
|    time_elapsed         | 45264       |
|    total_timesteps      | 8110080     |
| train/                  |             |
|    approx_kl            | 0.003966678 |
|    clip_fraction        | 0        