Script started on 2024-10-23 16:38:47-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/43" COLUMNS="185" LINES="53"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> conda activate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python code1_1_residual_custom_c.py
GPU 1: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
Iteration: 2 | Episodes: 100 | Median Reward: 22.25 | Max Reward: 29.66
Iteration: 4 | Episodes: 200 | Median Reward: 15.69 | Max Reward: 29.66
Iteration: 7 | Episodes: 300 | Median Reward: 10.40 | Max Reward: 41.84
Iteration: 9 | Episodes: 400 | Median Reward: 10.40 | Max Reward: 41.84
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -87.5      |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 10         |
|    time_elapsed         | 201        |
|    total_timesteps      | 40960      |
| train/                  |            |
|    approx_kl            | 0.44905666 |
|    clip_fraction        | 0.464      |
|    clip_range           | 0.3        |
|    entropy_loss         | -80.6      |
|    explained_variance   | 0.0136     |
|    learning_rate        | 0.0005     |
|    loss                 | 100        |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.0332     |
|    std                  | 1.01       |
|    value_loss           | 220        |
----------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 9.23 | Max Reward: 41.84
Iteration: 14 | Episodes: 600 | Median Reward: 13.22 | Max Reward: 41.84
Iteration: 17 | Episodes: 700 | Median Reward: 21.39 | Max Reward: 41.84
Iteration: 19 | Episodes: 800 | Median Reward: 22.66 | Max Reward: 41.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80         |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 20          |
|    time_elapsed         | 405         |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.008897891 |
|    clip_fraction        | 0.000415    |
|    clip_range           | 0.3         |
|    entropy_loss         | -82.9       |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0005      |
|    loss                 | 18.5        |
|    n_updates            | 190         |
|    policy_gradient_loss | 0.0113      |
|    std                  | 1.01        |
|    value_loss           | 66          |
-----------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 13.07 | Max Reward: 41.84
Iteration: 24 | Episodes: 1000 | Median Reward: 18.22 | Max Reward: 41.84
Iteration: 27 | Episodes: 1100 | Median Reward: 12.02 | Max Reward: 41.84
Iteration: 29 | Episodes: 1200 | Median Reward: 19.53 | Max Reward: 41.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -79.4       |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 30          |
|    time_elapsed         | 615         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.025659855 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.3         |
|    entropy_loss         | -83.4       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0005      |
|    loss                 | 3.31        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.016      |
|    std                  | 1.01        |
|    value_loss           | 22.5        |
-----------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 13.47 | Max Reward: 41.84
Iteration: 34 | Episodes: 1400 | Median Reward: 18.48 | Max Reward: 41.84
Iteration: 36 | Episodes: 1500 | Median Reward: 22.38 | Max Reward: 41.84
Iteration: 39 | Episodes: 1600 | Median Reward: 23.98 | Max Reward: 41.84
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.3       |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 40          |
|    time_elapsed         | 827         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.087083206 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.3         |
|    entropy_loss         | -98.6       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0005      |
|    loss                 | -8.07       |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.0121      |
|    std                  | 1.01        |
|    value_loss           | 24.8        |
-----------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 11.21 | Max Reward: 41.84
Iteration: 44 | Episodes: 1800 | Median Reward: 19.31 | Max Reward: 41.84
Iteration: 46 | Episodes: 1900 | Median Reward: 3.92 | Max Reward: 41.84
Iteration: 49 | Episodes: 2000 | Median Reward: 22.44 | Max Reward: 41.84
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.5         |
| time/                   |               |
|    fps                  | 197           |
|    iterations           | 50            |
|    time_elapsed         | 1034          |
|    total_timesteps      | 204800        |
| train/                  |               |
|    approx_kl            | 0.00013716255 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -108          |
|    explained_variance   | 0.985         |
|    learning_rate        | 0.0005        |
|    loss                 | -3.64         |
|    n_updates            | 490           |
|    policy_gradient_loss | 0.000576      |
|    std                  | 1.01          |
|    value_loss           | 26.8          |
-------------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 23.29 | Max Reward: 44.26
Iteration: 54 | Episodes: 2200 | Median Reward: 16.75 | Max Reward: 44.26
Iteration: 56 | Episodes: 2300 | Median Reward: 21.97 | Max Reward: 44.26
Iteration: 59 | Episodes: 2400 | Median Reward: 23.19 | Max Reward: 44.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -79.2       |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 60          |
|    time_elapsed         | 1246        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.019262522 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.3         |
|    entropy_loss         | -120        |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0005      |
|    loss                 | -9.62       |
|    n_updates            | 590         |
|    policy_gradient_loss | -0.021      |
|    std                  | 1.01        |
|    value_loss           | 4.16        |
-----------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 29.28 | Max Reward: 44.26
Iteration: 64 | Episodes: 2600 | Median Reward: 21.86 | Max Reward: 44.26
Iteration: 66 | Episodes: 2700 | Median Reward: 24.54 | Max Reward: 44.26
Iteration: 69 | Episodes: 2800 | Median Reward: 13.17 | Max Reward: 44.26
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -83.5      |
| time/                   |            |
|    fps                  | 197        |
|    iterations           | 70         |
|    time_elapsed         | 1452       |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.08160304 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.3        |
|    entropy_loss         | -126       |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0005     |
|    loss                 | -10.2      |
|    n_updates            | 690        |
|    policy_gradient_loss | 0.0148     |
|    std                  | 1.01       |
|    value_loss           | 3.46       |
----------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 22.15 | Max Reward: 44.26
Iteration: 73 | Episodes: 3000 | Median Reward: 21.49 | Max Reward: 44.26
Iteration: 76 | Episodes: 3100 | Median Reward: 24.88 | Max Reward: 44.26
Iteration: 78 | Episodes: 3200 | Median Reward: 27.71 | Max Reward: 44.26
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.8        |
| time/                   |              |
|    fps                  | 197          |
|    iterations           | 80           |
|    time_elapsed         | 1656         |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.0041967034 |
|    clip_fraction        | 0.00366      |
|    clip_range           | 0.3          |
|    entropy_loss         | -145         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.4        |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.000739    |
|    std                  | 1.01         |
|    value_loss           | 3.83         |
------------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 28.36 | Max Reward: 44.26
Iteration: 83 | Episodes: 3400 | Median Reward: 26.37 | Max Reward: 44.26
Iteration: 86 | Episodes: 3500 | Median Reward: 26.30 | Max Reward: 44.26
Iteration: 88 | Episodes: 3600 | Median Reward: 24.27 | Max Reward: 44.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.7       |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 90          |
|    time_elapsed         | 1858        |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.027650634 |
|    clip_fraction        | 0.00586     |
|    clip_range           | 0.3         |
|    entropy_loss         | -149        |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0005      |
|    loss                 | -14         |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00261    |
|    std                  | 1.02        |
|    value_loss           | 4.37        |
-----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 26.19 | Max Reward: 44.26
Iteration: 93 | Episodes: 3800 | Median Reward: 25.02 | Max Reward: 44.26
Iteration: 96 | Episodes: 3900 | Median Reward: 23.28 | Max Reward: 44.26
Iteration: 98 | Episodes: 4000 | Median Reward: 30.26 | Max Reward: 44.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -73.1       |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 100         |
|    time_elapsed         | 2055        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.023157164 |
|    clip_fraction        | 0.0789      |
|    clip_range           | 0.3         |
|    entropy_loss         | -161        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -13.7       |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0025     |
|    std                  | 1.02        |
|    value_loss           | 3.18        |
-----------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 29.24 | Max Reward: 44.26
Iteration: 103 | Episodes: 4200 | Median Reward: 28.25 | Max Reward: 44.26
Iteration: 106 | Episodes: 4300 | Median Reward: 27.37 | Max Reward: 44.26
Iteration: 108 | Episodes: 4400 | Median Reward: 26.83 | Max Reward: 44.26
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.1       |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 110         |
|    time_elapsed         | 2258        |
|    total_timesteps      | 450560      |
| train/                  |             |
|    approx_kl            | 0.004023765 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -166        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -15.1       |
|    n_updates            | 1090        |
|    policy_gradient_loss | 0.00122     |
|    std                  | 1.02        |
|    value_loss           | 3.24        |
-----------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 26.88 | Max Reward: 44.26
Iteration: 113 | Episodes: 4600 | Median Reward: 26.67 | Max Reward: 45.37
aIteration: 115 | Episodes: 4700 | Median Reward: 29.06 | Max Reward: 45.37
Iteration: 118 | Episodes: 4800 | Median Reward: 28.49 | Max Reward: 45.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -72.1       |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 120         |
|    time_elapsed         | 2459        |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.084429115 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.3         |
|    entropy_loss         | -172        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -13.4       |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0564     |
|    std                  | 1.02        |
|    value_loss           | 3.35        |
-----------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 34.90 | Max Reward: 45.37
Iteration: 123 | Episodes: 5000 | Median Reward: 31.96 | Max Reward: 45.37
Iteration: 125 | Episodes: 5100 | Median Reward: 31.52 | Max Reward: 45.37
Iteration: 128 | Episodes: 5200 | Median Reward: 29.58 | Max Reward: 45.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.3       |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 130         |
|    time_elapsed         | 2657        |
|    total_timesteps      | 532480      |
| train/                  |             |
|    approx_kl            | 0.011954738 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.3         |
|    entropy_loss         | -176        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -17         |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0264     |
|    std                  | 1.02        |
|    value_loss           | 3.49        |
-----------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 32.39 | Max Reward: 45.37
Iteration: 133 | Episodes: 5400 | Median Reward: 31.67 | Max Reward: 45.37
Iteration: 135 | Episodes: 5500 | Median Reward: 31.71 | Max Reward: 45.37
Iteration: 138 | Episodes: 5600 | Median Reward: 33.23 | Max Reward: 45.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.8       |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 140         |
|    time_elapsed         | 2863        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.022165015 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.3         |
|    entropy_loss         | -182        |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0005      |
|    loss                 | -15.9       |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0035     |
|    std                  | 1.02        |
|    value_loss           | 3.76        |
-----------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 30.92 | Max Reward: 45.37
Iteration: 143 | Episodes: 5800 | Median Reward: 30.61 | Max Reward: 45.37
Iteration: 145 | Episodes: 5900 | Median Reward: 35.43 | Max Reward: 45.37
Iteration: 147 | Episodes: 6000 | Median Reward: 35.47 | Max Reward: 45.37
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.2       |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 150         |
|    time_elapsed         | 3066        |
|    total_timesteps      | 614400      |
| train/                  |             |
|    approx_kl            | 0.012430488 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -186        |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0005      |
|    loss                 | -15.4       |
|    n_updates            | 1490        |
|    policy_gradient_loss | -0.0134     |
|    std                  | 1.03        |
|    value_loss           | 7.39        |
-----------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 32.00 | Max Reward: 45.37
Iteration: 152 | Episodes: 6200 | Median Reward: 32.80 | Max Reward: 46.64
Iteration: 155 | Episodes: 6300 | Median Reward: 30.84 | Max Reward: 46.64
Iteration: 157 | Episodes: 6400 | Median Reward: 31.74 | Max Reward: 46.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.1       |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 160         |
|    time_elapsed         | 3270        |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.011956437 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -189        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -17.1       |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00749    |
|    std                  | 1.03        |
|    value_loss           | 4.47        |
-----------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 34.42 | Max Reward: 46.64
Iteration: 162 | Episodes: 6600 | Median Reward: 35.00 | Max Reward: 46.64
Iteration: 165 | Episodes: 6700 | Median Reward: 37.03 | Max Reward: 46.64
Iteration: 167 | Episodes: 6800 | Median Reward: 37.97 | Max Reward: 46.64
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -64.2      |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 170        |
|    time_elapsed         | 3464       |
|    total_timesteps      | 696320     |
| train/                  |            |
|    approx_kl            | 0.07242125 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.3        |
|    entropy_loss         | -192       |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0005     |
|    loss                 | -18.8      |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0239    |
|    std                  | 1.03       |
|    value_loss           | 1.06       |
----------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 36.45 | Max Reward: 46.64
Iteration: 172 | Episodes: 7000 | Median Reward: 37.99 | Max Reward: 46.64
Iteration: 175 | Episodes: 7100 | Median Reward: 34.68 | Max Reward: 46.64
Iteration: 177 | Episodes: 7200 | Median Reward: 36.32 | Max Reward: 46.64
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.9       |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 180         |
|    time_elapsed         | 3655        |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.044327475 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.3         |
|    entropy_loss         | -198        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -17.1       |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.0437     |
|    std                  | 1.03        |
|    value_loss           | 2.64        |
-----------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 38.24 | Max Reward: 47.19
Iteration: 182 | Episodes: 7400 | Median Reward: 35.03 | Max Reward: 47.19
Iteration: 184 | Episodes: 7500 | Median Reward: 35.44 | Max Reward: 47.19
Iteration: 187 | Episodes: 7600 | Median Reward: 36.37 | Max Reward: 47.19
Iteration: 189 | Episodes: 7700 | Median Reward: 39.80 | Max Reward: 47.31
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.5        |
| time/                   |              |
|    fps                  | 201          |
|    iterations           | 190          |
|    time_elapsed         | 3853         |
|    total_timesteps      | 778240       |
| train/                  |              |
|    approx_kl            | 0.0021247112 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -200         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0005       |
|    loss                 | -19          |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.00501     |
|    std                  | 1.03         |
|    value_loss           | 3.76         |
------------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 36.40 | Max Reward: 47.31
Iteration: 194 | Episodes: 7900 | Median Reward: 38.50 | Max Reward: 47.31
Iteration: 197 | Episodes: 8000 | Median Reward: 39.06 | Max Reward: 47.31
Iteration: 199 | Episodes: 8100 | Median Reward: 40.16 | Max Reward: 47.31
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.6        |
| time/                   |              |
|    fps                  | 202          |
|    iterations           | 200          |
|    time_elapsed         | 4049         |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0040761065 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -202         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -11.9        |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00696     |
|    std                  | 1.03         |
|    value_loss           | 5.64         |
------------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 38.80 | Max Reward: 47.31
Iteration: 204 | Episodes: 8300 | Median Reward: 39.00 | Max Reward: 47.31
Iteration: 207 | Episodes: 8400 | Median Reward: 39.31 | Max Reward: 47.66
Iteration: 209 | Episodes: 8500 | Median Reward: 39.00 | Max Reward: 47.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.5       |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 210         |
|    time_elapsed         | 4245        |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.015609562 |
|    clip_fraction        | 0.00605     |
|    clip_range           | 0.3         |
|    entropy_loss         | -204        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.5       |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.0213     |
|    std                  | 1.04        |
|    value_loss           | 4.59        |
-----------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 34.28 | Max Reward: 47.73
Iteration: 214 | Episodes: 8700 | Median Reward: 39.74 | Max Reward: 47.73
Iteration: 216 | Episodes: 8800 | Median Reward: 40.15 | Max Reward: 47.73
Iteration: 219 | Episodes: 8900 | Median Reward: 40.28 | Max Reward: 47.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.6       |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 220         |
|    time_elapsed         | 4443        |
|    total_timesteps      | 901120      |
| train/                  |             |
|    approx_kl            | 0.010851913 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -206        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -19         |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.0168     |
|    std                  | 1.04        |
|    value_loss           | 1.61        |
-----------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 38.73 | Max Reward: 47.73
Iteration: 224 | Episodes: 9100 | Median Reward: 37.77 | Max Reward: 47.73
Iteration: 226 | Episodes: 9200 | Median Reward: 41.20 | Max Reward: 47.73
Iteration: 229 | Episodes: 9300 | Median Reward: 42.32 | Max Reward: 47.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.4       |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 230         |
|    time_elapsed         | 4648        |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.023686625 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.3         |
|    entropy_loss         | -208        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -17.8       |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.0483     |
|    std                  | 1.04        |
|    value_loss           | 3.81        |
-----------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 43.04 | Max Reward: 47.73
Iteration: 234 | Episodes: 9500 | Median Reward: 40.01 | Max Reward: 47.73
Iteration: 236 | Episodes: 9600 | Median Reward: 40.19 | Max Reward: 47.73
Iteration: 239 | Episodes: 9700 | Median Reward: 39.47 | Max Reward: 47.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.2        |
| time/                   |              |
|    fps                  | 202          |
|    iterations           | 240          |
|    time_elapsed         | 4850         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0022134818 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -210         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.9        |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00339     |
|    std                  | 1.04         |
|    value_loss           | 1.27         |
------------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 40.23 | Max Reward: 47.73
Iteration: 244 | Episodes: 9900 | Median Reward: 42.61 | Max Reward: 47.73
Iteration: 246 | Episodes: 10000 | Median Reward: 40.65 | Max Reward: 47.73
Iteration: 249 | Episodes: 10100 | Median Reward: 38.70 | Max Reward: 47.73
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -61.4      |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 250        |
|    time_elapsed         | 5051       |
|    total_timesteps      | 1024000    |
| train/                  |            |
|    approx_kl            | 0.47457904 |
|    clip_fraction        | 0.7        |
|    clip_range           | 0.3        |
|    entropy_loss         | -213       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -20.7      |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.0654    |
|    std                  | 1.05       |
|    value_loss           | 1.02       |
----------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 39.86 | Max Reward: 47.73
Iteration: 253 | Episodes: 10300 | Median Reward: 40.68 | Max Reward: 48.02
Iteration: 256 | Episodes: 10400 | Median Reward: 43.34 | Max Reward: 48.02
Iteration: 258 | Episodes: 10500 | Median Reward: 42.05 | Max Reward: 48.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.3       |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 260         |
|    time_elapsed         | 5246        |
|    total_timesteps      | 1064960     |
| train/                  |             |
|    approx_kl            | 0.017627614 |
|    clip_fraction        | 0.000488    |
|    clip_range           | 0.3         |
|    entropy_loss         | -215        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -20         |
|    n_updates            | 2590        |
|    policy_gradient_loss | -0.0247     |
|    std                  | 1.05        |
|    value_loss           | 2.25        |
-----------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 42.19 | Max Reward: 48.02
Iteration: 263 | Episodes: 10700 | Median Reward: 43.99 | Max Reward: 48.02
Iteration: 266 | Episodes: 10800 | Median Reward: 42.44 | Max Reward: 48.02
Iteration: 268 | Episodes: 10900 | Median Reward: 43.17 | Max Reward: 48.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 203          |
|    iterations           | 270          |
|    time_elapsed         | 5443         |
|    total_timesteps      | 1105920      |
| train/                  |              |
|    approx_kl            | 0.0017604011 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -216         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.5        |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.00404     |
|    std                  | 1.05         |
|    value_loss           | 5.51         |
------------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 44.85 | Max Reward: 48.02
Iteration: 273 | Episodes: 11100 | Median Reward: 43.33 | Max Reward: 48.02
Iteration: 276 | Episodes: 11200 | Median Reward: 45.88 | Max Reward: 48.02
Iteration: 278 | Episodes: 11300 | Median Reward: 43.03 | Max Reward: 48.02
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.7         |
| time/                   |               |
|    fps                  | 203           |
|    iterations           | 280           |
|    time_elapsed         | 5639          |
|    total_timesteps      | 1146880       |
| train/                  |               |
|    approx_kl            | 0.00037657964 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -217          |
|    explained_variance   | 0.988         |
|    learning_rate        | 0.0005        |
|    loss                 | -20.4         |
|    n_updates            | 2790          |
|    policy_gradient_loss | -0.000674     |
|    std                  | 1.05          |
|    value_loss           | 4.9           |
-------------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 42.97 | Max Reward: 48.02
Iteration: 283 | Episodes: 11500 | Median Reward: 42.57 | Max Reward: 48.02
Iteration: 286 | Episodes: 11600 | Median Reward: 42.61 | Max Reward: 48.02
Iteration: 288 | Episodes: 11700 | Median Reward: 45.49 | Max Reward: 48.02
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.5         |
| time/                   |               |
|    fps                  | 203           |
|    iterations           | 290           |
|    time_elapsed         | 5838          |
|    total_timesteps      | 1187840       |
| train/                  |               |
|    approx_kl            | 0.00016964205 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -218          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.0005        |
|    loss                 | -19.2         |
|    n_updates            | 2890          |
|    policy_gradient_loss | 2.52e-05      |
|    std                  | 1.06          |
|    value_loss           | 3.84          |
-------------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 43.94 | Max Reward: 48.02
Iteration: 293 | Episodes: 11900 | Median Reward: 40.23 | Max Reward: 48.02
Iteration: 295 | Episodes: 12000 | Median Reward: 42.63 | Max Reward: 48.02
Iteration: 298 | Episodes: 12100 | Median Reward: 43.52 | Max Reward: 48.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 203          |
|    iterations           | 300          |
|    time_elapsed         | 6037         |
|    total_timesteps      | 1228800      |
| train/                  |              |
|    approx_kl            | 0.0014710787 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -219         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0005       |
|    loss                 | -21.4        |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 1.06         |
|    value_loss           | 4.24         |
------------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 44.05 | Max Reward: 48.02
Iteration: 303 | Episodes: 12300 | Median Reward: 43.25 | Max Reward: 48.02
Iteration: 305 | Episodes: 12400 | Median Reward: 44.67 | Max Reward: 48.02
Iteration: 308 | Episodes: 12500 | Median Reward: 42.97 | Max Reward: 48.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 203          |
|    iterations           | 310          |
|    time_elapsed         | 6235         |
|    total_timesteps      | 1269760      |
| train/                  |              |
|    approx_kl            | 0.0023817224 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -221         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -21.4        |
|    n_updates            | 3090         |
|    policy_gradient_loss | -0.00485     |
|    std                  | 1.06         |
|    value_loss           | 2.71         |
------------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: 44.50 | Max Reward: 48.02
Iteration: 313 | Episodes: 12700 | Median Reward: 46.68 | Max Reward: 48.02
Iteration: 315 | Episodes: 12800 | Median Reward: 46.27 | Max Reward: 48.02
Iteration: 318 | Episodes: 12900 | Median Reward: 43.47 | Max Reward: 48.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.9       |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 320         |
|    time_elapsed         | 6433        |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.010464917 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.3         |
|    entropy_loss         | -221        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -21         |
|    n_updates            | 3190        |
|    policy_gradient_loss | -0.0111     |
|    std                  | 1.07        |
|    value_loss           | 0.991       |
-----------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: 42.65 | Max Reward: 48.02
Iteration: 323 | Episodes: 13100 | Median Reward: 43.39 | Max Reward: 48.02
Iteration: 325 | Episodes: 13200 | Median Reward: 42.83 | Max Reward: 48.02
Iteration: 327 | Episodes: 13300 | Median Reward: 43.69 | Max Reward: 48.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.5       |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 330         |
|    time_elapsed         | 6630        |
|    total_timesteps      | 1351680     |
| train/                  |             |
|    approx_kl            | 0.001921118 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -222        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.3       |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.00343    |
|    std                  | 1.07        |
|    value_loss           | 1.73        |
-----------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: 43.29 | Max Reward: 48.02
Iteration: 332 | Episodes: 13500 | Median Reward: 43.53 | Max Reward: 48.02
Iteration: 335 | Episodes: 13600 | Median Reward: 43.36 | Max Reward: 48.02
Iteration: 337 | Episodes: 13700 | Median Reward: 46.64 | Max Reward: 48.02
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.5         |
| time/                   |               |
|    fps                  | 203           |
|    iterations           | 340           |
|    time_elapsed         | 6828          |
|    total_timesteps      | 1392640       |
| train/                  |               |
|    approx_kl            | 0.00014896251 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -223          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -21.8         |
|    n_updates            | 3390          |
|    policy_gradient_loss | -0.000106     |
|    std                  | 1.07          |
|    value_loss           | 2.28          |
-------------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: 45.70 | Max Reward: 48.02
Iteration: 342 | Episodes: 13900 | Median Reward: 44.11 | Max Reward: 48.02
Iteration: 345 | Episodes: 14000 | Median Reward: 44.20 | Max Reward: 48.02
Iteration: 347 | Episodes: 14100 | Median Reward: 44.11 | Max Reward: 48.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.8        |
| time/                   |              |
|    fps                  | 204          |
|    iterations           | 350          |
|    time_elapsed         | 7027         |
|    total_timesteps      | 1433600      |
| train/                  |              |
|    approx_kl            | 0.0019565606 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -224         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -17.5        |
|    n_updates            | 3490         |
|    policy_gradient_loss | -0.00327     |
|    std                  | 1.07         |
|    value_loss           | 3.7          |
------------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: 46.75 | Max Reward: 48.02
Iteration: 352 | Episodes: 14300 | Median Reward: 43.28 | Max Reward: 48.02
Iteration: 355 | Episodes: 14400 | Median Reward: 45.84 | Max Reward: 48.02
Iteration: 357 | Episodes: 14500 | Median Reward: 43.92 | Max Reward: 48.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.5        |
| time/                   |              |
|    fps                  | 204          |
|    iterations           | 360          |
|    time_elapsed         | 7226         |
|    total_timesteps      | 1474560      |
| train/                  |              |
|    approx_kl            | 0.0015514744 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -225         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.3        |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.00227     |
|    std                  | 1.08         |
|    value_loss           | 1.25         |
------------------------------------------
Iteration: 360 | Episodes: 14600 | Median Reward: 46.37 | Max Reward: 48.02
Iteration: 362 | Episodes: 14700 | Median Reward: 47.20 | Max Reward: 48.02
Iteration: 364 | Episodes: 14800 | Median Reward: 43.14 | Max Reward: 48.02
Iteration: 367 | Episodes: 14900 | Median Reward: 43.92 | Max Reward: 48.02
Iteration: 369 | Episodes: 15000 | Median Reward: 44.13 | Max Reward: 48.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.7       |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 370         |
|    time_elapsed         | 7429        |
|    total_timesteps      | 1515520     |
| train/                  |             |
|    approx_kl            | 0.005589531 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -226        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -21.6       |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.00429    |
|    std                  | 1.08        |
|    value_loss           | 1.72        |
-----------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: 44.27 | Max Reward: 48.02
Iteration: 374 | Episodes: 15200 | Median Reward: 46.70 | Max Reward: 49.02
Iteration: 377 | Episodes: 15300 | Median Reward: 43.22 | Max Reward: 49.02
Iteration: 379 | Episodes: 15400 | Median Reward: 43.10 | Max Reward: 49.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 203          |
|    iterations           | 380          |
|    time_elapsed         | 7634         |
|    total_timesteps      | 1556480      |
| train/                  |              |
|    approx_kl            | 0.0009977818 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -227         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.3        |
|    n_updates            | 3790         |
|    policy_gradient_loss | 0.000882     |
|    std                  | 1.08         |
|    value_loss           | 1.15         |
------------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: 46.74 | Max Reward: 49.02
Iteration: 384 | Episodes: 15600 | Median Reward: 46.21 | Max Reward: 49.02
Iteration: 387 | Episodes: 15700 | Median Reward: 43.70 | Max Reward: 49.02
Iteration: 389 | Episodes: 15800 | Median Reward: 42.88 | Max Reward: 49.02
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.3         |
| time/                   |               |
|    fps                  | 203           |
|    iterations           | 390           |
|    time_elapsed         | 7836          |
|    total_timesteps      | 1597440       |
| train/                  |               |
|    approx_kl            | 0.00021875804 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -228          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -21.2         |
|    n_updates            | 3890          |
|    policy_gradient_loss | -0.000707     |
|    std                  | 1.09          |
|    value_loss           | 7.46          |
-------------------------------------------
Iteration: 392 | Episodes: 15900 | Median Reward: 42.32 | Max Reward: 49.02
Iteration: 394 | Episodes: 16000 | Median Reward: 44.46 | Max Reward: 49.02
Iteration: 396 | Episodes: 16100 | Median Reward: 45.44 | Max Reward: 49.02
Iteration: 399 | Episodes: 16200 | Median Reward: 43.30 | Max Reward: 49.02
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -56.8      |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 400        |
|    time_elapsed         | 8035       |
|    total_timesteps      | 1638400    |
| train/                  |            |
|    approx_kl            | 0.01681371 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -228       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -22.3      |
|    n_updates            | 3990       |
|    policy_gradient_loss | -0.0139    |
|    std                  | 1.09       |
|    value_loss           | 1.09       |
----------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: 46.68 | Max Reward: 49.02
Iteration: 404 | Episodes: 16400 | Median Reward: 44.05 | Max Reward: 49.02
Iteration: 406 | Episodes: 16500 | Median Reward: 44.00 | Max Reward: 49.02
Iteration: 409 | Episodes: 16600 | Median Reward: 46.70 | Max Reward: 49.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.7        |
| time/                   |              |
|    fps                  | 203          |
|    iterations           | 410          |
|    time_elapsed         | 8235         |
|    total_timesteps      | 1679360      |
| train/                  |              |
|    approx_kl            | 0.0014065925 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -229         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -21.4        |
|    n_updates            | 4090         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 1.09         |
|    value_loss           | 3.91         |
------------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: 46.37 | Max Reward: 49.02
Iteration: 414 | Episodes: 16800 | Median Reward: 46.37 | Max Reward: 49.02
Iteration: 416 | Episodes: 16900 | Median Reward: 43.75 | Max Reward: 49.02
Iteration: 419 | Episodes: 17000 | Median Reward: 47.45 | Max Reward: 49.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.4       |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 420         |
|    time_elapsed         | 8433        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.075485095 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.3         |
|    entropy_loss         | -231        |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.6       |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.0625     |
|    std                  | 1.1         |
|    value_loss           | 1.76        |
-----------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: 46.62 | Max Reward: 49.02
Iteration: 424 | Episodes: 17200 | Median Reward: 46.62 | Max Reward: 49.02
Iteration: 426 | Episodes: 17300 | Median Reward: 43.43 | Max Reward: 49.02
Iteration: 429 | Episodes: 17400 | Median Reward: 44.22 | Max Reward: 49.02
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.3        |
| time/                   |              |
|    fps                  | 204          |
|    iterations           | 430          |
|    time_elapsed         | 8630         |
|    total_timesteps      | 1761280      |
| train/                  |              |
|    approx_kl            | 0.0013965121 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -232         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.7        |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.00183     |
|    std                  | 1.1          |
|    value_loss           | 7.53         |
------------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: 44.08 | Max Reward: 49.02
Iteration: 433 | Episodes: 17600 | Median Reward: 44.04 | Max Reward: 49.02
Iteration: 436 | Episodes: 17700 | Median Reward: 43.08 | Max Reward: 49.02
Iteration: 438 | Episodes: 17800 | Median Reward: 46.68 | Max Reward: 49.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.2       |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 440         |
|    time_elapsed         | 8830        |
|    total_timesteps      | 1802240     |
| train/                  |             |
|    approx_kl            | 0.023571469 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -233        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.1       |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.0192     |
|    std                  | 1.1         |
|    value_loss           | 0.81        |
-----------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 46.36 | Max Reward: 49.02
Iteration: 443 | Episodes: 18000 | Median Reward: 43.58 | Max Reward: 49.02
Iteration: 446 | Episodes: 18100 | Median Reward: 45.92 | Max Reward: 49.02
Iteration: 448 | Episodes: 18200 | Median Reward: 46.35 | Max Reward: 49.02
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -55.9       |
| time/                   |             |
|    fps                  | 204  8035187 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -242          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -12.6         |
|    n_updates            | 4490          |
|    policy_gradient_loss | -0.000545     |
|    std                  | 1.12          |
|    value_loss           | 11.7          |
-------------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 43.91 | Max Reward: 49.17
Iteration: 453 | Episodes: 18400 | Median Reward: 47.34 | Max Reward: 49.17
Iteration: 456 | Episodes: 18500 | Median Reward: 46.70 | Max Reward: 49.17
Iteration: 458 | Episodes: 18600 | Median Reward: 45.21 | Max Reward: 49.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.2        |
| time/                   |              |
|    fps                  | 203          |
|    iterations           | 460          |
|    time_elapsed         | 9273         |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 0.0021144974 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -243         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.6        |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.00455     |
|    std                  | 1.13         |
|    value_loss           | 6.03         |
------------------------------------------
Iteration: 461 | Episodes: 18700 | Median Reward: 42.90 | Max Reward: 49.17
Iteration: 463 | Episodes: 18800 | Median Reward: 46.75 | Max Reward: 49.17
Iteration: 465 | Episodes: 18900 | Median Reward: 46.77 | Max Reward: 49.17
Iteration: 468 | Episodes: 19000 | Median Reward: 45.16 | Max Reward: 49.17
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -55.9      |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 470        |
|    time_elapsed         | 9471       |
|    total_timesteps      | 1925120    |
| train/                  |            |
|    approx_kl            | 0.05721045 |
|    clip_fraction        | 0.075      |
|    clip_range           | 0.3        |
|    entropy_loss         | -244       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0005     |
|    loss                 | -24        |
|    n_updates            | 4690       |
|    policy_gradient_loss | -0.0221    |
|    std                  | 1.13       |
|    value_loss           | 0.762      |
----------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 44.22 | Max Reward: 49.17
Iteration: 473 | Episodes: 19200 | Median Reward: 45.98 | Max Reward: 49.17
Iteration: 475 | Episodes: 19300 | Median Reward: 46.66 | Max Reward: 49.17
Iteration: 478 | Episodes: 19400 | Median Reward: 46.26 | Max Reward: 49.26
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.1        |
| time/                   |              |
|    fps                  | 203          |
|    iterations           | 480          |
|    time_elapsed         | 9671         |
|    total_timesteps      | 1966080      |
| train/                  |              |
|    approx_kl            | 0.0012067079 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -244         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.6        |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 1.14         |
|    value_loss           | 2.2          |
------------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 46.97 | Max Reward: 49.26
Iteration: 483 | Episodes: 19600 | Median Reward: 41.31 | Max Reward: 49.26
Iteration: 485 | Episodes: 19700 | Median Reward: 44.38 | Max Reward: 49.26
Iteration: 488 | Episodes: 19800 | Median Reward: 45.87 | Max Reward: 49.26
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -53.6      |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 490        |
|    time_elapsed         | 9871       |
|    total_timesteps      | 2007040    |
| train/                  |            |
|    approx_kl            | 0.01778692 |
|    clip_fraction        | 0.0257     |
|    clip_range           | 0.3        |
|    entropy_loss         | -245       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -24.4      |
|    n_updates            | 4890       |
|    policy_gradient_loss | -0.00582   |
|    std                  | 1.14       |
|    value_loss           | 0.523      |
----------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 47.37 | Max Reward: 49.26
Iteration: 493 | Episodes: 20000 | Median Reward: 47.39 | Max Reward: 49.26
Iteration: 495 | Episodes: 20100 | Median Reward: 47.28 | Max Reward: 49.26
Iteration: 498 | Episodes: 20200 | Median Reward: 43.71 | Max Reward: 49.26
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -54.1      |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 500        |
|    time_elapsed         | 10071      |
|    total_timesteps      | 2048000    |
| train/                  |            |
|    approx_kl            | 0.00890919 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -247       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -24        |
|    n_updates            | 4990       |
|    policy_gradient_loss | 0.00132    |
|    std                  | 1.15       |
|    value_loss           | 1.27       |
----------------------------------------
Training End | Episodes: 20280 | Median Reward: 47.14 | Max Reward: 49.26
Plot saved as fig_code1_1_residual_c.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> aaa[K[K(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> aa[K[Kexit
exit

Script done on 2024-10-24 01:14:58-04:00 [COMMAND_EXIT_CODE="0"]
[24m:[1m~/Bot_hand/codes[0m> a
aa-enabled            addgroup              aleph                 apport-collect        apt-key               arptables-restore     aulast                automat-visualize3
aa-exec               addpart               alias                 apport-unpack         apt-mark              arptables-save        aulastlog             autopep8
aa-remove-unknown     addr2line             allec                 appres                apt-sortpkgs          as                    aureport              autoreconf
aa-status             add-shell             alloc                 apropos               ar                    assistant             ausearch              autoscan
aa-teardown           adduser               anaconda              apt                   arch                  at                    ausyscall             autosp
accessdb              adig                  anaconda-navigator    apt-add-repository    arcstat               atd                   authorindex           autoupdate
aclocal               aec                   anaconda-project      apt-cache             arc_summary           atq                   autoconf              autrace
aclocal-1.16          afm2pl                apparmor_parser       apt-cdrom             arpd                  atrm                  autoheader            auvirt
acountry              afm2tfm               apparmor_status       apt-config            arptables             audispd               autom4te              awk
activate              agetty                applygnupgdefaults    apt-extracttemplates  arptables-nft         auditctl              automake              axohelp
add-apt-repository    ahost                 apport-bug            apt-ftparchive        arptables-nft-restore auditd                automake-1.16         
addgnupghome          ale-import-roms       apport-cli            apt-get               arptables-nft-save    augenrules            automat-visualize     
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> a[K(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> a
aa-enabled            afm2pl                appres                arptables-nft         ausyscall
aa-exec               afm2tfm               apropos               arptables-nft-restore authorindex
aa-remove-unknown     agetty                apt                   arptables-nft-save    autoconf
aa-status             ahost                 apt-add-repository    arptables-restore     autoheader
aa-teardown           ale-import-roms       apt-cache             arptables-save        autom4te
accessdb              aleph                 apt-cdrom             as                    automake
aclocal               alias                 apt-config            assistant             automake-1.16
aclocal-1.16          allec                 apt-extracttemplates  at                    automat-visualize
acountry              alloc                 apt-ftparchive        atd                   automat-visualize3
activate              anaconda              apt-get               atq                   autopep8
add-apt-repository    anaconda-navigator    apt-key               atrm                  autoreconf
addgnupghome          anaconda-project      apt-mark              audispd               autoscan
addgroup              apparmor_parser       apt-sortpkgs          auditctl              autosp
addpart               apparmor_status       ar                    auditd                autoupdate
addr2line             applygnupgdefaults    arch                  augenrules            autrace
add-shell             apport-bug            arcstat               aulast                auvirt
adduser               apport-cli            arc_summary           aulastlog             awk
adig                  apport-collect        arpd                  aureport              axohelp
aec                   apport-unpack         arptables             ausearch              
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> a[K[K(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> exit
exit

Script done on 2024-10-24 01:14:42-04:00 [COMMAND_EXIT_CODE="0"]
