Script started on 2024-10-24 17:18:19-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/27" COLUMNS="203" LINES="53"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> p[Kconda activate mujoco_test\[K\[K
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python code1_1_residual_r[Ke.py
GPU 3: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
/home/easgrad/dgusain/anaconda3/envs/mujoco_test/lib/python3.8/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])
  warnings.warn(
Iteration: 2 | Episodes: 100 | Median Reward: 20.57 | Avg Reward: 21.29 | Max Reward: 33.00
Iteration: 4 | Episodes: 200 | Median Reward: 26.01 | Avg Reward: 25.81 | Max Reward: 37.48
Iteration: 7 | Episodes: 300 | Median Reward: 22.47 | Avg Reward: 22.32 | Max Reward: 37.48
Iteration: 9 | Episodes: 400 | Median Reward: 22.44 | Avg Reward: 22.86 | Max Reward: 37.48
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -77         |
| time/                   |             |
|    fps                  | 987         |
|    iterations           | 10          |
|    time_elapsed         | 41          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.026724048 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.3         |
|    entropy_loss         | -61.9       |
|    explained_variance   | 0.159       |
|    learning_rate        | 0.0005      |
|    loss                 | 78.7        |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.0145      |
|    std                  | 1.01        |
|    value_loss           | 203         |
-----------------------------------------
Iteration: 12 | Episodes: 500 | Median Reward: 24.25 | Avg Reward: 25.55 | Max Reward: 38.13
Iteration: 14 | Episodes: 600 | Median Reward: 25.78 | Avg Reward: 26.01 | Max Reward: 38.13
Iteration: 17 | Episodes: 700 | Median Reward: 24.54 | Avg Reward: 23.85 | Max Reward: 38.13
Iteration: 19 | Episodes: 800 | Median Reward: 22.37 | Avg Reward: 22.78 | Max Reward: 38.13
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -77.6      |
| time/                   |            |
|    fps                  | 980        |
|    iterations           | 20         |
|    time_elapsed         | 83         |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.36935228 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.3        |
|    entropy_loss         | -62.4      |
|    explained_variance   | 0.0962     |
|    learning_rate        | 0.0005     |
|    loss                 | 76.5       |
|    n_updates            | 190        |
|    policy_gradient_loss | 0.0638     |
|    std                  | 1.01       |
|    value_loss           | 181        |
----------------------------------------
Iteration: 22 | Episodes: 900 | Median Reward: 22.63 | Avg Reward: 22.68 | Max Reward: 38.13
Iteration: 24 | Episodes: 1000 | Median Reward: 22.89 | Avg Reward: 21.79 | Max Reward: 38.13
Iteration: 27 | Episodes: 1100 | Median Reward: 21.59 | Avg Reward: 21.75 | Max Reward: 38.13
Iteration: 29 | Episodes: 1200 | Median Reward: 25.82 | Avg Reward: 25.28 | Max Reward: 38.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74.9       |
| time/                   |             |
|    fps                  | 979         |
|    iterations           | 30          |
|    time_elapsed         | 125         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.016135182 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.3         |
|    entropy_loss         | -69.6       |
|    explained_variance   | 0.896       |
|    learning_rate        | 0.0005      |
|    loss                 | 8.26        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0168     |
|    std                  | 1.01        |
|    value_loss           | 39.1        |
-----------------------------------------
Iteration: 32 | Episodes: 1300 | Median Reward: 23.21 | Avg Reward: 24.59 | Max Reward: 38.13
Iteration: 34 | Episodes: 1400 | Median Reward: 20.74 | Avg Reward: 21.71 | Max Reward: 38.13
Iteration: 36 | Episodes: 1500 | Median Reward: 26.83 | Avg Reward: 24.35 | Max Reward: 38.13
Iteration: 39 | Episodes: 1600 | Median Reward: 24.47 | Avg Reward: 24.98 | Max Reward: 38.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -75.5       |
| time/                   |             |
|    fps                  | 977         |
|    iterations           | 40          |
|    time_elapsed         | 167         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.032869846 |
|    clip_fraction        | 0.0997      |
|    clip_range           | 0.3         |
|    entropy_loss         | -83         |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0005      |
|    loss                 | -4.99       |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0111     |
|    std                  | 1.01        |
|    value_loss           | 10          |
-----------------------------------------
Iteration: 41 | Episodes: 1700 | Median Reward: 27.17 | Avg Reward: 26.09 | Max Reward: 38.13
Iteration: 44 | Episodes: 1800 | Median Reward: 22.51 | Avg Reward: 21.19 | Max Reward: 38.13
Iteration: 46 | Episodes: 1900 | Median Reward: 22.72 | Avg Reward: 24.05 | Max Reward: 38.13
Iteration: 49 | Episodes: 2000 | Median Reward: 20.66 | Avg Reward: 21.90 | Max Reward: 38.13
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.9       |
| time/                   |             |
|    fps                  | 974         |
|    iterations           | 50          |
|    time_elapsed         | 210         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.020351062 |
|    clip_fraction        | 0.000732    |
|    clip_range           | 0.3         |
|    entropy_loss         | -92.1       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0005      |
|    loss                 | -5.19       |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00681    |
|    std                  | 1.01        |
|    value_loss           | 25.1        |
-----------------------------------------
Iteration: 51 | Episodes: 2100 | Median Reward: 25.32 | Avg Reward: 24.36 | Max Reward: 38.13
Iteration: 54 | Episodes: 2200 | Median Reward: 26.70 | Avg Reward: 25.79 | Max Reward: 38.13
Iteration: 56 | Episodes: 2300 | Median Reward: 24.39 | Avg Reward: 26.32 | Max Reward: 39.94
Iteration: 59 | Episodes: 2400 | Median Reward: 26.52 | Avg Reward: 24.78 | Max Reward: 39.94
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -74.5      |
| time/                   |            |
|    fps                  | 972        |
|    iterations           | 60         |
|    time_elapsed         | 252        |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.01283689 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -104       |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0005     |
|    loss                 | -6.78      |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.015     |
|    std                  | 1.01       |
|    value_loss           | 6.77       |
----------------------------------------
Iteration: 61 | Episodes: 2500 | Median Reward: 25.64 | Avg Reward: 26.02 | Max Reward: 39.94
Iteration: 64 | Episodes: 2600 | Median Reward: 25.57 | Avg Reward: 24.83 | Max Reward: 39.94
Iteration: 66 | Episodes: 2700 | Median Reward: 25.43 | Avg Reward: 24.78 | Max Reward: 39.94
Iteration: 69 | Episodes: 2800 | Median Reward: 26.25 | Avg Reward: 25.97 | Max Reward: 39.94
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -72.9      |
| time/                   |            |
|    fps                  | 972        |
|    iterations           | 70         |
|    time_elapsed         | 294        |
|    total_timesteps      | 286720     |
| train/                  |            |
|    approx_kl            | 0.19190893 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.3        |
|    entropy_loss         | -103       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -7.69      |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0168    |
|    std                  | 1.02       |
|    value_loss           | 1.79       |
----------------------------------------
Iteration: 71 | Episodes: 2900 | Median Reward: 27.59 | Avg Reward: 27.74 | Max Reward: 39.94
Iteration: 73 | Episodes: 3000 | Median Reward: 26.94 | Avg Reward: 26.60 | Max Reward: 39.94
Iteration: 76 | Episodes: 3100 | Median Reward: 29.06 | Avg Reward: 27.60 | Max Reward: 39.94
Iteration: 78 | Episodes: 3200 | Median Reward: 25.61 | Avg Reward: 26.00 | Max Reward: 39.94
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -75.2      |
| time/                   |            |
|    fps                  | 971        |
|    iterations           | 80         |
|    time_elapsed         | 337        |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.32274985 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.3        |
|    entropy_loss         | -109       |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0005     |
|    loss                 | -8.64      |
|    n_updates            | 790        |
|    policy_gradient_loss | 0.0595     |
|    std                  | 1.02       |
|    value_loss           | 7.9        |
----------------------------------------
Iteration: 81 | Episodes: 3300 | Median Reward: 25.91 | Avg Reward: 23.82 | Max Reward: 39.94
Iteration: 83 | Episodes: 3400 | Median Reward: 28.78 | Avg Reward: 28.11 | Max Reward: 39.94
Iteration: 86 | Episodes: 3500 | Median Reward: 25.47 | Avg Reward: 24.90 | Max Reward: 39.94
Iteration: 88 | Episodes: 3600 | Median Reward: 27.82 | Avg Reward: 27.05 | Max Reward: 39.94
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -72.7      |
| time/                   |            |
|    fps                  | 970        |
|    iterations           | 90         |
|    time_elapsed         | 379        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.08111889 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.3        |
|    entropy_loss         | -129       |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0005     |
|    loss                 | -11.7      |
|    n_updates            | 890        |
|    policy_gradient_loss | 0.00826    |
|    std                  | 1.02       |
|    value_loss           | 4.78       |
----------------------------------------
Iteration: 91 | Episodes: 3700 | Median Reward: 28.21 | Avg Reward: 28.78 | Max Reward: 39.94
Iteration: 93 | Episodes: 3800 | Median Reward: 27.29 | Avg Reward: 27.05 | Max Reward: 39.94
Iteration: 96 | Episodes: 3900 | Median Reward: 28.87 | Avg Reward: 28.44 | Max Reward: 39.94
Iteration: 98 | Episodes: 4000 | Median Reward: 26.75 | Avg Reward: 28.61 | Max Reward: 39.94
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.8       |
| time/                   |             |
|    fps                  | 971         |
|    iterations           | 100         |
|    time_elapsed         | 421         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.045441568 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.3         |
|    entropy_loss         | -139        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -12.1       |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0204     |
|    std                  | 1.02        |
|    value_loss           | 3.2         |
-----------------------------------------
Iteration: 101 | Episodes: 4100 | Median Reward: 26.74 | Avg Reward: 27.20 | Max Reward: 39.94
Iteration: 103 | Episodes: 4200 | Median Reward: 31.08 | Avg Reward: 31.50 | Max Reward: 39.94
Iteration: 106 | Episodes: 4300 | Median Reward: 29.97 | Avg Reward: 29.93 | Max Reward: 39.94
Iteration: 108 | Episodes: 4400 | Median Reward: 31.24 | Avg Reward: 30.52 | Max Reward: 39.94
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -71.3      |
| time/                   |            |
|    fps                  | 971        |
|    iterations           | 110        |
|    time_elapsed         | 463        |
|    total_timesteps      | 450560     |
| train/                  |            |
|    approx_kl            | 0.18852793 |
|    clip_fraction        | 0.55       |
|    clip_range           | 0.3        |
|    entropy_loss         | -156       |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0005     |
|    loss                 | -12.6      |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.123     |
|    std                  | 1.03       |
|    value_loss           | 3.99       |
----------------------------------------
Iteration: 110 | Episodes: 4500 | Median Reward: 30.55 | Avg Reward: 30.20 | Max Reward: 39.94
Iteration: 113 | Episodes: 4600 | Median Reward: 30.59 | Avg Reward: 29.13 | Max Reward: 39.94
Iteration: 115 | Episodes: 4700 | Median Reward: 31.21 | Avg Reward: 30.10 | Max Reward: 39.94
Iteration: 118 | Episodes: 4800 | Median Reward: 31.98 | Avg Reward: 31.71 | Max Reward: 41.67
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.4        |
| time/                   |              |
|    fps                  | 970          |
|    iterations           | 120          |
|    time_elapsed         | 506          |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.0039765853 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -163         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0005       |
|    loss                 | -13.8        |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.0044      |
|    std                  | 1.03         |
|    value_loss           | 3.85         |
------------------------------------------
Iteration: 120 | Episodes: 4900 | Median Reward: 28.46 | Avg Reward: 30.04 | Max Reward: 41.67
Iteration: 123 | Episodes: 5000 | Median Reward: 31.18 | Avg Reward: 29.92 | Max Reward: 41.67
Iteration: 125 | Episodes: 5100 | Median Reward: 33.44 | Avg Reward: 32.76 | Max Reward: 41.67
Iteration: 128 | Episodes: 5200 | Median Reward: 33.59 | Avg Reward: 32.74 | Max Reward: 41.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.4       |
| time/                   |             |
|    fps                  | 970         |
|    iterations           | 130         |
|    time_elapsed         | 548         |
|    total_timesteps      | 532480      |
| train/                  |             |
|    approx_kl            | 0.012648283 |
|    clip_fraction        | 0.00244     |
|    clip_range           | 0.3         |
|    entropy_loss         | -167        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0005      |
|    loss                 | -16.2       |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.0208     |
|    std                  | 1.03        |
|    value_loss           | 2.83        |
-----------------------------------------
Iteration: 130 | Episodes: 5300 | Median Reward: 35.18 | Avg Reward: 34.12 | Max Reward: 41.67
Iteration: 133 | Episodes: 5400 | Median Reward: 31.35 | Avg Reward: 30.95 | Max Reward: 41.67
Iteration: 135 | Episodes: 5500 | Median Reward: 31.92 | Avg Reward: 32.99 | Max Reward: 41.67
Iteration: 138 | Episodes: 5600 | Median Reward: 32.84 | Avg Reward: 33.79 | Max Reward: 41.67
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.9       |
| time/                   |             |
|    fps                  | 970         |
|    iterations           | 140         |
|    time_elapsed         | 591         |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.031704694 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.3         |
|    entropy_loss         | -171        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0005      |
|    loss                 | -16.8       |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.0011     |
|    std                  | 1.03        |
|    value_loss           | 1.74        |
-----------------------------------------
Iteration: 140 | Episodes: 5700 | Median Reward: 32.53 | Avg Reward: 31.21 | Max Reward: 41.67
Iteration: 143 | Episodes: 5800 | Median Reward: 32.93 | Avg Reward: 32.98 | Max Reward: 41.67
Iteration: 145 | Episodes: 5900 | Median Reward: 34.17 | Avg Reward: 34.95 | Max Reward: 41.89
Iteration: 147 | Episodes: 6000 | Median Reward: 34.43 | Avg Reward: 34.60 | Max Reward: 44.01
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -65        |
| time/                   |            |
|    fps                  | 969        |
|    iterations           | 150        |
|    time_elapsed         | 633        |
|    total_timesteps      | 614400     |
| train/                  |            |
|    approx_kl            | 0.07818794 |
|    clip_fraction        | 0.182      |
|    clip_range           | 0.3        |
|    entropy_loss         | -182       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -16.1      |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0223    |
|    std                  | 1.04       |
|    value_loss           | 2.57       |
----------------------------------------
Iteration: 150 | Episodes: 6100 | Median Reward: 35.79 | Avg Reward: 34.94 | Max Reward: 44.01
Iteration: 152 | Episodes: 6200 | Median Reward: 34.85 | Avg Reward: 36.67 | Max Reward: 44.01
Iteration: 155 | Episodes: 6300 | Median Reward: 33.70 | Avg Reward: 34.02 | Max Reward: 44.01
Iteration: 157 | Episodes: 6400 | Median Reward: 33.75 | Avg Reward: 34.38 | Max Reward: 44.01
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -65.3      |
| time/                   |            |
|    fps                  | 969        |
|    iterations           | 160        |
|    time_elapsed         | 675        |
|    total_timesteps      | 655360     |
| train/                  |            |
|    approx_kl            | 0.03647962 |
|    clip_fraction        | 0.0156     |
|    clip_range           | 0.3        |
|    entropy_loss         | -186       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0005     |
|    loss                 | -15.6      |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0183    |
|    std                  | 1.04       |
|    value_loss           | 4.51       |
----------------------------------------
Iteration: 160 | Episodes: 6500 | Median Reward: 34.25 | Avg Reward: 34.75 | Max Reward: 44.01
Iteration: 162 | Episodes: 6600 | Median Reward: 36.42 | Avg Reward: 35.16 | Max Reward: 44.01
Iteration: 165 | Episodes: 6700 | Median Reward: 32.29 | Avg Reward: 32.38 | Max Reward: 44.01
Iteration: 167 | Episodes: 6800 | Median Reward: 36.08 | Avg Reward: 35.54 | Max Reward: 44.01
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.3       |
| time/                   |             |
|    fps                  | 969         |
|    iterations           | 170         |
|    time_elapsed         | 717         |
|    total_timesteps      | 696320      |
| train/                  |             |
|    approx_kl            | 0.048499826 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.3         |
|    entropy_loss         | -189        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -18.1       |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.00901    |
|    std                  | 1.04        |
|    value_loss           | 2.45        |
-----------------------------------------
Iteration: 170 | Episodes: 6900 | Median Reward: 38.67 | Avg Reward: 35.80 | Max Reward: 44.01
Iteration: 172 | Episodes: 7000 | Median Reward: 34.13 | Avg Reward: 33.60 | Max Reward: 44.01
Iteration: 175 | Episodes: 7100 | Median Reward: 37.43 | Avg Reward: 36.63 | Max Reward: 44.01
Iteration: 177 | Episodes: 7200 | Median Reward: 34.96 | Avg Reward: 34.10 | Max Reward: 44.01
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -64.8     |
| time/                   |           |
|    fps                  | 969       |
|    iterations           | 180       |
|    time_elapsed         | 760       |
|    total_timesteps      | 737280    |
| train/                  |           |
|    approx_kl            | 0.0929649 |
|    clip_fraction        | 0.276     |
|    clip_range           | 0.3       |
|    entropy_loss         | -194      |
|    explained_variance   | 0.997     |
|    learning_rate        | 0.0005    |
|    loss                 | -16.8     |
|    n_updates            | 1790      |
|    policy_gradient_loss | -0.0533   |
|    std                  | 1.05      |
|    value_loss           | 3.15      |
---------------------------------------
Iteration: 180 | Episodes: 7300 | Median Reward: 36.41 | Avg Reward: 35.48 | Max Reward: 44.21
Iteration: 182 | Episodes: 7400 | Median Reward: 37.46 | Avg Reward: 37.67 | Max Reward: 44.21
Iteration: 184 | Episodes: 7500 | Median Reward: 35.83 | Avg Reward: 36.75 | Max Reward: 44.21
Iteration: 187 | Episodes: 7600 | Median Reward: 36.56 | Avg Reward: 35.99 | Max Reward: 44.21
Iteration: 189 | Episodes: 7700 | Median Reward: 37.96 | Avg Reward: 36.18 | Max Reward: 44.21
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.8       |
| time/                   |             |
|    fps                  | 970         |
|    iterations           | 190         |
|    time_elapsed         | 802         |
|    total_timesteps      | 778240      |
| train/                  |             |
|    approx_kl            | 0.036344588 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.3         |
|    entropy_loss         | -197        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -19.4       |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0392     |
|    std                  | 1.05        |
|    value_loss           | 7.17        |
-----------------------------------------
Iteration: 192 | Episodes: 7800 | Median Reward: 36.50 | Avg Reward: 35.25 | Max Reward: 44.21
Iteration: 194 | Episodes: 7900 | Median Reward: 37.09 | Avg Reward: 36.48 | Max Reward: 44.21
Iteration: 197 | Episodes: 8000 | Median Reward: 37.25 | Avg Reward: 36.83 | Max Reward: 44.21
Iteration: 199 | Episodes: 8100 | Median Reward: 36.49 | Avg Reward: 35.88 | Max Reward: 44.21
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.9        |
| time/                   |              |
|    fps                  | 970          |
|    iterations           | 200          |
|    time_elapsed         | 844          |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0029019245 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -200         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -14.8        |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00438     |
|    std                  | 1.05         |
|    value_loss           | 10.3         |
------------------------------------------
Iteration: 202 | Episodes: 8200 | Median Reward: 38.07 | Avg Reward: 37.28 | Max Reward: 44.21
Iteration: 204 | Episodes: 8300 | Median Reward: 36.89 | Avg Reward: 36.17 | Max Reward: 44.21
Iteration: 207 | Episodes: 8400 | Median Reward: 37.16 | Avg Reward: 37.44 | Max Reward: 44.21
Iteration: 209 | Episodes: 8500 | Median Reward: 37.49 | Avg Reward: 36.91 | Max Reward: 44.21
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.8       |
| time/                   |             |
|    fps                  | 970         |
|    iterations           | 210         |
|    time_elapsed         | 886         |
|    total_timesteps      | 860160      |
| train/                  |             |
|    approx_kl            | 0.019345049 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.3         |
|    entropy_loss         | -204        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0005      |
|    loss                 | -19.4       |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.0107     |
|    std                  | 1.05        |
|    value_loss           | 2.87        |
-----------------------------------------
Iteration: 212 | Episodes: 8600 | Median Reward: 37.00 | Avg Reward: 37.90 | Max Reward: 44.21
Iteration: 214 | Episodes: 8700 | Median Reward: 38.30 | Avg Reward: 38.12 | Max Reward: 44.21
Iteration: 216 | Episodes: 8800 | Median Reward: 37.32 | Avg Reward: 38.25 | Max Reward: 44.60
Iteration: 219 | Episodes: 8900 | Median Reward: 39.54 | Avg Reward: 39.06 | Max Reward: 44.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.9       |
| time/                   |             |
|    fps                  | 971         |
|    iterations           | 220         |
|    time_elapsed         | 927         |
|    total_timesteps      | 901120      |
| train/                  |             |
|    approx_kl            | 0.013785936 |
|    clip_fraction        | 0.00244     |
|    clip_range           | 0.3         |
|    entropy_loss         | -207        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -20.2       |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.00517    |
|    std                  | 1.05        |
|    value_loss           | 1.2         |
-----------------------------------------
Iteration: 221 | Episodes: 9000 | Median Reward: 38.68 | Avg Reward: 37.90 | Max Reward: 44.60
Iteration: 224 | Episodes: 9100 | Median Reward: 37.36 | Avg Reward: 37.32 | Max Reward: 44.60
Iteration: 226 | Episodes: 9200 | Median Reward: 37.83 | Avg Reward: 37.46 | Max Reward: 44.60
Iteration: 229 | Episodes: 9300 | Median Reward: 39.78 | Avg Reward: 39.56 | Max Reward: 44.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.7       |
| time/                   |             |
|    fps                  | 971         |
|    iterations           | 230         |
|    time_elapsed         | 969         |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.001099572 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -208        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0005      |
|    loss                 | -20.6       |
|    n_updates            | 2290        |
|    policy_gradient_loss | 0.000425    |
|    std                  | 1.06        |
|    value_loss           | 2.11        |
-----------------------------------------
Iteration: 231 | Episodes: 9400 | Median Reward: 39.73 | Avg Reward: 39.00 | Max Reward: 44.60
Iteration: 234 | Episodes: 9500 | Median Reward: 37.27 | Avg Reward: 37.30 | Max Reward: 44.60
Iteration: 236 | Episodes: 9600 | Median Reward: 36.72 | Avg Reward: 37.85 | Max Reward: 44.60
Iteration: 239 | Episodes: 9700 | Median Reward: 37.19 | Avg Reward: 36.73 | Max Reward: 44.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.6        |
| time/                   |              |
|    fps                  | 971          |
|    iterations           | 240          |
|    time_elapsed         | 1012         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0012483803 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -210         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.6        |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00221     |
|    std                  | 1.06         |
|    value_loss           | 2.17         |
------------------------------------------
Iteration: 241 | Episodes: 9800 | Median Reward: 39.18 | Avg Reward: 39.35 | Max Reward: 44.60
Iteration: 244 | Episodes: 9900 | Median Reward: 39.25 | Avg Reward: 38.51 | Max Reward: 44.60
Iteration: 246 | Episodes: 10000 | Median Reward: 39.97 | Avg Reward: 38.77 | Max Reward: 44.60
Iteration: 249 | Episodes: 10100 | Median Reward: 39.04 | Avg Reward: 39.48 | Max Reward: 44.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.6        |
| time/                   |              |
|    fps                  | 970          |
|    iterations           | 250          |
|    time_elapsed         | 1054         |
|    total_timesteps      | 1024000      |
| train/                  |              |
|    approx_kl            | 0.0038840878 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -211         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.6        |
|    n_updates            | 2490         |
|    policy_gradient_loss | -0.00421     |
|    std                  | 1.06         |
|    value_loss           | 1.16         |
------------------------------------------
Iteration: 251 | Episodes: 10200 | Median Reward: 39.20 | Avg Reward: 39.13 | Max Reward: 44.60
Iteration: 253 | Episodes: 10300 | Median Reward: 39.08 | Avg Reward: 38.53 | Max Reward: 44.60
Iteration: 256 | Episodes: 10400 | Median Reward: 38.66 | Avg Reward: 38.03 | Max Reward: 44.60
Iteration: 258 | Episodes: 10500 | Median Reward: 39.52 | Avg Reward: 38.07 | Max Reward: 44.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.9        |
| time/                   |              |
|    fps                  | 970          |
|    iterations           | 260          |
|    time_elapsed         | 1097         |
|    total_timesteps      | 1064960      |
| train/                  |              |
|    approx_kl            | 0.0060729384 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -213         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -19.6        |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.00672     |
|    std                  | 1.07         |
|    value_loss           | 1.46         |
------------------------------------------
Iteration: 261 | Episodes: 10600 | Median Reward: 39.50 | Avg Reward: 38.61 | Max Reward: 44.60
Iteration: 263 | Episodes: 10700 | Median Reward: 39.40 | Avg Reward: 38.43 | Max Reward: 44.60
Iteration: 266 | Episodes: 10800 | Median Reward: 39.39 | Avg Reward: 37.76 | Max Reward: 44.60
Iteration: 268 | Episodes: 10900 | Median Reward: 35.89 | Avg Reward: 36.42 | Max Reward: 44.60
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -64.2      |
| time/                   |            |
|    fps                  | 970        |
|    iterations           | 270        |
|    time_elapsed         | 1139       |
|    total_timesteps      | 1105920    |
| train/                  |            |
|    approx_kl            | 0.09100704 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.3        |
|    entropy_loss         | -213       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -21.3      |
|    n_updates            | 2690       |
|    policy_gradient_loss | -0.0429    |
|    std                  | 1.07       |
|    value_loss           | 0.554      |
----------------------------------------
Iteration: 271 | Episodes: 11000 | Median Reward: 38.62 | Avg Reward: 37.52 | Max Reward: 44.60
Iteration: 273 | Episodes: 11100 | Median Reward: 40.47 | Avg Reward: 39.80 | Max Reward: 44.60
Iteration: 276 | Episodes: 11200 | Median Reward: 39.44 | Avg Reward: 38.92 | Max Reward: 44.60
Iteration: 278 | Episodes: 11300 | Median Reward: 40.80 | Avg Reward: 40.37 | Max Reward: 44.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.7        |
| time/                   |              |
|    fps                  | 970          |
|    iterations           | 280          |
|    time_elapsed         | 1181         |
|    total_timesteps      | 1146880      |
| train/                  |              |
|    approx_kl            | 0.0039254124 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -215         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -21          |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.000898    |
|    std                  | 1.07         |
|    value_loss           | 1.32         |
------------------------------------------
Iteration: 281 | Episodes: 11400 | Median Reward: 40.03 | Avg Reward: 39.98 | Max Reward: 44.60
Iteration: 283 | Episodes: 11500 | Median Reward: 41.91 | Avg Reward: 41.60 | Max Reward: 44.60
Iteration: 286 | Episodes: 11600 | Median Reward: 40.58 | Avg Reward: 39.83 | Max Reward: 44.60
Iteration: 288 | Episodes: 11700 | Median Reward: 39.36 | Avg Reward: 38.68 | Max Reward: 44.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.7        |
| time/                   |              |
|    fps                  | 970          |
|    iterations           | 290          |
|    time_elapsed         | 1224         |
|    total_timesteps      | 1187840      |
| train/                  |              |
|    approx_kl            | 0.0062605343 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -217         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.9        |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 1.08         |
|    value_loss           | 1.66         |
------------------------------------------
Iteration: 290 | Episodes: 11800 | Median Reward: 40.33 | Avg Reward: 39.64 | Max Reward: 44.60
Iteration: 293 | Episodes: 11900 | Median Reward: 38.54 | Avg Reward: 39.08 | Max Reward: 44.60
Iteration: 295 | Episodes: 12000 | Median Reward: 38.91 | Avg Reward: 40.11 | Max Reward: 44.60
Iteration: 298 | Episodes: 12100 | Median Reward: 39.12 | Avg Reward: 37.02 | Max Reward: 44.60
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -62.3      |
| time/                   |            |
|    fps                  | 970        |
|    iterations           | 300        |
|    time_elapsed         | 1266       |
|    total_timesteps      | 1228800    |
| train/                  |            |
|    approx_kl            | 0.07790912 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.3        |
|    entropy_loss         | -219       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -18.4      |
|    n_updates            | 2990       |
|    policy_gradient_loss | -0.0461    |
|    std                  | 1.08       |
|    value_loss           | 4.02       |
----------------------------------------
Iteration: 300 | Episodes: 12200 | Median Reward: 37.15 | Avg Reward: 38.35 | Max Reward: 44.60
Iteration: 303 | Episodes: 12300 | Median Reward: 39.11 | Avg Reward: 39.06 | Max Reward: 44.60
Iteration: 305 | Episodes: 12400 | Median Reward: 39.93 | Avg Reward: 39.93 | Max Reward: 44.60
Iteration: 308 | Episodes: 12500 | Median Reward: 41.03 | Avg Reward: 40.31 | Max Reward: 44.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.9        |
| time/                   |              |
|    fps                  | 969          |
|    iterations           | 310          |
|    time_elapsed         | 1309         |
|    total_timesteps      | 1269760      |
| train/                  |              |
|    approx_kl            | 0.0025993206 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -220         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.9        |
|    n_updates            | 3090         |
|    policy_gradient_loss | -0.00364     |
|    std                  | 1.08         |
|    value_loss           | 1.49         |
------------------------------------------
Iteration: 310 | Episodes: 12600 | Median Reward: 38.06 | Avg Reward: 38.79 | Max Reward: 44.60
Iteration: 313 | Episodes: 12700 | Median Reward: 39.54 | Avg Reward: 38.53 | Max Reward: 44.60
Iteration: 315 | Episodes: 12800 | Median Reward: 38.57 | Avg Reward: 38.25 | Max Reward: 44.60
Iteration: 318 | Episodes: 12900 | Median Reward: 40.00 | Avg Reward: 39.71 | Max Reward: 44.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61          |
| time/                   |              |
|    fps                  | 969          |
|    iterations           | 320          |
|    time_elapsed         | 1352         |
|    total_timesteps      | 1310720      |
| train/                  |              |
|    approx_kl            | 0.0023531371 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -220         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -19.7        |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.00218     |
|    std                  | 1.09         |
|    value_loss           | 2.69         |
------------------------------------------
Iteration: 320 | Episodes: 13000 | Median Reward: 38.90 | Avg Reward: 39.35 | Max Reward: 44.60
Iteration: 323 | Episodes: 13100 | Median Reward: 39.92 | Avg Reward: 39.65 | Max Reward: 44.60
Iteration: 325 | Episodes: 13200 | Median Reward: 39.75 | Avg Reward: 40.15 | Max Reward: 44.60
Iteration: 327 | Episodes: 13300 | Median Reward: 39.77 | Avg Reward: 39.92 | Max Reward: 44.60
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.1         |
| time/                   |               |
|    fps                  | 969           |
|    iterations           | 330           |
|    time_elapsed         | 1394          |
|    total_timesteps      | 1351680       |
| train/                  |               |
|    approx_kl            | 0.00033405068 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -222          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.0005        |
|    loss                 | -21.7         |
|    n_updates            | 3290          |
|    policy_gradient_loss | -0.000743     |
|    std                  | 1.09          |
|    value_loss           | 2.41          |
-------------------------------------------
Iteration: 330 | Episodes: 13400 | Median Reward: 39.83 | Avg Reward: 39.39 | Max Reward: 44.60
Iteration: 332 | Episodes: 13500 | Median Reward: 40.90 | Avg Reward: 39.48 | Max Reward: 44.60
Iteration: 335 | Episodes: 13600 | Median Reward: 41.88 | Avg Reward: 40.76 | Max Reward: 44.60
Iteration: 337 | Episodes: 13700 | Median Reward: 40.45 | Avg Reward: 39.93 | Max Reward: 44.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.8        |
| time/                   |              |
|    fps                  | 969          |
|    iterations           | 340          |
|    time_elapsed         | 1436         |
|    total_timesteps      | 1392640      |
| train/                  |              |
|    approx_kl            | 0.0009008908 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -224         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.5        |
|    n_updates            | 3390         |
|    policy_gradient_loss | -0.000469    |
|    std                  | 1.1          |
|    value_loss           | 3            |
------------------------------------------
Iteration: 340 | Episodes: 13800 | Median Reward: 40.01 | Avg Reward: 39.95 | Max Reward: 44.60
Iteration: 342 | Episodes: 13900 | Median Reward: 40.43 | Avg Reward: 40.25 | Max Reward: 44.60
Iteration: 345 | Episodes: 14000 | Median Reward: 39.98 | Avg Reward: 40.26 | Max Reward: 44.60
Iteration: 347 | Episodes: 14100 | Median Reward: 40.21 | Avg Reward: 39.56 | Max Reward: 44.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.2        |
| time/                   |              |
|    fps                  | 968          |
|    iterations           | 350          |
|    time_elapsed         | 1479         |
|    total_timesteps      | 1433600      |
| train/                  |              |
|    approx_kl            | 0.0029402173 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -223         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -22.2        |
|    n_updates            | 3490         |
|    policy_gradient_loss | -0.0044      |
|    std                  | 1.1          |
|    value_loss           | 0.473        |
------------------------------------------
Iteration: 350 | Episodes: 14200 | Median Reward: 40.46 | Avg Reward: 39.84 | Max Reward: 44.60
Iteration: 352 | Episodes: 14300 | Median Reward: 41.21 | Avg Reward: 40.67 | Max Reward: 44.60
Iteration: 355 | Episodes: 14400 | Median Reward: 39.07 | Avg Reward: 40.17 | Max Reward: 45.42
Iteration: 357 | Episodes: 14500 | Median Reward: 41.06 | Avg Reward: 40.40 | Max Reward: 45.42
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -60.2      |
| time/                   |            |
|    fps                  | 968        |
|    iterations           | 360        |
|    time_elapsed         | 1522       |
|    total_timesteps      | 1474560    |
| train/                  |            |
|    approx_kl            | 0.07762865 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.3        |
|    entropy_loss         | -226       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0005     |
|    loss                 | -21.1      |
|    n_updates            | 3590       |
|    policy_gradient_loss | -0.0459    |
|    std                  | 1.1        |
|    value_loss           | 1.11       |
----------------------------------------
Iteration: 360 | Episodes: 14600 | Median Reward: 41.61 | Avg Reward: 39.78 | Max Reward: 45.42
Iteration: 362 | Episodes: 14700 | Median Reward: 40.66 | Avg Reward: 39.97 | Max Reward: 45.42
Iteration: 364 | Episodes: 14800 | Median Reward: 40.58 | Avg Reward: 40.98 | Max Reward: 45.42
Iteration: 367 | Episodes: 14900 | Median Reward: 42.18 | Avg Reward: 41.49 | Max Reward: 45.42
Iteration: 369 | Episodes: 15000 | Median Reward: 41.26 | Avg Reward: 41.72 | Max Reward: 45.42
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.3         |
| time/                   |               |
|    fps                  | 968           |
|    iterations           | 370           |
|    time_elapsed         | 1565          |
|    total_timesteps      | 1515520       |
| train/                  |               |
|    approx_kl            | 0.00013553066 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -228          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -22.2         |
|    n_updates            | 3690          |
|    policy_gradient_loss | 0.000274      |
|    std                  | 1.11          |
|    value_loss           | 1.64          |
-------------------------------------------
Iteration: 372 | Episodes: 15100 | Median Reward: 39.24 | Avg Reward: 39.69 | Max Reward: 45.42
Iteration: 374 | Episodes: 15200 | Median Reward: 39.64 | Avg Reward: 39.71 | Max Reward: 45.42
Iteration: 377 | Episodes: 15300 | Median Reward: 39.99 | Avg Reward: 40.24 | Max Reward: 45.42
Iteration: 379 | Episodes: 15400 | Median Reward: 40.50 | Avg Reward: 40.77 | Max Reward: 45.42
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.3       |
| time/                   |             |
|    fps                  | 968         |
|    iterations           | 380         |
|    time_elapsed         | 1607        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.004028728 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -229        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -22.1       |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00461    |
|    std                  | 1.11        |
|    value_loss           | 5.8         |
-----------------------------------------
Iteration: 382 | Episodes: 15500 | Median Reward: 42.68 | Avg Reward: 41.44 | Max Reward: 45.73
Iteration: 384 | Episodes: 15600 | Median Reward: 39.83 | Avg Reward: 39.75 | Max Reward: 45.73
Iteration: 387 | Episodes: 15700 | Median Reward: 40.51 | Avg Reward: 40.66 | Max Reward: 45.73
Iteration: 389 | Episodes: 15800 | Median Reward: 40.51 | Avg Reward: 40.64 | Max Reward: 45.73
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.9       |
| time/                   |             |
|    fps                  | 968         |
|    iterations           | 390         |
|    time_elapsed         | 1649        |
|    total_timesteps      | 1597440     |
| train/                  |             |
|    approx_kl            | 0.020691706 |
|    clip_fraction        | 0.0255      |
|    clip_range           | 0.3         |
|    entropy_loss         | -231        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -22.3       |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.0145     |
|    std                  | 1.11        |
|    value_loss           | 1.18        |
-----------------------------------------
Iteration: 392 | Episodes: 15900 | Median Reward: 40.67 | Avg Reward: 40.79 | Max Reward: 45.73
Iteration: 394 | Episodes: 16000 | Median Reward: 41.12 | Avg Reward: 40.04 | Max Reward: 45.73
Iteration: 396 | Episodes: 16100 | Median Reward: 40.13 | Avg Reward: 40.70 | Max Reward: 45.73
Iteration: 399 | Episodes: 16200 | Median Reward: 41.10 | Avg Reward: 41.36 | Max Reward: 45.73
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.7        |
| time/                   |              |
|    fps                  | 968          |
|    iterations           | 400          |
|    time_elapsed         | 1692         |
|    total_timesteps      | 1638400      |
| train/                  |              |
|    approx_kl            | 0.0003983777 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -233         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.8        |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.000319    |
|    std                  | 1.12         |
|    value_loss           | 1.31         |
------------------------------------------
Iteration: 401 | Episodes: 16300 | Median Reward: 42.83 | Avg Reward: 41.96 | Max Reward: 45.73
Iteration: 404 | Episodes: 16400 | Median Reward: 42.03 | Avg Reward: 41.08 | Max Reward: 45.73
Iteration: 406 | Episodes: 16500 | Median Reward: 43.10 | Avg Reward: 42.17 | Max Reward: 45.73
Iteration: 409 | Episodes: 16600 | Median Reward: 41.93 | Avg Reward: 41.39 | Max Reward: 45.73
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.4         |
| time/                   |               |
|    fps                  | 968           |
|    iterations           | 410           |
|    time_elapsed         | 1734          |
|    total_timesteps      | 1679360       |
| train/                  |               |
|    approx_kl            | 0.00039459765 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -235          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -22.8         |
|    n_updates            | 4090          |
|    policy_gradient_loss | -0.000773     |
|    std                  | 1.12          |
|    value_loss           | 0.872         |
-------------------------------------------
Iteration: 411 | Episodes: 16700 | Median Reward: 40.44 | Avg Reward: 40.54 | Max Reward: 45.73
Iteration: 414 | Episodes: 16800 | Median Reward: 41.89 | Avg Reward: 41.27 | Max Reward: 45.73
Iteration: 416 | Episodes: 16900 | Median Reward: 40.87 | Avg Reward: 41.06 | Max Reward: 45.73
Iteration: 419 | Episodes: 17000 | Median Reward: 42.47 | Avg Reward: 42.40 | Max Reward: 46.15
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.7         |
| time/                   |               |
|    fps                  | 967           |
|    iterations           | 420           |
|    time_elapsed         | 1777          |
|    total_timesteps      | 1720320       |
| train/                  |               |
|    approx_kl            | 2.5667701e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -236          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -17.9         |
|    n_updates            | 4190          |
|    policy_gradient_loss | -6.47e-05     |
|    std                  | 1.13          |
|    value_loss           | 3.82          |
-------------------------------------------
Iteration: 421 | Episodes: 17100 | Median Reward: 41.89 | Avg Reward: 41.25 | Max Reward: 46.15
Iteration: 424 | Episodes: 17200 | Median Reward: 40.41 | Avg Reward: 40.65 | Max Reward: 46.15
Iteration: 426 | Episodes: 17300 | Median Reward: 41.06 | Avg Reward: 41.09 | Max Reward: 46.15
Iteration: 429 | Episodes: 17400 | Median Reward: 40.96 | Avg Reward: 40.82 | Max Reward: 46.15
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.4         |
| time/                   |               |
|    fps                  | 967           |
|    iterations           | 430           |
|    time_elapsed         | 1820          |
|    total_timesteps      | 1761280       |
| train/                  |               |
|    approx_kl            | 0.00037259836 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -236          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -23.2         |
|    n_updates            | 4290          |
|    policy_gradient_loss | -0.000416     |
|    std                  | 1.13          |
|    value_loss           | 1.21          |
-------------------------------------------
Iteration: 431 | Episodes: 17500 | Median Reward: 40.27 | Avg Reward: 40.75 | Max Reward: 46.15
Iteration: 433 | Episodes: 17600 | Median Reward: 40.73 | Avg Reward: 40.97 | Max Reward: 46.15
Iteration: 436 | Episodes: 17700 | Median Reward: 42.29 | Avg Reward: 41.58 | Max Reward: 46.15
Iteration: 438 | Episodes: 17800 | Median Reward: 41.92 | Avg Reward: 40.85 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.9        |
| time/                   |              |
|    fps                  | 967          |
|    iterations           | 440          |
|    time_elapsed         | 1862         |
|    total_timesteps      | 1802240      |
| train/                  |              |
|    approx_kl            | 0.0020038509 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -237         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -21          |
|    n_updates            | 4390         |
|    policy_gradient_loss | 0.000885     |
|    std                  | 1.14         |
|    value_loss           | 2.15         |
------------------------------------------
Iteration: 441 | Episodes: 17900 | Median Reward: 42.66 | Avg Reward: 42.07 | Max Reward: 46.15
Iteration: 443 | Episodes: 18000 | Median Reward: 41.97 | Avg Reward: 42.01 | Max Reward: 46.15
Iteration: 446 | Episodes: 18100 | Median Reward: 41.11 | Avg Reward: 41.61 | Max Reward: 46.15
Iteration: 448 | Episodes: 18200 | Median Reward: 42.80 | Avg Reward: 42.34 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.5        |
| time/                   |              |
|    fps                  | 967          |
|    iterations           | 450          |
|    time_elapsed         | 1905         |
|    total_timesteps      | 1843200      |
| train/                  |              |
|    approx_kl            | 0.0014992824 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -238         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -20.7        |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.000934    |
|    std                  | 1.14         |
|    value_loss           | 2.69         |
------------------------------------------
Iteration: 451 | Episodes: 18300 | Median Reward: 42.65 | Avg Reward: 41.62 | Max Reward: 46.15
Iteration: 453 | Episodes: 18400 | Median Reward: 42.35 | Avg Reward: 41.52 | Max Reward: 46.15
Iteration: 456 | Episodes: 18500 | Median Reward: 42.82 | Avg Reward: 41.59 | Max Reward: 46.15
Iteration: 458 | Episodes: 18600 | Median Reward: 41.05 | Avg Reward: 40.61 | Max Reward: 46.15
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.1         |
| time/                   |               |
|    fps                  | 967           |
|    iterations           | 460           |
|    time_elapsed         | 1948          |
|    total_timesteps      | 1884160       |
| train/                  |               |
|    approx_kl            | 0.00041381601 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -239          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -23.8         |
|    n_updates            | 4590          |
|    policy_gradient_loss | -0.0012       |
|    std                  | 1.14          |
|    value_loss           | 1.49          |
-------------------------------------------
Iteration: 461 | Episodes: 18700 | Median Reward: 42.71 | Avg Reward: 41.84 | Max Reward: 46.15
Iteration: 463 | Episodes: 18800 | Median Reward: 41.54 | Avg Reward: 41.54 | Max Reward: 46.15
Iteration: 466 | Episodes: 18900 | Median Reward: 43.38 | Avg Reward: 42.36 | Max Reward: 46.15
Iteration: 468 | Episodes: 19000 | Median Reward: 42.65 | Avg Reward: 41.78 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.9        |
| time/                   |              |
|    fps                  | 967          |
|    iterations           | 470          |
|    time_elapsed         | 1989         |
|    total_timesteps      | 1925120      |
| train/                  |              |
|    approx_kl            | 0.0065443637 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -240         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -23.7        |
|    n_updates            | 4690         |
|    policy_gradient_loss | -0.012       |
|    std                  | 1.15         |
|    value_loss           | 0.551        |
------------------------------------------
Iteration: 470 | Episodes: 19100 | Median Reward: 40.63 | Avg Reward: 41.26 | Max Reward: 46.15
Iteration: 473 | Episodes: 19200 | Median Reward: 40.46 | Avg Reward: 40.75 | Max Reward: 46.15
Iteration: 475 | Episodes: 19300 | Median Reward: 40.57 | Avg Reward: 41.24 | Max Reward: 46.15
Iteration: 478 | Episodes: 19400 | Median Reward: 43.71 | Avg Reward: 43.27 | Max Reward: 46.15
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.9         |
| time/                   |               |
|    fps                  | 967           |
|    iterations           | 480           |
|    time_elapsed         | 2032          |
|    total_timesteps      | 1966080       |
| train/                  |               |
|    approx_kl            | 0.00055115554 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -241          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -23.4         |
|    n_updates            | 4790          |
|    policy_gradient_loss | -0.00123      |
|    std                  | 1.16          |
|    value_loss           | 1.03          |
-------------------------------------------
Iteration: 480 | Episodes: 19500 | Median Reward: 42.77 | Avg Reward: 42.15 | Max Reward: 46.15
Iteration: 483 | Episodes: 19600 | Median Reward: 42.81 | Avg Reward: 41.90 | Max Reward: 46.15
Iteration: 485 | Episodes: 19700 | Median Reward: 41.13 | Avg Reward: 41.07 | Max Reward: 46.15
Iteration: 488 | Episodes: 19800 | Median Reward: 42.50 | Avg Reward: 41.34 | Max Reward: 46.15
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -57.8      |
| time/                   |            |
|    fps                  | 967        |
|    iterations           | 490        |
|    time_elapsed         | 2074       |
|    total_timesteps      | 2007040    |
| train/                  |            |
|    approx_kl            | 0.06603333 |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.3        |
|    entropy_loss         | -243       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -23.4      |
|    n_updates            | 4890       |
|    policy_gradient_loss | -0.025     |
|    std                  | 1.16       |
|    value_loss           | 0.632      |
----------------------------------------
Iteration: 490 | Episodes: 19900 | Median Reward: 43.05 | Avg Reward: 42.53 | Max Reward: 46.15
Iteration: 493 | Episodes: 20000 | Median Reward: 40.54 | Avg Reward: 40.74 | Max Reward: 46.15
Iteration: 495 | Episodes: 20100 | Median Reward: 42.81 | Avg Reward: 42.69 | Max Reward: 46.15
Iteration: 498 | Episodes: 20200 | Median Reward: 40.80 | Avg Reward: 40.93 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.8        |
| time/                   |              |
|    fps                  | 967          |
|    iterations           | 500          |
|    time_elapsed         | 2117         |
|    total_timesteps      | 2048000      |
| train/                  |              |
|    approx_kl            | 0.0025501607 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -244         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.1        |
|    n_updates            | 4990         |
|    policy_gradient_loss | -0.00744     |
|    std                  | 1.17         |
|    value_loss           | 4.45         |
------------------------------------------
Iteration: 500 | Episodes: 20300 | Median Reward: 42.29 | Avg Reward: 41.75 | Max Reward: 46.15
Iteration: 503 | Episodes: 20400 | Median Reward: 43.43 | Avg Reward: 41.72 | Max Reward: 46.15
Iteration: 505 | Episodes: 20500 | Median Reward: 42.90 | Avg Reward: 41.96 | Max Reward: 46.15
Iteration: 507 | Episodes: 20600 | Median Reward: 43.00 | Avg Reward: 41.98 | Max Reward: 46.15
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.3         |
| time/                   |               |
|    fps                  | 967           |
|    iterations           | 510           |
|    time_elapsed         | 2159          |
|    total_timesteps      | 2088960       |
| train/                  |               |
|    approx_kl            | 0.00037611724 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -246          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -24.5         |
|    n_updates            | 5090          |
|    policy_gradient_loss | -3.29e-05     |
|    std                  | 1.17          |
|    value_loss           | 0.51          |
-------------------------------------------
Iteration: 510 | Episodes: 20700 | Median Reward: 43.08 | Avg Reward: 42.60 | Max Reward: 46.15
Iteration: 512 | Episodes: 20800 | Median Reward: 42.54 | Avg Reward: 41.98 | Max Reward: 46.15
Iteration: 515 | Episodes: 20900 | Median Reward: 43.41 | Avg Reward: 42.78 | Max Reward: 46.15
Iteration: 517 | Episodes: 21000 | Median Reward: 42.90 | Avg Reward: 42.13 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.1        |
| time/                   |              |
|    fps                  | 950          |
|    iterations           | 520          |
|    time_elapsed         | 2239         |
|    total_timesteps      | 2129920      |
| train/                  |              |
|    approx_kl            | 0.0011183604 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -246         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -24.1        |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00304     |
|    std                  | 1.18         |
|    value_loss           | 2.93         |
------------------------------------------
Iteration: 520 | Episodes: 21100 | Median Reward: 42.72 | Avg Reward: 41.95 | Max Reward: 46.15
Iteration: 522 | Episodes: 21200 | Median Reward: 42.62 | Avg Reward: 41.88 | Max Reward: 46.15
Iteration: 525 | Episodes: 21300 | Median Reward: 43.49 | Avg Reward: 42.65 | Max Reward: 46.15
Iteration: 527 | Episodes: 21400 | Median Reward: 43.10 | Avg Reward: 42.46 | Max Reward: 46.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.8       |
| time/                   |             |
|    fps                  | 930         |
|    iterations           | 530         |
|    time_elapsed         | 2333        |
|    total_timesteps      | 2170880     |
| train/                  |             |
|    approx_kl            | 0.014495519 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -247        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -24.2       |
|    n_updates            | 5290        |
|    policy_gradient_loss | -0.0195     |
|    std                  | 1.18        |
|    value_loss           | 0.906       |
-----------------------------------------
Iteration: 530 | Episodes: 21500 | Median Reward: 43.49 | Avg Reward: 42.26 | Max Reward: 46.15
Iteration: 532 | Episodes: 21600 | Median Reward: 43.52 | Avg Reward: 42.48 | Max Reward: 46.15
Iteration: 535 | Episodes: 21700 | Median Reward: 43.68 | Avg Reward: 42.63 | Max Reward: 46.15
Iteration: 537 | Episodes: 21800 | Median Reward: 43.56 | Avg Reward: 43.04 | Max Reward: 46.15
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -58        |
| time/                   |            |
|    fps                  | 920        |
|    iterations           | 540        |
|    time_elapsed         | 2403       |
|    total_timesteps      | 2211840    |
| train/                  |            |
|    approx_kl            | 0.01254064 |
|    clip_fraction        | 0          |
|    clip_range           | 0.3        |
|    entropy_loss         | -248       |
|    explained_variance   | 1          |
|    learning_rate        | 0.0005     |
|    loss                 | -24.4      |
|    n_updates            | 5390       |
|    policy_gradient_loss | -0.0209    |
|    std                  | 1.19       |
|    value_loss           | 1.28       |
----------------------------------------
Iteration: 540 | Episodes: 21900 | Median Reward: 42.87 | Avg Reward: 41.89 | Max Reward: 46.15
Iteration: 542 | Episodes: 22000 | Median Reward: 41.19 | Avg Reward: 41.84 | Max Reward: 46.15
Iteration: 544 | Episodes: 22100 | Median Reward: 43.08 | Avg Reward: 42.27 | Max Reward: 46.15
Iteration: 547 | Episodes: 22200 | Median Reward: 43.05 | Avg Reward: 42.05 | Max Reward: 46.15
Iteration: 549 | Episodes: 22300 | Median Reward: 42.22 | Avg Reward: 41.72 | Max Reward: 46.15
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.2         |
| time/                   |               |
|    fps                  | 920           |
|    iterations           | 550           |
|    time_elapsed         | 2447          |
|    total_timesteps      | 2252800       |
| train/                  |               |
|    approx_kl            | 0.00028257593 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -248          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.0005        |
|    loss                 | -24.1         |
|    n_updates            | 5490          |
|    policy_gradient_loss | 0.00171       |
|    std                  | 1.19          |
|    value_loss           | 5.65          |
-------------------------------------------
Iteration: 552 | Episodes: 22400 | Median Reward: 43.04 | Avg Reward: 42.39 | Max Reward: 46.15
Iteration: 554 | Episodes: 22500 | Median Reward: 42.83 | Avg Reward: 42.00 | Max Reward: 46.15
Iteration: 557 | Episodes: 22600 | Median Reward: 41.03 | Avg Reward: 41.99 | Max Reward: 46.15
Iteration: 559 | Episodes: 22700 | Median Reward: 43.95 | Avg Reward: 43.47 | Max Reward: 46.15
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.6         |
| time/                   |               |
|    fps                  | 921           |
|    iterations           | 560           |
|    time_elapsed         | 2489          |
|    total_timesteps      | 2293760       |
| train/                  |               |
|    approx_kl            | 0.00027911476 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -248          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -24.7         |
|    n_updates            | 5590          |
|    policy_gradient_loss | -0.000175     |
|    std                  | 1.2           |
|    value_loss           | 1.32          |
-------------------------------------------
Iteration: 562 | Episodes: 22800 | Median Reward: 43.26 | Avg Reward: 41.99 | Max Reward: 46.15
Iteration: 564 | Episodes: 22900 | Median Reward: 43.81 | Avg Reward: 43.23 | Max Reward: 46.15
Iteration: 567 | Episodes: 23000 | Median Reward: 43.05 | Avg Reward: 42.10 | Max Reward: 46.15
Iteration: 569 | Episodes: 23100 | Median Reward: 41.98 | Avg Reward: 42.04 | Max Reward: 46.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.8       |
| time/                   |             |
|    fps                  | 921         |
|    iterations           | 570         |
|    time_elapsed         | 2533        |
|    total_timesteps      | 2334720     |
| train/                  |             |
|    approx_kl            | 0.045713738 |
|    clip_fraction        | 0.051       |
|    clip_range           | 0.3         |
|    entropy_loss         | -249        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -24.4       |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.05       |
|    std                  | 1.2         |
|    value_loss           | 0.873       |
-----------------------------------------
Iteration: 572 | Episodes: 23200 | Median Reward: 43.00 | Avg Reward: 41.85 | Max Reward: 46.15
Iteration: 574 | Episodes: 23300 | Median Reward: 43.45 | Avg Reward: 42.49 | Max Reward: 46.15
Iteration: 577 | Episodes: 23400 | Median Reward: 43.57 | Avg Reward: 42.95 | Max Reward: 46.15
Iteration: 579 | Episodes: 23500 | Median Reward: 43.53 | Avg Reward: 42.74 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 580          |
|    time_elapsed         | 2576         |
|    total_timesteps      | 2375680      |
| train/                  |              |
|    approx_kl            | 0.0001314277 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -22.5        |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.000398    |
|    std                  | 1.21         |
|    value_loss           | 2.74         |
------------------------------------------
Iteration: 581 | Episodes: 23600 | Median Reward: 42.77 | Avg Reward: 42.33 | Max Reward: 46.15
Iteration: 584 | Episodes: 23700 | Median Reward: 43.57 | Avg Reward: 42.29 | Max Reward: 46.15
Iteration: 586 | Episodes: 23800 | Median Reward: 42.67 | Avg Reward: 42.19 | Max Reward: 46.15
Iteration: 589 | Episodes: 23900 | Median Reward: 43.42 | Avg Reward: 41.54 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.3        |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 590          |
|    time_elapsed         | 2619         |
|    total_timesteps      | 2416640      |
| train/                  |              |
|    approx_kl            | 0.0020831232 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -250         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.4        |
|    n_updates            | 5890         |
|    policy_gradient_loss | -0.00344     |
|    std                  | 1.21         |
|    value_loss           | 3.81         |
------------------------------------------
Iteration: 591 | Episodes: 24000 | Median Reward: 43.40 | Avg Reward: 42.33 | Max Reward: 46.15
Iteration: 594 | Episodes: 24100 | Median Reward: 42.85 | Avg Reward: 42.05 | Max Reward: 46.15
Iteration: 596 | Episodes: 24200 | Median Reward: 42.83 | Avg Reward: 42.36 | Max Reward: 46.15
Iteration: 599 | Episodes: 24300 | Median Reward: 42.54 | Avg Reward: 42.00 | Max Reward: 46.15
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.2         |
| time/                   |               |
|    fps                  | 922           |
|    iterations           | 600           |
|    time_elapsed         | 2662          |
|    total_timesteps      | 2457600       |
| train/                  |               |
|    approx_kl            | 0.00052776036 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -250          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -22.4         |
|    n_updates            | 5990          |
|    policy_gradient_loss | -0.00168      |
|    std                  | 1.21          |
|    value_loss           | 2.71          |
-------------------------------------------
Iteration: 601 | Episodes: 24400 | Median Reward: 42.67 | Avg Reward: 42.14 | Max Reward: 46.15
Iteration: 604 | Episodes: 24500 | Median Reward: 43.71 | Avg Reward: 42.82 | Max Reward: 46.15
Iteration: 606 | Episodes: 24600 | Median Reward: 43.56 | Avg Reward: 42.76 | Max Reward: 46.15
Iteration: 609 | Episodes: 24700 | Median Reward: 41.13 | Avg Reward: 41.77 | Max Reward: 46.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 923         |
|    iterations           | 610         |
|    time_elapsed         | 2706        |
|    total_timesteps      | 2498560     |
| train/                  |             |
|    approx_kl            | 0.022648674 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.3         |
|    entropy_loss         | -251        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -25         |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.0229     |
|    std                  | 1.22        |
|    value_loss           | 0.437       |
-----------------------------------------
Iteration: 611 | Episodes: 24800 | Median Reward: 43.49 | Avg Reward: 42.35 | Max Reward: 46.15
Iteration: 613 | Episodes: 24900 | Median Reward: 43.41 | Avg Reward: 42.71 | Max Reward: 46.15
Iteration: 616 | Episodes: 25000 | Median Reward: 42.84 | Avg Reward: 42.27 | Max Reward: 46.15
Iteration: 618 | Episodes: 25100 | Median Reward: 43.09 | Avg Reward: 42.83 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.5        |
| time/                   |              |
|    fps                  | 923          |
|    iterations           | 620          |
|    time_elapsed         | 2749         |
|    total_timesteps      | 2539520      |
| train/                  |              |
|    approx_kl            | 0.0026801345 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -252         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0005       |
|    loss                 | -24.6        |
|    n_updates            | 6190         |
|    policy_gradient_loss | 0.000179     |
|    std                  | 1.23         |
|    value_loss           | 1.76         |
------------------------------------------
Iteration: 621 | Episodes: 25200 | Median Reward: 42.91 | Avg Reward: 41.34 | Max Reward: 46.15
Iteration: 623 | Episodes: 25300 | Median Reward: 43.10 | Avg Reward: 42.00 | Max Reward: 46.15
Iteration: 626 | Episodes: 25400 | Median Reward: 42.52 | Avg Reward: 41.84 | Max Reward: 46.15
Iteration: 628 | Episodes: 25500 | Median Reward: 43.09 | Avg Reward: 42.27 | Max Reward: 46.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.8       |
| time/                   |             |
|    fps                  | 924         |
|    iterations           | 630         |
|    time_elapsed         | 2792        |
|    total_timesteps      | 2580480     |
| train/                  |             |
|    approx_kl            | 0.001807246 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -253        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -24.3       |
|    n_updates            | 6290        |
|    policy_gradient_loss | -0.00274    |
|    std                  | 1.23        |
|    value_loss           | 1.42        |
-----------------------------------------
Iteration: 631 | Episodes: 25600 | Median Reward: 43.11 | Avg Reward: 42.63 | Max Reward: 46.15
Iteration: 633 | Episodes: 25700 | Median Reward: 43.07 | Avg Reward: 41.62 | Max Reward: 46.15
Iteration: 636 | Episodes: 25800 | Median Reward: 42.62 | Avg Reward: 42.16 | Max Reward: 46.15
Iteration: 638 | Episodes: 25900 | Median Reward: 43.78 | Avg Reward: 42.92 | Max Reward: 46.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.5       |
| time/                   |             |
|    fps                  | 924         |
|    iterations           | 640         |
|    time_elapsed         | 2834        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.015625015 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -254        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -25.3       |
|    n_updates            | 6390        |
|    policy_gradient_loss | -0.011      |
|    std                  | 1.24        |
|    value_loss           | 0.918       |
-----------------------------------------
Iteration: 641 | Episodes: 26000 | Median Reward: 43.24 | Avg Reward: 42.80 | Max Reward: 46.15
Iteration: 643 | Episodes: 26100 | Median Reward: 43.08 | Avg Reward: 42.39 | Max Reward: 46.15
Iteration: 646 | Episodes: 26200 | Median Reward: 43.15 | Avg Reward: 42.73 | Max Reward: 46.15
Iteration: 648 | Episodes: 26300 | Median Reward: 41.16 | Avg Reward: 41.24 | Max Reward: 46.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.6       |
| time/                   |             |
|    fps                  | 924         |
|    iterations           | 650         |
|    time_elapsed         | 2878        |
|    total_timesteps      | 2662400     |
| train/                  |             |
|    approx_kl            | 0.014091931 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -254        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.3       |
|    n_updates            | 6490        |
|    policy_gradient_loss | -0.0165     |
|    std                  | 1.24        |
|    value_loss           | 2.11        |
-----------------------------------------
Iteration: 650 | Episodes: 26400 | Median Reward: 41.21 | Avg Reward: 41.41 | Max Reward: 46.15
Iteration: 653 | Episodes: 26500 | Median Reward: 43.87 | Avg Reward: 43.22 | Max Reward: 46.15
Iteration: 655 | Episodes: 26600 | Median Reward: 43.11 | Avg Reward: 42.23 | Max Reward: 46.15
Iteration: 658 | Episodes: 26700 | Median Reward: 43.43 | Avg Reward: 42.06 | Max Reward: 46.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.9       |
| time/                   |             |
|    fps                  | 925         |
|    iterations           | 660         |
|    time_elapsed         | 2920        |
|    total_timesteps      | 2703360     |
| train/                  |             |
|    approx_kl            | 0.017057117 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.3         |
|    entropy_loss         | -255        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0005      |
|    loss                 | -23.9       |
|    n_updates            | 6590        |
|    policy_gradient_loss | -0.0136     |
|    std                  | 1.25        |
|    value_loss           | 1.68        |
-----------------------------------------
Iteration: 660 | Episodes: 26800 | Median Reward: 43.09 | Avg Reward: 42.26 | Max Reward: 46.15
Iteration: 663 | Episodes: 26900 | Median Reward: 43.88 | Avg Reward: 43.28 | Max Reward: 46.15
Iteration: 665 | Episodes: 27000 | Median Reward: 41.70 | Avg Reward: 42.00 | Max Reward: 46.15
Iteration: 668 | Episodes: 27100 | Median Reward: 42.79 | Avg Reward: 42.30 | Max Reward: 46.15
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57           |
| time/                   |               |
|    fps                  | 926           |
|    iterations           | 670           |
|    time_elapsed         | 2963          |
|    total_timesteps      | 2744320       |
| train/                  |               |
|    approx_kl            | 0.00045484075 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -256          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -25.1         |
|    n_updates            | 6690          |
|    policy_gradient_loss | -0.000251     |
|    std                  | 1.26          |
|    value_loss           | 2.1           |
-------------------------------------------
Iteration: 670 | Episodes: 27200 | Median Reward: 43.68 | Avg Reward: 42.46 | Max Reward: 46.15
Iteration: 673 | Episodes: 27300 | Median Reward: 43.93 | Avg Reward: 42.16 | Max Reward: 46.15
Iteration: 675 | Episodes: 27400 | Median Reward: 43.89 | Avg Reward: 43.02 | Max Reward: 46.15
Iteration: 678 | Episodes: 27500 | Median Reward: 43.05 | Avg Reward: 42.14 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 926          |
|    iterations           | 680          |
|    time_elapsed         | 3005         |
|    total_timesteps      | 2785280      |
| train/                  |              |
|    approx_kl            | 0.0034856633 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -257         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.6        |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.00654     |
|    std                  | 1.26         |
|    value_loss           | 1.7          |
------------------------------------------
Iteration: 680 | Episodes: 27600 | Median Reward: 43.19 | Avg Reward: 42.57 | Max Reward: 46.15
Iteration: 683 | Episodes: 27700 | Median Reward: 43.78 | Avg Reward: 42.99 | Max Reward: 46.15
Iteration: 685 | Episodes: 27800 | Median Reward: 43.73 | Avg Reward: 43.05 | Max Reward: 46.15
Iteration: 687 | Episodes: 27900 | Median Reward: 43.92 | Avg Reward: 43.43 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 927          |
|    iterations           | 690          |
|    time_elapsed         | 3048         |
|    total_timesteps      | 2826240      |
| train/                  |              |
|    approx_kl            | 0.0013366563 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -257         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.4        |
|    n_updates            | 6890         |
|    policy_gradient_loss | -0.00132     |
|    std                  | 1.27         |
|    value_loss           | 0.998        |
------------------------------------------
Iteration: 690 | Episodes: 28000 | Median Reward: 43.90 | Avg Reward: 42.85 | Max Reward: 46.15
Iteration: 692 | Episodes: 28100 | Median Reward: 41.69 | Avg Reward: 41.89 | Max Reward: 46.15
Iteration: 695 | Episodes: 28200 | Median Reward: 43.89 | Avg Reward: 42.54 | Max Reward: 46.15
Iteration: 697 | Episodes: 28300 | Median Reward: 42.97 | Avg Reward: 42.03 | Max Reward: 46.15
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.3       |
| time/                   |             |
|    fps                  | 921         |
|    iterations           | 700         |
|    time_elapsed         | 3112        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.005623592 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -258        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -24.8       |
|    n_updates            | 6990        |
|    policy_gradient_loss | -0.00254    |
|    std                  | 1.27        |
|    value_loss           | 1.34        |
-----------------------------------------
Iteration: 700 | Episodes: 28400 | Median Reward: 42.82 | Avg Reward: 41.75 | Max Reward: 46.15
Iteration: 702 | Episodes: 28500 | Median Reward: 43.51 | Avg Reward: 42.46 | Max Reward: 46.15
Iteration: 705 | Episodes: 28600 | Median Reward: 43.07 | Avg Reward: 42.31 | Max Reward: 46.15
Iteration: 707 | Episodes: 28700 | Median Reward: 43.87 | Avg Reward: 42.76 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.8        |
| time/                   |              |
|    fps                  | 914          |
|    iterations           | 710          |
|    time_elapsed         | 3180         |
|    total_timesteps      | 2908160      |
| train/                  |              |
|    approx_kl            | 0.0007886954 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -259         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.8        |
|    n_updates            | 7090         |
|    policy_gradient_loss | -0.000899    |
|    std                  | 1.28         |
|    value_loss           | 0.673        |
------------------------------------------
Iteration: 710 | Episodes: 28800 | Median Reward: 43.37 | Avg Reward: 43.05 | Max Reward: 46.15
Iteration: 712 | Episodes: 28900 | Median Reward: 41.94 | Avg Reward: 42.00 | Max Reward: 46.15
Iteration: 715 | Episodes: 29000 | Median Reward: 43.70 | Avg Reward: 43.33 | Max Reward: 46.15
Iteration: 717 | Episodes: 29100 | Median Reward: 43.25 | Avg Reward: 42.47 | Max Reward: 46.15
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 905          |
|    iterations           | 720          |
|    time_elapsed         | 3256         |
|    total_timesteps      | 2949120      |
| train/                  |              |
|    approx_kl            | 0.0009773378 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -259         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.4        |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.28         |
|    value_loss           | 0.704        |
------------------------------------------
Iteration: 720 | Episodes: 29200 | Median Reward: 43.09 | Avg Reward: 42.54 | Max Reward: 46.15
Iteration: 722 | Episodes: 29300 | Median Reward: 43.73 | Avg Reward: 42.88 | Max Reward: 46.61
Iteration: 724 | Episodes: 29400 | Median Reward: 43.91 | Avg Reward: 42.50 | Max Reward: 46.61
Iteration: 727 | Episodes: 29500 | Median Reward: 41.81 | Avg Reward: 42.00 | Max Reward: 46.61
Iteration: 729 | Episodes: 29600 | Median Reward: 43.81 | Avg Reward: 42.84 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 896          |
|    iterations           | 730          |
|    time_elapsed         | 3334         |
|    total_timesteps      | 2990080      |
| train/                  |              |
|    approx_kl            | 0.0018561695 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -260         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -25          |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 1.29         |
|    value_loss           | 0.608        |
------------------------------------------
Iteration: 732 | Episodes: 29700 | Median Reward: 43.90 | Avg Reward: 41.89 | Max Reward: 46.61
Iteration: 734 | Episodes: 29800 | Median Reward: 43.77 | Avg Reward: 42.90 | Max Reward: 46.61
Iteration: 737 | Episodes: 29900 | Median Reward: 43.57 | Avg Reward: 42.47 | Max Reward: 46.61
Iteration: 739 | Episodes: 30000 | Median Reward: 41.41 | Avg Reward: 42.27 | Max Reward: 46.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.9         |
| time/                   |               |
|    fps                  | 897           |
|    iterations           | 740           |
|    time_elapsed         | 3377          |
|    total_timesteps      | 3031040       |
| train/                  |               |
|    approx_kl            | 0.00025064807 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -261          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -26.1         |
|    n_updates            | 7390          |
|    policy_gradient_loss | -0.000418     |
|    std                  | 1.3           |
|    value_loss           | 0.432         |
-------------------------------------------
Iteration: 742 | Episodes: 30100 | Median Reward: 43.86 | Avg Reward: 43.06 | Max Reward: 46.61
Iteration: 744 | Episodes: 30200 | Median Reward: 43.89 | Avg Reward: 42.91 | Max Reward: 46.61
Iteration: 747 | Episodes: 30300 | Median Reward: 41.30 | Avg Reward: 41.75 | Max Reward: 46.61
Iteration: 749 | Episodes: 30400 | Median Reward: 43.12 | Avg Reward: 43.28 | Max Reward: 46.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.6       |
| time/                   |             |
|    fps                  | 898         |
|    iterations           | 750         |
|    time_elapsed         | 3420        |
|    total_timesteps      | 3072000     |
| train/                  |             |
|    approx_kl            | 0.002158349 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -262        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -23.6       |
|    n_updates            | 7490        |
|    policy_gradient_loss | -0.00276    |
|    std                  | 1.31        |
|    value_loss           | 1.01        |
-----------------------------------------
Iteration: 752 | Episodes: 30500 | Median Reward: 43.89 | Avg Reward: 42.74 | Max Reward: 46.61
Iteration: 754 | Episodes: 30600 | Median Reward: 42.93 | Avg Reward: 42.44 | Max Reward: 46.61
Iteration: 757 | Episodes: 30700 | Median Reward: 43.68 | Avg Reward: 42.83 | Max Reward: 46.61
Iteration: 759 | Episodes: 30800 | Median Reward: 43.04 | Avg Reward: 42.41 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.9        |
| time/                   |              |
|    fps                  | 898          |
|    iterations           | 760          |
|    time_elapsed         | 3463         |
|    total_timesteps      | 3112960      |
| train/                  |              |
|    approx_kl            | 0.0018762222 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -263         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -25.8        |
|    n_updates            | 7590         |
|    policy_gradient_loss | -0.00263     |
|    std                  | 1.31         |
|    value_loss           | 1.19         |
------------------------------------------
Iteration: 761 | Episodes: 30900 | Median Reward: 43.41 | Avg Reward: 42.49 | Max Reward: 46.61
Iteration: 764 | Episodes: 31000 | Median Reward: 43.29 | Avg Reward: 42.72 | Max Reward: 46.61
Iteration: 766 | Episodes: 31100 | Median Reward: 43.71 | Avg Reward: 43.14 | Max Reward: 46.61
Iteration: 769 | Episodes: 31200 | Median Reward: 43.41 | Avg Reward: 42.85 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 899          |
|    iterations           | 770          |
|    time_elapsed         | 3506         |
|    total_timesteps      | 3153920      |
| train/                  |              |
|    approx_kl            | 0.0014883769 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -263         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26.2        |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.32         |
|    value_loss           | 0.425        |
------------------------------------------
Iteration: 771 | Episodes: 31300 | Median Reward: 43.39 | Avg Reward: 42.93 | Max Reward: 46.61
Iteration: 774 | Episodes: 31400 | Median Reward: 43.38 | Avg Reward: 42.24 | Max Reward: 46.61
Iteration: 776 | Episodes: 31500 | Median Reward: 43.46 | Avg Reward: 42.75 | Max Reward: 46.61
Iteration: 779 | Episodes: 31600 | Median Reward: 43.73 | Avg Reward: 42.22 | Max Reward: 46.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.7       |
| time/                   |             |
|    fps                  | 900         |
|    iterations           | 780         |
|    time_elapsed         | 3549        |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.027759524 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.3         |
|    entropy_loss         | -264        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -25.3       |
|    n_updates            | 7790        |
|    policy_gradient_loss | -0.0231     |
|    std                  | 1.33        |
|    value_loss           | 1.41        |
-----------------------------------------
Iteration: 781 | Episodes: 31700 | Median Reward: 43.81 | Avg Reward: 42.75 | Max Reward: 46.61
Iteration: 784 | Episodes: 31800 | Median Reward: 41.73 | Avg Reward: 42.27 | Max Reward: 46.61
Iteration: 786 | Episodes: 31900 | Median Reward: 43.93 | Avg Reward: 42.78 | Max Reward: 46.61
Iteration: 789 | Episodes: 32000 | Median Reward: 43.33 | Avg Reward: 42.06 | Max Reward: 46.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.5       |
| time/                   |             |
|    fps                  | 888         |
|    iterations           | 790         |
|    time_elapsed         | 3643        |
|    total_timesteps      | 3235840     |
| train/                  |             |
|    approx_kl            | 0.051796444 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.3         |
|    entropy_loss         | -264        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -26.4       |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.0677     |
|    std                  | 1.33        |
|    value_loss           | 0.354       |
-----------------------------------------
Iteration: 791 | Episodes: 32100 | Median Reward: 43.70 | Avg Reward: 42.94 | Max Reward: 46.61
Iteration: 793 | Episodes: 32200 | Median Reward: 43.38 | Avg Reward: 42.90 | Max Reward: 46.61
Iteration: 796 | Episodes: 32300 | Median Reward: 43.89 | Avg Reward: 43.35 | Max Reward: 46.61
Iteration: 798 | Episodes: 32400 | Median Reward: 42.00 | Avg Reward: 41.97 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58          |
| time/                   |              |
|    fps                  | 874          |
|    iterations           | 800          |
|    time_elapsed         | 3746         |
|    total_timesteps      | 3276800      |
| train/                  |              |
|    approx_kl            | 0.0010655688 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -265         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.3        |
|    n_updates            | 7990         |
|    policy_gradient_loss | 6.3e-05      |
|    std                  | 1.34         |
|    value_loss           | 0.664        |
------------------------------------------
Iteration: 801 | Episodes: 32500 | Median Reward: 42.86 | Avg Reward: 42.80 | Max Reward: 46.61
Iteration: 803 | Episodes: 32600 | Median Reward: 43.72 | Avg Reward: 42.48 | Max Reward: 46.61
Iteration: 806 | Episodes: 32700 | Median Reward: 43.88 | Avg Reward: 43.19 | Max Reward: 46.61
Iteration: 808 | Episodes: 32800 | Median Reward: 42.90 | Avg Reward: 42.28 | Max Reward: 46.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57           |
| time/                   |               |
|    fps                  | 863           |
|    iterations           | 810           |
|    time_elapsed         | 3844          |
|    total_timesteps      | 3317760       |
| train/                  |               |
|    approx_kl            | 0.00017186247 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -266          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -26.5         |
|    n_updates            | 8090          |
|    policy_gradient_loss | 0.000601      |
|    std                  | 1.35          |
|    value_loss           | 0.949         |
-------------------------------------------
Iteration: 811 | Episodes: 32900 | Median Reward: 43.27 | Avg Reward: 43.17 | Max Reward: 46.61
Iteration: 813 | Episodes: 33000 | Median Reward: 43.76 | Avg Reward: 42.21 | Max Reward: 46.61
Iteration: 816 | Episodes: 33100 | Median Reward: 43.95 | Avg Reward: 43.11 | Max Reward: 46.61
Iteration: 818 | Episodes: 33200 | Median Reward: 43.73 | Avg Reward: 42.84 | Max Reward: 46.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58         |
| time/                   |             |
|    fps                  | 852         |
|    iterations           | 820         |
|    time_elapsed         | 3941        |
|    total_timesteps      | 3358720     |
| train/                  |             |
|    approx_kl            | 0.006138745 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -266        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -26         |
|    n_updates            | 8190        |
|    policy_gradient_loss | -0.01       |
|    std                  | 1.36        |
|    value_loss           | 1.26        |
-----------------------------------------
Iteration: 821 | Episodes: 33300 | Median Reward: 43.95 | Avg Reward: 42.36 | Max Reward: 46.61
Iteration: 823 | Episodes: 33400 | Median Reward: 43.93 | Avg Reward: 43.69 | Max Reward: 46.61
Iteration: 826 | Episodes: 33500 | Median Reward: 43.93 | Avg Reward: 43.26 | Max Reward: 46.61
Iteration: 828 | Episodes: 33600 | Median Reward: 43.70 | Avg Reward: 42.91 | Max Reward: 46.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.5         |
| time/                   |               |
|    fps                  | 841           |
|    iterations           | 830           |
|    time_elapsed         | 4039          |
|    total_timesteps      | 3399680       |
| train/                  |               |
|    approx_kl            | 0.00011641916 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -267          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -26.4         |
|    n_updates            | 8290          |
|    policy_gradient_loss | -0.000424     |
|    std                  | 1.37          |
|    value_loss           | 1.73          |
-------------------------------------------
Iteration: 830 | Episodes: 33700 | Median Reward: 43.81 | Avg Reward: 43.47 | Max Reward: 46.61
Iteration: 833 | Episodes: 33800 | Median Reward: 43.66 | Avg Reward: 41.97 | Max Reward: 46.61
Iteration: 835 | Episodes: 33900 | Median Reward: 41.93 | Avg Reward: 41.65 | Max Reward: 46.61
Iteration: 838 | Episodes: 34000 | Median Reward: 41.67 | Avg Reward: 42.15 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.9        |
| time/                   |              |
|    fps                  | 831          |
|    iterations           | 840          |
|    time_elapsed         | 4138         |
|    total_timesteps      | 3440640      |
| train/                  |              |
|    approx_kl            | 0.0006125977 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -267         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.6        |
|    n_updates            | 8390         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.38         |
|    value_loss           | 1.06         |
------------------------------------------
Iteration: 840 | Episodes: 34100 | Median Reward: 43.85 | Avg Reward: 42.35 | Max Reward: 46.61
Iteration: 843 | Episodes: 34200 | Median Reward: 41.08 | Avg Reward: 41.44 | Max Reward: 46.61
Iteration: 845 | Episodes: 34300 | Median Reward: 42.87 | Avg Reward: 42.43 | Max Reward: 46.61
Iteration: 848 | Episodes: 34400 | Median Reward: 43.89 | Avg Reward: 43.30 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.7        |
| time/                   |              |
|    fps                  | 829          |
|    iterations           | 850          |
|    time_elapsed         | 4197         |
|    total_timesteps      | 3481600      |
| train/                  |              |
|    approx_kl            | 0.0025586016 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -267         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -22.6        |
|    n_updates            | 8490         |
|    policy_gradient_loss | -0.00886     |
|    std                  | 1.38         |
|    value_loss           | 5.76         |
------------------------------------------
Iteration: 850 | Episodes: 34500 | Median Reward: 43.93 | Avg Reward: 42.66 | Max Reward: 46.61
Iteration: 853 | Episodes: 34600 | Median Reward: 43.88 | Avg Reward: 42.97 | Max Reward: 46.61
Iteration: 855 | Episodes: 34700 | Median Reward: 42.85 | Avg Reward: 42.05 | Max Reward: 46.61
Iteration: 858 | Episodes: 34800 | Median Reward: 43.51 | Avg Reward: 43.25 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 822          |
|    iterations           | 860          |
|    time_elapsed         | 4283         |
|    total_timesteps      | 3522560      |
| train/                  |              |
|    approx_kl            | 0.0025419183 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -269         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -26          |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.00632     |
|    std                  | 1.39         |
|    value_loss           | 0.835        |
------------------------------------------
Iteration: 860 | Episodes: 34900 | Median Reward: 42.70 | Avg Reward: 42.41 | Max Reward: 46.61
Iteration: 863 | Episodes: 35000 | Median Reward: 43.94 | Avg Reward: 43.50 | Max Reward: 46.61
Iteration: 865 | Episodes: 35100 | Median Reward: 43.93 | Avg Reward: 43.31 | Max Reward: 46.61
Iteration: 867 | Episodes: 35200 | Median Reward: 43.76 | Avg Reward: 43.05 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.3        |
| time/                   |              |
|    fps                  | 818          |
|    iterations           | 870          |
|    time_elapsed         | 4352         |
|    total_timesteps      | 3563520      |
| train/                  |              |
|    approx_kl            | 0.0061269854 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -269         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26.7        |
|    n_updates            | 8690         |
|    policy_gradient_loss | -0.0166      |
|    std                  | 1.4          |
|    value_loss           | 2.76         |
------------------------------------------
Iteration: 870 | Episodes: 35300 | Median Reward: 43.49 | Avg Reward: 42.63 | Max Reward: 46.61
Iteration: 872 | Episodes: 35400 | Median Reward: 43.96 | Avg Reward: 43.08 | Max Reward: 46.61
Iteration: 875 | Episodes: 35500 | Median Reward: 43.96 | Avg Reward: 43.54 | Max Reward: 46.61
Iteration: 877 | Episodes: 35600 | Median Reward: 42.58 | Avg Reward: 42.54 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 819          |
|    iterations           | 880          |
|    time_elapsed         | 4395         |
|    total_timesteps      | 3604480      |
| train/                  |              |
|    approx_kl            | 0.0010737001 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -270         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -26.9        |
|    n_updates            | 8790         |
|    policy_gradient_loss | -0.00343     |
|    std                  | 1.41         |
|    value_loss           | 2.55         |
------------------------------------------
Iteration: 880 | Episodes: 35700 | Median Reward: 43.45 | Avg Reward: 42.78 | Max Reward: 46.61
Iteration: 882 | Episodes: 35800 | Median Reward: 43.41 | Avg Reward: 41.93 | Max Reward: 46.61
Iteration: 885 | Episodes: 35900 | Median Reward: 43.92 | Avg Reward: 42.73 | Max Reward: 46.61
Iteration: 887 | Episodes: 36000 | Median Reward: 43.94 | Avg Reward: 42.77 | Max Reward: 46.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.6         |
| time/                   |               |
|    fps                  | 821           |
|    iterations           | 890           |
|    time_elapsed         | 4439          |
|    total_timesteps      | 3645440       |
| train/                  |               |
|    approx_kl            | 5.2927848e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -271          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.0005        |
|    loss                 | -26.6         |
|    n_updates            | 8890          |
|    policy_gradient_loss | -3.76e-05     |
|    std                  | 1.42          |
|    value_loss           | 1.15          |
-------------------------------------------
Iteration: 890 | Episodes: 36100 | Median Reward: 43.90 | Avg Reward: 43.36 | Max Reward: 46.61
Iteration: 892 | Episodes: 36200 | Median Reward: 41.71 | Avg Reward: 41.96 | Max Reward: 46.61
Iteration: 895 | Episodes: 36300 | Median Reward: 41.36 | Avg Reward: 41.92 | Max Reward: 46.61
Iteration: 897 | Episodes: 36400 | Median Reward: 43.06 | Avg Reward: 42.72 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57          |
| time/                   |              |
|    fps                  | 822          |
|    iterations           | 900          |
|    time_elapsed         | 4482         |
|    total_timesteps      | 3686400      |
| train/                  |              |
|    approx_kl            | 0.0021876853 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -271         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -25.5        |
|    n_updates            | 8990         |
|    policy_gradient_loss | -0.00596     |
|    std                  | 1.43         |
|    value_loss           | 1.49         |
------------------------------------------
Iteration: 900 | Episodes: 36500 | Median Reward: 43.69 | Avg Reward: 42.95 | Max Reward: 46.61
Iteration: 902 | Episodes: 36600 | Median Reward: 43.91 | Avg Reward: 42.91 | Max Reward: 46.61
Iteration: 904 | Episodes: 36700 | Median Reward: 43.95 | Avg Reward: 43.26 | Max Reward: 46.61
Iteration: 907 | Episodes: 36800 | Median Reward: 43.89 | Avg Reward: 42.76 | Max Reward: 46.61
Iteration: 909 | Episodes: 36900 | Median Reward: 43.89 | Avg Reward: 42.76 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 823          |
|    iterations           | 910          |
|    time_elapsed         | 4526         |
|    total_timesteps      | 3727360      |
| train/                  |              |
|    approx_kl            | 3.414818e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -271         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0005       |
|    loss                 | -26.4        |
|    n_updates            | 9090         |
|    policy_gradient_loss | 4.11e-06     |
|    std                  | 1.44         |
|    value_loss           | 0.969        |
------------------------------------------
Iteration: 912 | Episodes: 37000 | Median Reward: 43.05 | Avg Reward: 42.20 | Max Reward: 46.61
Iteration: 914 | Episodes: 37100 | Median Reward: 42.94 | Avg Reward: 42.23 | Max Reward: 46.61
Iteration: 917 | Episodes: 37200 | Median Reward: 43.88 | Avg Reward: 42.73 | Max Reward: 46.61
Iteration: 919 | Episodes: 37300 | Median Reward: 43.87 | Avg Reward: 43.55 | Max Reward: 46.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.5       |
| time/                   |             |
|    fps                  | 819         |
|    iterations           | 920         |
|    time_elapsed         | 4598        |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.011457274 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -272        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -26.3       |
|    n_updates            | 9190        |
|    policy_gradient_loss | -0.00552    |
|    std                  | 1.44        |
|    value_loss           | 0.545       |
-----------------------------------------
Iteration: 922 | Episodes: 37400 | Median Reward: 42.42 | Avg Reward: 42.38 | Max Reward: 46.61
Iteration: 924 | Episodes: 37500 | Median Reward: 43.66 | Avg Reward: 42.65 | Max Reward: 46.61
Iteration: 927 | Episodes: 37600 | Median Reward: 43.45 | Avg Reward: 42.35 | Max Reward: 46.61
Iteration: 929 | Episodes: 37700 | Median Reward: 42.17 | Avg Reward: 42.42 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.3        |
| time/                   |              |
|    fps                  | 808          |
|    iterations           | 930          |
|    time_elapsed         | 4711         |
|    total_timesteps      | 3809280      |
| train/                  |              |
|    approx_kl            | 0.0009301699 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -273         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -22.2        |
|    n_updates            | 9290         |
|    policy_gradient_loss | -0.00318     |
|    std                  | 1.45         |
|    value_loss           | 5.92         |
------------------------------------------
Iteration: 932 | Episodes: 37800 | Median Reward: 43.69 | Avg Reward: 42.85 | Max Reward: 46.61
Iteration: 934 | Episodes: 37900 | Median Reward: 43.83 | Avg Reward: 43.04 | Max Reward: 46.61
Iteration: 937 | Episodes: 38000 | Median Reward: 43.89 | Avg Reward: 43.51 | Max Reward: 46.61
Iteration: 939 | Episodes: 38100 | Median Reward: 43.82 | Avg Reward: 43.20 | Max Reward: 46.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.9         |
| time/                   |               |
|    fps                  | 808           |
|    iterations           | 940           |
|    time_elapsed         | 4762          |
|    total_timesteps      | 3850240       |
| train/                  |               |
|    approx_kl            | 0.00028878928 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -273          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.0005        |
|    loss                 | -27.2         |
|    n_updates            | 9390          |
|    policy_gradient_loss | -0.000349     |
|    std                  | 1.46          |
|    value_loss           | 0.63          |
-------------------------------------------
Iteration: 941 | Episodes: 38200 | Median Reward: 43.87 | Avg Reward: 43.21 | Max Reward: 46.61
Iteration: 944 | Episodes: 38300 | Median Reward: 43.89 | Avg Reward: 42.25 | Max Reward: 46.61
Iteration: 946 | Episodes: 38400 | Median Reward: 42.37 | Avg Reward: 42.49 | Max Reward: 46.61
Iteration: 949 | Episodes: 38500 | Median Reward: 43.54 | Avg Reward: 42.68 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.2        |
| time/                   |              |
|    fps                  | 809          |
|    iterations           | 950          |
|    time_elapsed         | 4806         |
|    total_timesteps      | 3891200      |
| train/                  |              |
|    approx_kl            | 0.0013676859 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -274         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27.2        |
|    n_updates            | 9490         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.46         |
|    value_loss           | 0.953        |
------------------------------------------
Iteration: 951 | Episodes: 38600 | Median Reward: 43.45 | Avg Reward: 42.35 | Max Reward: 46.61
Iteration: 954 | Episodes: 38700 | Median Reward: 43.71 | Avg Reward: 43.14 | Max Reward: 46.61
Iteration: 956 | Episodes: 38800 | Median Reward: 43.38 | Avg Reward: 42.52 | Max Reward: 46.61
Iteration: 959 | Episodes: 38900 | Median Reward: 43.46 | Avg Reward: 43.04 | Max Reward: 46.61
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.4        |
| time/                   |              |
|    fps                  | 810          |
|    iterations           | 960          |
|    time_elapsed         | 4850         |
|    total_timesteps      | 3932160      |
| train/                  |              |
|    approx_kl            | 0.0019647495 |
|    clip_fraction        | 0            |
|    clip_range           | 0.3          |
|    entropy_loss         | -274         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0005       |
|    loss                 | -27.1        |
|    n_updates            | 9590         |
|    policy_gradient_loss | -0.00193     |
|    std                  | 1.47         |
|    value_loss           | 0.663        |
------------------------------------------
Iteration: 961 | Episodes: 39000 | Median Reward: 43.44 | Avg Reward: 42.49 | Max Reward: 46.61
Iteration: 964 | Episodes: 39100 | Median Reward: 43.89 | Avg Reward: 43.18 | Max Reward: 46.61
Iteration: 966 | Episodes: 39200 | Median Reward: 43.94 | Avg Reward: 43.01 | Max Reward: 46.61
Iteration: 969 | Episodes: 39300 | Median Reward: 43.73 | Avg Reward: 43.62 | Max Reward: 46.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.4         |
| time/                   |               |
|    fps                  | 811           |
|    iterations           | 970           |
|    time_elapsed         | 4894          |
|    total_timesteps      | 3973120       |
| train/                  |               |
|    approx_kl            | 0.00028335498 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -275          |
|    explained_variance   | 1             |
|    learning_rate        | 0.0005        |
|    loss                 | -26.4         |
|    n_updates            | 9690          |
|    policy_gradient_loss | -0.000308     |
|    std                  | 1.48          |
|    value_loss           | 0.61          |
-------------------------------------------
Iteration: 971 | Episodes: 39400 | Median Reward: 43.10 | Avg Reward: 42.55 | Max Reward: 46.61
Iteration: 973 | Episodes: 39500 | Median Reward: 43.93 | Avg Reward: 42.94 | Max Reward: 46.61
Iteration: 976 | Episodes: 39600 | Median Reward: 43.39 | Avg Reward: 42.66 | Max Reward: 46.61
Iteration: 978 | Episodes: 39700 | Median Reward: 43.91 | Avg Reward: 42.81 | Max Reward: 46.61
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57           |
| time/                   |               |
|    fps                  | 812           |
|    iterations           | 980           |
|    time_elapsed         | 4937          |
|    total_timesteps      | 4014080       |
| train/                  |               |
|    approx_kl            | 0.00027750325 |
|    clip_fraction        | 0             |
|    clip_range           | 0.3           |
|    entropy_loss         | -276          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.0005        |
|    loss                 | -27.5         |
|    n_updates            | 9790          |
|    policy_gradient_loss | -0.000175     |
|    std                  | 1.49          |
|    value_loss           | 0.517         |
-------------------------------------------
Iteration: 981 | Episodes: 39800 | Median Reward: 43.57 | Avg Reward: 42.83 | Max Reward: 46.61
Iteration: 983 | Episodes: 39900 | Median Reward: 43.90 | Avg Reward: 42.93 | Max Reward: 46.61
Iteration: 986 | Episodes: 40000 | Median Reward: 43.06 | Avg Reward: 42.25 | Max Reward: 46.61
Iteration: 988 | Episodes: 40100 | Median Reward: 42.87 | Avg Reward: 42.16 | Max Reward: 46.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.5       |
| time/                   |             |
|    fps                  | 814         |
|    iterations           | 990         |
|    time_elapsed         | 4980        |
|    total_timesteps      | 4055040     |
| train/                  |             |
|    approx_kl            | 0.012486054 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -277        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0005      |
|    loss                 | -27.3       |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.0197     |
|    std                  | 1.5         |
|    value_loss           | 0.855       |
-----------------------------------------
Iteration: 991 | Episodes: 40200 | Median Reward: 43.41 | Avg Reward: 41.98 | Max Reward: 46.61
Iteration: 993 | Episodes: 40300 | Median Reward: 43.70 | Avg Reward: 42.91 | Max Reward: 46.61
Iteration: 996 | Episodes: 40400 | Median Reward: 43.46 | Avg Reward: 42.84 | Max Reward: 46.61
Iteration: 998 | Episodes: 40500 | Median Reward: 43.92 | Avg Reward: 43.04 | Max Reward: 46.61
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -56.3       |
| time/                   |             |
|    fps                  | 815         |
|    iterations           | 1000        |
|    time_elapsed         | 5024        |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.007667826 |
|    clip_fraction        | 0           |
|    clip_range           | 0.3         |
|    entropy_loss         | -278        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0005      |
|    loss                 | -27.7       |
|    n_updates            | 9990        |
|    policy_gradient_loss | -0.00296    |
|    std                  | 1.5         |
|    value_loss           | 0.41        |
-----------------------------------------
Training End | Episodes: 40552 | Median Reward: 43.93 | Avg Reward: 43.90 | Max Reward: 46.61
Plot saved as fig_code1_1_residual_e.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> exit
exit

Script done on 2024-10-24 20:52:12-04:00 [COMMAND_EXIT_CODE="0"]
