Script started on 2024-10-22 21:00:43-04:00 [TERM="screen.xterm-256color" TTY="/dev/pts/23" COLUMNS="282" LINES="75"]
[4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> cona[Kda activate mujoco_test
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> python code1_4.pt[Ky
GPU 4: Using cuda device
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
INFO:root:Attempting to load Mujoco model from: /home/easgrad/dgusain/Bot_hand/bot_hand.xml
INFO:root:Mujoco model loaded successfully.
Using cuda device
Iteration: 1 | Episodes: 100 | Median Reward: 23.48 | Max Reward: 35.79
Iteration: 2 | Episodes: 200 | Median Reward: 22.40 | Max Reward: 35.79
Iteration: 3 | Episodes: 300 | Median Reward: 17.77 | Max Reward: 35.79
Iteration: 4 | Episodes: 400 | Median Reward: 24.11 | Max Reward: 35.79
Iteration: 6 | Episodes: 500 | Median Reward: 20.44 | Max Reward: 35.79
Iteration: 7 | Episodes: 600 | Median Reward: 20.44 | Max Reward: 35.79
Iteration: 8 | Episodes: 700 | Median Reward: 17.16 | Max Reward: 35.79
Iteration: 9 | Episodes: 800 | Median Reward: 13.82 | Max Reward: 35.79
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -86.3      |
| time/                   |            |
|    fps                  | 152        |
|    iterations           | 10         |
|    time_elapsed         | 536        |
|    total_timesteps      | 81920      |
| train/                  |            |
|    approx_kl            | 0.03629289 |
|    clip_fraction        | 0.00322    |
|    clip_range           | 0.4        |
|    entropy_loss         | -60.1      |
|    explained_variance   | -0.0145    |
|    learning_rate        | 0.000707   |
|    loss                 | 81.6       |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.00403    |
|    std                  | 1.01       |
|    value_loss           | 173        |
----------------------------------------
Iteration: 11 | Episodes: 900 | Median Reward: 6.57 | Max Reward: 35.79
Iteration: 12 | Episodes: 1000 | Median Reward: 25.01 | Max Reward: 35.79
Iteration: 13 | Episodes: 1100 | Median Reward: 12.66 | Max Reward: 35.79
Iteration: 14 | Episodes: 1200 | Median Reward: 22.19 | Max Reward: 35.79
Iteration: 16 | Episodes: 1300 | Median Reward: 22.19 | Max Reward: 35.79
Iteration: 17 | Episodes: 1400 | Median Reward: 11.38 | Max Reward: 35.79
Iteration: 18 | Episodes: 1500 | Median Reward: 10.97 | Max Reward: 35.79
Iteration: 19 | Episodes: 1600 | Median Reward: 4.09 | Max Reward: 35.79
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -97.3      |
| time/                   |            |
|    fps                  | 151        |
|    iterations           | 20         |
|    time_elapsed         | 1083       |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.02788977 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -64.6      |
|    explained_variance   | -0.00538   |
|    learning_rate        | 0.000707   |
|    loss                 | 68.3       |
|    n_updates            | 190        |
|    policy_gradient_loss | 0.00114    |
|    std                  | 1.02       |
|    value_loss           | 146        |
----------------------------------------
Iteration: 20 | Episodes: 1700 | Median Reward: 20.90 | Max Reward: 35.79
Iteration: 22 | Episodes: 1800 | Median Reward: 22.11 | Max Reward: 35.79
Iteration: 23 | Episodes: 1900 | Median Reward: 22.13 | Max Reward: 35.79
Iteration: 24 | Episodes: 2000 | Median Reward: 16.16 | Max Reward: 35.79
Iteration: 25 | Episodes: 2100 | Median Reward: 5.27 | Max Reward: 35.79
Iteration: 27 | Episodes: 2200 | Median Reward: 6.60 | Max Reward: 35.79
Iteration: 28 | Episodes: 2300 | Median Reward: 19.24 | Max Reward: 35.79
Iteration: 29 | Episodes: 2400 | Median Reward: 24.63 | Max Reward: 35.79
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -86.5      |
| time/                   |            |
|    fps                  | 151        |
|    iterations           | 30         |
|    time_elapsed         | 1625       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.17000511 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.4        |
|    entropy_loss         | -61.8      |
|    explained_variance   | -0.000992  |
|    learning_rate        | 0.000707   |
|    loss                 | 108        |
|    n_updates            | 290        |
|    policy_gradient_loss | 0.0322     |
|    std                  | 1.04       |
|    value_loss           | 224        |
----------------------------------------
Iteration: 30 | Episodes: 2500 | Median Reward: 9.57 | Max Reward: 35.79
Iteration: 32 | Episodes: 2600 | Median Reward: 14.68 | Max Reward: 35.79
Iteration: 33 | Episodes: 2700 | Median Reward: 18.95 | Max Reward: 35.79
Iteration: 34 | Episodes: 2800 | Median Reward: 10.77 | Max Reward: 35.79
Iteration: 35 | Episodes: 2900 | Median Reward: 3.62 | Max Reward: 35.79
Iteration: 36 | Episodes: 3000 | Median Reward: 13.77 | Max Reward: 37.36
Iteration: 38 | Episodes: 3100 | Median Reward: 18.83 | Max Reward: 37.36
Iteration: 39 | Episodes: 3200 | Median Reward: 21.22 | Max Reward: 37.36
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -81.5       |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 40          |
|    time_elapsed         | 2161        |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.008949138 |
|    clip_fraction        | 0.00244     |
|    clip_range           | 0.4         |
|    entropy_loss         | -79.6       |
|    explained_variance   | -0.00039    |
|    learning_rate        | 0.000707    |
|    loss                 | 106         |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.000879    |
|    std                  | 1.05        |
|    value_loss           | 221         |
-----------------------------------------
Iteration: 40 | Episodes: 3300 | Median Reward: 17.02 | Max Reward: 37.36
Iteration: 41 | Episodes: 3400 | Median Reward: 18.25 | Max Reward: 37.93
Iteration: 43 | Episodes: 3500 | Median Reward: 25.25 | Max Reward: 37.93
Iteration: 44 | Episodes: 3600 | Median Reward: 15.12 | Max Reward: 37.93
Iteration: 45 | Episodes: 3700 | Median Reward: 19.43 | Max Reward: 37.93
Iteration: 46 | Episodes: 3800 | Median Reward: 18.40 | Max Reward: 37.93
Iteration: 48 | Episodes: 3900 | Median Reward: 22.08 | Max Reward: 37.93
Iteration: 49 | Episodes: 4000 | Median Reward: 35.55 | Max Reward: 37.93
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -80.5     |
| time/                   |           |
|    fps                  | 151       |
|    iterations           | 50        |
|    time_elapsed         | 2701      |
|    total_timesteps      | 409600    |
| train/                  |           |
|    approx_kl            | 0.6359198 |
|    clip_fraction        | 0.142     |
|    clip_range           | 0.4       |
|    entropy_loss         | -80.4     |
|    explained_variance   | -0.000115 |
|    learning_rate        | 0.000707  |
|    loss                 | 169       |
|    n_updates            | 490       |
|    policy_gradient_loss | -0.0293   |
|    std                  | 1.08      |
|    value_loss           | 345       |
---------------------------------------
Iteration: 50 | Episodes: 4100 | Median Reward: 17.82 | Max Reward: 37.93
Iteration: 51 | Episodes: 4200 | Median Reward: 19.81 | Max Reward: 37.93
Iteration: 53 | Episodes: 4300 | Median Reward: 20.31 | Max Reward: 37.93
Iteration: 54 | Episodes: 4400 | Median Reward: 23.12 | Max Reward: 37.93
Iteration: 55 | Episodes: 4500 | Median Reward: 9.89 | Max Reward: 37.93
Iteration: 56 | Episodes: 4600 | Median Reward: 28.03 | Max Reward: 37.93
Iteration: 57 | Episodes: 4700 | Median Reward: 22.00 | Max Reward: 37.93
Iteration: 59 | Episodes: 4800 | Median Reward: 24.13 | Max Reward: 38.30
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -72.5      |
| time/                   |            |
|    fps                  | 151        |
|    iterations           | 60         |
|    time_elapsed         | 3243       |
|    total_timesteps      | 491520     |
| train/                  |            |
|    approx_kl            | 0.46023566 |
|    clip_fraction        | 0.0625     |
|    clip_range           | 0.4        |
|    entropy_loss         | -82.1      |
|    explained_variance   | -5.88e-05  |
|    learning_rate        | 0.000707   |
|    loss                 | 99.8       |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0299    |
|    std                  | 1.12       |
|    value_loss           | 208        |
----------------------------------------
Iteration: 60 | Episodes: 4900 | Median Reward: 24.13 | Max Reward: 38.30
Iteration: 61 | Episodes: 5000 | Median Reward: 19.25 | Max Reward: 38.30
Iteration: 62 | Episodes: 5100 | Median Reward: 19.86 | Max Reward: 38.30
Iteration: 64 | Episodes: 5200 | Median Reward: 16.74 | Max Reward: 38.30
Iteration: 65 | Episodes: 5300 | Median Reward: 9.31 | Max Reward: 38.30
Iteration: 66 | Episodes: 5400 | Median Reward: 7.21 | Max Reward: 38.30
Iteration: 67 | Episodes: 5500 | Median Reward: 27.70 | Max Reward: 38.30
Iteration: 69 | Episodes: 5600 | Median Reward: 17.02 | Max Reward: 38.30
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -83.7      |
| time/                   |            |
|    fps                  | 151        |
|    iterations           | 70         |
|    time_elapsed         | 3783       |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.03322287 |
|    clip_fraction        | 0.013      |
|    clip_range           | 0.4        |
|    entropy_loss         | -83.8      |
|    explained_variance   | -3.83e-05  |
|    learning_rate        | 0.000707   |
|    loss                 | 90.9       |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0128    |
|    std                  | 1.16       |
|    value_loss           | 190        |
----------------------------------------
Iteration: 70 | Episodes: 5700 | Median Reward: 22.69 | Max Reward: 38.30
Iteration: 71 | Episodes: 5800 | Median Reward: 27.11 | Max Reward: 38.30
Iteration: 72 | Episodes: 5900 | Median Reward: 29.41 | Max Reward: 38.30
Iteration: 73 | Episodes: 6000 | Median Reward: 21.46 | Max Reward: 38.30
Iteration: 75 | Episodes: 6100 | Median Reward: 5.41 | Max Reward: 38.30
Iteration: 76 | Episodes: 6200 | Median Reward: 0.86 | Max Reward: 38.30
Iteration: 77 | Episodes: 6300 | Median Reward: 15.39 | Max Reward: 38.30
Iteration: 78 | Episodes: 6400 | Median Reward: 17.74 | Max Reward: 38.30
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -81.9     |
| time/                   |           |
|    fps                  | 151       |
|    iterations           | 80        |
|    time_elapsed         | 4324      |
|    total_timesteps      | 655360    |
| train/                  |           |
|    approx_kl            | 1.8723879 |
|    clip_fraction        | 0.183     |
|    clip_range           | 0.4       |
|    entropy_loss         | -81       |
|    explained_variance   | -1.66e-05 |
|    learning_rate        | 0.000707  |
|    loss                 | 126       |
|    n_updates            | 790       |
|    policy_gradient_loss | -0.0368   |
|    std                  | 1.23      |
|    value_loss           | 261       |
---------------------------------------
Iteration: 80 | Episodes: 6500 | Median Reward: 18.82 | Max Reward: 38.30
Iteration: 81 | Episodes: 6600 | Median Reward: 20.24 | Max Reward: 38.30
Iteration: 82 | Episodes: 6700 | Median Reward: 22.64 | Max Reward: 38.30
Iteration: 83 | Episodes: 6800 | Median Reward: 17.75 | Max Reward: 38.30
Iteration: 85 | Episodes: 6900 | Median Reward: 15.23 | Max Reward: 38.30
Iteration: 86 | Episodes: 7000 | Median Reward: 19.18 | Max Reward: 38.30
Iteration: 87 | Episodes: 7100 | Median Reward: 17.70 | Max Reward: 38.30
Iteration: 88 | Episodes: 7200 | Median Reward: 24.50 | Max Reward: 38.30
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -80.2       |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 90          |
|    time_elapsed         | 4869        |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.018420167 |
|    clip_fraction        | 0.0015      |
|    clip_range           | 0.4         |
|    entropy_loss         | -84.5       |
|    explained_variance   | -7.03e-06   |
|    learning_rate        | 0.000707    |
|    loss                 | 169         |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00257    |
|    std                  | 1.28        |
|    value_loss           | 346         |
-----------------------------------------
Iteration: 90 | Episodes: 7300 | Median Reward: 26.34 | Max Reward: 38.30
Iteration: 91 | Episodes: 7400 | Median Reward: 29.92 | Max Reward: 38.92
Iteration: 92 | Episodes: 7500 | Median Reward: 22.72 | Max Reward: 38.92
Iteration: 93 | Episodes: 7600 | Median Reward: 12.36 | Max Reward: 38.92
Iteration: 94 | Episodes: 7700 | Median Reward: 17.27 | Max Reward: 38.92
Iteration: 96 | Episodes: 7800 | Median Reward: 25.44 | Max Reward: 38.92
Iteration: 97 | Episodes: 7900 | Median Reward: 23.18 | Max Reward: 38.92
Iteration: 98 | Episodes: 8000 | Median Reward: 16.80 | Max Reward: 38.92
Iteration: 99 | Episodes: 8100 | Median Reward: 5.49 | Max Reward: 38.92
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -94.2       |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 100         |
|    time_elapsed         | 5404        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.020463552 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -87         |
|    explained_variance   | -5.36e-06   |
|    learning_rate        | 0.000707    |
|    loss                 | 110         |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.0107     |
|    std                  | 1.31        |
|    value_loss           | 229         |
-----------------------------------------
Iteration: 101 | Episodes: 8200 | Median Reward: 6.12 | Max Reward: 38.92
Iteration: 102 | Episodes: 8300 | Median Reward: 29.70 | Max Reward: 38.92
Iteration: 103 | Episodes: 8400 | Median Reward: 23.04 | Max Reward: 38.92
Iteration: 104 | Episodes: 8500 | Median Reward: 21.77 | Max Reward: 38.92
Iteration: 106 | Episodes: 8600 | Median Reward: 19.86 | Max Reward: 38.92
Iteration: 107 | Episodes: 8700 | Median Reward: 30.35 | Max Reward: 38.92
Iteration: 108 | Episodes: 8800 | Median Reward: 32.24 | Max Reward: 39.49
Iteration: 109 | Episodes: 8900 | Median Reward: 19.63 | Max Reward: 39.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -84.2        |
| time/                   |              |
|    fps                  | 151          |
|    iterations           | 110          |
|    time_elapsed         | 5934         |
|    total_timesteps      | 901120       |
| train/                  |              |
|    approx_kl            | 0.0028732694 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -89.1        |
|    explained_variance   | -3.22e-06    |
|    learning_rate        | 0.000707     |
|    loss                 | 171          |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00302     |
|    std                  | 1.35         |
|    value_loss           | 352          |
------------------------------------------
Iteration: 110 | Episodes: 9000 | Median Reward: 15.66 | Max Reward: 39.49
Iteration: 112 | Episodes: 9100 | Median Reward: 21.67 | Max Reward: 39.49
Iteration: 113 | Episodes: 9200 | Median Reward: 24.51 | Max Reward: 39.49
Iteration: 114 | Episodes: 9300 | Median Reward: 25.16 | Max Reward: 39.49
Iteration: 115 | Episodes: 9400 | Median Reward: 15.67 | Max Reward: 39.49
Iteration: 117 | Episodes: 9500 | Median Reward: 5.86 | Max Reward: 39.49
Iteration: 118 | Episodes: 9600 | Median Reward: 21.59 | Max Reward: 39.49
Iteration: 119 | Episodes: 9700 | Median Reward: 26.77 | Max Reward: 39.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80.3         |
| time/                   |               |
|    fps                  | 151           |
|    iterations           | 120           |
|    time_elapsed         | 6471          |
|    total_timesteps      | 983040        |
| train/                  |               |
|    approx_kl            | 0.00014888133 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -89.6         |
|    explained_variance   | -3.22e-06     |
|    learning_rate        | 0.000707      |
|    loss                 | 145           |
|    n_updates            | 1190          |
|    policy_gradient_loss | -1.94e-05     |
|    std                  | 1.36          |
|    value_loss           | 299           |
-------------------------------------------
Iteration: 120 | Episodes: 9800 | Median Reward: 19.51 | Max Reward: 39.49
Iteration: 122 | Episodes: 9900 | Median Reward: 28.32 | Max Reward: 39.49
Iteration: 123 | Episodes: 10000 | Median Reward: 10.61 | Max Reward: 39.49
Iteration: 124 | Episodes: 10100 | Median Reward: 7.11 | Max Reward: 39.49
Iteration: 125 | Episodes: 10200 | Median Reward: 17.60 | Max Reward: 39.49
Iteration: 126 | Episodes: 10300 | Median Reward: 17.46 | Max Reward: 39.49
Iteration: 128 | Episodes: 10400 | Median Reward: 30.33 | Max Reward: 39.49
Iteration: 129 | Episodes: 10500 | Median Reward: 17.70 | Max Reward: 39.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83.1        |
| time/                   |              |
|    fps                  | 152          |
|    iterations           | 130          |
|    time_elapsed         | 6966         |
|    total_timesteps      | 1064960      |
| train/                  |              |
|    approx_kl            | 0.0053714244 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -90.6        |
|    explained_variance   | -3.22e-06    |
|    learning_rate        | 0.000707     |
|    loss                 | 129          |
|    n_updates            | 1290         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.38         |
|    value_loss           | 266          |
------------------------------------------
Iteration: 130 | Episodes: 10600 | Median Reward: 11.50 | Max Reward: 39.49
Iteration: 131 | Episodes: 10700 | Median Reward: 11.50 | Max Reward: 39.49
Iteration: 133 | Episodes: 10800 | Median Reward: 6.35 | Max Reward: 39.49
Iteration: 134 | Episodes: 10900 | Median Reward: 6.35 | Max Reward: 39.49
Iteration: 135 | Episodes: 11000 | Median Reward: 22.77 | Max Reward: 39.49
Iteration: 136 | Episodes: 11100 | Median Reward: 25.90 | Max Reward: 39.49
Iteration: 138 | Episodes: 11200 | Median Reward: 21.26 | Max Reward: 39.49
Iteration: 139 | Episodes: 11300 | Median Reward: 18.06 | Max Reward: 41.34
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -68.8      |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 140        |
|    time_elapsed         | 7461       |
|    total_timesteps      | 1146880    |
| train/                  |            |
|    approx_kl            | 0.00114836 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -91.5      |
|    explained_variance   | -2.74e-06  |
|    learning_rate        | 0.000707   |
|    loss                 | 106        |
|    n_updates            | 1390       |
|    policy_gradient_loss | 0.00014    |
|    std                  | 1.4        |
|    value_loss           | 220        |
----------------------------------------
Iteration: 140 | Episodes: 11400 | Median Reward: 28.88 | Max Reward: 41.34
Iteration: 141 | Episodes: 11500 | Median Reward: 16.43 | Max Reward: 41.34
Iteration: 143 | Episodes: 11600 | Median Reward: 25.61 | Max Reward: 41.34
Iteration: 144 | Episodes: 11700 | Median Reward: 22.55 | Max Reward: 41.34
Iteration: 145 | Episodes: 11800 | Median Reward: 23.20 | Max Reward: 41.34
Iteration: 146 | Episodes: 11900 | Median Reward: 31.63 | Max Reward: 45.99
Iteration: 147 | Episodes: 12000 | Median Reward: 24.81 | Max Reward: 45.99
Iteration: 149 | Episodes: 12100 | Median Reward: 24.81 | Max Reward: 45.99
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -73.4        |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 150          |
|    time_elapsed         | 7966         |
|    total_timesteps      | 1228800      |
| train/                  |              |
|    approx_kl            | 0.0010856532 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -92.2        |
|    explained_variance   | -2.03e-06    |
|    learning_rate        | 0.000707     |
|    loss                 | 146          |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.000632    |
|    std                  | 1.44         |
|    value_loss           | 300          |
------------------------------------------
Iteration: 150 | Episodes: 12200 | Median Reward: 26.45 | Max Reward: 45.99
Iteration: 151 | Episodes: 12300 | Median Reward: 29.57 | Max Reward: 45.99
Iteration: 152 | Episodes: 12400 | Median Reward: 27.12 | Max Reward: 45.99
Iteration: 154 | Episodes: 12500 | Median Reward: 16.98 | Max Reward: 45.99
Iteration: 155 | Episodes: 12600 | Median Reward: 17.91 | Max Reward: 45.99
Iteration: 156 | Episodes: 12700 | Median Reward: 27.85 | Max Reward: 45.99
Iteration: 157 | Episodes: 12800 | Median Reward: 21.87 | Max Reward: 45.99
Iteration: 159 | Episodes: 12900 | Median Reward: 18.33 | Max Reward: 45.99
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -93.3        |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 160          |
|    time_elapsed         | 8466         |
|    total_timesteps      | 1310720      |
| train/                  |              |
|    approx_kl            | 0.0065943887 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -91.9        |
|    explained_variance   | -1.91e-06    |
|    learning_rate        | 0.000707     |
|    loss                 | 112          |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.00308     |
|    std                  | 1.46         |
|    value_loss           | 234          |
------------------------------------------
Iteration: 160 | Episodes: 13000 | Median Reward: 14.82 | Max Reward: 45.99
Iteration: 161 | Episodes: 13100 | Median Reward: 14.82 | Max Reward: 45.99
Iteration: 162 | Episodes: 13200 | Median Reward: 22.00 | Max Reward: 45.99
Iteration: 163 | Episodes: 13300 | Median Reward: 15.50 | Max Reward: 45.99
Iteration: 165 | Episodes: 13400 | Median Reward: 27.38 | Max Reward: 45.99
Iteration: 166 | Episodes: 13500 | Median Reward: 31.11 | Max Reward: 45.99
Iteration: 167 | Episodes: 13600 | Median Reward: 31.30 | Max Reward: 45.99
Iteration: 168 | Episodes: 13700 | Median Reward: 18.67 | Max Reward: 45.99
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.1         |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 170           |
|    time_elapsed         | 8977          |
|    total_timesteps      | 1392640       |
| train/                  |               |
|    approx_kl            | 0.00022080768 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -92.9         |
|    explained_variance   | -1.91e-06     |
|    learning_rate        | 0.000707      |
|    loss                 | 92            |
|    n_updates            | 1690          |
|    policy_gradient_loss | -0.000582     |
|    std                  | 1.49          |
|    value_loss           | 193           |
-------------------------------------------
Iteration: 170 | Episodes: 13800 | Median Reward: 18.67 | Max Reward: 45.99
Iteration: 171 | Episodes: 13900 | Median Reward: 14.90 | Max Reward: 45.99
Iteration: 172 | Episodes: 14000 | Median Reward: 23.34 | Max Reward: 45.99
Iteration: 173 | Episodes: 14100 | Median Reward: 23.07 | Max Reward: 45.99
Iteration: 175 | Episodes: 14200 | Median Reward: 30.39 | Max Reward: 45.99
Iteration: 176 | Episodes: 14300 | Median Reward: 29.69 | Max Reward: 45.99
Iteration: 177 | Episodes: 14400 | Median Reward: 15.72 | Max Reward: 45.99
Iteration: 178 | Episodes: 14500 | Median Reward: 23.40 | Max Reward: 45.99
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71          |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 180          |
|    time_elapsed         | 9484         |
|    total_timesteps      | 1474560      |
| train/                  |              |
|    approx_kl            | 0.0016028468 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -93.6        |
|    explained_variance   | -1.43e-06    |
|    learning_rate        | 0.000707     |
|    loss                 | 146          |
|    n_updates            | 1790         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 1.52         |
|    value_loss           | 302          |
------------------------------------------
Iteration: 180 | Episodes: 14600 | Median Reward: 29.07 | Max Reward: 45.99
Iteration: 181 | Episodes: 14700 | Median Reward: 21.62 | Max Reward: 45.99
Iteration: 182 | Episodes: 14800 | Median Reward: 22.25 | Max Reward: 45.99
Iteration: 183 | Episodes: 14900 | Median Reward: 25.31 | Max Reward: 45.99
Iteration: 184 | Episodes: 15000 | Median Reward: 30.31 | Max Reward: 45.99
Iteration: 186 | Episodes: 15100 | Median Reward: 26.45 | Max Reward: 45.99
Iteration: 187 | Episodes: 15200 | Median Reward: 28.80 | Max Reward: 45.99
Iteration: 188 | Episodes: 15300 | Median Reward: 23.47 | Max Reward: 45.99
Iteration: 189 | Episodes: 15400 | Median Reward: 22.85 | Max Reward: 45.99
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -79.1       |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 190         |
|    time_elapsed         | 9996        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.073263414 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.4         |
|    entropy_loss         | -93.4       |
|    explained_variance   | -1.43e-06   |
|    learning_rate        | 0.000707    |
|    loss                 | 124         |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.0431     |
|    std                  | 1.54        |
|    value_loss           | 258         |
-----------------------------------------
Iteration: 191 | Episodes: 15500 | Median Reward: 13.77 | Max Reward: 45.99
Iteration: 192 | Episodes: 15600 | Median Reward: 17.51 | Max Reward: 45.99
Iteration: 193 | Episodes: 15700 | Median Reward: 17.73 | Max Reward: 45.99
Iteration: 194 | Episodes: 15800 | Median Reward: 34.77 | Max Reward: 45.99
Iteration: 196 | Episodes: 15900 | Median Reward: 31.69 | Max Reward: 45.99
Iteration: 197 | Episodes: 16000 | Median Reward: 25.13 | Max Reward: 45.99
Iteration: 198 | Episodes: 16100 | Median Reward: 23.40 | Max Reward: 45.99
Iteration: 199 | Episodes: 16200 | Median Reward: 26.17 | Max Reward: 45.99
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.5       |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 200         |
|    time_elapsed         | 10508       |
|    total_timesteps      | 1638400     |
| train/                  |             |
|    approx_kl            | 0.007767453 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -94.6       |
|    explained_variance   | -1.19e-06   |
|    learning_rate        | 0.000707    |
|    loss                 | 144         |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00246    |
|    std                  | 1.58        |
|    value_loss           | 298         |
-----------------------------------------
Iteration: 200 | Episodes: 16300 | Median Reward: 15.35 | Max Reward: 45.99
Iteration: 202 | Episodes: 16400 | Median Reward: 23.54 | Max Reward: 45.99
Iteration: 203 | Episodes: 16500 | Median Reward: 23.96 | Max Reward: 45.99
Iteration: 204 | Episodes: 16600 | Median Reward: 13.57 | Max Reward: 45.99
Iteration: 205 | Episodes: 16700 | Median Reward: 28.45 | Max Reward: 45.99
Iteration: 207 | Episodes: 16800 | Median Reward: 24.40 | Max Reward: 45.99
Iteration: 208 | Episodes: 16900 | Median Reward: 22.16 | Max Reward: 45.99
Iteration: 209 | Episodes: 17000 | Median Reward: 13.92 | Max Reward: 45.99
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.8         |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 210           |
|    time_elapsed         | 11017         |
|    total_timesteps      | 1720320       |
| train/                  |               |
|    approx_kl            | 0.00075864326 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -95.4         |
|    explained_variance   | -1.07e-06     |
|    learning_rate        | 0.000707      |
|    loss                 | 118           |
|    n_updates            | 2090          |
|    policy_gradient_loss | 0.000347      |
|    std                  | 1.63          |
|    value_loss           | 245           |
-------------------------------------------
Iteration: 210 | Episodes: 17100 | Median Reward: 25.82 | Max Reward: 45.99
Iteration: 212 | Episodes: 17200 | Median Reward: 26.10 | Max Reward: 45.99
Iteration: 213 | Episodes: 17300 | Median Reward: 27.96 | Max Reward: 45.99
Iteration: 214 | Episodes: 17400 | Median Reward: 29.39 | Max Reward: 45.99
Iteration: 215 | Episodes: 17500 | Median Reward: 26.45 | Max Reward: 45.99
Iteration: 216 | Episodes: 17600 | Median Reward: 26.69 | Max Reward: 45.99
Iteration: 218 | Episodes: 17700 | Median Reward: 31.12 | Max Reward: 45.99
Iteration: 219 | Episodes: 17800 | Median Reward: 33.43 | Max Reward: 45.99
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75.9         |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 220           |
|    time_elapsed         | 11528         |
|    total_timesteps      | 1802240       |
| train/                  |               |
|    approx_kl            | 0.00018108799 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -96           |
|    explained_variance   | -7.15e-07     |
|    learning_rate        | 0.000707      |
|    loss                 | 174           |
|    n_updates            | 2190          |
|    policy_gradient_loss | -6.1e-06      |
|    std                  | 1.67          |
|    value_loss           | 357           |
-------------------------------------------
Iteration: 220 | Episodes: 17900 | Median Reward: 25.19 | Max Reward: 45.99
Iteration: 221 | Episodes: 18000 | Median Reward: 25.19 | Max Reward: 45.99
Iteration: 223 | Episodes: 18100 | Median Reward: 23.04 | Max Reward: 45.99
Iteration: 224 | Episodes: 18200 | Median Reward: 20.44 | Max Reward: 45.99
Iteration: 225 | Episodes: 18300 | Median Reward: 15.93 | Max Reward: 45.99
Iteration: 226 | Episodes: 18400 | Median Reward: 16.23 | Max Reward: 45.99
Iteration: 228 | Episodes: 18500 | Median Reward: 15.06 | Max Reward: 45.99
Iteration: 229 | Episodes: 18600 | Median Reward: 22.66 | Max Reward: 45.99
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.5        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 230          |
|    time_elapsed         | 12038        |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 0.0023572545 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -96.7        |
|    explained_variance   | -9.54e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 125          |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.000629    |
|    std                  | 1.72         |
|    value_loss           | 259          |
------------------------------------------
Iteration: 230 | Episodes: 18700 | Median Reward: 33.42 | Max Reward: 45.99
Iteration: 231 | Episodes: 18800 | Median Reward: 23.33 | Max Reward: 45.99
Iteration: 233 | Episodes: 18900 | Median Reward: 23.33 | Max Reward: 45.99
Iteration: 234 | Episodes: 19000 | Median Reward: 35.31 | Max Reward: 45.99
Iteration: 235 | Episodes: 19100 | Median Reward: 28.16 | Max Reward: 45.99
Iteration: 236 | Episodes: 19200 | Median Reward: 11.58 | Max Reward: 45.99
Iteration: 237 | Episodes: 19300 | Median Reward: 26.38 | Max Reward: 45.99
Iteration: 239 | Episodes: 19400 | Median Reward: 32.81 | Max Reward: 45.99
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.4         |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 240           |
|    time_elapsed         | 12545         |
|    total_timesteps      | 1966080       |
| train/                  |               |
|    approx_kl            | 3.6197278e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -97           |
|    explained_variance   | -7.15e-07     |
|    learning_rate        | 0.000707      |
|    loss                 | 171           |
|    n_updates            | 2390          |
|    policy_gradient_loss | -8.13e-05     |
|    std                  | 1.74          |
|    value_loss           | 351           |
-------------------------------------------
Iteration: 240 | Episodes: 19500 | Median Reward: 32.92 | Max Reward: 45.99
Iteration: 241 | Episodes: 19600 | Median Reward: 18.97 | Max Reward: 45.99
Iteration: 242 | Episodes: 19700 | Median Reward: 18.78 | Max Reward: 45.99
Iteration: 244 | Episodes: 19800 | Median Reward: 22.19 | Max Reward: 45.99
Iteration: 245 | Episodes: 19900 | Median Reward: 21.65 | Max Reward: 45.99
Iteration: 246 | Episodes: 20000 | Median Reward: 29.06 | Max Reward: 45.99
Iteration: 247 | Episodes: 20100 | Median Reward: 30.66 | Max Reward: 45.99
Iteration: 249 | Episodes: 20200 | Median Reward: 22.12 | Max Reward: 45.99
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -76         |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 250         |
|    time_elapsed         | 13060       |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.003267602 |
|    clip_fraction        | 0.0025      |
|    clip_range           | 0.4         |
|    entropy_loss         | -97.2       |
|    explained_variance   | -4.77e-07   |
|    learning_rate        | 0.000707    |
|    loss                 | 168         |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00339    |
|    std                  | 1.77        |
|    value_loss           | 346         |
-----------------------------------------
Iteration: 250 | Episodes: 20300 | Median Reward: 23.30 | Max Reward: 45.99
Iteration: 251 | Episodes: 20400 | Median Reward: 27.19 | Max Reward: 45.99
Iteration: 252 | Episodes: 20500 | Median Reward: 26.54 | Max Reward: 45.99
Iteration: 253 | Episodes: 20600 | Median Reward: 20.22 | Max Reward: 45.99
Iteration: 255 | Episodes: 20700 | Median Reward: 23.28 | Max Reward: 45.99
Iteration: 256 | Episodes: 20800 | Median Reward: 29.54 | Max Reward: 45.99
Iteration: 257 | Episodes: 20900 | Median Reward: 28.10 | Max Reward: 45.99
Iteration: 258 | Episodes: 21000 | Median Reward: 30.79 | Max Reward: 45.99
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.3        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 260          |
|    time_elapsed         | 13583        |
|    total_timesteps      | 2129920      |
| train/                  |              |
|    approx_kl            | 0.0028982204 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -97.7        |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 120          |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.000804    |
|    std                  | 1.83         |
|    value_loss           | 250          |
------------------------------------------
Iteration: 260 | Episodes: 21100 | Median Reward: 29.52 | Max Reward: 45.99
Iteration: 261 | Episodes: 21200 | Median Reward: 32.95 | Max Reward: 45.99
Iteration: 262 | Episodes: 21300 | Median Reward: 25.02 | Max Reward: 45.99
Iteration: 263 | Episodes: 21400 | Median Reward: 20.00 | Max Reward: 45.99
Iteration: 265 | Episodes: 21500 | Median Reward: 30.41 | Max Reward: 45.99
Iteration: 266 | Episodes: 21600 | Median Reward: 26.00 | Max Reward: 45.99
Iteration: 267 | Episodes: 21700 | Median Reward: 27.80 | Max Reward: 45.99
Iteration: 268 | Episodes: 21800 | Median Reward: 30.69 | Max Reward: 45.99
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -82.1        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 270          |
|    time_elapsed         | 14096        |
|    total_timesteps      | 2211840      |
| train/                  |              |
|    approx_kl            | 0.0033473098 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -99.1        |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 142          |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.9          |
|    value_loss           | 295          |
------------------------------------------
Iteration: 270 | Episodes: 21900 | Median Reward: 27.67 | Max Reward: 45.99
Iteration: 271 | Episodes: 22000 | Median Reward: 23.51 | Max Reward: 45.99
Iteration: 272 | Episodes: 22100 | Median Reward: 22.87 | Max Reward: 45.99
Iteration: 273 | Episodes: 22200 | Median Reward: 21.93 | Max Reward: 45.99
Iteration: 274 | Episodes: 22300 | Median Reward: 25.88 | Max Reward: 45.99
Iteration: 276 | Episodes: 22400 | Median Reward: 26.34 | Max Reward: 45.99
Iteration: 277 | Episodes: 22500 | Median Reward: 24.54 | Max Reward: 45.99
Iteration: 278 | Episodes: 22600 | Median Reward: 27.63 | Max Reward: 45.99
Iteration: 279 | Episodes: 22700 | Median Reward: 29.97 | Max Reward: 45.99
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.5       |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 280         |
|    time_elapsed         | 14609       |
|    total_timesteps      | 2293760     |
| train/                  |             |
|    approx_kl            | 0.099527806 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.4         |
|    entropy_loss         | -99.5       |
|    explained_variance   | -4.77e-07   |
|    learning_rate        | 0.000707    |
|    loss                 | 140         |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.0249     |
|    std                  | 1.93        |
|    value_loss           | 291         |
-----------------------------------------
Iteration: 281 | Episodes: 22800 | Median Reward: 27.15 | Max Reward: 45.99
Iteration: 282 | Episodes: 22900 | Median Reward: 20.18 | Max Reward: 45.99
Iteration: 283 | Episodes: 23000 | Median Reward: 20.84 | Max Reward: 45.99
Iteration: 284 | Episodes: 23100 | Median Reward: 31.00 | Max Reward: 45.99
Iteration: 286 | Episodes: 23200 | Median Reward: 35.39 | Max Reward: 45.99
Iteration: 287 | Episodes: 23300 | Median Reward: 32.28 | Max Reward: 45.99
Iteration: 288 | Episodes: 23400 | Median Reward: 31.43 | Max Reward: 45.99
Iteration: 289 | Episodes: 23500 | Median Reward: 31.35 | Max Reward: 45.99
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.8        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 290          |
|    time_elapsed         | 15118        |
|    total_timesteps      | 2375680      |
| train/                  |              |
|    approx_kl            | 0.0011888476 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -100         |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 152          |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.000599    |
|    std                  | 1.98         |
|    value_loss           | 314          |
------------------------------------------
Iteration: 290 | Episodes: 23600 | Median Reward: 25.61 | Max Reward: 45.99
Iteration: 292 | Episodes: 23700 | Median Reward: 25.84 | Max Reward: 45.99
Iteration: 293 | Episodes: 23800 | Median Reward: 23.73 | Max Reward: 45.99
Iteration: 294 | Episodes: 23900 | Median Reward: 28.96 | Max Reward: 45.99
Iteration: 295 | Episodes: 24000 | Median Reward: 32.58 | Max Reward: 45.99
Iteration: 297 | Episodes: 24100 | Median Reward: 34.43 | Max Reward: 45.99
Iteration: 298 | Episodes: 24200 | Median Reward: 27.40 | Max Reward: 45.99
Iteration: 299 | Episodes: 24300 | Median Reward: 34.97 | Max Reward: 45.99
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.5        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 300          |
|    time_elapsed         | 15626        |
|    total_timesteps      | 2457600      |
| train/                  |              |
|    approx_kl            | 0.0005016427 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -101         |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 122          |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.000889    |
|    std                  | 2.02         |
|    value_loss           | 255          |
------------------------------------------
Iteration: 300 | Episodes: 24400 | Median Reward: 39.47 | Max Reward: 45.99
Iteration: 302 | Episodes: 24500 | Median Reward: 30.97 | Max Reward: 45.99
Iteration: 303 | Episodes: 24600 | Median Reward: 26.04 | Max Reward: 45.99
Iteration: 304 | Episodes: 24700 | Median Reward: 23.64 | Max Reward: 45.99
Iteration: 305 | Episodes: 24800 | Median Reward: 23.87 | Max Reward: 45.99
Iteration: 306 | Episodes: 24900 | Median Reward: 33.88 | Max Reward: 46.65
Iteration: 308 | Episodes: 25000 | Median Reward: 24.86 | Max Reward: 46.65
Iteration: 309 | Episodes: 25100 | Median Reward: 21.11 | Max Reward: 46.65
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.9         |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 310           |
|    time_elapsed         | 16137         |
|    total_timesteps      | 2539520       |
| train/                  |               |
|    approx_kl            | 0.00019683348 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -101          |
|    explained_variance   | -4.77e-07     |
|    learning_rate        | 0.000707      |
|    loss                 | 107           |
|    n_updates            | 3090          |
|    policy_gradient_loss | 0.00109       |
|    std                  | 2.06          |
|    value_loss           | 224           |
-------------------------------------------
Iteration: 310 | Episodes: 25200 | Median Reward: 35.24 | Max Reward: 46.65
Iteration: 311 | Episodes: 25300 | Median Reward: 28.68 | Max Reward: 46.65
Iteration: 313 | Episodes: 25400 | Median Reward: 26.09 | Max Reward: 46.65
Iteration: 314 | Episodes: 25500 | Median Reward: 22.07 | Max Reward: 46.65
Iteration: 315 | Episodes: 25600 | Median Reward: 31.08 | Max Reward: 46.65
Iteration: 316 | Episodes: 25700 | Median Reward: 27.61 | Max Reward: 46.65
Iteration: 318 | Episodes: 25800 | Median Reward: 26.88 | Max Reward: 46.65
Iteration: 319 | Episodes: 25900 | Median Reward: 33.03 | Max Reward: 46.65
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66           |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 320           |
|    time_elapsed         | 16648         |
|    total_timesteps      | 2621440       |
| train/                  |               |
|    approx_kl            | 1.7905797e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -101          |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.000707      |
|    loss                 | 162           |
|    n_updates            | 3190          |
|    policy_gradient_loss | 0.000112      |
|    std                  | 2.09          |
|    value_loss           | 334           |
-------------------------------------------
Iteration: 320 | Episodes: 26000 | Median Reward: 31.23 | Max Reward: 46.65
Iteration: 321 | Episodes: 26100 | Median Reward: 23.26 | Max Reward: 46.65
Iteration: 323 | Episodes: 26200 | Median Reward: 31.72 | Max Reward: 46.65
Iteration: 324 | Episodes: 26300 | Median Reward: 23.51 | Max Reward: 46.65
Iteration: 325 | Episodes: 26400 | Median Reward: 20.53 | Max Reward: 46.65
Iteration: 326 | Episodes: 26500 | Median Reward: 26.58 | Max Reward: 46.65
Iteration: 327 | Episodes: 26600 | Median Reward: 28.99 | Max Reward: 46.65
Iteration: 329 | Episodes: 26700 | Median Reward: 33.34 | Max Reward: 46.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.2        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 330          |
|    time_elapsed         | 17173        |
|    total_timesteps      | 2703360      |
| train/                  |              |
|    approx_kl            | 7.922878e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 167          |
|    n_updates            | 3290         |
|    policy_gradient_loss | -4.72e-05    |
|    std                  | 2.1          |
|    value_loss           | 345          |
------------------------------------------
Iteration: 330 | Episodes: 26800 | Median Reward: 31.06 | Max Reward: 46.65
Iteration: 331 | Episodes: 26900 | Median Reward: 25.70 | Max Reward: 46.65
Iteration: 332 | Episodes: 27000 | Median Reward: 23.05 | Max Reward: 46.65
Iteration: 334 | Episodes: 27100 | Median Reward: 23.40 | Max Reward: 46.65
Iteration: 335 | Episodes: 27200 | Median Reward: 25.39 | Max Reward: 46.65
Iteration: 336 | Episodes: 27300 | Median Reward: 25.70 | Max Reward: 46.65
Iteration: 337 | Episodes: 27400 | Median Reward: 20.71 | Max Reward: 46.65
Iteration: 339 | Episodes: 27500 | Median Reward: 36.10 | Max Reward: 46.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.6        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 340          |
|    time_elapsed         | 17691        |
|    total_timesteps      | 2785280      |
| train/                  |              |
|    approx_kl            | 0.0035843942 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -102         |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 160          |
|    n_updates            | 3390         |
|    policy_gradient_loss | -0.000779    |
|    std                  | 2.15         |
|    value_loss           | 330          |
------------------------------------------
Iteration: 340 | Episodes: 27600 | Median Reward: 26.34 | Max Reward: 46.65
Iteration: 341 | Episodes: 27700 | Median Reward: 14.37 | Max Reward: 46.65
Iteration: 342 | Episodes: 27800 | Median Reward: 21.91 | Max Reward: 46.65
Iteration: 343 | Episodes: 27900 | Median Reward: 27.97 | Max Reward: 46.65
Iteration: 345 | Episodes: 28000 | Median Reward: 30.50 | Max Reward: 46.65
Iteration: 346 | Episodes: 28100 | Median Reward: 29.95 | Max Reward: 46.65
Iteration: 347 | Episodes: 28200 | Median Reward: 21.00 | Max Reward: 46.65
Iteration: 348 | Episodes: 28300 | Median Reward: 32.91 | Max Reward: 46.65
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.3         |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 350           |
|    time_elapsed         | 18211         |
|    total_timesteps      | 2867200       |
| train/                  |               |
|    approx_kl            | 1.9057043e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -102          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.000707      |
|    loss                 | 184           |
|    n_updates            | 3490          |
|    policy_gradient_loss | 2.54e-07      |
|    std                  | 2.18          |
|    value_loss           | 379           |
-------------------------------------------
Iteration: 350 | Episodes: 28400 | Median Reward: 32.91 | Max Reward: 46.65
Iteration: 351 | Episodes: 28500 | Median Reward: 30.66 | Max Reward: 46.65
Iteration: 352 | Episodes: 28600 | Median Reward: 13.13 | Max Reward: 46.65
Iteration: 353 | Episodes: 28700 | Median Reward: 32.28 | Max Reward: 46.65
Iteration: 355 | Episodes: 28800 | Median Reward: 24.78 | Max Reward: 46.65
Iteration: 356 | Episodes: 28900 | Median Reward: 26.68 | Max Reward: 46.65
Iteration: 357 | Episodes: 29000 | Median Reward: 34.94 | Max Reward: 46.65
Iteration: 358 | Episodes: 29100 | Median Reward: 15.82 | Max Reward: 46.65
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.9         |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 360           |
|    time_elapsed         | 18725         |
|    total_timesteps      | 2949120       |
| train/                  |               |
|    approx_kl            | 0.00074662396 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | -3.58e-07     |
|    learning_rate        | 0.000707      |
|    loss                 | 115           |
|    n_updates            | 3590          |
|    policy_gradient_loss | -0.00162      |
|    std                  | 2.2           |
|    value_loss           | 240           |
-------------------------------------------
Iteration: 360 | Episodes: 29200 | Median Reward: 21.51 | Max Reward: 46.65
Iteration: 361 | Episodes: 29300 | Median Reward: 20.82 | Max Reward: 46.65
Iteration: 362 | Episodes: 29400 | Median Reward: 12.99 | Max Reward: 46.65
Iteration: 363 | Episodes: 29500 | Median Reward: 11.02 | Max Reward: 46.65
Iteration: 364 | Episodes: 29600 | Median Reward: 21.03 | Max Reward: 46.65
Iteration: 366 | Episodes: 29700 | Median Reward: 30.87 | Max Reward: 46.65
Iteration: 367 | Episodes: 29800 | Median Reward: 19.48 | Max Reward: 46.65
Iteration: 368 | Episodes: 29900 | Median Reward: 24.32 | Max Reward: 46.65
Iteration: 369 | Episodes: 30000 | Median Reward: 26.22 | Max Reward: 46.65
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.6         |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 370           |
|    time_elapsed         | 19244         |
|    total_timesteps      | 3031040       |
| train/                  |               |
|    approx_kl            | 0.00014960935 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -103          |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.000707      |
|    loss                 | 138           |
|    n_updates            | 3690          |
|    policy_gradient_loss | -0.00028      |
|    std                  | 2.24          |
|    value_loss           | 285           |
-------------------------------------------
Iteration: 371 | Episodes: 30100 | Median Reward: 24.86 | Max Reward: 46.65
Iteration: 372 | Episodes: 30200 | Median Reward: 19.06 | Max Reward: 46.65
Iteration: 373 | Episodes: 30300 | Median Reward: 18.90 | Max Reward: 46.65
Iteration: 374 | Episodes: 30400 | Median Reward: 35.42 | Max Reward: 46.65
Iteration: 376 | Episodes: 30500 | Median Reward: 22.89 | Max Reward: 46.65
Iteration: 377 | Episodes: 30600 | Median Reward: 28.09 | Max Reward: 46.65
Iteration: 378 | Episodes: 30700 | Median Reward: 26.29 | Max Reward: 46.65
Iteration: 379 | Episodes: 30800 | Median Reward: 25.84 | Max Reward: 46.65
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 101       |
|    ep_rew_mean          | -74.2     |
| time/                   |           |
|    fps                  | 157       |
|    iterations           | 380       |
|    time_elapsed         | 19763     |
|    total_timesteps      | 3112960   |
| train/                  |           |
|    approx_kl            | 0.0523545 |
|    clip_fraction        | 0.0485    |
|    clip_range           | 0.4       |
|    entropy_loss         | -103      |
|    explained_variance   | -3.58e-07 |
|    learning_rate        | 0.000707  |
|    loss                 | 133       |
|    n_updates            | 3790      |
|    policy_gradient_loss | -0.00602  |
|    std                  | 2.27      |
|    value_loss           | 277       |
---------------------------------------
Iteration: 380 | Episodes: 30900 | Median Reward: 28.34 | Max Reward: 46.65
Iteration: 382 | Episodes: 31000 | Median Reward: 26.66 | Max Reward: 46.65
Iteration: 383 | Episodes: 31100 | Median Reward: 31.93 | Max Reward: 46.65
Iteration: 384 | Episodes: 31200 | Median Reward: 23.92 | Max Reward: 46.65
Iteration: 385 | Episodes: 31300 | Median Reward: 28.81 | Max Reward: 46.65
Iteration: 387 | Episodes: 31400 | Median Reward: 28.35 | Max Reward: 46.65
Iteration: 388 | Episodes: 31500 | Median Reward: 24.63 | Max Reward: 46.65
Iteration: 389 | Episodes: 31600 | Median Reward: 25.69 | Max Reward: 46.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.9        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 390          |
|    time_elapsed         | 20288        |
|    total_timesteps      | 3194880      |
| train/                  |              |
|    approx_kl            | 0.0012122232 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -104         |
|    explained_variance   | -2.38e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 106          |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.00231     |
|    std                  | 2.33         |
|    value_loss           | 223          |
------------------------------------------
Iteration: 390 | Episodes: 31700 | Median Reward: 30.38 | Max Reward: 46.65
Iteration: 392 | Episodes: 31800 | Median Reward: 29.64 | Max Reward: 46.65
Iteration: 393 | Episodes: 31900 | Median Reward: 15.15 | Max Reward: 46.65
Iteration: 394 | Episodes: 32000 | Median Reward: 18.04 | Max Reward: 46.65
Iteration: 395 | Episodes: 32100 | Median Reward: 23.77 | Max Reward: 46.65
Iteration: 396 | Episodes: 32200 | Median Reward: 32.55 | Max Reward: 46.65
Iteration: 398 | Episodes: 32300 | Median Reward: 26.33 | Max Reward: 46.65
Iteration: 399 | Episodes: 32400 | Median Reward: 25.36 | Max Reward: 46.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.9       |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 400         |
|    time_elapsed         | 20812       |
|    total_timesteps      | 3276800     |
| train/                  |             |
|    approx_kl            | 0.002656904 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -104        |
|    explained_variance   | -3.58e-07   |
|    learning_rate        | 0.000707    |
|    loss                 | 116         |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.00402    |
|    std                  | 2.37        |
|    value_loss           | 243         |
-----------------------------------------
Iteration: 400 | Episodes: 32500 | Median Reward: 24.45 | Max Reward: 46.65
Iteration: 401 | Episodes: 32600 | Median Reward: 26.64 | Max Reward: 46.65
Iteration: 403 | Episodes: 32700 | Median Reward: 27.78 | Max Reward: 46.65
Iteration: 404 | Episodes: 32800 | Median Reward: 24.49 | Max Reward: 46.65
Iteration: 405 | Episodes: 32900 | Median Reward: 25.11 | Max Reward: 46.65
Iteration: 406 | Episodes: 33000 | Median Reward: 28.87 | Max Reward: 46.65
Iteration: 408 | Episodes: 33100 | Median Reward: 29.04 | Max Reward: 46.65
Iteration: 409 | Episodes: 33200 | Median Reward: 35.19 | Max Reward: 46.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.4        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 410          |
|    time_elapsed         | 21332        |
|    total_timesteps      | 3358720      |
| train/                  |              |
|    approx_kl            | 0.0017006968 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -105         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 178          |
|    n_updates            | 4090         |
|    policy_gradient_loss | -3.84e-05    |
|    std                  | 2.43         |
|    value_loss           | 368          |
------------------------------------------
Iteration: 410 | Episodes: 33300 | Median Reward: 29.65 | Max Reward: 46.65
Iteration: 411 | Episodes: 33400 | Median Reward: 27.96 | Max Reward: 46.65
Iteration: 413 | Episodes: 33500 | Median Reward: 33.10 | Max Reward: 46.65
Iteration: 414 | Episodes: 33600 | Median Reward: 32.43 | Max Reward: 46.65
Iteration: 415 | Episodes: 33700 | Median Reward: 15.95 | Max Reward: 46.65
Iteration: 416 | Episodes: 33800 | Median Reward: 22.79 | Max Reward: 46.65
Iteration: 417 | Episodes: 33900 | Median Reward: 27.30 | Max Reward: 46.65
Iteration: 419 | Episodes: 34000 | Median Reward: 26.40 | Max Reward: 46.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.1        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 420          |
|    time_elapsed         | 21860        |
|    total_timesteps      | 3440640      |
| train/                  |              |
|    approx_kl            | 0.0005345807 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -105         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 139          |
|    n_updates            | 4190         |
|    policy_gradient_loss | -0.000423    |
|    std                  | 2.46         |
|    value_loss           | 289          |
------------------------------------------
Iteration: 420 | Episodes: 34100 | Median Reward: 28.21 | Max Reward: 46.65
Iteration: 421 | Episodes: 34200 | Median Reward: 29.20 | Max Reward: 46.65
Iteration: 422 | Episodes: 34300 | Median Reward: 31.61 | Max Reward: 46.65
Iteration: 424 | Episodes: 34400 | Median Reward: 35.67 | Max Reward: 46.65
Iteration: 425 | Episodes: 34500 | Median Reward: 32.09 | Max Reward: 46.65
Iteration: 426 | Episodes: 34600 | Median Reward: 31.06 | Max Reward: 46.65
Iteration: 427 | Episodes: 34700 | Median Reward: 20.53 | Max Reward: 46.65
Iteration: 429 | Episodes: 34800 | Median Reward: 20.53 | Max Reward: 46.65
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.7       |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 430         |
|    time_elapsed         | 22383       |
|    total_timesteps      | 3522560     |
| train/                  |             |
|    approx_kl            | 0.018121038 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -106        |
|    explained_variance   | -1.19e-07   |
|    learning_rate        | 0.000707    |
|    loss                 | 140         |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.00613    |
|    std                  | 2.53        |
|    value_loss           | 291         |
-----------------------------------------
Iteration: 430 | Episodes: 34900 | Median Reward: 33.33 | Max Reward: 46.65
Iteration: 431 | Episodes: 35000 | Median Reward: 34.98 | Max Reward: 46.65
Iteration: 432 | Episodes: 35100 | Median Reward: 34.42 | Max Reward: 46.65
Iteration: 433 | Episodes: 35200 | Median Reward: 25.81 | Max Reward: 46.65
Iteration: 435 | Episodes: 35300 | Median Reward: 31.03 | Max Reward: 46.65
Iteration: 436 | Episodes: 35400 | Median Reward: 29.82 | Max Reward: 46.65
Iteration: 437 | Episodes: 35500 | Median Reward: 31.48 | Max Reward: 46.65
Iteration: 438 | Episodes: 35600 | Median Reward: 30.20 | Max Reward: 46.65
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -71        |
| time/                   |            |
|    fps                  | 157        |
|    iterations           | 440        |
|    time_elapsed         | 22906      |
|    total_timesteps      | 3604480    |
| train/                  |            |
|    approx_kl            | 0.06401411 |
|    clip_fraction        | 0.05       |
|    clip_range           | 0.4        |
|    entropy_loss         | -107       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.000707   |
|    loss                 | 146        |
|    n_updates            | 4390       |
|    policy_gradient_loss | -0.0122    |
|    std                  | 2.61       |
|    value_loss           | 303        |
----------------------------------------
Iteration: 440 | Episodes: 35700 | Median Reward: 31.60 | Max Reward: 46.65
Iteration: 441 | Episodes: 35800 | Median Reward: 28.15 | Max Reward: 46.65
Iteration: 442 | Episodes: 35900 | Median Reward: 28.71 | Max Reward: 46.65
Iteration: 443 | Episodes: 36000 | Median Reward: 31.04 | Max Reward: 46.65
Iteration: 445 | Episodes: 36100 | Median Reward: 28.91 | Max Reward: 46.65
Iteration: 446 | Episodes: 36200 | Median Reward: 22.49 | Max Reward: 46.65
Iteration: 447 | Episodes: 36300 | Median Reward: 19.55 | Max Reward: 46.65
Iteration: 448 | Episodes: 36400 | Median Reward: 32.99 | Max Reward: 46.65
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.1        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 450          |
|    time_elapsed         | 23435        |
|    total_timesteps      | 3686400      |
| train/                  |              |
|    approx_kl            | 0.0006852497 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -107         |
|    explained_variance   | 0            |
|    learning_rate        | 0.000707     |
|    loss                 | 180          |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.000689    |
|    std                  | 2.72         |
|    value_loss           | 371          |
------------------------------------------
Iteration: 450 | Episodes: 36500 | Median Reward: 37.35 | Max Reward: 46.79
Iteration: 451 | Episodes: 36600 | Median Reward: 31.35 | Max Reward: 46.79
Iteration: 452 | Episodes: 36700 | Median Reward: 22.97 | Max Reward: 46.79
Iteration: 453 | Episodes: 36800 | Median Reward: 36.60 | Max Reward: 46.79
Iteration: 454 | Episodes: 36900 | Median Reward: -12.37 | Max Reward: 46.79
Iteration: 456 | Episodes: 37000 | Median Reward: -6.79 | Max Reward: 46.79
Iteration: 457 | Episodes: 37100 | Median Reward: -0.70 | Max Reward: 46.79
Iteration: 458 | Episodes: 37200 | Median Reward: -1.11 | Max Reward: 46.79
Iteration: 459 | Episodes: 37300 | Median Reward: -5.52 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -109          |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 460           |
|    time_elapsed         | 23987         |
|    total_timesteps      | 3768320       |
| train/                  |               |
|    approx_kl            | 1.1758675e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -107          |
|    explained_variance   | -1.19e-07     |
|    learning_rate        | 0.000707      |
|    loss                 | 50.9          |
|    n_updates            | 4590          |
|    policy_gradient_loss | -7.38e-05     |
|    std                  | 2.74          |
|    value_loss           | 114           |
-------------------------------------------
Iteration: 461 | Episodes: 37400 | Median Reward: -8.07 | Max Reward: 46.79
Iteration: 462 | Episodes: 37500 | Median Reward: -13.23 | Max Reward: 46.79
Iteration: 463 | Episodes: 37600 | Median Reward: -12.37 | Max Reward: 46.79
Iteration: 464 | Episodes: 37700 | Median Reward: 3.99 | Max Reward: 46.79
Iteration: 466 | Episodes: 37800 | Median Reward: -2.43 | Max Reward: 46.79
Iteration: 467 | Episodes: 37900 | Median Reward: 7.16 | Max Reward: 46.79
Iteration: 468 | Episodes: 38000 | Median Reward: -2.81 | Max Reward: 46.79
Iteration: 469 | Episodes: 38100 | Median Reward: -16.23 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -118         |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 470          |
|    time_elapsed         | 24474        |
|    total_timesteps      | 3850240      |
| train/                  |              |
|    approx_kl            | 0.0025955103 |
|    clip_fraction        | 0.001        |
|    clip_range           | 0.4          |
|    entropy_loss         | -107         |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 38.7         |
|    n_updates            | 4690         |
|    policy_gradient_loss | -0.00312     |
|    std                  | 2.74         |
|    value_loss           | 90.1         |
------------------------------------------
Iteration: 470 | Episodes: 38200 | Median Reward: -17.11 | Max Reward: 46.79
Iteration: 472 | Episodes: 38300 | Median Reward: -17.69 | Max Reward: 46.79
Iteration: 473 | Episodes: 38400 | Median Reward: 2.16 | Max Reward: 46.79
Iteration: 474 | Episodes: 38500 | Median Reward: 2.88 | Max Reward: 46.79
Iteration: 475 | Episodes: 38600 | Median Reward: -1.48 | Max Reward: 46.79
Iteration: 477 | Episodes: 38700 | Median Reward: -0.76 | Max Reward: 46.79
Iteration: 478 | Episodes: 38800 | Median Reward: -2.26 | Max Reward: 46.79
Iteration: 479 | Episodes: 38900 | Median Reward: 1.17 | Max Reward: 46.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -93.3       |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 480         |
|    time_elapsed         | 24967       |
|    total_timesteps      | 3932160     |
| train/                  |             |
|    approx_kl            | 0.017538518 |
|    clip_fraction        | 0.00293     |
|    clip_range           | 0.4         |
|    entropy_loss         | -104        |
|    explained_variance   | -3.58e-07   |
|    learning_rate        | 0.000707    |
|    loss                 | 57.3        |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.00425    |
|    std                  | 2.75        |
|    value_loss           | 126         |
-----------------------------------------
Iteration: 480 | Episodes: 39000 | Median Reward: 12.66 | Max Reward: 46.79
Iteration: 482 | Episodes: 39100 | Median Reward: 9.83 | Max Reward: 46.79
Iteration: 483 | Episodes: 39200 | Median Reward: -7.63 | Max Reward: 46.79
Iteration: 484 | Episodes: 39300 | Median Reward: -8.99 | Max Reward: 46.79
Iteration: 485 | Episodes: 39400 | Median Reward: -5.54 | Max Reward: 46.79
Iteration: 486 | Episodes: 39500 | Median Reward: -4.56 | Max Reward: 46.79
Iteration: 488 | Episodes: 39600 | Median Reward: -1.03 | Max Reward: 46.79
Iteration: 489 | Episodes: 39700 | Median Reward: -8.75 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -110          |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 490           |
|    time_elapsed         | 25458         |
|    total_timesteps      | 4014080       |
| train/                  |               |
|    approx_kl            | 9.6115946e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -108          |
|    explained_variance   | -3.58e-07     |
|    learning_rate        | 0.000707      |
|    loss                 | 58.5          |
|    n_updates            | 4890          |
|    policy_gradient_loss | 0.000881      |
|    std                  | 2.78          |
|    value_loss           | 128           |
-------------------------------------------
Iteration: 490 | Episodes: 39800 | Median Reward: -5.34 | Max Reward: 46.79
Iteration: 491 | Episodes: 39900 | Median Reward: -1.84 | Max Reward: 46.79
Iteration: 493 | Episodes: 40000 | Median Reward: -11.32 | Max Reward: 46.79
Iteration: 494 | Episodes: 40100 | Median Reward: -11.32 | Max Reward: 46.79
Iteration: 495 | Episodes: 40200 | Median Reward: -1.09 | Max Reward: 46.79
Iteration: 496 | Episodes: 40300 | Median Reward: -4.24 | Max Reward: 46.79
Iteration: 498 | Episodes: 40400 | Median Reward: -5.94 | Max Reward: 46.79
Iteration: 499 | Episodes: 40500 | Median Reward: -7.51 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -105         |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 500          |
|    time_elapsed         | 25947        |
|    total_timesteps      | 4096000      |
| train/                  |              |
|    approx_kl            | 0.0006429816 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -108         |
|    explained_variance   | -3.58e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 44.1         |
|    n_updates            | 4990         |
|    policy_gradient_loss | 0.000556     |
|    std                  | 2.84         |
|    value_loss           | 100          |
------------------------------------------
Iteration: 500 | Episodes: 40600 | Median Reward: -3.13 | Max Reward: 46.79
Iteration: 501 | Episodes: 40700 | Median Reward: -11.74 | Max Reward: 46.79
Iteration: 503 | Episodes: 40800 | Median Reward: 1.48 | Max Reward: 46.79
Iteration: 504 | Episodes: 40900 | Median Reward: -1.52 | Max Reward: 46.79
Iteration: 505 | Episodes: 41000 | Median Reward: 4.37 | Max Reward: 46.79
Iteration: 506 | Episodes: 41100 | Median Reward: 4.37 | Max Reward: 46.79
Iteration: 507 | Episodes: 41200 | Median Reward: 2.81 | Max Reward: 46.79
Iteration: 509 | Episodes: 41300 | Median Reward: 2.31 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -106          |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 510           |
|    time_elapsed         | 26446         |
|    total_timesteps      | 4177920       |
| train/                  |               |
|    approx_kl            | 0.00026277747 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -109          |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.000707      |
|    loss                 | 122           |
|    n_updates            | 5090          |
|    policy_gradient_loss | 0.000492      |
|    std                  | 2.91          |
|    value_loss           | 256           |
-------------------------------------------
Iteration: 510 | Episodes: 41400 | Median Reward: -0.41 | Max Reward: 46.79
Iteration: 511 | Episodes: 41500 | Median Reward: 10.49 | Max Reward: 46.79
Iteration: 512 | Episodes: 41600 | Median Reward: 7.47 | Max Reward: 46.79
Iteration: 514 | Episodes: 41700 | Median Reward: 8.27 | Max Reward: 46.79
Iteration: 515 | Episodes: 41800 | Median Reward: 9.96 | Max Reward: 46.79
Iteration: 516 | Episodes: 41900 | Median Reward: 6.70 | Max Reward: 46.79
Iteration: 517 | Episodes: 42000 | Median Reward: 6.22 | Max Reward: 46.79
Iteration: 519 | Episodes: 42100 | Median Reward: 0.53 | Max Reward: 46.79
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -102       |
| time/                   |            |
|    fps                  | 158        |
|    iterations           | 520        |
|    time_elapsed         | 26942      |
|    total_timesteps      | 4259840    |
| train/                  |            |
|    approx_kl            | 0.04535935 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.4        |
|    entropy_loss         | -108       |
|    explained_variance   | -1.19e-07  |
|    learning_rate        | 0.000707   |
|    loss                 | 87.5       |
|    n_updates            | 5190       |
|    policy_gradient_loss | -0.00926   |
|    std                  | 2.97       |
|    value_loss           | 186        |
----------------------------------------
Iteration: 520 | Episodes: 42200 | Median Reward: 0.37 | Max Reward: 46.79
Iteration: 521 | Episodes: 42300 | Median Reward: 3.53 | Max Reward: 46.79
Iteration: 522 | Episodes: 42400 | Median Reward: -7.64 | Max Reward: 46.79
Iteration: 523 | Episodes: 42500 | Median Reward: 11.90 | Max Reward: 46.79
Iteration: 525 | Episodes: 42600 | Median Reward: 3.57 | Max Reward: 46.79
Iteration: 526 | Episodes: 42700 | Median Reward: -1.98 | Max Reward: 46.79
Iteration: 527 | Episodes: 42800 | Median Reward: -2.88 | Max Reward: 46.79
Iteration: 528 | Episodes: 42900 | Median Reward: -2.29 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -96.8        |
| time/                   |              |
|    fps                  | 158          |
|    iterations           | 530          |
|    time_elapsed         | 27426        |
|    total_timesteps      | 4341760      |
| train/                  |              |
|    approx_kl            | 0.0011633947 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -109         |
|    explained_variance   | -1.19e-07    |
|    learning_rate        | 0.000707     |
|    loss                 | 119          |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.00221     |
|    std                  | 3.01         |
|    value_loss           | 250          |
------------------------------------------
Iteration: 530 | Episodes: 43000 | Median Reward: 8.37 | Max Reward: 46.79
Iteration: 531 | Episodes: 43100 | Median Reward: 14.06 | Max Reward: 46.79
Iteration: 532 | Episodes: 43200 | Median Reward: 15.22 | Max Reward: 46.79
Iteration: 533 | Episodes: 43300 | Median Reward: 12.46 | Max Reward: 46.79
Iteration: 535 | Episodes: 43400 | Median Reward: 18.21 | Max Reward: 46.79
Iteration: 536 | Episodes: 43500 | Median Reward: 2.40 | Max Reward: 46.79
Iteration: 537 | Episodes: 43600 | Median Reward: -8.08 | Max Reward: 46.79
Iteration: 538 | Episodes: 43700 | Median Reward: -4.81 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -83.2         |
| time/                   |               |
|    fps                  | 158           |
|    iterations           | 540           |
|    time_elapsed         | 27910         |
|    total_timesteps      | 4423680       |
| train/                  |               |
|    approx_kl            | 0.00047849087 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -109          |
|    explained_variance   | 1.19e-07      |
|    learning_rate        | 0.000707      |
|    loss                 | 65.7          |
|    n_updates            | 5390          |
|    policy_gradient_loss | -0.00177      |
|    std                  | 3.04          |
|    value_loss           | 143           |
-------------------------------------------
Iteration: 540 | Episodes: 43800 | Median Reward: 15.94 | Max Reward: 46.79
Iteration: 541 | Episodes: 43900 | Median Reward: 14.40 | Max Reward: 46.79
Iteration: 542 | Episodes: 44000 | Median Reward: 0.61 | Max Reward: 46.79
Iteration: 543 | Episodes: 44100 | Median Reward: 13.74 | Max Reward: 46.79
Iteration: 544 | Episodes: 44200 | Median Reward: 7.04 | Max Reward: 46.79
Iteration: 546 | Episodes: 44300 | Median Reward: 8.90 | Max Reward: 46.79
Iteration: 547 | Episodes: 44400 | Median Reward: 8.77 | Max Reward: 46.79
Iteration: 548 | Episodes: 44500 | Median Reward: 6.04 | Max Reward: 46.79
Iteration: 549 | Episodes: 44600 | Median Reward: 15.43 | Max Reward: 46.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -84.9       |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 550         |
|    time_elapsed         | 28403       |
|    total_timesteps      | 4505600     |
| train/                  |             |
|    approx_kl            | 0.021817397 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -110        |
|    explained_variance   | 3.46e-06    |
|    learning_rate        | 0.000707    |
|    loss                 | 93.1        |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00384    |
|    std                  | 3.09        |
|    value_loss           | 197         |
-----------------------------------------
Iteration: 551 | Episodes: 44700 | Median Reward: 16.96 | Max Reward: 46.79
Iteration: 552 | Episodes: 44800 | Median Reward: 9.22 | Max Reward: 46.79
Iteration: 553 | Episodes: 44900 | Median Reward: -7.56 | Max Reward: 46.79
Iteration: 554 | Episodes: 45000 | Median Reward: 8.77 | Max Reward: 46.79
Iteration: 556 | Episodes: 45100 | Median Reward: 13.13 | Max Reward: 46.79
Iteration: 557 | Episodes: 45200 | Median Reward: 10.15 | Max Reward: 46.79
Iteration: 558 | Episodes: 45300 | Median Reward: 9.93 | Max Reward: 46.79
Iteration: 559 | Episodes: 45400 | Median Reward: 19.59 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.6         |
| time/                   |               |
|    fps                  | 158           |
|    iterations           | 560           |
|    time_elapsed         | 28899         |
|    total_timesteps      | 4587520       |
| train/                  |               |
|    approx_kl            | 0.00045368815 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -110          |
|    explained_variance   | 0.00945       |
|    learning_rate        | 0.000707      |
|    loss                 | 102           |
|    n_updates            | 5590          |
|    policy_gradient_loss | -0.00171      |
|    std                  | 3.13          |
|    value_loss           | 207           |
-------------------------------------------
Iteration: 560 | Episodes: 45500 | Median Reward: 19.36 | Max Reward: 46.79
Iteration: 562 | Episodes: 45600 | Median Reward: 9.47 | Max Reward: 46.79
Iteration: 563 | Episodes: 45700 | Median Reward: 10.60 | Max Reward: 46.79
Iteration: 564 | Episodes: 45800 | Median Reward: 15.03 | Max Reward: 46.79
Iteration: 565 | Episodes: 45900 | Median Reward: 3.27 | Max Reward: 46.79
Iteration: 567 | Episodes: 46000 | Median Reward: 5.27 | Max Reward: 46.79
Iteration: 568 | Episodes: 46100 | Median Reward: -2.24 | Max Reward: 46.79
Iteration: 569 | Episodes: 46200 | Median Reward: 8.76 | Max Reward: 46.79
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -89.5       |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 570         |
|    time_elapsed         | 29398       |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 8.10092e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -110        |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.000707    |
|    loss                 | 13.7        |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.000501   |
|    std                  | 3.13        |
|    value_loss           | 59.8        |
-----------------------------------------
Iteration: 570 | Episodes: 46300 | Median Reward: 7.35 | Max Reward: 46.79
Iteration: 572 | Episodes: 46400 | Median Reward: 9.90 | Max Reward: 46.79
Iteration: 573 | Episodes: 46500 | Median Reward: 8.71 | Max Reward: 46.79
Iteration: 574 | Episodes: 46600 | Median Reward: 3.94 | Max Reward: 46.79
Iteration: 575 | Episodes: 46700 | Median Reward: 4.35 | Max Reward: 46.79
Iteration: 577 | Episodes: 46800 | Median Reward: 16.22 | Max Reward: 46.79
Iteration: 578 | Episodes: 46900 | Median Reward: -1.53 | Max Reward: 46.79
Iteration: 579 | Episodes: 47000 | Median Reward: 3.38 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.8         |
| time/                   |               |
|    fps                  | 158           |
|    iterations           | 580           |
|    time_elapsed         | 29893         |
|    total_timesteps      | 4751360       |
| train/                  |               |
|    approx_kl            | 0.00011073957 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -110          |
|    explained_variance   | 0.829         |
|    learning_rate        | 0.000707      |
|    loss                 | 10.6          |
|    n_updates            | 5790          |
|    policy_gradient_loss | -0.00201      |
|    std                  | 3.14          |
|    value_loss           | 33.6          |
-------------------------------------------
Iteration: 580 | Episodes: 47100 | Median Reward: 16.70 | Max Reward: 46.79
Iteration: 581 | Episodes: 47200 | Median Reward: 9.85 | Max Reward: 46.79
Iteration: 583 | Episodes: 47300 | Median Reward: 9.31 | Max Reward: 46.79
Iteration: 584 | Episodes: 47400 | Median Reward: 10.44 | Max Reward: 46.79
Iteration: 585 | Episodes: 47500 | Median Reward: 16.05 | Max Reward: 46.79
Iteration: 586 | Episodes: 47600 | Median Reward: 11.10 | Max Reward: 46.79
Iteration: 588 | Episodes: 47700 | Median Reward: 14.11 | Max Reward: 46.79
Iteration: 589 | Episodes: 47800 | Median Reward: 23.70 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -94.1         |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 590           |
|    time_elapsed         | 30384         |
|    total_timesteps      | 4833280       |
| train/                  |               |
|    approx_kl            | 5.4829485e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -110          |
|    explained_variance   | 0.924         |
|    learning_rate        | 0.000707      |
|    loss                 | 6.86          |
|    n_updates            | 5890          |
|    policy_gradient_loss | -0.000803     |
|    std                  | 3.14          |
|    value_loss           | 30.5          |
-------------------------------------------
Iteration: 590 | Episodes: 47900 | Median Reward: 2.68 | Max Reward: 46.79
Iteration: 591 | Episodes: 48000 | Median Reward: -4.56 | Max Reward: 46.79
Iteration: 593 | Episodes: 48100 | Median Reward: -4.56 | Max Reward: 46.79
Iteration: 594 | Episodes: 48200 | Median Reward: 9.35 | Max Reward: 46.79
Iteration: 595 | Episodes: 48300 | Median Reward: 4.84 | Max Reward: 46.79
Iteration: 596 | Episodes: 48400 | Median Reward: -0.92 | Max Reward: 46.79
Iteration: 597 | Episodes: 48500 | Median Reward: 16.59 | Max Reward: 46.79
Iteration: 599 | Episodes: 48600 | Median Reward: 19.07 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -81.8         |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 600           |
|    time_elapsed         | 30870         |
|    total_timesteps      | 4915200       |
| train/                  |               |
|    approx_kl            | 1.5867408e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -110          |
|    explained_variance   | 0.726         |
|    learning_rate        | 0.000707      |
|    loss                 | 17            |
|    n_updates            | 5990          |
|    policy_gradient_loss | -9.69e-06     |
|    std                  | 3.14          |
|    value_loss           | 68.2          |
-------------------------------------------
Iteration: 600 | Episodes: 48700 | Median Reward: 19.72 | Max Reward: 46.79
Iteration: 601 | Episodes: 48800 | Median Reward: 9.08 | Max Reward: 46.79
Iteration: 602 | Episodes: 48900 | Median Reward: 19.96 | Max Reward: 46.79
Iteration: 604 | Episodes: 49000 | Median Reward: 7.77 | Max Reward: 46.79
Iteration: 605 | Episodes: 49100 | Median Reward: 6.54 | Max Reward: 46.79
Iteration: 606 | Episodes: 49200 | Median Reward: 3.80 | Max Reward: 46.79
Iteration: 607 | Episodes: 49300 | Median Reward: 7.26 | Max Reward: 46.79
Iteration: 609 | Episodes: 49400 | Median Reward: 13.82 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -83          |
| time/                   |              |
|    fps                  | 159          |
|    iterations           | 610          |
|    time_elapsed         | 31362        |
|    total_timesteps      | 4997120      |
| train/                  |              |
|    approx_kl            | 3.121837e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -110         |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.000707     |
|    loss                 | 0.294        |
|    n_updates            | 6090         |
|    policy_gradient_loss | 0.000607     |
|    std                  | 3.14         |
|    value_loss           | 15.5         |
------------------------------------------
Iteration: 610 | Episodes: 49500 | Median Reward: 16.93 | Max Reward: 46.79
Iteration: 611 | Episodes: 49600 | Median Reward: -2.42 | Max Reward: 46.79
Iteration: 612 | Episodes: 49700 | Median Reward: 19.96 | Max Reward: 46.79
Iteration: 613 | Episodes: 49800 | Median Reward: 19.96 | Max Reward: 46.79
Iteration: 615 | Episodes: 49900 | Median Reward: 20.27 | Max Reward: 46.79
Iteration: 616 | Episodes: 50000 | Median Reward: 14.47 | Max Reward: 46.79
Iteration: 617 | Episodes: 50100 | Median Reward: 13.98 | Max Reward: 46.79
Iteration: 618 | Episodes: 50200 | Median Reward: 24.35 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -88.1         |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 620           |
|    time_elapsed         | 31856         |
|    total_timesteps      | 5079040       |
| train/                  |               |
|    approx_kl            | 3.7743142e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -110          |
|    explained_variance   | 0.989         |
|    learning_rate        | 0.000707      |
|    loss                 | 9.12          |
|    n_updates            | 6190          |
|    policy_gradient_loss | -0.0016       |
|    std                  | 3.14          |
|    value_loss           | 24.2          |
-------------------------------------------
Iteration: 620 | Episodes: 50300 | Median Reward: 15.25 | Max Reward: 46.79
Iteration: 621 | Episodes: 50400 | Median Reward: 15.18 | Max Reward: 46.79
Iteration: 622 | Episodes: 50500 | Median Reward: 10.19 | Max Reward: 46.79
Iteration: 623 | Episodes: 50600 | Median Reward: 17.44 | Max Reward: 46.79
Iteration: 625 | Episodes: 50700 | Median Reward: 16.62 | Max Reward: 46.79
Iteration: 626 | Episodes: 50800 | Median Reward: 9.89 | Max Reward: 46.79
Iteration: 627 | Episodes: 50900 | Median Reward: 21.59 | Max Reward: 46.79
Iteration: 628 | Episodes: 51000 | Median Reward: 20.72 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.2         |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 630           |
|    time_elapsed         | 32361         |
|    total_timesteps      | 5160960       |
| train/                  |               |
|    approx_kl            | 1.4327859e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -110          |
|    explained_variance   | 0.979         |
|    learning_rate        | 0.000707      |
|    loss                 | -2.65         |
|    n_updates            | 6290          |
|    policy_gradient_loss | -0.000312     |
|    std                  | 3.14          |
|    value_loss           | 7.48          |
-------------------------------------------
Iteration: 630 | Episodes: 51100 | Median Reward: 21.11 | Max Reward: 46.79
Iteration: 631 | Episodes: 51200 | Median Reward: 18.80 | Max Reward: 46.79
Iteration: 632 | Episodes: 51300 | Median Reward: 13.05 | Max Reward: 46.79
Iteration: 633 | Episodes: 51400 | Median Reward: 17.03 | Max Reward: 46.79
Iteration: 634 | Episodes: 51500 | Median Reward: 14.71 | Max Reward: 46.79
Iteration: 636 | Episodes: 51600 | Median Reward: 3.91 | Max Reward: 46.79
Iteration: 637 | Episodes: 51700 | Median Reward: 3.91 | Max Reward: 46.79
Iteration: 638 | Episodes: 51800 | Median Reward: 18.33 | Max Reward: 46.79
Iteration: 639 | Episodes: 51900 | Median Reward: 19.26 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -87.3         |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 640           |
|    time_elapsed         | 32847         |
|    total_timesteps      | 5242880       |
| train/                  |               |
|    approx_kl            | 3.5970952e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -110          |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.000707      |
|    loss                 | 0.3           |
|    n_updates            | 6390          |
|    policy_gradient_loss | -7.92e-06     |
|    std                  | 3.15          |
|    value_loss           | 13.5          |
-------------------------------------------
Iteration: 641 | Episodes: 52000 | Median Reward: 17.62 | Max Reward: 46.79
Iteration: 642 | Episodes: 52100 | Median Reward: 20.73 | Max Reward: 46.79
Iteration: 643 | Episodes: 52200 | Median Reward: 19.32 | Max Reward: 46.79
Iteration: 644 | Episodes: 52300 | Median Reward: 20.15 | Max Reward: 46.79
Iteration: 646 | Episodes: 52400 | Median Reward: 22.28 | Max Reward: 46.79
Iteration: 647 | Episodes: 52500 | Median Reward: 19.25 | Max Reward: 46.79
Iteration: 648 | Episodes: 52600 | Median Reward: 14.12 | Max Reward: 46.79
Iteration: 649 | Episodes: 52700 | Median Reward: 14.65 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.3        |
| time/                   |              |
|    fps                  | 159          |
|    iterations           | 650          |
|    time_elapsed         | 33331        |
|    total_timesteps      | 5324800      |
| train/                  |              |
|    approx_kl            | 7.147377e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -110         |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.000707     |
|    loss                 | -1.59        |
|    n_updates            | 6490         |
|    policy_gradient_loss | -0.000191    |
|    std                  | 3.15         |
|    value_loss           | 7.95         |
------------------------------------------
Iteration: 650 | Episodes: 52800 | Median Reward: 22.86 | Max Reward: 46.79
Iteration: 652 | Episodes: 52900 | Median Reward: 11.73 | Max Reward: 46.79
Iteration: 653 | Episodes: 53000 | Median Reward: -0.12 | Max Reward: 46.79
Iteration: 654 | Episodes: 53100 | Median Reward: 6.88 | Max Reward: 46.79
Iteration: 655 | Episodes: 53200 | Median Reward: 10.78 | Max Reward: 46.79
Iteration: 657 | Episodes: 53300 | Median Reward: 26.56 | Max Reward: 46.79
Iteration: 658 | Episodes: 53400 | Median Reward: 27.15 | Max Reward: 46.79
Iteration: 659 | Episodes: 53500 | Median Reward: 16.02 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -84.2         |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 660           |
|    time_elapsed         | 33817         |
|    total_timesteps      | 5406720       |
| train/                  |               |
|    approx_kl            | 0.00011702383 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -111          |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.000707      |
|    loss                 | 0.86          |
|    n_updates            | 6590          |
|    policy_gradient_loss | -0.00235      |
|    std                  | 3.15          |
|    value_loss           | 12.1          |
-------------------------------------------
Iteration: 660 | Episodes: 53600 | Median Reward: 14.67 | Max Reward: 46.79
Iteration: 662 | Episodes: 53700 | Median Reward: 9.75 | Max Reward: 46.79
Iteration: 663 | Episodes: 53800 | Median Reward: 26.32 | Max Reward: 46.79
Iteration: 664 | Episodes: 53900 | Median Reward: 25.47 | Max Reward: 46.79
Iteration: 665 | Episodes: 54000 | Median Reward: 26.06 | Max Reward: 46.79
Iteration: 667 | Episodes: 54100 | Median Reward: 20.45 | Max Reward: 46.79
Iteration: 668 | Episodes: 54200 | Median Reward: 16.16 | Max Reward: 46.79
Iteration: 669 | Episodes: 54300 | Median Reward: 2.32 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -96.9        |
| time/                   |              |
|    fps                  | 159          |
|    iterations           | 670          |
|    time_elapsed         | 34315        |
|    total_timesteps      | 5488640      |
| train/                  |              |
|    approx_kl            | 0.0001910584 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.000707     |
|    loss                 | -2.42        |
|    n_updates            | 6690         |
|    policy_gradient_loss | -0.00265     |
|    std                  | 3.16         |
|    value_loss           | 6.97         |
------------------------------------------
Iteration: 670 | Episodes: 54400 | Median Reward: 14.15 | Max Reward: 46.79
Iteration: 671 | Episodes: 54500 | Median Reward: 17.77 | Max Reward: 46.79
Iteration: 673 | Episodes: 54600 | Median Reward: 16.05 | Max Reward: 46.79
Iteration: 674 | Episodes: 54700 | Median Reward: 21.53 | Max Reward: 46.79
Iteration: 675 | Episodes: 54800 | Median Reward: 26.02 | Max Reward: 46.79
Iteration: 676 | Episodes: 54900 | Median Reward: 14.22 | Max Reward: 46.79
Iteration: 678 | Episodes: 55000 | Median Reward: 20.34 | Max Reward: 46.79
Iteration: 679 | Episodes: 55100 | Median Reward: 31.31 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -80.9        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 680          |
|    time_elapsed         | 34807        |
|    total_timesteps      | 5570560      |
| train/                  |              |
|    approx_kl            | 4.473135e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.000707     |
|    loss                 | -2.21        |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.000643    |
|    std                  | 3.16         |
|    value_loss           | 13.1         |
------------------------------------------
Iteration: 680 | Episodes: 55200 | Median Reward: 15.65 | Max Reward: 46.79
Iteration: 681 | Episodes: 55300 | Median Reward: 23.99 | Max Reward: 46.79
Iteration: 683 | Episodes: 55400 | Median Reward: 23.05 | Max Reward: 46.79
Iteration: 684 | Episodes: 55500 | Median Reward: 11.74 | Max Reward: 46.79
Iteration: 685 | Episodes: 55600 | Median Reward: 11.92 | Max Reward: 46.79
Iteration: 686 | Episodes: 55700 | Median Reward: 14.13 | Max Reward: 46.79
Iteration: 687 | Episodes: 55800 | Median Reward: 22.83 | Max Reward: 46.79
Iteration: 689 | Episodes: 55900 | Median Reward: 22.93 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.3         |
| time/                   |               |
|    fps                  | 160           |
|    iterations           | 690           |
|    time_elapsed         | 35295         |
|    total_timesteps      | 5652480       |
| train/                  |               |
|    approx_kl            | 0.00010664894 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -111          |
|    explained_variance   | 0.982         |
|    learning_rate        | 0.000707      |
|    loss                 | -2.1          |
|    n_updates            | 6890          |
|    policy_gradient_loss | -0.00129      |
|    std                  | 3.16          |
|    value_loss           | 6.43          |
-------------------------------------------
Iteration: 690 | Episodes: 56000 | Median Reward: 24.82 | Max Reward: 46.79
Iteration: 691 | Episodes: 56100 | Median Reward: 9.03 | Max Reward: 46.79
Iteration: 692 | Episodes: 56200 | Median Reward: 23.99 | Max Reward: 46.79
Iteration: 694 | Episodes: 56300 | Median Reward: 20.65 | Max Reward: 46.79
Iteration: 695 | Episodes: 56400 | Median Reward: 15.38 | Max Reward: 46.79
Iteration: 696 | Episodes: 56500 | Median Reward: 5.88 | Max Reward: 46.79
Iteration: 697 | Episodes: 56600 | Median Reward: 18.31 | Max Reward: 46.79
Iteration: 699 | Episodes: 56700 | Median Reward: 24.62 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -73.6         |
| time/                   |               |
|    fps                  | 160           |
|    iterations           | 700           |
|    time_elapsed         | 35795         |
|    total_timesteps      | 5734400       |
| train/                  |               |
|    approx_kl            | 0.00028366316 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -111          |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.000707      |
|    loss                 | -2.12         |
|    n_updates            | 6990          |
|    policy_gradient_loss | -0.00472      |
|    std                  | 3.16          |
|    value_loss           | 8.22          |
-------------------------------------------
Iteration: 700 | Episodes: 56800 | Median Reward: 28.36 | Max Reward: 46.79
Iteration: 701 | Episodes: 56900 | Median Reward: 18.95 | Max Reward: 46.79
Iteration: 702 | Episodes: 57000 | Median Reward: 26.25 | Max Reward: 46.79
Iteration: 703 | Episodes: 57100 | Median Reward: 26.82 | Max Reward: 46.79
Iteration: 705 | Episodes: 57200 | Median Reward: 25.18 | Max Reward: 46.79
Iteration: 706 | Episodes: 57300 | Median Reward: 24.69 | Max Reward: 46.79
Iteration: 707 | Episodes: 57400 | Median Reward: 24.51 | Max Reward: 46.79
Iteration: 708 | Episodes: 57500 | Median Reward: 13.08 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -89           |
| time/                   |               |
|    fps                  | 160           |
|    iterations           | 710           |
|    time_elapsed         | 36291         |
|    total_timesteps      | 5816320       |
| train/                  |               |
|    approx_kl            | 0.00045135483 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -111          |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.000707      |
|    loss                 | -1.81         |
|    n_updates            | 7090          |
|    policy_gradient_loss | -0.00174      |
|    std                  | 3.17          |
|    value_loss           | 7.09          |
-------------------------------------------
Iteration: 710 | Episodes: 57600 | Median Reward: 12.47 | Max Reward: 46.79
Iteration: 711 | Episodes: 57700 | Median Reward: 17.51 | Max Reward: 46.79
Iteration: 712 | Episodes: 57800 | Median Reward: 27.47 | Max Reward: 46.79
Iteration: 713 | Episodes: 57900 | Median Reward: 29.03 | Max Reward: 46.79
Iteration: 715 | Episodes: 58000 | Median Reward: 27.24 | Max Reward: 46.79
Iteration: 716 | Episodes: 58100 | Median Reward: 24.22 | Max Reward: 46.79
Iteration: 717 | Episodes: 58200 | Median Reward: 29.05 | Max Reward: 46.79
Iteration: 718 | Episodes: 58300 | Median Reward: 11.66 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -86.8        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 720          |
|    time_elapsed         | 36804        |
|    total_timesteps      | 5898240      |
| train/                  |              |
|    approx_kl            | 4.094321e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.000707     |
|    loss                 | -3.62        |
|    n_updates            | 7190         |
|    policy_gradient_loss | -0.000788    |
|    std                  | 3.17         |
|    value_loss           | 4.72         |
------------------------------------------
Iteration: 720 | Episodes: 58400 | Median Reward: 15.01 | Max Reward: 46.79
Iteration: 721 | Episodes: 58500 | Median Reward: 21.95 | Max Reward: 46.79
Iteration: 722 | Episodes: 58600 | Median Reward: 21.23 | Max Reward: 46.79
Iteration: 723 | Episodes: 58700 | Median Reward: 16.62 | Max Reward: 46.79
Iteration: 724 | Episodes: 58800 | Median Reward: 24.12 | Max Reward: 46.79
Iteration: 726 | Episodes: 58900 | Median Reward: 19.83 | Max Reward: 46.79
Iteration: 727 | Episodes: 59000 | Median Reward: 17.16 | Max Reward: 46.79
Iteration: 728 | Episodes: 59100 | Median Reward: 12.06 | Max Reward: 46.79
Iteration: 729 | Episodes: 59200 | Median Reward: 27.48 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.3        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 730          |
|    time_elapsed         | 37337        |
|    total_timesteps      | 5980160      |
| train/                  |              |
|    approx_kl            | 7.114529e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.000707     |
|    loss                 | -2.95        |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.000589    |
|    std                  | 3.17         |
|    value_loss           | 6.43         |
------------------------------------------
Iteration: 731 | Episodes: 59300 | Median Reward: 27.43 | Max Reward: 46.79
Iteration: 732 | Episodes: 59400 | Median Reward: 9.40 | Max Reward: 46.79
Iteration: 733 | Episodes: 59500 | Median Reward: 13.61 | Max Reward: 46.79
Iteration: 734 | Episodes: 59600 | Median Reward: 23.88 | Max Reward: 46.79
Iteration: 736 | Episodes: 59700 | Median Reward: 23.91 | Max Reward: 46.79
Iteration: 737 | Episodes: 59800 | Median Reward: 35.47 | Max Reward: 46.79
Iteration: 738 | Episodes: 59900 | Median Reward: 27.66 | Max Reward: 46.79
Iteration: 739 | Episodes: 60000 | Median Reward: 22.29 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.7         |
| time/                   |               |
|    fps                  | 160           |
|    iterations           | 740           |
|    time_elapsed         | 37847         |
|    total_timesteps      | 6062080       |
| train/                  |               |
|    approx_kl            | 1.7777827e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -111          |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.31         |
|    n_updates            | 7390          |
|    policy_gradient_loss | -0.000209     |
|    std                  | 3.18          |
|    value_loss           | 4.57          |
-------------------------------------------
Iteration: 740 | Episodes: 60100 | Median Reward: 22.29 | Max Reward: 46.79
Iteration: 742 | Episodes: 60200 | Median Reward: 17.97 | Max Reward: 46.79
Iteration: 743 | Episodes: 60300 | Median Reward: 25.00 | Max Reward: 46.79
Iteration: 744 | Episodes: 60400 | Median Reward: 8.14 | Max Reward: 46.79
Iteration: 745 | Episodes: 60500 | Median Reward: 12.16 | Max Reward: 46.79
Iteration: 747 | Episodes: 60600 | Median Reward: 22.67 | Max Reward: 46.79
Iteration: 748 | Episodes: 60700 | Median Reward: 29.98 | Max Reward: 46.79
Iteration: 749 | Episodes: 60800 | Median Reward: 22.06 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -86.8         |
| time/                   |               |
|    fps                  | 160           |
|    iterations           | 750           |
|    time_elapsed         | 38352         |
|    total_timesteps      | 6144000       |
| train/                  |               |
|    approx_kl            | 1.6845137e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -111          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.8          |
|    n_updates            | 7490          |
|    policy_gradient_loss | -0.000413     |
|    std                  | 3.18          |
|    value_loss           | 2.34          |
-------------------------------------------
Iteration: 750 | Episodes: 60900 | Median Reward: 10.90 | Max Reward: 46.79
Iteration: 752 | Episodes: 61000 | Median Reward: 26.97 | Max Reward: 46.79
Iteration: 753 | Episodes: 61100 | Median Reward: 25.63 | Max Reward: 46.79
Iteration: 754 | Episodes: 61200 | Median Reward: 17.76 | Max Reward: 46.79
Iteration: 755 | Episodes: 61300 | Median Reward: 15.45 | Max Reward: 46.79
Iteration: 757 | Episodes: 61400 | Median Reward: 27.29 | Max Reward: 46.79
Iteration: 758 | Episodes: 61500 | Median Reward: 21.98 | Max Reward: 46.79
Iteration: 759 | Episodes: 61600 | Median Reward: 16.95 | Max Reward: 46.79
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -85.4         |
| time/                   |               |
|    fps                  | 160           |
|    iterations           | 760           |
|    time_elapsed         | 38852         |
|    total_timesteps      | 6225920       |
| train/                  |               |
|    approx_kl            | 0.00025949703 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -111          |
|    explained_variance   | 0.988         |
|    learning_rate        | 0.000707      |
|    loss                 | -3.08         |
|    n_updates            | 7590          |
|    policy_gradient_loss | -0.00342      |
|    std                  | 3.19          |
|    value_loss           | 3.86          |
-------------------------------------------
Iteration: 760 | Episodes: 61700 | Median Reward: 21.62 | Max Reward: 46.79
Iteration: 761 | Episodes: 61800 | Median Reward: 22.90 | Max Reward: 46.79
Iteration: 763 | Episodes: 61900 | Median Reward: 15.98 | Max Reward: 46.79
Iteration: 764 | Episodes: 62000 | Median Reward: 13.15 | Max Reward: 46.79
Iteration: 765 | Episodes: 62100 | Median Reward: 23.18 | Max Reward: 46.79
Iteration: 766 | Episodes: 62200 | Median Reward: 20.11 | Max Reward: 46.79
Iteration: 768 | Episodes: 62300 | Median Reward: 14.57 | Max Reward: 46.79
Iteration: 769 | Episodes: 62400 | Median Reward: 21.48 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.9        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 770          |
|    time_elapsed         | 39356        |
|    total_timesteps      | 6307840      |
| train/                  |              |
|    approx_kl            | 0.0001253387 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.14        |
|    n_updates            | 7690         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 3.2          |
|    value_loss           | 2.36         |
------------------------------------------
Iteration: 770 | Episodes: 62500 | Median Reward: 30.42 | Max Reward: 46.79
Iteration: 771 | Episodes: 62600 | Median Reward: 30.43 | Max Reward: 46.79
Iteration: 773 | Episodes: 62700 | Median Reward: 24.60 | Max Reward: 46.79
Iteration: 774 | Episodes: 62800 | Median Reward: 21.22 | Max Reward: 46.79
Iteration: 775 | Episodes: 62900 | Median Reward: 27.08 | Max Reward: 46.79
Iteration: 776 | Episodes: 63000 | Median Reward: 29.52 | Max Reward: 46.79
Iteration: 777 | Episodes: 63100 | Median Reward: 34.36 | Max Reward: 46.79
Iteration: 779 | Episodes: 63200 | Median Reward: 21.95 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.7        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 780          |
|    time_elapsed         | 39861        |
|    total_timesteps      | 6389760      |
| train/                  |              |
|    approx_kl            | 0.0005666172 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.49        |
|    n_updates            | 7790         |
|    policy_gradient_loss | -0.00739     |
|    std                  | 3.2          |
|    value_loss           | 2.41         |
------------------------------------------
Iteration: 780 | Episodes: 63300 | Median Reward: 19.49 | Max Reward: 46.79
Iteration: 781 | Episodes: 63400 | Median Reward: 16.28 | Max Reward: 46.79
Iteration: 782 | Episodes: 63500 | Median Reward: 17.63 | Max Reward: 46.79
Iteration: 784 | Episodes: 63600 | Median Reward: 23.24 | Max Reward: 46.79
Iteration: 785 | Episodes: 63700 | Median Reward: 25.45 | Max Reward: 46.79
Iteration: 786 | Episodes: 63800 | Median Reward: 20.65 | Max Reward: 46.79
Iteration: 787 | Episodes: 63900 | Median Reward: 26.36 | Max Reward: 46.79
Iteration: 789 | Episodes: 64000 | Median Reward: 9.63 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -79.5        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 790          |
|    time_elapsed         | 40366        |
|    total_timesteps      | 6471680      |
| train/                  |              |
|    approx_kl            | 8.503867e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.000707     |
|    loss                 | -3.98        |
|    n_updates            | 7890         |
|    policy_gradient_loss | -0.000468    |
|    std                  | 3.2          |
|    value_loss           | 3.23         |
------------------------------------------
Iteration: 790 | Episodes: 64100 | Median Reward: 23.01 | Max Reward: 46.79
Iteration: 791 | Episodes: 64200 | Median Reward: 25.44 | Max Reward: 46.79
Iteration: 792 | Episodes: 64300 | Median Reward: 12.35 | Max Reward: 46.79
Iteration: 793 | Episodes: 64400 | Median Reward: 26.23 | Max Reward: 46.79
Iteration: 795 | Episodes: 64500 | Median Reward: 19.90 | Max Reward: 46.79
Iteration: 796 | Episodes: 64600 | Median Reward: 22.52 | Max Reward: 46.79
Iteration: 797 | Episodes: 64700 | Median Reward: 14.40 | Max Reward: 46.79
Iteration: 798 | Episodes: 64800 | Median Reward: 23.90 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -78.1        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 800          |
|    time_elapsed         | 40862        |
|    total_timesteps      | 6553600      |
| train/                  |              |
|    approx_kl            | 6.485274e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.000707     |
|    loss                 | -3.68        |
|    n_updates            | 7990         |
|    policy_gradient_loss | -0.000125    |
|    std                  | 3.21         |
|    value_loss           | 3.92         |
------------------------------------------
Iteration: 800 | Episodes: 64900 | Median Reward: 20.36 | Max Reward: 46.79
Iteration: 801 | Episodes: 65000 | Median Reward: 17.57 | Max Reward: 46.79
Iteration: 802 | Episodes: 65100 | Median Reward: 16.46 | Max Reward: 46.79
Iteration: 803 | Episodes: 65200 | Median Reward: 22.65 | Max Reward: 46.79
Iteration: 805 | Episodes: 65300 | Median Reward: 32.03 | Max Reward: 46.79
Iteration: 806 | Episodes: 65400 | Median Reward: 36.21 | Max Reward: 46.79
Iteration: 807 | Episodes: 65500 | Median Reward: 24.86 | Max Reward: 46.79
Iteration: 808 | Episodes: 65600 | Median Reward: 25.38 | Max Reward: 46.79
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.9        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 810          |
|    time_elapsed         | 41370        |
|    total_timesteps      | 6635520      |
| train/                  |              |
|    approx_kl            | 0.0022366457 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -111         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.7         |
|    n_updates            | 8090         |
|    policy_gradient_loss | -0.015       |
|    std                  | 3.21         |
|    value_loss           | 1.85         |
------------------------------------------
Iteration: 810 | Episodes: 65700 | Median Reward: 20.46 | Max Reward: 47.60
Iteration: 811 | Episodes: 65800 | Median Reward: 10.48 | Max Reward: 47.60
Iteration: 812 | Episodes: 65900 | Median Reward: 10.48 | Max Reward: 47.60
Iteration: 813 | Episodes: 66000 | Median Reward: 23.98 | Max Reward: 47.60
Iteration: 814 | Episodes: 66100 | Median Reward: 26.52 | Max Reward: 47.60
Iteration: 816 | Episodes: 66200 | Median Reward: 30.58 | Max Reward: 47.60
Iteration: 817 | Episodes: 66300 | Median Reward: 27.48 | Max Reward: 47.60
Iteration: 818 | Episodes: 66400 | Median Reward: 15.23 | Max Reward: 47.60
Iteration: 819 | Episodes: 66500 | Median Reward: 33.83 | Max Reward: 47.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.2        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 820          |
|    time_elapsed         | 41851        |
|    total_timesteps      | 6717440      |
| train/                  |              |
|    approx_kl            | 0.0004508228 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.09        |
|    n_updates            | 8190         |
|    policy_gradient_loss | -0.00343     |
|    std                  | 3.22         |
|    value_loss           | 7.49         |
------------------------------------------
Iteration: 821 | Episodes: 66600 | Median Reward: 30.55 | Max Reward: 47.60
Iteration: 822 | Episodes: 66700 | Median Reward: 25.19 | Max Reward: 47.60
Iteration: 823 | Episodes: 66800 | Median Reward: 23.38 | Max Reward: 47.60
Iteration: 824 | Episodes: 66900 | Median Reward: 18.51 | Max Reward: 47.60
Iteration: 826 | Episodes: 67000 | Median Reward: 19.15 | Max Reward: 47.60
Iteration: 827 | Episodes: 67100 | Median Reward: 25.71 | Max Reward: 47.60
Iteration: 828 | Episodes: 67200 | Median Reward: 24.42 | Max Reward: 47.60
Iteration: 829 | Episodes: 67300 | Median Reward: 27.60 | Max Reward: 47.60
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -74.9      |
| time/                   |            |
|    fps                  | 160        |
|    iterations           | 830        |
|    time_elapsed         | 42321      |
|    total_timesteps      | 6799360    |
| train/                  |            |
|    approx_kl            | 5.5782e-06 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -112       |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.000707   |
|    loss                 | -3.81      |
|    n_updates            | 8290       |
|    policy_gradient_loss | -0.00012   |
|    std                  | 3.22       |
|    value_loss           | 4.54       |
----------------------------------------
Iteration: 830 | Episodes: 67400 | Median Reward: 29.73 | Max Reward: 47.60
Iteration: 832 | Episodes: 67500 | Median Reward: 23.50 | Max Reward: 47.60
Iteration: 833 | Episodes: 67600 | Median Reward: 18.96 | Max Reward: 47.60
Iteration: 834 | Episodes: 67700 | Median Reward: 19.62 | Max Reward: 47.60
Iteration: 835 | Episodes: 67800 | Median Reward: 23.43 | Max Reward: 47.60
Iteration: 837 | Episodes: 67900 | Median Reward: 29.66 | Max Reward: 47.60
Iteration: 838 | Episodes: 68000 | Median Reward: 23.84 | Max Reward: 47.60
Iteration: 839 | Episodes: 68100 | Median Reward: 27.51 | Max Reward: 47.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74          |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 840          |
|    time_elapsed         | 42792        |
|    total_timesteps      | 6881280      |
| train/                  |              |
|    approx_kl            | 0.0018849713 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.000707     |
|    loss                 | -3.42        |
|    n_updates            | 8390         |
|    policy_gradient_loss | -0.0126      |
|    std                  | 3.23         |
|    value_loss           | 4.3          |
------------------------------------------
Iteration: 840 | Episodes: 68200 | Median Reward: 29.92 | Max Reward: 47.60
Iteration: 842 | Episodes: 68300 | Median Reward: 24.80 | Max Reward: 47.60
Iteration: 843 | Episodes: 68400 | Median Reward: 23.20 | Max Reward: 47.60
Iteration: 844 | Episodes: 68500 | Median Reward: 26.95 | Max Reward: 47.60
Iteration: 845 | Episodes: 68600 | Median Reward: 24.63 | Max Reward: 47.60
Iteration: 847 | Episodes: 68700 | Median Reward: 21.09 | Max Reward: 47.60
Iteration: 848 | Episodes: 68800 | Median Reward: 19.62 | Max Reward: 47.60
Iteration: 849 | Episodes: 68900 | Median Reward: 19.01 | Max Reward: 47.60
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76.6         |
| time/                   |               |
|    fps                  | 160           |
|    iterations           | 850           |
|    time_elapsed         | 43280         |
|    total_timesteps      | 6963200       |
| train/                  |               |
|    approx_kl            | 2.1119355e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.000707      |
|    loss                 | -3.5          |
|    n_updates            | 8490          |
|    policy_gradient_loss | -0.000501     |
|    std                  | 3.23          |
|    value_loss           | 4.26          |
-------------------------------------------
Iteration: 850 | Episodes: 69000 | Median Reward: 16.33 | Max Reward: 47.60
Iteration: 851 | Episodes: 69100 | Median Reward: 25.12 | Max Reward: 47.60
Iteration: 853 | Episodes: 69200 | Median Reward: 24.62 | Max Reward: 47.60
Iteration: 854 | Episodes: 69300 | Median Reward: 23.63 | Max Reward: 47.60
Iteration: 855 | Episodes: 69400 | Median Reward: 28.55 | Max Reward: 47.60
Iteration: 856 | Episodes: 69500 | Median Reward: 28.59 | Max Reward: 47.60
Iteration: 858 | Episodes: 69600 | Median Reward: 23.25 | Max Reward: 47.60
Iteration: 859 | Episodes: 69700 | Median Reward: 25.24 | Max Reward: 47.60
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -78.7         |
| time/                   |               |
|    fps                  | 160           |
|    iterations           | 860           |
|    time_elapsed         | 43826         |
|    total_timesteps      | 7045120       |
| train/                  |               |
|    approx_kl            | 2.1546148e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | 0.989         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.07         |
|    n_updates            | 8590          |
|    policy_gradient_loss | 7.62e-05      |
|    std                  | 3.24          |
|    value_loss           | 3.78          |
-------------------------------------------
Iteration: 860 | Episodes: 69800 | Median Reward: 26.79 | Max Reward: 47.60
Iteration: 861 | Episodes: 69900 | Median Reward: 21.33 | Max Reward: 47.60
Iteration: 863 | Episodes: 70000 | Median Reward: 26.48 | Max Reward: 47.60
Iteration: 864 | Episodes: 70100 | Median Reward: 30.97 | Max Reward: 47.60
Iteration: 865 | Episodes: 70200 | Median Reward: 30.64 | Max Reward: 47.60
Iteration: 866 | Episodes: 70300 | Median Reward: 19.64 | Max Reward: 47.60
Iteration: 867 | Episodes: 70400 | Median Reward: 17.29 | Max Reward: 47.60
Iteration: 869 | Episodes: 70500 | Median Reward: 21.78 | Max Reward: 47.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -76.3        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 870          |
|    time_elapsed         | 44363        |
|    total_timesteps      | 7127040      |
| train/                  |              |
|    approx_kl            | 0.0019416343 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.76        |
|    n_updates            | 8690         |
|    policy_gradient_loss | -0.00791     |
|    std                  | 3.24         |
|    value_loss           | 0.781        |
------------------------------------------
Iteration: 870 | Episodes: 70600 | Median Reward: 23.44 | Max Reward: 47.60
Iteration: 871 | Episodes: 70700 | Median Reward: 28.53 | Max Reward: 47.60
Iteration: 872 | Episodes: 70800 | Median Reward: 31.05 | Max Reward: 47.60
Iteration: 874 | Episodes: 70900 | Median Reward: 23.43 | Max Reward: 47.60
Iteration: 875 | Episodes: 71000 | Median Reward: 12.78 | Max Reward: 47.60
Iteration: 876 | Episodes: 71100 | Median Reward: 21.33 | Max Reward: 47.60
Iteration: 877 | Episodes: 71200 | Median Reward: 22.93 | Max Reward: 47.60
Iteration: 879 | Episodes: 71300 | Median Reward: 35.44 | Max Reward: 47.60
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.9         |
| time/                   |               |
|    fps                  | 160           |
|    iterations           | 880           |
|    time_elapsed         | 44903         |
|    total_timesteps      | 7208960       |
| train/                  |               |
|    approx_kl            | 0.00046852874 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.07         |
|    n_updates            | 8790          |
|    policy_gradient_loss | -0.000253     |
|    std                  | 3.24          |
|    value_loss           | 3.08          |
-------------------------------------------
Iteration: 880 | Episodes: 71400 | Median Reward: 34.26 | Max Reward: 47.60
Iteration: 881 | Episodes: 71500 | Median Reward: 22.35 | Max Reward: 47.60
Iteration: 882 | Episodes: 71600 | Median Reward: 24.61 | Max Reward: 47.60
Iteration: 883 | Episodes: 71700 | Median Reward: 26.63 | Max Reward: 47.60
Iteration: 885 | Episodes: 71800 | Median Reward: 30.80 | Max Reward: 47.60
Iteration: 886 | Episodes: 71900 | Median Reward: 26.46 | Max Reward: 47.60
Iteration: 887 | Episodes: 72000 | Median Reward: 29.15 | Max Reward: 47.60
Iteration: 888 | Episodes: 72100 | Median Reward: 14.92 | Max Reward: 47.60
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.9         |
| time/                   |               |
|    fps                  | 160           |
|    iterations           | 890           |
|    time_elapsed         | 45449         |
|    total_timesteps      | 7290880       |
| train/                  |               |
|    approx_kl            | 5.6697143e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.32         |
|    n_updates            | 8890          |
|    policy_gradient_loss | -0.000403     |
|    std                  | 3.25          |
|    value_loss           | 2.68          |
-------------------------------------------
Iteration: 890 | Episodes: 72200 | Median Reward: 39.36 | Max Reward: 47.60
Iteration: 891 | Episodes: 72300 | Median Reward: 21.45 | Max Reward: 47.60
Iteration: 892 | Episodes: 72400 | Median Reward: 20.27 | Max Reward: 47.60
Iteration: 893 | Episodes: 72500 | Median Reward: 23.58 | Max Reward: 47.60
Iteration: 895 | Episodes: 72600 | Median Reward: 20.68 | Max Reward: 47.60
Iteration: 896 | Episodes: 72700 | Median Reward: 19.43 | Max Reward: 47.60
Iteration: 897 | Episodes: 72800 | Median Reward: 28.44 | Max Reward: 47.60
Iteration: 898 | Episodes: 72900 | Median Reward: 27.07 | Max Reward: 47.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -72.2       |
| time/                   |             |
|    fps                  | 160         |
|    iterations           | 900         |
|    time_elapsed         | 45990       |
|    total_timesteps      | 7372800     |
| train/                  |             |
|    approx_kl            | 9.33612e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -112        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.02       |
|    n_updates            | 8990        |
|    policy_gradient_loss | -0.00118    |
|    std                  | 3.26        |
|    value_loss           | 4.37        |
-----------------------------------------
Iteration: 900 | Episodes: 73000 | Median Reward: 28.73 | Max Reward: 47.60
Iteration: 901 | Episodes: 73100 | Median Reward: 29.84 | Max Reward: 47.60
Iteration: 902 | Episodes: 73200 | Median Reward: 23.30 | Max Reward: 47.60
Iteration: 903 | Episodes: 73300 | Median Reward: 24.32 | Max Reward: 47.60
Iteration: 904 | Episodes: 73400 | Median Reward: 20.98 | Max Reward: 47.60
Iteration: 906 | Episodes: 73500 | Median Reward: 25.20 | Max Reward: 47.60
Iteration: 907 | Episodes: 73600 | Median Reward: 16.58 | Max Reward: 47.60
Iteration: 908 | Episodes: 73700 | Median Reward: 20.94 | Max Reward: 47.60
Iteration: 909 | Episodes: 73800 | Median Reward: 22.87 | Max Reward: 47.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -85.2        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 910          |
|    time_elapsed         | 46537        |
|    total_timesteps      | 7454720      |
| train/                  |              |
|    approx_kl            | 2.607055e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.02        |
|    n_updates            | 9090         |
|    policy_gradient_loss | 3.24e-05     |
|    std                  | 3.27         |
|    value_loss           | 2.01         |
------------------------------------------
Iteration: 911 | Episodes: 73900 | Median Reward: 27.15 | Max Reward: 47.60
Iteration: 912 | Episodes: 74000 | Median Reward: 31.72 | Max Reward: 47.60
Iteration: 913 | Episodes: 74100 | Median Reward: 19.62 | Max Reward: 47.60
Iteration: 914 | Episodes: 74200 | Median Reward: 26.98 | Max Reward: 47.60
Iteration: 916 | Episodes: 74300 | Median Reward: 26.47 | Max Reward: 47.60
Iteration: 917 | Episodes: 74400 | Median Reward: 23.27 | Max Reward: 47.60
Iteration: 918 | Episodes: 74500 | Median Reward: 26.76 | Max Reward: 47.60
Iteration: 919 | Episodes: 74600 | Median Reward: 23.93 | Max Reward: 47.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.1        |
| time/                   |              |
|    fps                  | 160          |
|    iterations           | 920          |
|    time_elapsed         | 47080        |
|    total_timesteps      | 7536640      |
| train/                  |              |
|    approx_kl            | 0.0004740656 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.04        |
|    n_updates            | 9190         |
|    policy_gradient_loss | -0.00473     |
|    std                  | 3.28         |
|    value_loss           | 2.4          |
------------------------------------------
Iteration: 920 | Episodes: 74700 | Median Reward: 27.41 | Max Reward: 47.60
Iteration: 922 | Episodes: 74800 | Median Reward: 33.80 | Max Reward: 47.60
Iteration: 923 | Episodes: 74900 | Median Reward: 32.66 | Max Reward: 47.60
Iteration: 924 | Episodes: 75000 | Median Reward: 23.95 | Max Reward: 47.60
Iteration: 925 | Episodes: 75100 | Median Reward: 20.96 | Max Reward: 47.60
Iteration: 927 | Episodes: 75200 | Median Reward: 29.95 | Max Reward: 47.60
Iteration: 928 | Episodes: 75300 | Median Reward: 29.95 | Max Reward: 47.60
Iteration: 929 | Episodes: 75400 | Median Reward: 20.57 | Max Reward: 47.60
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -76           |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 930           |
|    time_elapsed         | 47621         |
|    total_timesteps      | 7618560       |
| train/                  |               |
|    approx_kl            | 5.2056384e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | 0.994         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.84         |
|    n_updates            | 9290          |
|    policy_gradient_loss | -0.000281     |
|    std                  | 3.29          |
|    value_loss           | 2.21          |
-------------------------------------------
Iteration: 930 | Episodes: 75500 | Median Reward: 19.63 | Max Reward: 47.60
Iteration: 932 | Episodes: 75600 | Median Reward: 22.47 | Max Reward: 47.60
Iteration: 933 | Episodes: 75700 | Median Reward: 21.73 | Max Reward: 47.60
Iteration: 934 | Episodes: 75800 | Median Reward: 17.80 | Max Reward: 47.60
Iteration: 935 | Episodes: 75900 | Median Reward: 13.15 | Max Reward: 47.60
Iteration: 937 | Episodes: 76000 | Median Reward: 24.94 | Max Reward: 47.60
Iteration: 938 | Episodes: 76100 | Median Reward: 27.56 | Max Reward: 47.60
Iteration: 939 | Episodes: 76200 | Median Reward: 27.10 | Max Reward: 47.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -75          |
| time/                   |              |
|    fps                  | 159          |
|    iterations           | 940          |
|    time_elapsed         | 48164        |
|    total_timesteps      | 7700480      |
| train/                  |              |
|    approx_kl            | 0.0034013109 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -112         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.24        |
|    n_updates            | 9390         |
|    policy_gradient_loss | -0.00972     |
|    std                  | 3.3          |
|    value_loss           | 2.17         |
------------------------------------------
Iteration: 940 | Episodes: 76300 | Median Reward: 20.34 | Max Reward: 47.60
Iteration: 941 | Episodes: 76400 | Median Reward: 17.88 | Max Reward: 47.60
Iteration: 943 | Episodes: 76500 | Median Reward: 20.80 | Max Reward: 47.60
Iteration: 944 | Episodes: 76600 | Median Reward: 19.08 | Max Reward: 47.60
Iteration: 945 | Episodes: 76700 | Median Reward: 37.21 | Max Reward: 47.60
Iteration: 946 | Episodes: 76800 | Median Reward: 29.90 | Max Reward: 47.60
Iteration: 948 | Episodes: 76900 | Median Reward: 18.48 | Max Reward: 47.60
Iteration: 949 | Episodes: 77000 | Median Reward: 29.00 | Max Reward: 47.60
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71.9         |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 950           |
|    time_elapsed         | 48712         |
|    total_timesteps      | 7782400       |
| train/                  |               |
|    approx_kl            | 0.00029760986 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -112          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.05         |
|    n_updates            | 9490          |
|    policy_gradient_loss | 0.000503      |
|    std                  | 3.32          |
|    value_loss           | 1.47          |
-------------------------------------------
Iteration: 950 | Episodes: 77100 | Median Reward: 27.19 | Max Reward: 47.60
Iteration: 951 | Episodes: 77200 | Median Reward: 25.01 | Max Reward: 47.60
Iteration: 953 | Episodes: 77300 | Median Reward: 31.07 | Max Reward: 47.60
Iteration: 954 | Episodes: 77400 | Median Reward: 31.68 | Max Reward: 47.60
Iteration: 955 | Episodes: 77500 | Median Reward: 21.24 | Max Reward: 47.60
Iteration: 956 | Episodes: 77600 | Median Reward: 23.26 | Max Reward: 47.60
Iteration: 957 | Episodes: 77700 | Median Reward: 32.91 | Max Reward: 47.60
Iteration: 959 | Episodes: 77800 | Median Reward: 33.69 | Max Reward: 47.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.3        |
| time/                   |              |
|    fps                  | 159          |
|    iterations           | 960          |
|    time_elapsed         | 49251        |
|    total_timesteps      | 7864320      |
| train/                  |              |
|    approx_kl            | 7.475838e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -113         |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.3         |
|    n_updates            | 9590         |
|    policy_gradient_loss | -0.00091     |
|    std                  | 3.33         |
|    value_loss           | 4.96         |
------------------------------------------
Iteration: 960 | Episodes: 77900 | Median Reward: 29.78 | Max Reward: 47.60
Iteration: 961 | Episodes: 78000 | Median Reward: 35.41 | Max Reward: 47.60
Iteration: 962 | Episodes: 78100 | Median Reward: 32.13 | Max Reward: 47.60
Iteration: 964 | Episodes: 78200 | Median Reward: 21.29 | Max Reward: 47.60
Iteration: 965 | Episodes: 78300 | Median Reward: 18.87 | Max Reward: 47.60
Iteration: 966 | Episodes: 78400 | Median Reward: 19.86 | Max Reward: 47.60
Iteration: 967 | Episodes: 78500 | Median Reward: 14.07 | Max Reward: 47.60
Iteration: 969 | Episodes: 78600 | Median Reward: 20.72 | Max Reward: 47.60
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -75           |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 970           |
|    time_elapsed         | 49791         |
|    total_timesteps      | 7946240       |
| train/                  |               |
|    approx_kl            | 0.00021610741 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | 0.991         |
|    learning_rate        | 0.000707      |
|    loss                 | -3.99         |
|    n_updates            | 9690          |
|    policy_gradient_loss | -0.00182      |
|    std                  | 3.34          |
|    value_loss           | 3.48          |
-------------------------------------------
Iteration: 970 | Episodes: 78700 | Median Reward: 19.33 | Max Reward: 47.60
Iteration: 971 | Episodes: 78800 | Median Reward: 20.32 | Max Reward: 47.60
Iteration: 972 | Episodes: 78900 | Median Reward: 38.01 | Max Reward: 47.60
Iteration: 973 | Episodes: 79000 | Median Reward: 25.64 | Max Reward: 47.60
Iteration: 975 | Episodes: 79100 | Median Reward: 22.08 | Max Reward: 47.60
Iteration: 976 | Episodes: 79200 | Median Reward: 29.52 | Max Reward: 47.60
Iteration: 977 | Episodes: 79300 | Median Reward: 35.26 | Max Reward: 47.60
Iteration: 978 | Episodes: 79400 | Median Reward: 32.78 | Max Reward: 47.60
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74.9       |
| time/                   |             |
|    fps                  | 159         |
|    iterations           | 980         |
|    time_elapsed         | 50332       |
|    total_timesteps      | 8028160     |
| train/                  |             |
|    approx_kl            | 0.003345003 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -113        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.2        |
|    n_updates            | 9790        |
|    policy_gradient_loss | -0.0218     |
|    std                  | 3.35        |
|    value_loss           | 1.73        |
-----------------------------------------
Iteration: 980 | Episodes: 79500 | Median Reward: 23.41 | Max Reward: 47.60
Iteration: 981 | Episodes: 79600 | Median Reward: 34.05 | Max Reward: 47.60
Iteration: 982 | Episodes: 79700 | Median Reward: 26.29 | Max Reward: 47.60
Iteration: 983 | Episodes: 79800 | Median Reward: 24.39 | Max Reward: 47.60
Iteration: 985 | Episodes: 79900 | Median Reward: 31.35 | Max Reward: 47.60
Iteration: 986 | Episodes: 80000 | Median Reward: 31.46 | Max Reward: 47.60
Iteration: 987 | Episodes: 80100 | Median Reward: 30.60 | Max Reward: 47.60
Iteration: 988 | Episodes: 80200 | Median Reward: 30.32 | Max Reward: 47.60
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.5         |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 990           |
|    time_elapsed         | 50883         |
|    total_timesteps      | 8110080       |
| train/                  |               |
|    approx_kl            | 0.00012090547 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.06         |
|    n_updates            | 9890          |
|    policy_gradient_loss | -0.00128      |
|    std                  | 3.35          |
|    value_loss           | 3             |
-------------------------------------------
Iteration: 990 | Episodes: 80300 | Median Reward: 33.31 | Max Reward: 47.60
Iteration: 991 | Episodes: 80400 | Median Reward: 26.25 | Max Reward: 47.60
Iteration: 992 | Episodes: 80500 | Median Reward: 21.53 | Max Reward: 47.60
Iteration: 993 | Episodes: 80600 | Median Reward: 20.96 | Max Reward: 47.60
Iteration: 994 | Episodes: 80700 | Median Reward: 15.54 | Max Reward: 47.60
Iteration: 996 | Episodes: 80800 | Median Reward: 23.87 | Max Reward: 47.60
Iteration: 997 | Episodes: 80900 | Median Reward: 11.86 | Max Reward: 47.60
Iteration: 998 | Episodes: 81000 | Median Reward: 23.33 | Max Reward: 47.60
Iteration: 999 | Episodes: 81100 | Median Reward: 28.27 | Max Reward: 47.60
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -74.7        |
| time/                   |              |
|    fps                  | 159          |
|    iterations           | 1000         |
|    time_elapsed         | 51426        |
|    total_timesteps      | 8192000      |
| train/                  |              |
|    approx_kl            | 0.0010383804 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -113         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.000707     |
|    loss                 | -3.6         |
|    n_updates            | 9990         |
|    policy_gradient_loss | -0.00476     |
|    std                  | 3.37         |
|    value_loss           | 4.38         |
------------------------------------------
Iteration: 1001 | Episodes: 81200 | Median Reward: 31.08 | Max Reward: 47.60
Iteration: 1002 | Episodes: 81300 | Median Reward: 33.55 | Max Reward: 47.60
Iteration: 1003 | Episodes: 81400 | Median Reward: 32.15 | Max Reward: 47.60
Iteration: 1004 | Episodes: 81500 | Median Reward: 24.76 | Max Reward: 47.60
Iteration: 1006 | Episodes: 81600 | Median Reward: 25.17 | Max Reward: 47.60
Iteration: 1007 | Episodes: 81700 | Median Reward: 27.56 | Max Reward: 47.77
Iteration: 1008 | Episodes: 81800 | Median Reward: 29.78 | Max Reward: 47.77
Iteration: 1009 | Episodes: 81900 | Median Reward: 31.12 | Max Reward: 47.77
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.1         |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 1010          |
|    time_elapsed         | 51972         |
|    total_timesteps      | 8273920       |
| train/                  |               |
|    approx_kl            | 1.9570653e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | 0.99          |
|    learning_rate        | 0.000707      |
|    loss                 | -3.24         |
|    n_updates            | 10090         |
|    policy_gradient_loss | -1.93e-05     |
|    std                  | 3.38          |
|    value_loss           | 4.12          |
-------------------------------------------
Iteration: 1010 | Episodes: 82000 | Median Reward: 35.26 | Max Reward: 47.77
Iteration: 1012 | Episodes: 82100 | Median Reward: 37.54 | Max Reward: 47.77
Iteration: 1013 | Episodes: 82200 | Median Reward: 22.69 | Max Reward: 47.77
Iteration: 1014 | Episodes: 82300 | Median Reward: 33.05 | Max Reward: 47.77
Iteration: 1015 | Episodes: 82400 | Median Reward: 29.76 | Max Reward: 47.77
Iteration: 1017 | Episodes: 82500 | Median Reward: 23.23 | Max Reward: 47.77
Iteration: 1018 | Episodes: 82600 | Median Reward: 23.23 | Max Reward: 47.77
Iteration: 1019 | Episodes: 82700 | Median Reward: 31.34 | Max Reward: 47.77
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -70.9         |
| time/                   |               |
|    fps                  | 159           |
|    iterations           | 1020          |
|    time_elapsed         | 52526         |
|    total_timesteps      | 8355840       |
| train/                  |               |
|    approx_kl            | 0.00063379656 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.92         |
|    n_updates            | 10190         |
|    policy_gradient_loss | -0.00552      |
|    std                  | 3.4           |
|    value_loss           | 1.95          |
-------------------------------------------
Iteration: 1020 | Episodes: 82800 | Median Reward: 26.61 | Max Reward: 47.77
Iteration: 1022 | Episodes: 82900 | Median Reward: 24.31 | Max Reward: 47.77
Iteration: 1023 | Episodes: 83000 | Median Reward: 22.66 | Max Reward: 47.77
Iteration: 1024 | Episodes: 83100 | Median Reward: 23.35 | Max Reward: 47.77
Iteration: 1025 | Episodes: 83200 | Median Reward: 30.26 | Max Reward: 47.77
Iteration: 1027 | Episodes: 83300 | Median Reward: 28.12 | Max Reward: 47.77
Iteration: 1028 | Episodes: 83400 | Median Reward: 35.85 | Max Reward: 47.77
Iteration: 1029 | Episodes: 83500 | Median Reward: 17.62 | Max Reward: 47.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -77.5        |
| time/                   |              |
|    fps                  | 158          |
|    iterations           | 1030         |
|    time_elapsed         | 53072        |
|    total_timesteps      | 8437760      |
| train/                  |              |
|    approx_kl            | 0.0011962487 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -113         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.07        |
|    n_updates            | 10290        |
|    policy_gradient_loss | -0.00299     |
|    std                  | 3.4          |
|    value_loss           | 1.86         |
------------------------------------------
Iteration: 1030 | Episodes: 83600 | Median Reward: 37.34 | Max Reward: 47.77
Iteration: 1031 | Episodes: 83700 | Median Reward: 38.68 | Max Reward: 47.77
Iteration: 1033 | Episodes: 83800 | Median Reward: 23.42 | Max Reward: 47.77
Iteration: 1034 | Episodes: 83900 | Median Reward: 25.12 | Max Reward: 47.77
Iteration: 1035 | Episodes: 84000 | Median Reward: 25.01 | Max Reward: 47.77
Iteration: 1036 | Episodes: 84100 | Median Reward: 16.69 | Max Reward: 47.77
Iteration: 1038 | Episodes: 84200 | Median Reward: 27.15 | Max Reward: 47.77
Iteration: 1039 | Episodes: 84300 | Median Reward: 23.65 | Max Reward: 47.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -78.3       |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 1040        |
|    time_elapsed         | 53622       |
|    total_timesteps      | 8519680     |
| train/                  |             |
|    approx_kl            | 0.007940674 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -113        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.000707    |
|    loss                 | -3.96       |
|    n_updates            | 10390       |
|    policy_gradient_loss | -0.0226     |
|    std                  | 3.41        |
|    value_loss           | 4.3         |
-----------------------------------------
Iteration: 1040 | Episodes: 84400 | Median Reward: 25.97 | Max Reward: 47.77
Iteration: 1041 | Episodes: 84500 | Median Reward: 29.23 | Max Reward: 47.77
Iteration: 1043 | Episodes: 84600 | Median Reward: 32.88 | Max Reward: 47.77
Iteration: 1044 | Episodes: 84700 | Median Reward: 23.70 | Max Reward: 47.77
Iteration: 1045 | Episodes: 84800 | Median Reward: 29.90 | Max Reward: 47.77
Iteration: 1046 | Episodes: 84900 | Median Reward: 23.30 | Max Reward: 47.77
Iteration: 1047 | Episodes: 85000 | Median Reward: 21.22 | Max Reward: 47.77
Iteration: 1049 | Episodes: 85100 | Median Reward: 25.62 | Max Reward: 47.77
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -72        |
| time/                   |            |
|    fps                  | 158        |
|    iterations           | 1050       |
|    time_elapsed         | 54175      |
|    total_timesteps      | 8601600    |
| train/                  |            |
|    approx_kl            | 0.04905391 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -113       |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.000707   |
|    loss                 | -5.27      |
|    n_updates            | 10490      |
|    policy_gradient_loss | -0.0699    |
|    std                  | 3.42       |
|    value_loss           | 1.26       |
----------------------------------------
Iteration: 1050 | Episodes: 85200 | Median Reward: 26.07 | Max Reward: 47.77
Iteration: 1051 | Episodes: 85300 | Median Reward: 28.99 | Max Reward: 47.77
Iteration: 1052 | Episodes: 85400 | Median Reward: 34.62 | Max Reward: 47.77
Iteration: 1054 | Episodes: 85500 | Median Reward: 22.78 | Max Reward: 47.77
Iteration: 1055 | Episodes: 85600 | Median Reward: 22.78 | Max Reward: 47.77
Iteration: 1056 | Episodes: 85700 | Median Reward: 33.06 | Max Reward: 47.77
Iteration: 1057 | Episodes: 85800 | Median Reward: 14.96 | Max Reward: 47.77
Iteration: 1059 | Episodes: 85900 | Median Reward: 20.62 | Max Reward: 47.77
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -80           |
| time/                   |               |
|    fps                  | 158           |
|    iterations           | 1060          |
|    time_elapsed         | 54730         |
|    total_timesteps      | 8683520       |
| train/                  |               |
|    approx_kl            | 8.0477475e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | 0.992         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.85         |
|    n_updates            | 10590         |
|    policy_gradient_loss | 3.6e-05       |
|    std                  | 3.43          |
|    value_loss           | 2.56          |
-------------------------------------------
Iteration: 1060 | Episodes: 86000 | Median Reward: 30.04 | Max Reward: 47.77
Iteration: 1061 | Episodes: 86100 | Median Reward: 35.79 | Max Reward: 47.77
Iteration: 1062 | Episodes: 86200 | Median Reward: 35.06 | Max Reward: 47.77
Iteration: 1064 | Episodes: 86300 | Median Reward: 26.01 | Max Reward: 47.77
Iteration: 1065 | Episodes: 86400 | Median Reward: 28.43 | Max Reward: 47.77
Iteration: 1066 | Episodes: 86500 | Median Reward: 29.84 | Max Reward: 47.77
Iteration: 1067 | Episodes: 86600 | Median Reward: 27.72 | Max Reward: 47.77
Iteration: 1068 | Episodes: 86700 | Median Reward: 30.88 | Max Reward: 47.77
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.7         |
| time/                   |               |
|    fps                  | 158           |
|    iterations           | 1070          |
|    time_elapsed         | 55266         |
|    total_timesteps      | 8765440       |
| train/                  |               |
|    approx_kl            | 0.00020799994 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | 0.991         |
|    learning_rate        | 0.000707      |
|    loss                 | -3.84         |
|    n_updates            | 10690         |
|    policy_gradient_loss | -0.000917     |
|    std                  | 3.45          |
|    value_loss           | 4.87          |
-------------------------------------------
Iteration: 1070 | Episodes: 86800 | Median Reward: 33.58 | Max Reward: 47.77
Iteration: 1071 | Episodes: 86900 | Median Reward: 29.57 | Max Reward: 47.77
Iteration: 1072 | Episodes: 87000 | Median Reward: 29.87 | Max Reward: 47.77
Iteration: 1073 | Episodes: 87100 | Median Reward: 33.27 | Max Reward: 47.77
Iteration: 1075 | Episodes: 87200 | Median Reward: 28.72 | Max Reward: 47.77
Iteration: 1076 | Episodes: 87300 | Median Reward: 20.72 | Max Reward: 47.77
Iteration: 1077 | Episodes: 87400 | Median Reward: 29.59 | Max Reward: 47.77
Iteration: 1078 | Episodes: 87500 | Median Reward: 35.16 | Max Reward: 47.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.1        |
| time/                   |              |
|    fps                  | 158          |
|    iterations           | 1080         |
|    time_elapsed         | 55810        |
|    total_timesteps      | 8847360      |
| train/                  |              |
|    approx_kl            | 0.0006178708 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -113         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.42        |
|    n_updates            | 10790        |
|    policy_gradient_loss | -0.00231     |
|    std                  | 3.46         |
|    value_loss           | 2.89         |
------------------------------------------
Iteration: 1080 | Episodes: 87600 | Median Reward: 31.21 | Max Reward: 47.77
Iteration: 1081 | Episodes: 87700 | Median Reward: 29.76 | Max Reward: 47.77
Iteration: 1082 | Episodes: 87800 | Median Reward: 31.88 | Max Reward: 47.77
Iteration: 1083 | Episodes: 87900 | Median Reward: 27.96 | Max Reward: 47.77
Iteration: 1084 | Episodes: 88000 | Median Reward: 29.92 | Max Reward: 47.77
Iteration: 1086 | Episodes: 88100 | Median Reward: 25.20 | Max Reward: 47.77
Iteration: 1087 | Episodes: 88200 | Median Reward: 40.34 | Max Reward: 47.77
Iteration: 1088 | Episodes: 88300 | Median Reward: 36.82 | Max Reward: 47.77
Iteration: 1089 | Episodes: 88400 | Median Reward: 32.37 | Max Reward: 47.77
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.7         |
| time/                   |               |
|    fps                  | 158           |
|    iterations           | 1090          |
|    time_elapsed         | 56359         |
|    total_timesteps      | 8929280       |
| train/                  |               |
|    approx_kl            | 0.00089222693 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -113          |
|    explained_variance   | 0.99          |
|    learning_rate        | 0.000707      |
|    loss                 | -4.58         |
|    n_updates            | 10890         |
|    policy_gradient_loss | -0.00403      |
|    std                  | 3.47          |
|    value_loss           | 2.89          |
-------------------------------------------
Iteration: 1091 | Episodes: 88500 | Median Reward: 25.36 | Max Reward: 47.77
Iteration: 1092 | Episodes: 88600 | Median Reward: 31.18 | Max Reward: 47.77
Iteration: 1093 | Episodes: 88700 | Median Reward: 26.26 | Max Reward: 47.77
Iteration: 1094 | Episodes: 88800 | Median Reward: 38.19 | Max Reward: 47.77
Iteration: 1096 | Episodes: 88900 | Median Reward: 29.01 | Max Reward: 47.77
Iteration: 1097 | Episodes: 89000 | Median Reward: 31.44 | Max Reward: 47.77
Iteration: 1098 | Episodes: 89100 | Median Reward: 41.71 | Max Reward: 47.77
Iteration: 1099 | Episodes: 89200 | Median Reward: 30.32 | Max Reward: 47.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68.9       |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 1100        |
|    time_elapsed         | 56906       |
|    total_timesteps      | 9011200     |
| train/                  |             |
|    approx_kl            | 0.003137861 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -113        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.22       |
|    n_updates            | 10990       |
|    policy_gradient_loss | -0.0145     |
|    std                  | 3.48        |
|    value_loss           | 2.57        |
-----------------------------------------
Iteration: 1100 | Episodes: 89300 | Median Reward: 31.10 | Max Reward: 47.77
Iteration: 1102 | Episodes: 89400 | Median Reward: 32.09 | Max Reward: 47.77
Iteration: 1103 | Episodes: 89500 | Median Reward: 30.01 | Max Reward: 47.77
Iteration: 1104 | Episodes: 89600 | Median Reward: 35.68 | Max Reward: 47.77
Iteration: 1105 | Episodes: 89700 | Median Reward: 29.79 | Max Reward: 47.77
Iteration: 1107 | Episodes: 89800 | Median Reward: 34.71 | Max Reward: 47.77
Iteration: 1108 | Episodes: 89900 | Median Reward: 34.71 | Max Reward: 47.77
Iteration: 1109 | Episodes: 90000 | Median Reward: 27.12 | Max Reward: 47.77
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -72.4         |
| time/                   |               |
|    fps                  | 158           |
|    iterations           | 1110          |
|    time_elapsed         | 57452         |
|    total_timesteps      | 9093120       |
| train/                  |               |
|    approx_kl            | 6.0186743e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -114          |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.000707      |
|    loss                 | -2.59         |
|    n_updates            | 11090         |
|    policy_gradient_loss | -0.000361     |
|    std                  | 3.5           |
|    value_loss           | 3.71          |
-------------------------------------------
Iteration: 1110 | Episodes: 90100 | Median Reward: 27.64 | Max Reward: 47.77
Iteration: 1112 | Episodes: 90200 | Median Reward: 31.81 | Max Reward: 47.77
Iteration: 1113 | Episodes: 90300 | Median Reward: 32.68 | Max Reward: 47.77
Iteration: 1114 | Episodes: 90400 | Median Reward: 34.58 | Max Reward: 47.77
Iteration: 1115 | Episodes: 90500 | Median Reward: 37.96 | Max Reward: 47.77
Iteration: 1117 | Episodes: 90600 | Median Reward: 31.70 | Max Reward: 47.77
Iteration: 1118 | Episodes: 90700 | Median Reward: 30.10 | Max Reward: 47.77
Iteration: 1119 | Episodes: 90800 | Median Reward: 32.82 | Max Reward: 47.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68         |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 1120        |
|    time_elapsed         | 58000       |
|    total_timesteps      | 9175040     |
| train/                  |             |
|    approx_kl            | 0.000473069 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -114        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.18       |
|    n_updates            | 11190       |
|    policy_gradient_loss | -0.000324   |
|    std                  | 3.51        |
|    value_loss           | 1.29        |
-----------------------------------------
Iteration: 1120 | Episodes: 90900 | Median Reward: 33.12 | Max Reward: 47.77
Iteration: 1121 | Episodes: 91000 | Median Reward: 33.14 | Max Reward: 47.77
Iteration: 1123 | Episodes: 91100 | Median Reward: 28.88 | Max Reward: 47.77
Iteration: 1124 | Episodes: 91200 | Median Reward: 26.27 | Max Reward: 47.77
Iteration: 1125 | Episodes: 91300 | Median Reward: 26.95 | Max Reward: 47.77
Iteration: 1126 | Episodes: 91400 | Median Reward: 18.73 | Max Reward: 47.77
Iteration: 1128 | Episodes: 91500 | Median Reward: 32.21 | Max Reward: 47.77
Iteration: 1129 | Episodes: 91600 | Median Reward: 33.68 | Max Reward: 47.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -73          |
| time/                   |              |
|    fps                  | 158          |
|    iterations           | 1130         |
|    time_elapsed         | 58539        |
|    total_timesteps      | 9256960      |
| train/                  |              |
|    approx_kl            | 0.0012609651 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -114         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.45        |
|    n_updates            | 11290        |
|    policy_gradient_loss | -0.00145     |
|    std                  | 3.52         |
|    value_loss           | 1.06         |
------------------------------------------
Iteration: 1130 | Episodes: 91700 | Median Reward: 29.06 | Max Reward: 47.77
Iteration: 1131 | Episodes: 91800 | Median Reward: 20.41 | Max Reward: 47.77
Iteration: 1133 | Episodes: 91900 | Median Reward: 30.69 | Max Reward: 47.77
Iteration: 1134 | Episodes: 92000 | Median Reward: 36.10 | Max Reward: 47.77
Iteration: 1135 | Episodes: 92100 | Median Reward: 34.28 | Max Reward: 47.77
Iteration: 1136 | Episodes: 92200 | Median Reward: 29.20 | Max Reward: 47.77
Iteration: 1137 | Episodes: 92300 | Median Reward: 34.61 | Max Reward: 47.77
Iteration: 1139 | Episodes: 92400 | Median Reward: 30.32 | Max Reward: 47.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.3        |
| time/                   |              |
|    fps                  | 158          |
|    iterations           | 1140         |
|    time_elapsed         | 59087        |
|    total_timesteps      | 9338880      |
| train/                  |              |
|    approx_kl            | 0.0020297307 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -114         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.96        |
|    n_updates            | 11390        |
|    policy_gradient_loss | -0.0117      |
|    std                  | 3.55         |
|    value_loss           | 1.04         |
------------------------------------------
Iteration: 1140 | Episodes: 92500 | Median Reward: 36.48 | Max Reward: 47.77
Iteration: 1141 | Episodes: 92600 | Median Reward: 34.22 | Max Reward: 47.77
Iteration: 1142 | Episodes: 92700 | Median Reward: 34.79 | Max Reward: 47.77
Iteration: 1144 | Episodes: 92800 | Median Reward: 35.80 | Max Reward: 47.77
Iteration: 1145 | Episodes: 92900 | Median Reward: 35.25 | Max Reward: 47.77
Iteration: 1146 | Episodes: 93000 | Median Reward: 32.03 | Max Reward: 47.77
Iteration: 1147 | Episodes: 93100 | Median Reward: 30.96 | Max Reward: 47.77
Iteration: 1149 | Episodes: 93200 | Median Reward: 32.67 | Max Reward: 47.77
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.8         |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 1150          |
|    time_elapsed         | 59638         |
|    total_timesteps      | 9420800       |
| train/                  |               |
|    approx_kl            | 0.00020181571 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -114          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.57         |
|    n_updates            | 11490         |
|    policy_gradient_loss | -0.000796     |
|    std                  | 3.57          |
|    value_loss           | 1.9           |
-------------------------------------------
Iteration: 1150 | Episodes: 93300 | Median Reward: 33.35 | Max Reward: 47.77
Iteration: 1151 | Episodes: 93400 | Median Reward: 28.16 | Max Reward: 47.77
Iteration: 1152 | Episodes: 93500 | Median Reward: 31.97 | Max Reward: 47.77
Iteration: 1154 | Episodes: 93600 | Median Reward: 17.79 | Max Reward: 47.77
Iteration: 1155 | Episodes: 93700 | Median Reward: 33.08 | Max Reward: 47.77
Iteration: 1156 | Episodes: 93800 | Median Reward: 39.20 | Max Reward: 47.77
Iteration: 1157 | Episodes: 93900 | Median Reward: 37.14 | Max Reward: 47.77
Iteration: 1158 | Episodes: 94000 | Median Reward: 31.86 | Max Reward: 47.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.8        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 1160         |
|    time_elapsed         | 60179        |
|    total_timesteps      | 9502720      |
| train/                  |              |
|    approx_kl            | 0.0037597257 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -114         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.27        |
|    n_updates            | 11590        |
|    policy_gradient_loss | -0.0135      |
|    std                  | 3.59         |
|    value_loss           | 1.56         |
------------------------------------------
Iteration: 1160 | Episodes: 94100 | Median Reward: 32.92 | Max Reward: 47.77
Iteration: 1161 | Episodes: 94200 | Median Reward: 33.59 | Max Reward: 47.77
Iteration: 1162 | Episodes: 94300 | Median Reward: 27.82 | Max Reward: 47.77
Iteration: 1163 | Episodes: 94400 | Median Reward: 27.65 | Max Reward: 47.77
Iteration: 1165 | Episodes: 94500 | Median Reward: 38.26 | Max Reward: 47.77
Iteration: 1166 | Episodes: 94600 | Median Reward: 30.66 | Max Reward: 47.77
Iteration: 1167 | Episodes: 94700 | Median Reward: 31.77 | Max Reward: 47.77
Iteration: 1168 | Episodes: 94800 | Median Reward: 25.92 | Max Reward: 47.77
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -74.3         |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 1170          |
|    time_elapsed         | 60726         |
|    total_timesteps      | 9584640       |
| train/                  |               |
|    approx_kl            | 0.00050619507 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -114          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.86         |
|    n_updates            | 11690         |
|    policy_gradient_loss | 0.000542      |
|    std                  | 3.61          |
|    value_loss           | 1.48          |
-------------------------------------------
Iteration: 1170 | Episodes: 94900 | Median Reward: 25.38 | Max Reward: 47.77
Iteration: 1171 | Episodes: 95000 | Median Reward: 33.30 | Max Reward: 47.77
Iteration: 1172 | Episodes: 95100 | Median Reward: 35.05 | Max Reward: 47.77
Iteration: 1173 | Episodes: 95200 | Median Reward: 34.08 | Max Reward: 47.77
Iteration: 1174 | Episodes: 95300 | Median Reward: 33.36 | Max Reward: 47.77
Iteration: 1176 | Episodes: 95400 | Median Reward: 26.41 | Max Reward: 47.77
Iteration: 1177 | Episodes: 95500 | Median Reward: 37.51 | Max Reward: 47.77
Iteration: 1178 | Episodes: 95600 | Median Reward: 27.81 | Max Reward: 47.77
Iteration: 1179 | Episodes: 95700 | Median Reward: 27.81 | Max Reward: 47.77
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.6         |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 1180          |
|    time_elapsed         | 61276         |
|    total_timesteps      | 9666560       |
| train/                  |               |
|    approx_kl            | 0.00022299396 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -114          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.59         |
|    n_updates            | 11790         |
|    policy_gradient_loss | -0.000768     |
|    std                  | 3.61          |
|    value_loss           | 0.567         |
-------------------------------------------
Iteration: 1181 | Episodes: 95800 | Median Reward: 31.22 | Max Reward: 47.77
Iteration: 1182 | Episodes: 95900 | Median Reward: 35.50 | Max Reward: 47.77
Iteration: 1183 | Episodes: 96000 | Median Reward: 40.17 | Max Reward: 47.77
Iteration: 1184 | Episodes: 96100 | Median Reward: 23.22 | Max Reward: 47.77
Iteration: 1186 | Episodes: 96200 | Median Reward: 29.44 | Max Reward: 47.77
Iteration: 1187 | Episodes: 96300 | Median Reward: 36.56 | Max Reward: 47.77
Iteration: 1188 | Episodes: 96400 | Median Reward: 36.93 | Max Reward: 47.77
Iteration: 1189 | Episodes: 96500 | Median Reward: 30.52 | Max Reward: 47.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -71.3        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 1190         |
|    time_elapsed         | 61828        |
|    total_timesteps      | 9748480      |
| train/                  |              |
|    approx_kl            | 0.0013005111 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.92        |
|    n_updates            | 11890        |
|    policy_gradient_loss | -0.00512     |
|    std                  | 3.63         |
|    value_loss           | 1.61         |
------------------------------------------
Iteration: 1190 | Episodes: 96600 | Median Reward: 24.84 | Max Reward: 47.77
Iteration: 1192 | Episodes: 96700 | Median Reward: 34.93 | Max Reward: 47.77
Iteration: 1193 | Episodes: 96800 | Median Reward: 28.59 | Max Reward: 47.77
Iteration: 1194 | Episodes: 96900 | Median Reward: 28.97 | Max Reward: 47.77
Iteration: 1195 | Episodes: 97000 | Median Reward: 35.69 | Max Reward: 47.77
Iteration: 1197 | Episodes: 97100 | Median Reward: 24.30 | Max Reward: 47.77
Iteration: 1198 | Episodes: 97200 | Median Reward: 24.11 | Max Reward: 47.77
Iteration: 1199 | Episodes: 97300 | Median Reward: 31.08 | Max Reward: 47.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.3        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 1200         |
|    time_elapsed         | 62368        |
|    total_timesteps      | 9830400      |
| train/                  |              |
|    approx_kl            | 0.0009802921 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.84        |
|    n_updates            | 11990        |
|    policy_gradient_loss | -0.000685    |
|    std                  | 3.63         |
|    value_loss           | 1.68         |
------------------------------------------
Iteration: 1200 | Episodes: 97400 | Median Reward: 34.48 | Max Reward: 47.77
Iteration: 1202 | Episodes: 97500 | Median Reward: 34.00 | Max Reward: 47.77
Iteration: 1203 | Episodes: 97600 | Median Reward: 31.93 | Max Reward: 47.77
Iteration: 1204 | Episodes: 97700 | Median Reward: 27.40 | Max Reward: 47.77
Iteration: 1205 | Episodes: 97800 | Median Reward: 31.70 | Max Reward: 47.77
Iteration: 1207 | Episodes: 97900 | Median Reward: 31.07 | Max Reward: 47.77
Iteration: 1208 | Episodes: 98000 | Median Reward: 28.84 | Max Reward: 47.77
Iteration: 1209 | Episodes: 98100 | Median Reward: 28.56 | Max Reward: 47.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.7       |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 1210        |
|    time_elapsed         | 62924       |
|    total_timesteps      | 9912320     |
| train/                  |             |
|    approx_kl            | 0.003685865 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -115        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.000707    |
|    loss                 | -4.45       |
|    n_updates            | 12090       |
|    policy_gradient_loss | -0.0117     |
|    std                  | 3.65        |
|    value_loss           | 2.16        |
-----------------------------------------
Iteration: 1210 | Episodes: 98200 | Median Reward: 30.47 | Max Reward: 47.77
Iteration: 1211 | Episodes: 98300 | Median Reward: 34.97 | Max Reward: 47.77
Iteration: 1213 | Episodes: 98400 | Median Reward: 31.99 | Max Reward: 47.77
Iteration: 1214 | Episodes: 98500 | Median Reward: 31.99 | Max Reward: 47.77
Iteration: 1215 | Episodes: 98600 | Median Reward: 34.44 | Max Reward: 47.77
Iteration: 1216 | Episodes: 98700 | Median Reward: 31.98 | Max Reward: 47.77
Iteration: 1218 | Episodes: 98800 | Median Reward: 30.20 | Max Reward: 47.77
Iteration: 1219 | Episodes: 98900 | Median Reward: 26.06 | Max Reward: 47.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.8       |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 1220        |
|    time_elapsed         | 63474       |
|    total_timesteps      | 9994240     |
| train/                  |             |
|    approx_kl            | 0.010482926 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -115        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.52       |
|    n_updates            | 12190       |
|    policy_gradient_loss | -0.0321     |
|    std                  | 3.68        |
|    value_loss           | 1.5         |
-----------------------------------------
Iteration: 1220 | Episodes: 99000 | Median Reward: 30.65 | Max Reward: 47.77
Iteration: 1221 | Episodes: 99100 | Median Reward: 26.37 | Max Reward: 47.77
Iteration: 1223 | Episodes: 99200 | Median Reward: 26.37 | Max Reward: 47.77
Iteration: 1224 | Episodes: 99300 | Median Reward: 33.74 | Max Reward: 47.77
Iteration: 1225 | Episodes: 99400 | Median Reward: 32.59 | Max Reward: 47.77
Iteration: 1226 | Episodes: 99500 | Median Reward: 34.53 | Max Reward: 47.77
Iteration: 1227 | Episodes: 99600 | Median Reward: 41.64 | Max Reward: 47.77
Iteration: 1229 | Episodes: 99700 | Median Reward: 37.58 | Max Reward: 47.77
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.8        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 1230         |
|    time_elapsed         | 64021        |
|    total_timesteps      | 10076160     |
| train/                  |              |
|    approx_kl            | 0.0011753837 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -3.81        |
|    n_updates            | 12290        |
|    policy_gradient_loss | -0.00644     |
|    std                  | 3.69         |
|    value_loss           | 3.42         |
------------------------------------------
Iteration: 1230 | Episodes: 99800 | Median Reward: 33.98 | Max Reward: 47.77
Iteration: 1231 | Episodes: 99900 | Median Reward: 37.29 | Max Reward: 47.77
Iteration: 1232 | Episodes: 100000 | Median Reward: 38.28 | Max Reward: 47.77
Iteration: 1234 | Episodes: 100100 | Median Reward: 30.76 | Max Reward: 47.77
Iteration: 1235 | Episodes: 100200 | Median Reward: 33.55 | Max Reward: 47.77
Iteration: 1236 | Episodes: 100300 | Median Reward: 35.17 | Max Reward: 47.77
Iteration: 1237 | Episodes: 100400 | Median Reward: 35.06 | Max Reward: 47.77
Iteration: 1239 | Episodes: 100500 | Median Reward: 32.05 | Max Reward: 47.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66         |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 1240        |
|    time_elapsed         | 64572       |
|    total_timesteps      | 10158080    |
| train/                  |             |
|    approx_kl            | 0.000748417 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -115        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.16       |
|    n_updates            | 12390       |
|    policy_gradient_loss | -0.00176    |
|    std                  | 3.7         |
|    value_loss           | 1.7         |
-----------------------------------------
Iteration: 1240 | Episodes: 100600 | Median Reward: 37.36 | Max Reward: 47.77
Iteration: 1241 | Episodes: 100700 | Median Reward: 38.56 | Max Reward: 47.77
Iteration: 1242 | Episodes: 100800 | Median Reward: 32.55 | Max Reward: 47.77
Iteration: 1244 | Episodes: 100900 | Median Reward: 29.92 | Max Reward: 47.77
Iteration: 1245 | Episodes: 101000 | Median Reward: 29.46 | Max Reward: 47.77
Iteration: 1246 | Episodes: 101100 | Median Reward: 33.85 | Max Reward: 47.77
Iteration: 1247 | Episodes: 101200 | Median Reward: 41.08 | Max Reward: 47.77
Iteration: 1248 | Episodes: 101300 | Median Reward: 37.27 | Max Reward: 47.77
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.6       |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 1250        |
|    time_elapsed         | 65124       |
|    total_timesteps      | 10240000    |
| train/                  |             |
|    approx_kl            | 0.002439791 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -115        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.48       |
|    n_updates            | 12490       |
|    policy_gradient_loss | -0.00722    |
|    std                  | 3.72        |
|    value_loss           | 0.594       |
-----------------------------------------
Iteration: 1250 | Episodes: 101400 | Median Reward: 34.61 | Max Reward: 47.77
Iteration: 1251 | Episodes: 101500 | Median Reward: 29.65 | Max Reward: 47.77
Iteration: 1252 | Episodes: 101600 | Median Reward: 29.49 | Max Reward: 47.77
Iteration: 1253 | Episodes: 101700 | Median Reward: 31.03 | Max Reward: 47.77
Iteration: 1255 | Episodes: 101800 | Median Reward: 34.51 | Max Reward: 47.77
Iteration: 1256 | Episodes: 101900 | Median Reward: 45.32 | Max Reward: 48.03
Iteration: 1257 | Episodes: 102000 | Median Reward: 46.69 | Max Reward: 48.03
aIteration: 1258 | Episodes: 102100 | Median Reward: 41.19 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.6         |
| time/                   |               |
|    fps                  | 157           |
|    iterations           | 1260          |
|    time_elapsed         | 65665         |
|    total_timesteps      | 10321920      |
| train/                  |               |
|    approx_kl            | 5.6273013e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -115          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.01         |
|    n_updates            | 12590         |
|    policy_gradient_loss | -6.35e-05     |
|    std                  | 3.73          |
|    value_loss           | 3.2           |
-------------------------------------------
Iteration: 1260 | Episodes: 102200 | Median Reward: 41.00 | Max Reward: 48.03
Iteration: 1261 | Episodes: 102300 | Median Reward: 40.51 | Max Reward: 48.03
Iteration: 1262 | Episodes: 102400 | Median Reward: 40.66 | Max Reward: 48.03
Iteration: 1263 | Episodes: 102500 | Median Reward: 40.56 | Max Reward: 48.03
Iteration: 1264 | Episodes: 102600 | Median Reward: 28.23 | Max Reward: 48.03
Iteration: 1266 | Episodes: 102700 | Median Reward: 35.77 | Max Reward: 48.03
Iteration: 1267 | Episodes: 102800 | Median Reward: 38.04 | Max Reward: 48.03
Iteration: 1268 | Episodes: 102900 | Median Reward: 32.27 | Max Reward: 48.03
Iteration: 1269 | Episodes: 103000 | Median Reward: 31.60 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.6        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 1270         |
|    time_elapsed         | 66218        |
|    total_timesteps      | 10403840     |
| train/                  |              |
|    approx_kl            | 0.0016692268 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -3.25        |
|    n_updates            | 12690        |
|    policy_gradient_loss | -0.00956     |
|    std                  | 3.75         |
|    value_loss           | 4.54         |
------------------------------------------
Iteration: 1271 | Episodes: 103100 | Median Reward: 28.90 | Max Reward: 48.03
Iteration: 1272 | Episodes: 103200 | Median Reward: 29.39 | Max Reward: 48.03
Iteration: 1273 | Episodes: 103300 | Median Reward: 33.86 | Max Reward: 48.03
Iteration: 1274 | Episodes: 103400 | Median Reward: 34.55 | Max Reward: 48.03
Iteration: 1276 | Episodes: 103500 | Median Reward: 36.97 | Max Reward: 48.03
Iteration: 1277 | Episodes: 103600 | Median Reward: 41.80 | Max Reward: 48.03
Iteration: 1278 | Episodes: 103700 | Median Reward: 37.42 | Max Reward: 48.03
Iteration: 1279 | Episodes: 103800 | Median Reward: 40.63 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.9        |
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 1280         |
|    time_elapsed         | 66774        |
|    total_timesteps      | 10485760     |
| train/                  |              |
|    approx_kl            | 0.0017691927 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -115         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.35        |
|    n_updates            | 12790        |
|    policy_gradient_loss | -0.00461     |
|    std                  | 3.77         |
|    value_loss           | 1.33         |
------------------------------------------
Iteration: 1280 | Episodes: 103900 | Median Reward: 39.92 | Max Reward: 48.03
Iteration: 1282 | Episodes: 104000 | Median Reward: 35.65 | Max Reward: 48.03
Iteration: 1283 | Episodes: 104100 | Median Reward: 39.40 | Max Reward: 48.03
Iteration: 1284 | Episodes: 104200 | Median Reward: 38.16 | Max Reward: 48.03
Iteration: 1285 | Episodes: 104300 | Median Reward: 35.57 | Max Reward: 48.03
Iteration: 1287 | Episodes: 104400 | Median Reward: 36.49 | Max Reward: 48.03
Iteration: 1288 | Episodes: 104500 | Median Reward: 37.25 | Max Reward: 48.03
Iteration: 1289 | Episodes: 104600 | Median Reward: 40.88 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.6        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 1290         |
|    time_elapsed         | 67326        |
|    total_timesteps      | 10567680     |
| train/                  |              |
|    approx_kl            | 0.0031296322 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.06        |
|    n_updates            | 12890        |
|    policy_gradient_loss | -0.00706     |
|    std                  | 3.79         |
|    value_loss           | 2.37         |
------------------------------------------
Iteration: 1290 | Episodes: 104700 | Median Reward: 40.13 | Max Reward: 48.03
Iteration: 1292 | Episodes: 104800 | Median Reward: 33.99 | Max Reward: 48.03
Iteration: 1293 | Episodes: 104900 | Median Reward: 37.47 | Max Reward: 48.03
Iteration: 1294 | Episodes: 105000 | Median Reward: 36.49 | Max Reward: 48.03
Iteration: 1295 | Episodes: 105100 | Median Reward: 37.20 | Max Reward: 48.03
Iteration: 1297 | Episodes: 105200 | Median Reward: 37.85 | Max Reward: 48.03
Iteration: 1298 | Episodes: 105300 | Median Reward: 38.06 | Max Reward: 48.03
Iteration: 1299 | Episodes: 105400 | Median Reward: 35.55 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.7        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 1300         |
|    time_elapsed         | 67867        |
|    total_timesteps      | 10649600     |
| train/                  |              |
|    approx_kl            | 0.0020627077 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.47        |
|    n_updates            | 12990        |
|    policy_gradient_loss | -0.00478     |
|    std                  | 3.81         |
|    value_loss           | 0.78         |
------------------------------------------
Iteration: 1300 | Episodes: 105500 | Median Reward: 39.78 | Max Reward: 48.03
Iteration: 1301 | Episodes: 105600 | Median Reward: 40.51 | Max Reward: 48.03
Iteration: 1303 | Episodes: 105700 | Median Reward: 38.01 | Max Reward: 48.03
Iteration: 1304 | Episodes: 105800 | Median Reward: 39.19 | Max Reward: 48.03
Iteration: 1305 | Episodes: 105900 | Median Reward: 34.84 | Max Reward: 48.03
Iteration: 1306 | Episodes: 106000 | Median Reward: 32.85 | Max Reward: 48.03
Iteration: 1308 | Episodes: 106100 | Median Reward: 38.32 | Max Reward: 48.03
Iteration: 1309 | Episodes: 106200 | Median Reward: 32.89 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.7        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 1310         |
|    time_elapsed         | 68415        |
|    total_timesteps      | 10731520     |
| train/                  |              |
|    approx_kl            | 0.0018155852 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.94        |
|    n_updates            | 13090        |
|    policy_gradient_loss | -0.00755     |
|    std                  | 3.85         |
|    value_loss           | 3.67         |
------------------------------------------
Iteration: 1310 | Episodes: 106300 | Median Reward: 37.00 | Max Reward: 48.03
Iteration: 1311 | Episodes: 106400 | Median Reward: 34.29 | Max Reward: 48.03
Iteration: 1313 | Episodes: 106500 | Median Reward: 31.75 | Max Reward: 48.03
Iteration: 1314 | Episodes: 106600 | Median Reward: 35.40 | Max Reward: 48.03
Iteration: 1315 | Episodes: 106700 | Median Reward: 27.94 | Max Reward: 48.03
Iteration: 1316 | Episodes: 106800 | Median Reward: 28.84 | Max Reward: 48.03
Iteration: 1317 | Episodes: 106900 | Median Reward: 31.77 | Max Reward: 48.03
Iteration: 1319 | Episodes: 107000 | Median Reward: 29.51 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.1        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 1320         |
|    time_elapsed         | 68971        |
|    total_timesteps      | 10813440     |
| train/                  |              |
|    approx_kl            | 0.0010836236 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.27        |
|    n_updates            | 13190        |
|    policy_gradient_loss | -0.00253     |
|    std                  | 3.88         |
|    value_loss           | 1.17         |
------------------------------------------
Iteration: 1320 | Episodes: 107100 | Median Reward: 33.16 | Max Reward: 48.03
Iteration: 1321 | Episodes: 107200 | Median Reward: 28.42 | Max Reward: 48.03
Iteration: 1322 | Episodes: 107300 | Median Reward: 34.00 | Max Reward: 48.03
Iteration: 1324 | Episodes: 107400 | Median Reward: 32.77 | Max Reward: 48.03
Iteration: 1325 | Episodes: 107500 | Median Reward: 35.22 | Max Reward: 48.03
Iteration: 1326 | Episodes: 107600 | Median Reward: 37.33 | Max Reward: 48.03
Iteration: 1327 | Episodes: 107700 | Median Reward: 37.00 | Max Reward: 48.03
Iteration: 1329 | Episodes: 107800 | Median Reward: 33.22 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.4         |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 1330          |
|    time_elapsed         | 69517         |
|    total_timesteps      | 10895360      |
| train/                  |               |
|    approx_kl            | 1.8415049e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -116          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.41         |
|    n_updates            | 13290         |
|    policy_gradient_loss | 2.17e-05      |
|    std                  | 3.9           |
|    value_loss           | 1.41          |
-------------------------------------------
Iteration: 1330 | Episodes: 107900 | Median Reward: 38.99 | Max Reward: 48.03
Iteration: 1331 | Episodes: 108000 | Median Reward: 34.00 | Max Reward: 48.03
Iteration: 1332 | Episodes: 108100 | Median Reward: 32.92 | Max Reward: 48.03
Iteration: 1334 | Episodes: 108200 | Median Reward: 41.22 | Max Reward: 48.03
Iteration: 1335 | Episodes: 108300 | Median Reward: 41.88 | Max Reward: 48.03
Iteration: 1336 | Episodes: 108400 | Median Reward: 42.79 | Max Reward: 48.03
Iteration: 1337 | Episodes: 108500 | Median Reward: 33.15 | Max Reward: 48.03
Iteration: 1338 | Episodes: 108600 | Median Reward: 38.30 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.9        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 1340         |
|    time_elapsed         | 70069        |
|    total_timesteps      | 10977280     |
| train/                  |              |
|    approx_kl            | 0.0010265703 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.42        |
|    n_updates            | 13390        |
|    policy_gradient_loss | -0.00236     |
|    std                  | 3.9          |
|    value_loss           | 1.54         |
------------------------------------------
Iteration: 1340 | Episodes: 108700 | Median Reward: 38.16 | Max Reward: 48.03
Iteration: 1341 | Episodes: 108800 | Median Reward: 36.65 | Max Reward: 48.03
Iteration: 1342 | Episodes: 108900 | Median Reward: 35.81 | Max Reward: 48.03
Iteration: 1343 | Episodes: 109000 | Median Reward: 37.50 | Max Reward: 48.03
Iteration: 1345 | Episodes: 109100 | Median Reward: 39.99 | Max Reward: 48.03
Iteration: 1346 | Episodes: 109200 | Median Reward: 38.95 | Max Reward: 48.03
Iteration: 1347 | Episodes: 109300 | Median Reward: 31.49 | Max Reward: 48.03
Iteration: 1348 | Episodes: 109400 | Median Reward: 34.59 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 1350         |
|    time_elapsed         | 70619        |
|    total_timesteps      | 11059200     |
| train/                  |              |
|    approx_kl            | 0.0028344085 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.41        |
|    n_updates            | 13490        |
|    policy_gradient_loss | -0.00825     |
|    std                  | 3.91         |
|    value_loss           | 1.65         |
------------------------------------------
Iteration: 1350 | Episodes: 109500 | Median Reward: 39.00 | Max Reward: 48.03
Iteration: 1351 | Episodes: 109600 | Median Reward: 32.19 | Max Reward: 48.03
Iteration: 1352 | Episodes: 109700 | Median Reward: 30.53 | Max Reward: 48.03
Iteration: 1353 | Episodes: 109800 | Median Reward: 35.53 | Max Reward: 48.03
Iteration: 1354 | Episodes: 109900 | Median Reward: 35.63 | Max Reward: 48.03
Iteration: 1356 | Episodes: 110000 | Median Reward: 33.53 | Max Reward: 48.03
Iteration: 1357 | Episodes: 110100 | Median Reward: 32.46 | Max Reward: 48.03
Iteration: 1358 | Episodes: 110200 | Median Reward: 30.64 | Max Reward: 48.03
Iteration: 1359 | Episodes: 110300 | Median Reward: 31.44 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.6        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 1360         |
|    time_elapsed         | 71167        |
|    total_timesteps      | 11141120     |
| train/                  |              |
|    approx_kl            | 0.0017224692 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -116         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.27        |
|    n_updates            | 13590        |
|    policy_gradient_loss | -0.00666     |
|    std                  | 3.93         |
|    value_loss           | 1.87         |
------------------------------------------
Iteration: 1361 | Episodes: 110400 | Median Reward: 33.82 | Max Reward: 48.03
Iteration: 1362 | Episodes: 110500 | Median Reward: 37.10 | Max Reward: 48.03
Iteration: 1363 | Episodes: 110600 | Median Reward: 33.92 | Max Reward: 48.03
Iteration: 1364 | Episodes: 110700 | Median Reward: 32.77 | Max Reward: 48.03
Iteration: 1366 | Episodes: 110800 | Median Reward: 38.66 | Max Reward: 48.03
Iteration: 1367 | Episodes: 110900 | Median Reward: 30.21 | Max Reward: 48.03
Iteration: 1368 | Episodes: 111000 | Median Reward: 37.55 | Max Reward: 48.03
Iteration: 1369 | Episodes: 111100 | Median Reward: 35.15 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.1         |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 1370          |
|    time_elapsed         | 71729         |
|    total_timesteps      | 11223040      |
| train/                  |               |
|    approx_kl            | 0.00082304305 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -117          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.16         |
|    n_updates            | 13690         |
|    policy_gradient_loss | -0.00298      |
|    std                  | 3.96          |
|    value_loss           | 2.42          |
-------------------------------------------
Iteration: 1370 | Episodes: 111200 | Median Reward: 34.35 | Max Reward: 48.03
Iteration: 1372 | Episodes: 111300 | Median Reward: 39.72 | Max Reward: 48.03
Iteration: 1373 | Episodes: 111400 | Median Reward: 42.77 | Max Reward: 48.03
Iteration: 1374 | Episodes: 111500 | Median Reward: 27.08 | Max Reward: 48.03
Iteration: 1375 | Episodes: 111600 | Median Reward: 35.77 | Max Reward: 48.03
Iteration: 1377 | Episodes: 111700 | Median Reward: 36.65 | Max Reward: 48.03
Iteration: 1378 | Episodes: 111800 | Median Reward: 38.08 | Max Reward: 48.03
Iteration: 1379 | Episodes: 111900 | Median Reward: 27.33 | Max Reward: 48.03
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -71.4      |
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 1380       |
|    time_elapsed         | 72285      |
|    total_timesteps      | 11304960   |
| train/                  |            |
|    approx_kl            | 0.00474145 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -117       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000707   |
|    loss                 | -5.29      |
|    n_updates            | 13790      |
|    policy_gradient_loss | -0.0143    |
|    std                  | 3.96       |
|    value_loss           | 1.14       |
----------------------------------------
Iteration: 1380 | Episodes: 112000 | Median Reward: 38.73 | Max Reward: 48.03
Iteration: 1382 | Episodes: 112100 | Median Reward: 42.23 | Max Reward: 48.03
Iteration: 1383 | Episodes: 112200 | Median Reward: 38.13 | Max Reward: 48.03
Iteration: 1384 | Episodes: 112300 | Median Reward: 39.00 | Max Reward: 48.03
Iteration: 1385 | Episodes: 112400 | Median Reward: 35.70 | Max Reward: 48.03
Iteration: 1387 | Episodes: 112500 | Median Reward: 38.75 | Max Reward: 48.03
Iteration: 1388 | Episodes: 112600 | Median Reward: 39.62 | Max Reward: 48.03
Iteration: 1389 | Episodes: 112700 | Median Reward: 39.32 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.6         |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 1390          |
|    time_elapsed         | 72830         |
|    total_timesteps      | 11386880      |
| train/                  |               |
|    approx_kl            | 0.00011585465 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -117          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.86         |
|    n_updates            | 13890         |
|    policy_gradient_loss | -0.000747     |
|    std                  | 4             |
|    value_loss           | 2.89          |
-------------------------------------------
Iteration: 1390 | Episodes: 112800 | Median Reward: 38.95 | Max Reward: 48.03
Iteration: 1391 | Episodes: 112900 | Median Reward: 31.61 | Max Reward: 48.03
Iteration: 1393 | Episodes: 113000 | Median Reward: 35.65 | Max Reward: 48.03
Iteration: 1394 | Episodes: 113100 | Median Reward: 33.79 | Max Reward: 48.03
Iteration: 1395 | Episodes: 113200 | Median Reward: 33.75 | Max Reward: 48.03
Iteration: 1396 | Episodes: 113300 | Median Reward: 28.14 | Max Reward: 48.03
Iteration: 1398 | Episodes: 113400 | Median Reward: 32.22 | Max Reward: 48.03
Iteration: 1399 | Episodes: 113500 | Median Reward: 34.54 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 1400         |
|    time_elapsed         | 73387        |
|    total_timesteps      | 11468800     |
| train/                  |              |
|    approx_kl            | 0.0007440556 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -117         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -3.98        |
|    n_updates            | 13990        |
|    policy_gradient_loss | -0.00343     |
|    std                  | 4.03         |
|    value_loss           | 1.83         |
------------------------------------------
Iteration: 1400 | Episodes: 113600 | Median Reward: 42.83 | Max Reward: 48.03
Iteration: 1401 | Episodes: 113700 | Median Reward: 39.78 | Max Reward: 48.03
Iteration: 1403 | Episodes: 113800 | Median Reward: 37.78 | Max Reward: 48.03
Iteration: 1404 | Episodes: 113900 | Median Reward: 34.31 | Max Reward: 48.03
Iteration: 1405 | Episodes: 114000 | Median Reward: 35.72 | Max Reward: 48.03
Iteration: 1406 | Episodes: 114100 | Median Reward: 34.27 | Max Reward: 48.03
Iteration: 1407 | Episodes: 114200 | Median Reward: 35.93 | Max Reward: 48.03
Iteration: 1409 | Episodes: 114300 | Median Reward: 34.05 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66           |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 1410          |
|    time_elapsed         | 73936         |
|    total_timesteps      | 11550720      |
| train/                  |               |
|    approx_kl            | 0.00057113636 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -117          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.68         |
|    n_updates            | 14090         |
|    policy_gradient_loss | -0.00283      |
|    std                  | 4.06          |
|    value_loss           | 0.913         |
-------------------------------------------
Iteration: 1410 | Episodes: 114400 | Median Reward: 36.93 | Max Reward: 48.03
Iteration: 1411 | Episodes: 114500 | Median Reward: 35.59 | Max Reward: 48.03
Iteration: 1412 | Episodes: 114600 | Median Reward: 35.92 | Max Reward: 48.03
Iteration: 1414 | Episodes: 114700 | Median Reward: 43.42 | Max Reward: 48.03
Iteration: 1415 | Episodes: 114800 | Median Reward: 37.17 | Max Reward: 48.03
Iteration: 1416 | Episodes: 114900 | Median Reward: 36.65 | Max Reward: 48.03
Iteration: 1417 | Episodes: 115000 | Median Reward: 35.45 | Max Reward: 48.03
Iteration: 1419 | Episodes: 115100 | Median Reward: 35.18 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.2         |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 1420          |
|    time_elapsed         | 74460         |
|    total_timesteps      | 11632640      |
| train/                  |               |
|    approx_kl            | 1.3179182e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -117          |
|    explained_variance   | 0.995         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.47         |
|    n_updates            | 14190         |
|    policy_gradient_loss | 2.6e-05       |
|    std                  | 4.08          |
|    value_loss           | 1.54          |
-------------------------------------------
Iteration: 1420 | Episodes: 115200 | Median Reward: 37.02 | Max Reward: 48.03
Iteration: 1421 | Episodes: 115300 | Median Reward: 37.03 | Max Reward: 48.03
Iteration: 1422 | Episodes: 115400 | Median Reward: 37.00 | Max Reward: 48.03
Iteration: 1424 | Episodes: 115500 | Median Reward: 28.72 | Max Reward: 48.03
Iteration: 1425 | Episodes: 115600 | Median Reward: 34.88 | Max Reward: 48.03
Iteration: 1426 | Episodes: 115700 | Median Reward: 30.83 | Max Reward: 48.03
Iteration: 1427 | Episodes: 115800 | Median Reward: 35.81 | Max Reward: 48.03
Iteration: 1428 | Episodes: 115900 | Median Reward: 35.97 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.5        |
| time/                   |              |
|    fps                  | 156          |
|    iterations           | 1430         |
|    time_elapsed         | 75005        |
|    total_timesteps      | 11714560     |
| train/                  |              |
|    approx_kl            | 0.0018528522 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -117         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.44        |
|    n_updates            | 14290        |
|    policy_gradient_loss | -0.00658     |
|    std                  | 4.09         |
|    value_loss           | 1.41         |
------------------------------------------
Iteration: 1430 | Episodes: 116000 | Median Reward: 30.83 | Max Reward: 48.03
Iteration: 1431 | Episodes: 116100 | Median Reward: 30.83 | Max Reward: 48.03
Iteration: 1432 | Episodes: 116200 | Median Reward: 46.94 | Max Reward: 48.03
Iteration: 1433 | Episodes: 116300 | Median Reward: 43.98 | Max Reward: 48.03
Iteration: 1435 | Episodes: 116400 | Median Reward: 39.86 | Max Reward: 48.03
Iteration: 1436 | Episodes: 116500 | Median Reward: 35.44 | Max Reward: 48.03
Iteration: 1437 | Episodes: 116600 | Median Reward: 36.04 | Max Reward: 48.03
Iteration: 1438 | Episodes: 116700 | Median Reward: 36.58 | Max Reward: 48.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.5       |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 1440        |
|    time_elapsed         | 75547       |
|    total_timesteps      | 11796480    |
| train/                  |             |
|    approx_kl            | 0.012685984 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -117        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -4.46       |
|    n_updates            | 14390       |
|    policy_gradient_loss | -0.0395     |
|    std                  | 4.11        |
|    value_loss           | 2.02        |
-----------------------------------------
Iteration: 1440 | Episodes: 116800 | Median Reward: 38.02 | Max Reward: 48.03
Iteration: 1441 | Episodes: 116900 | Median Reward: 38.56 | Max Reward: 48.03
Iteration: 1442 | Episodes: 117000 | Median Reward: 36.85 | Max Reward: 48.03
Iteration: 1443 | Episodes: 117100 | Median Reward: 43.50 | Max Reward: 48.03
Iteration: 1444 | Episodes: 117200 | Median Reward: 44.56 | Max Reward: 48.03
Iteration: 1446 | Episodes: 117300 | Median Reward: 38.34 | Max Reward: 48.03
Iteration: 1447 | Episodes: 117400 | Median Reward: 36.35 | Max Reward: 48.03
Iteration: 1448 | Episodes: 117500 | Median Reward: 38.05 | Max Reward: 48.03
Iteration: 1449 | Episodes: 117600 | Median Reward: 38.44 | Max Reward: 48.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63         |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 1450        |
|    time_elapsed         | 76099       |
|    total_timesteps      | 11878400    |
| train/                  |             |
|    approx_kl            | 0.002206448 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -117        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.34       |
|    n_updates            | 14490       |
|    policy_gradient_loss | -0.00123    |
|    std                  | 4.13        |
|    value_loss           | 1.17        |
-----------------------------------------
Iteration: 1451 | Episodes: 117700 | Median Reward: 38.83 | Max Reward: 48.03
Iteration: 1452 | Episodes: 117800 | Median Reward: 38.49 | Max Reward: 48.03
Iteration: 1453 | Episodes: 117900 | Median Reward: 38.80 | Max Reward: 48.03
Iteration: 1454 | Episodes: 118000 | Median Reward: 34.36 | Max Reward: 48.03
Iteration: 1456 | Episodes: 118100 | Median Reward: 35.77 | Max Reward: 48.03
Iteration: 1457 | Episodes: 118200 | Median Reward: 42.16 | Max Reward: 48.03
Iteration: 1458 | Episodes: 118300 | Median Reward: 40.22 | Max Reward: 48.03
Iteration: 1459 | Episodes: 118400 | Median Reward: 39.04 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.2         |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 1460          |
|    time_elapsed         | 76654         |
|    total_timesteps      | 11960320      |
| train/                  |               |
|    approx_kl            | 1.5911588e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -118          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.71         |
|    n_updates            | 14590         |
|    policy_gradient_loss | -3.05e-05     |
|    std                  | 4.15          |
|    value_loss           | 0.833         |
-------------------------------------------
Iteration: 1460 | Episodes: 118500 | Median Reward: 38.29 | Max Reward: 48.03
Iteration: 1462 | Episodes: 118600 | Median Reward: 34.28 | Max Reward: 48.03
Iteration: 1463 | Episodes: 118700 | Median Reward: 28.78 | Max Reward: 48.03
Iteration: 1464 | Episodes: 118800 | Median Reward: 43.76 | Max Reward: 48.03
Iteration: 1465 | Episodes: 118900 | Median Reward: 40.77 | Max Reward: 48.03
Iteration: 1467 | Episodes: 119000 | Median Reward: 37.22 | Max Reward: 48.03
Iteration: 1468 | Episodes: 119100 | Median Reward: 37.22 | Max Reward: 48.03
Iteration: 1469 | Episodes: 119200 | Median Reward: 36.11 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -70.2        |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 1470         |
|    time_elapsed         | 77196        |
|    total_timesteps      | 12042240     |
| train/                  |              |
|    approx_kl            | 0.0055081886 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -118         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.28        |
|    n_updates            | 14690        |
|    policy_gradient_loss | -0.00519     |
|    std                  | 4.2          |
|    value_loss           | 0.873        |
------------------------------------------
Iteration: 1470 | Episodes: 119300 | Median Reward: 30.62 | Max Reward: 48.03
Iteration: 1472 | Episodes: 119400 | Median Reward: 35.46 | Max Reward: 48.03
Iteration: 1473 | Episodes: 119500 | Median Reward: 38.03 | Max Reward: 48.03
Iteration: 1474 | Episodes: 119600 | Median Reward: 36.15 | Max Reward: 48.03
Iteration: 1475 | Episodes: 119700 | Median Reward: 35.64 | Max Reward: 48.03
Iteration: 1477 | Episodes: 119800 | Median Reward: 35.28 | Max Reward: 48.03
Iteration: 1478 | Episodes: 119900 | Median Reward: 28.74 | Max Reward: 48.03
Iteration: 1479 | Episodes: 120000 | Median Reward: 38.77 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.4         |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 1480          |
|    time_elapsed         | 77750         |
|    total_timesteps      | 12124160      |
| train/                  |               |
|    approx_kl            | 0.00013658742 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -118          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.66         |
|    n_updates            | 14790         |
|    policy_gradient_loss | -0.000364     |
|    std                  | 4.22          |
|    value_loss           | 0.834         |
-------------------------------------------
Iteration: 1480 | Episodes: 120100 | Median Reward: 41.14 | Max Reward: 48.03
Iteration: 1481 | Episodes: 120200 | Median Reward: 41.63 | Max Reward: 48.03
Iteration: 1483 | Episodes: 120300 | Median Reward: 32.81 | Max Reward: 48.03
Iteration: 1484 | Episodes: 120400 | Median Reward: 30.24 | Max Reward: 48.03
Iteration: 1485 | Episodes: 120500 | Median Reward: 35.54 | Max Reward: 48.03
Iteration: 1486 | Episodes: 120600 | Median Reward: 41.55 | Max Reward: 48.03
Iteration: 1488 | Episodes: 120700 | Median Reward: 41.35 | Max Reward: 48.03
Iteration: 1489 | Episodes: 120800 | Median Reward: 36.28 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.7        |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 1490         |
|    time_elapsed         | 78298        |
|    total_timesteps      | 12206080     |
| train/                  |              |
|    approx_kl            | 6.392562e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -118         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.55        |
|    n_updates            | 14890        |
|    policy_gradient_loss | -0.000124    |
|    std                  | 4.25         |
|    value_loss           | 0.926        |
------------------------------------------
Iteration: 1490 | Episodes: 120900 | Median Reward: 37.50 | Max Reward: 48.03
Iteration: 1491 | Episodes: 121000 | Median Reward: 40.06 | Max Reward: 48.03
Iteration: 1493 | Episodes: 121100 | Median Reward: 33.69 | Max Reward: 48.03
Iteration: 1494 | Episodes: 121200 | Median Reward: 35.06 | Max Reward: 48.03
Iteration: 1495 | Episodes: 121300 | Median Reward: 42.52 | Max Reward: 48.03
Iteration: 1496 | Episodes: 121400 | Median Reward: 43.89 | Max Reward: 48.03
Iteration: 1497 | Episodes: 121500 | Median Reward: 43.76 | Max Reward: 48.03
Iteration: 1499 | Episodes: 121600 | Median Reward: 31.91 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.9         |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 1500          |
|    time_elapsed         | 78842         |
|    total_timesteps      | 12288000      |
| train/                  |               |
|    approx_kl            | 0.00017560228 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -118          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.6          |
|    n_updates            | 14990         |
|    policy_gradient_loss | -0.00115      |
|    std                  | 4.27          |
|    value_loss           | 1.43          |
-------------------------------------------
Iteration: 1500 | Episodes: 121700 | Median Reward: 33.65 | Max Reward: 48.03
Iteration: 1501 | Episodes: 121800 | Median Reward: 35.53 | Max Reward: 48.03
Iteration: 1502 | Episodes: 121900 | Median Reward: 36.77 | Max Reward: 48.03
Iteration: 1504 | Episodes: 122000 | Median Reward: 36.88 | Max Reward: 48.03
Iteration: 1505 | Episodes: 122100 | Median Reward: 35.85 | Max Reward: 48.03
Iteration: 1506 | Episodes: 122200 | Median Reward: 34.36 | Max Reward: 48.03
Iteration: 1507 | Episodes: 122300 | Median Reward: 31.57 | Max Reward: 48.03
Iteration: 1509 | Episodes: 122400 | Median Reward: 37.91 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.2        |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 1510         |
|    time_elapsed         | 79395        |
|    total_timesteps      | 12369920     |
| train/                  |              |
|    approx_kl            | 0.0014839113 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -118         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.55        |
|    n_updates            | 15090        |
|    policy_gradient_loss | 0.00237      |
|    std                  | 4.29         |
|    value_loss           | 1.89         |
------------------------------------------
Iteration: 1510 | Episodes: 122500 | Median Reward: 40.89 | Max Reward: 48.03
Iteration: 1511 | Episodes: 122600 | Median Reward: 39.41 | Max Reward: 48.03
Iteration: 1512 | Episodes: 122700 | Median Reward: 42.90 | Max Reward: 48.03
Iteration: 1514 | Episodes: 122800 | Median Reward: 42.90 | Max Reward: 48.03
Iteration: 1515 | Episodes: 122900 | Median Reward: 32.79 | Max Reward: 48.03
Iteration: 1516 | Episodes: 123000 | Median Reward: 39.61 | Max Reward: 48.03
Iteration: 1517 | Episodes: 123100 | Median Reward: 29.83 | Max Reward: 48.03
Iteration: 1518 | Episodes: 123200 | Median Reward: 43.26 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.2         |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 1520          |
|    time_elapsed         | 79947         |
|    total_timesteps      | 12451840      |
| train/                  |               |
|    approx_kl            | 3.7108977e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -118          |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.21         |
|    n_updates            | 15190         |
|    policy_gradient_loss | 0.00021       |
|    std                  | 4.3           |
|    value_loss           | 5.12          |
-------------------------------------------
Iteration: 1520 | Episodes: 123300 | Median Reward: 41.60 | Max Reward: 48.03
Iteration: 1521 | Episodes: 123400 | Median Reward: 36.17 | Max Reward: 48.03
Iteration: 1522 | Episodes: 123500 | Median Reward: 32.30 | Max Reward: 48.03
Iteration: 1523 | Episodes: 123600 | Median Reward: 30.93 | Max Reward: 48.03
Iteration: 1525 | Episodes: 123700 | Median Reward: 32.93 | Max Reward: 48.03
Iteration: 1526 | Episodes: 123800 | Median Reward: 36.63 | Max Reward: 48.03
Iteration: 1527 | Episodes: 123900 | Median Reward: 39.49 | Max Reward: 48.03
Iteration: 1528 | Episodes: 124000 | Median Reward: 34.41 | Max Reward: 48.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -60.5       |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 1530        |
|    time_elapsed         | 80491       |
|    total_timesteps      | 12533760    |
| train/                  |             |
|    approx_kl            | 0.010022359 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -118        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.43       |
|    n_updates            | 15290       |
|    policy_gradient_loss | -0.0256     |
|    std                  | 4.32        |
|    value_loss           | 0.92        |
-----------------------------------------
Iteration: 1530 | Episodes: 124100 | Median Reward: 38.55 | Max Reward: 48.03
Iteration: 1531 | Episodes: 124200 | Median Reward: 38.48 | Max Reward: 48.03
Iteration: 1532 | Episodes: 124300 | Median Reward: 40.88 | Max Reward: 48.03
Iteration: 1533 | Episodes: 124400 | Median Reward: 42.21 | Max Reward: 48.03
Iteration: 1534 | Episodes: 124500 | Median Reward: 35.11 | Max Reward: 48.03
Iteration: 1536 | Episodes: 124600 | Median Reward: 36.37 | Max Reward: 48.03
Iteration: 1537 | Episodes: 124700 | Median Reward: 37.00 | Max Reward: 48.03
Iteration: 1538 | Episodes: 124800 | Median Reward: 38.43 | Max Reward: 48.03
Iteration: 1539 | Episodes: 124900 | Median Reward: 34.16 | Max Reward: 48.03
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -65.1          |
| time/                   |                |
|    fps                  | 155            |
|    iterations           | 1540           |
|    time_elapsed         | 81037          |
|    total_timesteps      | 12615680       |
| train/                  |                |
|    approx_kl            | 0.000121860656 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -119           |
|    explained_variance   | 0.998          |
|    learning_rate        | 0.000707       |
|    loss                 | -3.42          |
|    n_updates            | 15390          |
|    policy_gradient_loss | -0.000996      |
|    std                  | 4.37           |
|    value_loss           | 2.4            |
--------------------------------------------
Iteration: 1541 | Episodes: 125000 | Median Reward: 31.61 | Max Reward: 48.03
Iteration: 1542 | Episodes: 125100 | Median Reward: 38.10 | Max Reward: 48.03
Iteration: 1543 | Episodes: 125200 | Median Reward: 37.05 | Max Reward: 48.03
Iteration: 1544 | Episodes: 125300 | Median Reward: 40.50 | Max Reward: 48.03
Iteration: 1546 | Episodes: 125400 | Median Reward: 39.55 | Max Reward: 48.03
Iteration: 1547 | Episodes: 125500 | Median Reward: 37.47 | Max Reward: 48.03
Iteration: 1548 | Episodes: 125600 | Median Reward: 41.10 | Max Reward: 48.03
Iteration: 1549 | Episodes: 125700 | Median Reward: 39.52 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.3        |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 1550         |
|    time_elapsed         | 81583        |
|    total_timesteps      | 12697600     |
| train/                  |              |
|    approx_kl            | 0.0072840424 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -119         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.6         |
|    n_updates            | 15490        |
|    policy_gradient_loss | -0.0094      |
|    std                  | 4.41         |
|    value_loss           | 0.949        |
------------------------------------------
Iteration: 1551 | Episodes: 125800 | Median Reward: 38.06 | Max Reward: 48.03
Iteration: 1552 | Episodes: 125900 | Median Reward: 34.24 | Max Reward: 48.03
Iteration: 1553 | Episodes: 126000 | Median Reward: 35.40 | Max Reward: 48.03
Iteration: 1554 | Episodes: 126100 | Median Reward: 35.62 | Max Reward: 48.03
Iteration: 1555 | Episodes: 126200 | Median Reward: 33.26 | Max Reward: 48.03
Iteration: 1557 | Episodes: 126300 | Median Reward: 42.71 | Max Reward: 48.03
Iteration: 1558 | Episodes: 126400 | Median Reward: 35.55 | Max Reward: 48.03
Iteration: 1559 | Episodes: 126500 | Median Reward: 46.80 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.3        |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 1560         |
|    time_elapsed         | 82129        |
|    total_timesteps      | 12779520     |
| train/                  |              |
|    approx_kl            | 8.228751e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -119         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.28        |
|    n_updates            | 15590        |
|    policy_gradient_loss | -0.000407    |
|    std                  | 4.43         |
|    value_loss           | 2.39         |
------------------------------------------
Iteration: 1560 | Episodes: 126600 | Median Reward: 39.74 | Max Reward: 48.03
Iteration: 1562 | Episodes: 126700 | Median Reward: 39.79 | Max Reward: 48.03
Iteration: 1563 | Episodes: 126800 | Median Reward: 38.21 | Max Reward: 48.03
Iteration: 1564 | Episodes: 126900 | Median Reward: 36.90 | Max Reward: 48.03
Iteration: 1565 | Episodes: 127000 | Median Reward: 37.19 | Max Reward: 48.03
Iteration: 1567 | Episodes: 127100 | Median Reward: 39.43 | Max Reward: 48.03
Iteration: 1568 | Episodes: 127200 | Median Reward: 36.16 | Max Reward: 48.03
Iteration: 1569 | Episodes: 127300 | Median Reward: 42.16 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.4         |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 1570          |
|    time_elapsed         | 82674         |
|    total_timesteps      | 12861440      |
| train/                  |               |
|    approx_kl            | 4.9152586e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -119          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -4.92         |
|    n_updates            | 15690         |
|    policy_gradient_loss | 4.03e-06      |
|    std                  | 4.47          |
|    value_loss           | 4             |
-------------------------------------------
Iteration: 1570 | Episodes: 127400 | Median Reward: 39.09 | Max Reward: 48.03
Iteration: 1571 | Episodes: 127500 | Median Reward: 34.75 | Max Reward: 48.03
Iteration: 1573 | Episodes: 127600 | Median Reward: 34.78 | Max Reward: 48.03
Iteration: 1574 | Episodes: 127700 | Median Reward: 38.42 | Max Reward: 48.03
Iteration: 1575 | Episodes: 127800 | Median Reward: 42.79 | Max Reward: 48.03
Iteration: 1576 | Episodes: 127900 | Median Reward: 41.63 | Max Reward: 48.03
Iteration: 1578 | Episodes: 128000 | Median Reward: 40.38 | Max Reward: 48.03
Iteration: 1579 | Episodes: 128100 | Median Reward: 39.31 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.6         |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 1580          |
|    time_elapsed         | 83223         |
|    total_timesteps      | 12943360      |
| train/                  |               |
|    approx_kl            | 3.0088682e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -119          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.8          |
|    n_updates            | 15790         |
|    policy_gradient_loss | 3.05e-05      |
|    std                  | 4.48          |
|    value_loss           | 1.04          |
-------------------------------------------
Iteration: 1580 | Episodes: 128200 | Median Reward: 34.16 | Max Reward: 48.03
Iteration: 1581 | Episodes: 128300 | Median Reward: 38.37 | Max Reward: 48.03
Iteration: 1583 | Episodes: 128400 | Median Reward: 39.20 | Max Reward: 48.03
Iteration: 1584 | Episodes: 128500 | Median Reward: 31.35 | Max Reward: 48.03
Iteration: 1585 | Episodes: 128600 | Median Reward: 31.96 | Max Reward: 48.03
Iteration: 1586 | Episodes: 128700 | Median Reward: 39.30 | Max Reward: 48.03
Iteration: 1587 | Episodes: 128800 | Median Reward: 41.81 | Max Reward: 48.03
Iteration: 1589 | Episodes: 128900 | Median Reward: 33.03 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.8        |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 1590         |
|    time_elapsed         | 83765        |
|    total_timesteps      | 13025280     |
| train/                  |              |
|    approx_kl            | 0.0003018903 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -119         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.82        |
|    n_updates            | 15890        |
|    policy_gradient_loss | -8.99e-05    |
|    std                  | 4.5          |
|    value_loss           | 1.21         |
------------------------------------------
Iteration: 1590 | Episodes: 129000 | Median Reward: 32.68 | Max Reward: 48.03
Iteration: 1591 | Episodes: 129100 | Median Reward: 40.50 | Max Reward: 48.03
Iteration: 1592 | Episodes: 129200 | Median Reward: 41.85 | Max Reward: 48.03
Iteration: 1594 | Episodes: 129300 | Median Reward: 35.61 | Max Reward: 48.03
Iteration: 1595 | Episodes: 129400 | Median Reward: 38.86 | Max Reward: 48.03
Iteration: 1596 | Episodes: 129500 | Median Reward: 41.64 | Max Reward: 48.03
Iteration: 1597 | Episodes: 129600 | Median Reward: 32.28 | Max Reward: 48.03
Iteration: 1599 | Episodes: 129700 | Median Reward: 41.09 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62          |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 1600         |
|    time_elapsed         | 84301        |
|    total_timesteps      | 13107200     |
| train/                  |              |
|    approx_kl            | 8.997522e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.41        |
|    n_updates            | 15990        |
|    policy_gradient_loss | -8.43e-07    |
|    std                  | 4.54         |
|    value_loss           | 1.15         |
------------------------------------------
Iteration: 1600 | Episodes: 129800 | Median Reward: 36.50 | Max Reward: 48.03
Iteration: 1601 | Episodes: 129900 | Median Reward: 37.19 | Max Reward: 48.03
Iteration: 1602 | Episodes: 130000 | Median Reward: 34.76 | Max Reward: 48.03
Iteration: 1604 | Episodes: 130100 | Median Reward: 33.44 | Max Reward: 48.03
Iteration: 1605 | Episodes: 130200 | Median Reward: 39.09 | Max Reward: 48.03
Iteration: 1606 | Episodes: 130300 | Median Reward: 38.64 | Max Reward: 48.03
Iteration: 1607 | Episodes: 130400 | Median Reward: 33.53 | Max Reward: 48.03
Iteration: 1608 | Episodes: 130500 | Median Reward: 32.53 | Max Reward: 48.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.1       |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 1610        |
|    time_elapsed         | 84876       |
|    total_timesteps      | 13189120    |
| train/                  |             |
|    approx_kl            | 0.013559751 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -120        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.18       |
|    n_updates            | 16090       |
|    policy_gradient_loss | -0.0514     |
|    std                  | 4.58        |
|    value_loss           | 2.18        |
-----------------------------------------
Iteration: 1610 | Episodes: 130600 | Median Reward: 35.43 | Max Reward: 48.03
Iteration: 1611 | Episodes: 130700 | Median Reward: 38.71 | Max Reward: 48.03
Iteration: 1612 | Episodes: 130800 | Median Reward: 32.93 | Max Reward: 48.03
Iteration: 1613 | Episodes: 130900 | Median Reward: 29.73 | Max Reward: 48.03
Iteration: 1615 | Episodes: 131000 | Median Reward: 37.00 | Max Reward: 48.03
Iteration: 1616 | Episodes: 131100 | Median Reward: 39.53 | Max Reward: 48.03
Iteration: 1617 | Episodes: 131200 | Median Reward: 40.11 | Max Reward: 48.03
Iteration: 1618 | Episodes: 131300 | Median Reward: 40.06 | Max Reward: 48.03
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.8       |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 1620        |
|    time_elapsed         | 85450       |
|    total_timesteps      | 13271040    |
| train/                  |             |
|    approx_kl            | 0.001632377 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -120        |
|    explained_variance   | 1           |
|    learning_rate        | 0.000707    |
|    loss                 | -5.95       |
|    n_updates            | 16190       |
|    policy_gradient_loss | -0.0023     |
|    std                  | 4.59        |
|    value_loss           | 0.723       |
-----------------------------------------
Iteration: 1620 | Episodes: 131400 | Median Reward: 39.91 | Max Reward: 48.03
Iteration: 1621 | Episodes: 131500 | Median Reward: 35.95 | Max Reward: 48.03
Iteration: 1622 | Episodes: 131600 | Median Reward: 37.57 | Max Reward: 48.03
Iteration: 1623 | Episodes: 131700 | Median Reward: 28.65 | Max Reward: 48.03
Iteration: 1624 | Episodes: 131800 | Median Reward: 31.71 | Max Reward: 48.03
Iteration: 1626 | Episodes: 131900 | Median Reward: 33.80 | Max Reward: 48.03
Iteration: 1627 | Episodes: 132000 | Median Reward: 26.72 | Max Reward: 48.03
Iteration: 1628 | Episodes: 132100 | Median Reward: 35.92 | Max Reward: 48.03
Iteration: 1629 | Episodes: 132200 | Median Reward: 36.83 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.5         |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 1630          |
|    time_elapsed         | 86023         |
|    total_timesteps      | 13352960      |
| train/                  |               |
|    approx_kl            | 0.00097535754 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -120          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.24         |
|    n_updates            | 16290         |
|    policy_gradient_loss | -0.00267      |
|    std                  | 4.61          |
|    value_loss           | 0.964         |
-------------------------------------------
Iteration: 1631 | Episodes: 132300 | Median Reward: 38.40 | Max Reward: 48.03
Iteration: 1632 | Episodes: 132400 | Median Reward: 41.16 | Max Reward: 48.03
Iteration: 1633 | Episodes: 132500 | Median Reward: 40.59 | Max Reward: 48.03
Iteration: 1634 | Episodes: 132600 | Median Reward: 41.95 | Max Reward: 48.03
Iteration: 1636 | Episodes: 132700 | Median Reward: 41.36 | Max Reward: 48.03
Iteration: 1637 | Episodes: 132800 | Median Reward: 38.48 | Max Reward: 48.03
Iteration: 1638 | Episodes: 132900 | Median Reward: 39.80 | Max Reward: 48.03
Iteration: 1639 | Episodes: 133000 | Median Reward: 43.63 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.5         |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 1640          |
|    time_elapsed         | 86589         |
|    total_timesteps      | 13434880      |
| train/                  |               |
|    approx_kl            | 0.00071313593 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -120          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.6          |
|    n_updates            | 16390         |
|    policy_gradient_loss | -0.00233      |
|    std                  | 4.64          |
|    value_loss           | 2.3           |
-------------------------------------------
Iteration: 1641 | Episodes: 133100 | Median Reward: 43.18 | Max Reward: 48.03
Iteration: 1642 | Episodes: 133200 | Median Reward: 39.82 | Max Reward: 48.03
Iteration: 1643 | Episodes: 133300 | Median Reward: 39.30 | Max Reward: 48.03
Iteration: 1644 | Episodes: 133400 | Median Reward: 41.47 | Max Reward: 48.03
Iteration: 1645 | Episodes: 133500 | Median Reward: 41.76 | Max Reward: 48.03
Iteration: 1647 | Episodes: 133600 | Median Reward: 43.48 | Max Reward: 48.03
Iteration: 1648 | Episodes: 133700 | Median Reward: 43.85 | Max Reward: 48.03
Iteration: 1649 | Episodes: 133800 | Median Reward: 32.49 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.5        |
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 1650         |
|    time_elapsed         | 87165        |
|    total_timesteps      | 13516800     |
| train/                  |              |
|    approx_kl            | 0.0025125807 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.46        |
|    n_updates            | 16490        |
|    policy_gradient_loss | -0.0041      |
|    std                  | 4.67         |
|    value_loss           | 1.68         |
------------------------------------------
Iteration: 1650 | Episodes: 133900 | Median Reward: 37.06 | Max Reward: 48.03
Iteration: 1652 | Episodes: 134000 | Median Reward: 40.49 | Max Reward: 48.03
Iteration: 1653 | Episodes: 134100 | Median Reward: 39.78 | Max Reward: 48.03
Iteration: 1654 | Episodes: 134200 | Median Reward: 33.65 | Max Reward: 48.03
Iteration: 1655 | Episodes: 134300 | Median Reward: 30.35 | Max Reward: 48.03
Iteration: 1657 | Episodes: 134400 | Median Reward: 33.03 | Max Reward: 48.03
Iteration: 1658 | Episodes: 134500 | Median Reward: 42.49 | Max Reward: 48.03
Iteration: 1659 | Episodes: 134600 | Median Reward: 42.41 | Max Reward: 48.03
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.2        |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 1660         |
|    time_elapsed         | 87742        |
|    total_timesteps      | 13598720     |
| train/                  |              |
|    approx_kl            | 0.0005862039 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -120         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.79        |
|    n_updates            | 16590        |
|    policy_gradient_loss | -0.00269     |
|    std                  | 4.69         |
|    value_loss           | 0.715        |
------------------------------------------
Iteration: 1660 | Episodes: 134700 | Median Reward: 42.01 | Max Reward: 48.03
Iteration: 1661 | Episodes: 134800 | Median Reward: 37.57 | Max Reward: 48.03
Iteration: 1663 | Episodes: 134900 | Median Reward: 38.14 | Max Reward: 48.03
Iteration: 1664 | Episodes: 135000 | Median Reward: 38.14 | Max Reward: 48.03
Iteration: 1665 | Episodes: 135100 | Median Reward: 33.61 | Max Reward: 48.03
Iteration: 1666 | Episodes: 135200 | Median Reward: 42.78 | Max Reward: 48.03
Iteration: 1668 | Episodes: 135300 | Median Reward: 41.06 | Max Reward: 48.03
Iteration: 1669 | Episodes: 135400 | Median Reward: 36.04 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58           |
| time/                   |               |
|    fps                  | 154           |
|    iterations           | 1670          |
|    time_elapsed         | 88313         |
|    total_timesteps      | 13680640      |
| train/                  |               |
|    approx_kl            | 0.00027286645 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -120          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.16         |
|    n_updates            | 16690         |
|    policy_gradient_loss | -0.000583     |
|    std                  | 4.71          |
|    value_loss           | 1.98          |
-------------------------------------------
Iteration: 1670 | Episodes: 135500 | Median Reward: 46.05 | Max Reward: 48.03
Iteration: 1671 | Episodes: 135600 | Median Reward: 34.81 | Max Reward: 48.03
Iteration: 1673 | Episodes: 135700 | Median Reward: 36.56 | Max Reward: 48.03
Iteration: 1674 | Episodes: 135800 | Median Reward: 42.75 | Max Reward: 48.03
Iteration: 1675 | Episodes: 135900 | Median Reward: 39.38 | Max Reward: 48.03
Iteration: 1676 | Episodes: 136000 | Median Reward: 34.71 | Max Reward: 48.03
Iteration: 1677 | Episodes: 136100 | Median Reward: 42.51 | Max Reward: 48.03
Iteration: 1679 | Episodes: 136200 | Median Reward: 37.32 | Max Reward: 48.03
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.5         |
| time/                   |               |
|    fps                  | 154           |
|    iterations           | 1680          |
|    time_elapsed         | 88895         |
|    total_timesteps      | 13762560      |
| train/                  |               |
|    approx_kl            | 3.3827993e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -121          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.93         |
|    n_updates            | 16790         |
|    policy_gradient_loss | 0.000115      |
|    std                  | 4.75          |
|    value_loss           | 0.995         |
-------------------------------------------
Iteration: 1680 | Episodes: 136300 | Median Reward: 35.80 | Max Reward: 48.03
Iteration: 1681 | Episodes: 136400 | Median Reward: 42.28 | Max Reward: 48.03
Iteration: 1682 | Episodes: 136500 | Median Reward: 41.31 | Max Reward: 48.03
Iteration: 1684 | Episodes: 136600 | Median Reward: 35.99 | Max Reward: 48.03
Iteration: 1685 | Episodes: 136700 | Median Reward: 30.58 | Max Reward: 48.03
Iteration: 1686 | Episodes: 136800 | Median Reward: 31.43 | Max Reward: 48.03
Iteration: 1687 | Episodes: 136900 | Median Reward: 37.01 | Max Reward: 48.03
Iteration: 1689 | Episodes: 137000 | Median Reward: 41.29 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.9       |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 1690        |
|    time_elapsed         | 89474       |
|    total_timesteps      | 13844480    |
| train/                  |             |
|    approx_kl            | 0.001336033 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -121        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.47       |
|    n_updates            | 16890       |
|    policy_gradient_loss | -0.00894    |
|    std                  | 4.78        |
|    value_loss           | 0.987       |
-----------------------------------------
Iteration: 1690 | Episodes: 137100 | Median Reward: 39.57 | Max Reward: 48.17
Iteration: 1691 | Episodes: 137200 | Median Reward: 38.12 | Max Reward: 48.17
Iteration: 1692 | Episodes: 137300 | Median Reward: 41.53 | Max Reward: 48.17
Iteration: 1694 | Episodes: 137400 | Median Reward: 44.37 | Max Reward: 48.17
Iteration: 1695 | Episodes: 137500 | Median Reward: 34.59 | Max Reward: 48.17
Iteration: 1696 | Episodes: 137600 | Median Reward: 36.54 | Max Reward: 48.17
Iteration: 1697 | Episodes: 137700 | Median Reward: 41.67 | Max Reward: 48.17
Iteration: 1698 | Episodes: 137800 | Median Reward: 43.49 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -53.9        |
| time/                   |              |
|    fps                  | 154          |
|    iterations           | 1700         |
|    time_elapsed         | 90050        |
|    total_timesteps      | 13926400     |
| train/                  |              |
|    approx_kl            | 0.0002618335 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -121         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.48        |
|    n_updates            | 16990        |
|    policy_gradient_loss | -0.00187     |
|    std                  | 4.82         |
|    value_loss           | 1.14         |
------------------------------------------
Iteration: 1700 | Episodes: 137900 | Median Reward: 45.72 | Max Reward: 48.17
Iteration: 1701 | Episodes: 138000 | Median Reward: 36.79 | Max Reward: 48.17
Iteration: 1702 | Episodes: 138100 | Median Reward: 38.36 | Max Reward: 48.17
Iteration: 1703 | Episodes: 138200 | Median Reward: 38.67 | Max Reward: 48.17
Iteration: 1705 | Episodes: 138300 | Median Reward: 35.32 | Max Reward: 48.17
Iteration: 1706 | Episodes: 138400 | Median Reward: 36.11 | Max Reward: 48.17
Iteration: 1707 | Episodes: 138500 | Median Reward: 37.63 | Max Reward: 48.17
Iteration: 1708 | Episodes: 138600 | Median Reward: 37.86 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65           |
| time/                   |               |
|    fps                  | 154           |
|    iterations           | 1710          |
|    time_elapsed         | 90628         |
|    total_timesteps      | 14008320      |
| train/                  |               |
|    approx_kl            | 0.00046623917 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -121          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.52         |
|    n_updates            | 17090         |
|    policy_gradient_loss | -0.002        |
|    std                  | 4.86          |
|    value_loss           | 1.14          |
-------------------------------------------
Iteration: 1710 | Episodes: 138700 | Median Reward: 37.49 | Max Reward: 48.17
Iteration: 1711 | Episodes: 138800 | Median Reward: 38.55 | Max Reward: 48.17
Iteration: 1712 | Episodes: 138900 | Median Reward: 36.66 | Max Reward: 48.17
Iteration: 1713 | Episodes: 139000 | Median Reward: 34.04 | Max Reward: 48.17
Iteration: 1714 | Episodes: 139100 | Median Reward: 35.62 | Max Reward: 48.17
Iteration: 1716 | Episodes: 139200 | Median Reward: 31.93 | Max Reward: 48.17
Iteration: 1717 | Episodes: 139300 | Median Reward: 39.07 | Max Reward: 48.17
Iteration: 1718 | Episodes: 139400 | Median Reward: 38.18 | Max Reward: 48.17
Iteration: 1719 | Episodes: 139500 | Median Reward: 42.17 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.6         |
| time/                   |               |
|    fps                  | 155           |
|    iterations           | 1720          |
|    time_elapsed         | 90845         |
|    total_timesteps      | 14090240      |
| train/                  |               |
|    approx_kl            | 0.00035371684 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -121          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.78         |
|    n_updates            | 17190         |
|    policy_gradient_loss | -0.000734     |
|    std                  | 4.87          |
|    value_loss           | 0.869         |
-------------------------------------------
Iteration: 1721 | Episodes: 139600 | Median Reward: 41.11 | Max Reward: 48.17
Iteration: 1722 | Episodes: 139700 | Median Reward: 37.46 | Max Reward: 48.17
Iteration: 1723 | Episodes: 139800 | Median Reward: 41.06 | Max Reward: 48.17
Iteration: 1724 | Episodes: 139900 | Median Reward: 40.44 | Max Reward: 48.17
Iteration: 1726 | Episodes: 140000 | Median Reward: 43.83 | Max Reward: 48.17
Iteration: 1727 | Episodes: 140100 | Median Reward: 40.37 | Max Reward: 48.17
Iteration: 1728 | Episodes: 140200 | Median Reward: 36.87 | Max Reward: 48.17
Iteration: 1729 | Episodes: 140300 | Median Reward: 37.64 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.1       |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 1730        |
|    time_elapsed         | 90933       |
|    total_timesteps      | 14172160    |
| train/                  |             |
|    approx_kl            | 0.009965306 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -121        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.88       |
|    n_updates            | 17290       |
|    policy_gradient_loss | -0.0194     |
|    std                  | 4.91        |
|    value_loss           | 0.765       |
-----------------------------------------
Iteration: 1731 | Episodes: 140400 | Median Reward: 29.22 | Max Reward: 48.17
Iteration: 1732 | Episodes: 140500 | Median Reward: 36.32 | Max Reward: 48.17
Iteration: 1733 | Episodes: 140600 | Median Reward: 37.55 | Max Reward: 48.17
Iteration: 1734 | Episodes: 140700 | Median Reward: 35.40 | Max Reward: 48.17
Iteration: 1735 | Episodes: 140800 | Median Reward: 36.60 | Max Reward: 48.17
Iteration: 1737 | Episodes: 140900 | Median Reward: 37.85 | Max Reward: 48.17
Iteration: 1738 | Episodes: 141000 | Median Reward: 40.53 | Max Reward: 48.17
Iteration: 1739 | Episodes: 141100 | Median Reward: 37.70 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.6         |
| time/                   |               |
|    fps                  | 156           |
|    iterations           | 1740          |
|    time_elapsed         | 91022         |
|    total_timesteps      | 14254080      |
| train/                  |               |
|    approx_kl            | 1.0169104e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -122          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.02         |
|    n_updates            | 17390         |
|    policy_gradient_loss | 8.09e-05      |
|    std                  | 4.95          |
|    value_loss           | 1.84          |
-------------------------------------------
Iteration: 1740 | Episodes: 141200 | Median Reward: 39.18 | Max Reward: 48.17
Iteration: 1742 | Episodes: 141300 | Median Reward: 40.10 | Max Reward: 48.17
Iteration: 1743 | Episodes: 141400 | Median Reward: 40.10 | Max Reward: 48.17
Iteration: 1744 | Episodes: 141500 | Median Reward: 33.28 | Max Reward: 48.17
Iteration: 1745 | Episodes: 141600 | Median Reward: 39.87 | Max Reward: 48.17
Iteration: 1747 | Episodes: 141700 | Median Reward: 33.25 | Max Reward: 48.17
Iteration: 1748 | Episodes: 141800 | Median Reward: 35.40 | Max Reward: 48.17
Iteration: 1749 | Episodes: 141900 | Median Reward: 32.23 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.3       |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 1750        |
|    time_elapsed         | 91110       |
|    total_timesteps      | 14336000    |
| train/                  |             |
|    approx_kl            | 0.003168275 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -122        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.49       |
|    n_updates            | 17490       |
|    policy_gradient_loss | -0.00909    |
|    std                  | 4.98        |
|    value_loss           | 1.44        |
-----------------------------------------
Iteration: 1750 | Episodes: 142000 | Median Reward: 33.16 | Max Reward: 48.17
Iteration: 1751 | Episodes: 142100 | Median Reward: 33.27 | Max Reward: 48.17
Iteration: 1753 | Episodes: 142200 | Median Reward: 34.93 | Max Reward: 48.17
Iteration: 1754 | Episodes: 142300 | Median Reward: 37.92 | Max Reward: 48.17
Iteration: 1755 | Episodes: 142400 | Median Reward: 33.53 | Max Reward: 48.17
Iteration: 1756 | Episodes: 142500 | Median Reward: 36.36 | Max Reward: 48.17
Iteration: 1758 | Episodes: 142600 | Median Reward: 37.21 | Max Reward: 48.17
Iteration: 1759 | Episodes: 142700 | Median Reward: 36.73 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.8        |
| time/                   |              |
|    fps                  | 158          |
|    iterations           | 1760         |
|    time_elapsed         | 91199        |
|    total_timesteps      | 14417920     |
| train/                  |              |
|    approx_kl            | 0.0015568617 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -122         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.3         |
|    n_updates            | 17590        |
|    policy_gradient_loss | -0.00223     |
|    std                  | 5.01         |
|    value_loss           | 1.45         |
------------------------------------------
Iteration: 1760 | Episodes: 142800 | Median Reward: 36.72 | Max Reward: 48.17
Iteration: 1761 | Episodes: 142900 | Median Reward: 31.19 | Max Reward: 48.17
Iteration: 1763 | Episodes: 143000 | Median Reward: 35.78 | Max Reward: 48.17
Iteration: 1764 | Episodes: 143100 | Median Reward: 35.26 | Max Reward: 48.17
Iteration: 1765 | Episodes: 143200 | Median Reward: 37.12 | Max Reward: 48.17
Iteration: 1766 | Episodes: 143300 | Median Reward: 39.71 | Max Reward: 48.17
Iteration: 1767 | Episodes: 143400 | Median Reward: 40.62 | Max Reward: 48.17
Iteration: 1769 | Episodes: 143500 | Median Reward: 41.64 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.5        |
| time/                   |              |
|    fps                  | 158          |
|    iterations           | 1770         |
|    time_elapsed         | 91288        |
|    total_timesteps      | 14499840     |
| train/                  |              |
|    approx_kl            | 0.0005031642 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -122         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.95        |
|    n_updates            | 17690        |
|    policy_gradient_loss | -0.00146     |
|    std                  | 5.03         |
|    value_loss           | 0.571        |
------------------------------------------
Iteration: 1770 | Episodes: 143600 | Median Reward: 37.78 | Max Reward: 48.17
Iteration: 1771 | Episodes: 143700 | Median Reward: 39.45 | Max Reward: 48.17
Iteration: 1772 | Episodes: 143800 | Median Reward: 37.08 | Max Reward: 48.17
Iteration: 1774 | Episodes: 143900 | Median Reward: 37.09 | Max Reward: 48.17
Iteration: 1775 | Episodes: 144000 | Median Reward: 35.33 | Max Reward: 48.17
Iteration: 1776 | Episodes: 144100 | Median Reward: 36.32 | Max Reward: 48.17
Iteration: 1777 | Episodes: 144200 | Median Reward: 39.25 | Max Reward: 48.17
Iteration: 1779 | Episodes: 144300 | Median Reward: 35.04 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.5        |
| time/                   |              |
|    fps                  | 159          |
|    iterations           | 1780         |
|    time_elapsed         | 91376        |
|    total_timesteps      | 14581760     |
| train/                  |              |
|    approx_kl            | 0.0006649522 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -122         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.63        |
|    n_updates            | 17790        |
|    policy_gradient_loss | -0.00312     |
|    std                  | 5.06         |
|    value_loss           | 2.22         |
------------------------------------------
Iteration: 1780 | Episodes: 144400 | Median Reward: 41.79 | Max Reward: 48.17
Iteration: 1781 | Episodes: 144500 | Median Reward: 42.46 | Max Reward: 48.17
Iteration: 1782 | Episodes: 144600 | Median Reward: 28.48 | Max Reward: 48.17
Iteration: 1784 | Episodes: 144700 | Median Reward: 40.04 | Max Reward: 48.17
Iteration: 1785 | Episodes: 144800 | Median Reward: 41.66 | Max Reward: 48.17
Iteration: 1786 | Episodes: 144900 | Median Reward: 38.26 | Max Reward: 48.17
Iteration: 1787 | Episodes: 145000 | Median Reward: 34.37 | Max Reward: 48.17
Iteration: 1788 | Episodes: 145100 | Median Reward: 34.81 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.6         |
| time/                   |               |
|    fps                  | 160           |
|    iterations           | 1790          |
|    time_elapsed         | 91465         |
|    total_timesteps      | 14663680      |
| train/                  |               |
|    approx_kl            | 0.00033238583 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -122          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.02         |
|    n_updates            | 17890         |
|    policy_gradient_loss | -0.00241      |
|    std                  | 5.08          |
|    value_loss           | 1.79          |
-------------------------------------------
Iteration: 1790 | Episodes: 145200 | Median Reward: 42.28 | Max Reward: 48.17
Iteration: 1791 | Episodes: 145300 | Median Reward: 42.28 | Max Reward: 48.17
Iteration: 1792 | Episodes: 145400 | Median Reward: 31.68 | Max Reward: 48.17
Iteration: 1793 | Episodes: 145500 | Median Reward: 31.25 | Max Reward: 48.17
Iteration: 1795 | Episodes: 145600 | Median Reward: 33.67 | Max Reward: 48.17
Iteration: 1796 | Episodes: 145700 | Median Reward: 33.84 | Max Reward: 48.17
Iteration: 1797 | Episodes: 145800 | Median Reward: 42.56 | Max Reward: 48.17
Iteration: 1798 | Episodes: 145900 | Median Reward: 39.34 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.7         |
| time/                   |               |
|    fps                  | 161           |
|    iterations           | 1800          |
|    time_elapsed         | 91554         |
|    total_timesteps      | 14745600      |
| train/                  |               |
|    approx_kl            | 0.00019707932 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -122          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.69         |
|    n_updates            | 17990         |
|    policy_gradient_loss | -0.00123      |
|    std                  | 5.1           |
|    value_loss           | 1.06          |
-------------------------------------------
Iteration: 1800 | Episodes: 146000 | Median Reward: 41.01 | Max Reward: 48.17
Iteration: 1801 | Episodes: 146100 | Median Reward: 40.00 | Max Reward: 48.17
Iteration: 1802 | Episodes: 146200 | Median Reward: 34.51 | Max Reward: 48.17
Iteration: 1803 | Episodes: 146300 | Median Reward: 34.68 | Max Reward: 48.17
Iteration: 1804 | Episodes: 146400 | Median Reward: 37.43 | Max Reward: 48.17
Iteration: 1806 | Episodes: 146500 | Median Reward: 37.54 | Max Reward: 48.17
Iteration: 1807 | Episodes: 146600 | Median Reward: 40.50 | Max Reward: 48.17
Iteration: 1808 | Episodes: 146700 | Median Reward: 35.15 | Max Reward: 48.17
Iteration: 1809 | Episodes: 146800 | Median Reward: 44.14 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.5        |
| time/                   |              |
|    fps                  | 161          |
|    iterations           | 1810         |
|    time_elapsed         | 91643        |
|    total_timesteps      | 14827520     |
| train/                  |              |
|    approx_kl            | 0.0018754301 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -122         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.57        |
|    n_updates            | 18090        |
|    policy_gradient_loss | -0.00646     |
|    std                  | 5.12         |
|    value_loss           | 0.988        |
------------------------------------------
Iteration: 1811 | Episodes: 146900 | Median Reward: 33.40 | Max Reward: 48.17
Iteration: 1812 | Episodes: 147000 | Median Reward: 38.33 | Max Reward: 48.17
Iteration: 1813 | Episodes: 147100 | Median Reward: 40.65 | Max Reward: 48.17
Iteration: 1814 | Episodes: 147200 | Median Reward: 36.13 | Max Reward: 48.17
Iteration: 1816 | Episodes: 147300 | Median Reward: 43.13 | Max Reward: 48.17
Iteration: 1817 | Episodes: 147400 | Median Reward: 40.27 | Max Reward: 48.17
Iteration: 1818 | Episodes: 147500 | Median Reward: 41.44 | Max Reward: 48.17
Iteration: 1819 | Episodes: 147600 | Median Reward: 41.44 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.9         |
| time/                   |               |
|    fps                  | 162           |
|    iterations           | 1820          |
|    time_elapsed         | 91731         |
|    total_timesteps      | 14909440      |
| train/                  |               |
|    approx_kl            | 0.00028972892 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -122          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.95         |
|    n_updates            | 18190         |
|    policy_gradient_loss | -0.000986     |
|    std                  | 5.14          |
|    value_loss           | 1.34          |
-------------------------------------------
Iteration: 1821 | Episodes: 147700 | Median Reward: 37.41 | Max Reward: 48.17
Iteration: 1822 | Episodes: 147800 | Median Reward: 41.96 | Max Reward: 48.17
Iteration: 1823 | Episodes: 147900 | Median Reward: 32.52 | Max Reward: 48.17
Iteration: 1824 | Episodes: 148000 | Median Reward: 39.30 | Max Reward: 48.17
Iteration: 1825 | Episodes: 148100 | Median Reward: 34.79 | Max Reward: 48.17
Iteration: 1827 | Episodes: 148200 | Median Reward: 41.62 | Max Reward: 48.17
Iteration: 1828 | Episodes: 148300 | Median Reward: 34.88 | Max Reward: 48.17
Iteration: 1829 | Episodes: 148400 | Median Reward: 32.07 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -68         |
| time/                   |             |
|    fps                  | 163         |
|    iterations           | 1830        |
|    time_elapsed         | 91820       |
|    total_timesteps      | 14991360    |
| train/                  |             |
|    approx_kl            | 0.016626542 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -123        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.02       |
|    n_updates            | 18290       |
|    policy_gradient_loss | -0.0475     |
|    std                  | 5.17        |
|    value_loss           | 1.01        |
-----------------------------------------
Iteration: 1830 | Episodes: 148500 | Median Reward: 42.67 | Max Reward: 48.17
Iteration: 1832 | Episodes: 148600 | Median Reward: 40.10 | Max Reward: 48.17
Iteration: 1833 | Episodes: 148700 | Median Reward: 44.13 | Max Reward: 48.17
Iteration: 1834 | Episodes: 148800 | Median Reward: 37.27 | Max Reward: 48.17
Iteration: 1835 | Episodes: 148900 | Median Reward: 42.60 | Max Reward: 48.17
Iteration: 1837 | Episodes: 149000 | Median Reward: 39.23 | Max Reward: 48.17
Iteration: 1838 | Episodes: 149100 | Median Reward: 43.17 | Max Reward: 48.17
Iteration: 1839 | Episodes: 149200 | Median Reward: 39.49 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 164          |
|    iterations           | 1840         |
|    time_elapsed         | 91908        |
|    total_timesteps      | 15073280     |
| train/                  |              |
|    approx_kl            | 0.0009331198 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -123         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -2.81        |
|    n_updates            | 18390        |
|    policy_gradient_loss | -0.00455     |
|    std                  | 5.2          |
|    value_loss           | 4.36         |
------------------------------------------
Iteration: 1840 | Episodes: 149300 | Median Reward: 37.47 | Max Reward: 48.17
Iteration: 1841 | Episodes: 149400 | Median Reward: 36.77 | Max Reward: 48.17
Iteration: 1843 | Episodes: 149500 | Median Reward: 38.75 | Max Reward: 48.17
Iteration: 1844 | Episodes: 149600 | Median Reward: 36.66 | Max Reward: 48.17
Iteration: 1845 | Episodes: 149700 | Median Reward: 39.36 | Max Reward: 48.17
Iteration: 1846 | Episodes: 149800 | Median Reward: 42.05 | Max Reward: 48.17
Iteration: 1848 | Episodes: 149900 | Median Reward: 34.79 | Max Reward: 48.17
Iteration: 1849 | Episodes: 150000 | Median Reward: 35.38 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.7         |
| time/                   |               |
|    fps                  | 164           |
|    iterations           | 1850          |
|    time_elapsed         | 91998         |
|    total_timesteps      | 15155200      |
| train/                  |               |
|    approx_kl            | 0.00063164637 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.25         |
|    n_updates            | 18490         |
|    policy_gradient_loss | -0.000803     |
|    std                  | 5.22          |
|    value_loss           | 0.567         |
-------------------------------------------
Iteration: 1850 | Episodes: 150100 | Median Reward: 42.03 | Max Reward: 48.17
Iteration: 1851 | Episodes: 150200 | Median Reward: 38.99 | Max Reward: 48.17
Iteration: 1853 | Episodes: 150300 | Median Reward: 38.06 | Max Reward: 48.17
Iteration: 1854 | Episodes: 150400 | Median Reward: 39.05 | Max Reward: 48.17
Iteration: 1855 | Episodes: 150500 | Median Reward: 43.22 | Max Reward: 48.17
Iteration: 1856 | Episodes: 150600 | Median Reward: 38.03 | Max Reward: 48.17
Iteration: 1857 | Episodes: 150700 | Median Reward: 38.03 | Max Reward: 48.17
Iteration: 1859 | Episodes: 150800 | Median Reward: 32.30 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.8         |
| time/                   |               |
|    fps                  | 165           |
|    iterations           | 1860          |
|    time_elapsed         | 92086         |
|    total_timesteps      | 15237120      |
| train/                  |               |
|    approx_kl            | 0.00054786046 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.33         |
|    n_updates            | 18590         |
|    policy_gradient_loss | -0.00406      |
|    std                  | 5.26          |
|    value_loss           | 2.23          |
-------------------------------------------
Iteration: 1860 | Episodes: 150900 | Median Reward: 36.68 | Max Reward: 48.17
Iteration: 1861 | Episodes: 151000 | Median Reward: 34.31 | Max Reward: 48.17
Iteration: 1862 | Episodes: 151100 | Median Reward: 36.90 | Max Reward: 48.17
Iteration: 1864 | Episodes: 151200 | Median Reward: 42.89 | Max Reward: 48.17
Iteration: 1865 | Episodes: 151300 | Median Reward: 40.05 | Max Reward: 48.17
Iteration: 1866 | Episodes: 151400 | Median Reward: 38.76 | Max Reward: 48.17
Iteration: 1867 | Episodes: 151500 | Median Reward: 37.33 | Max Reward: 48.17
Iteration: 1869 | Episodes: 151600 | Median Reward: 38.15 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.2         |
| time/                   |               |
|    fps                  | 166           |
|    iterations           | 1870          |
|    time_elapsed         | 92174         |
|    total_timesteps      | 15319040      |
| train/                  |               |
|    approx_kl            | 0.00038393668 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -123          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.39         |
|    n_updates            | 18690         |
|    policy_gradient_loss | -0.00312      |
|    std                  | 5.3           |
|    value_loss           | 1.75          |
-------------------------------------------
Iteration: 1870 | Episodes: 151700 | Median Reward: 36.33 | Max Reward: 48.17
Iteration: 1871 | Episodes: 151800 | Median Reward: 38.82 | Max Reward: 48.17
Iteration: 1872 | Episodes: 151900 | Median Reward: 39.11 | Max Reward: 48.17
Iteration: 1874 | Episodes: 152000 | Median Reward: 38.67 | Max Reward: 48.17
Iteration: 1875 | Episodes: 152100 | Median Reward: 41.23 | Max Reward: 48.17
Iteration: 1876 | Episodes: 152200 | Median Reward: 38.13 | Max Reward: 48.17
Iteration: 1877 | Episodes: 152300 | Median Reward: 35.28 | Max Reward: 48.17
Iteration: 1878 | Episodes: 152400 | Median Reward: 31.51 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.1        |
| time/                   |              |
|    fps                  | 166          |
|    iterations           | 1880         |
|    time_elapsed         | 92263        |
|    total_timesteps      | 15400960     |
| train/                  |              |
|    approx_kl            | 0.0011819957 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -123         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.56        |
|    n_updates            | 18790        |
|    policy_gradient_loss | -0.00326     |
|    std                  | 5.34         |
|    value_loss           | 0.959        |
------------------------------------------
Iteration: 1880 | Episodes: 152500 | Median Reward: 36.88 | Max Reward: 48.17
Iteration: 1881 | Episodes: 152600 | Median Reward: 38.70 | Max Reward: 48.17
Iteration: 1882 | Episodes: 152700 | Median Reward: 38.35 | Max Reward: 48.17
Iteration: 1883 | Episodes: 152800 | Median Reward: 40.27 | Max Reward: 48.17
Iteration: 1885 | Episodes: 152900 | Median Reward: 38.83 | Max Reward: 48.17
Iteration: 1886 | Episodes: 153000 | Median Reward: 43.78 | Max Reward: 48.17
Iteration: 1887 | Episodes: 153100 | Median Reward: 43.06 | Max Reward: 48.17
Iteration: 1888 | Episodes: 153200 | Median Reward: 39.88 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.6        |
| time/                   |              |
|    fps                  | 167          |
|    iterations           | 1890         |
|    time_elapsed         | 92352        |
|    total_timesteps      | 15482880     |
| train/                  |              |
|    approx_kl            | 3.158721e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -123         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.84        |
|    n_updates            | 18890        |
|    policy_gradient_loss | 4.38e-06     |
|    std                  | 5.36         |
|    value_loss           | 1.28         |
------------------------------------------
Iteration: 1890 | Episodes: 153300 | Median Reward: 34.81 | Max Reward: 48.17
Iteration: 1891 | Episodes: 153400 | Median Reward: 34.54 | Max Reward: 48.17
Iteration: 1892 | Episodes: 153500 | Median Reward: 32.84 | Max Reward: 48.17
Iteration: 1893 | Episodes: 153600 | Median Reward: 39.90 | Max Reward: 48.17
Iteration: 1894 | Episodes: 153700 | Median Reward: 41.42 | Max Reward: 48.17
Iteration: 1896 | Episodes: 153800 | Median Reward: 42.57 | Max Reward: 48.17
Iteration: 1897 | Episodes: 153900 | Median Reward: 42.50 | Max Reward: 48.17
Iteration: 1898 | Episodes: 154000 | Median Reward: 32.63 | Max Reward: 48.17
Iteration: 1899 | Episodes: 154100 | Median Reward: 32.88 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.4       |
| time/                   |             |
|    fps                  | 168         |
|    iterations           | 1900        |
|    time_elapsed         | 92440       |
|    total_timesteps      | 15564800    |
| train/                  |             |
|    approx_kl            | 0.000282467 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -124        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.000707    |
|    loss                 | -4.46       |
|    n_updates            | 18990       |
|    policy_gradient_loss | -2.48e-05   |
|    std                  | 5.38        |
|    value_loss           | 2.36        |
-----------------------------------------
Iteration: 1901 | Episodes: 154200 | Median Reward: 35.23 | Max Reward: 48.17
Iteration: 1902 | Episodes: 154300 | Median Reward: 38.78 | Max Reward: 48.17
Iteration: 1903 | Episodes: 154400 | Median Reward: 42.19 | Max Reward: 48.17
Iteration: 1904 | Episodes: 154500 | Median Reward: 39.33 | Max Reward: 48.17
Iteration: 1906 | Episodes: 154600 | Median Reward: 41.31 | Max Reward: 48.17
Iteration: 1907 | Episodes: 154700 | Median Reward: 33.22 | Max Reward: 48.17
Iteration: 1908 | Episodes: 154800 | Median Reward: 33.12 | Max Reward: 48.17
Iteration: 1909 | Episodes: 154900 | Median Reward: 33.50 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -67.7       |
| time/                   |             |
|    fps                  | 169         |
|    iterations           | 1910        |
|    time_elapsed         | 92528       |
|    total_timesteps      | 15646720    |
| train/                  |             |
|    approx_kl            | 0.004900837 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -124        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.75       |
|    n_updates            | 19090       |
|    policy_gradient_loss | -0.00993    |
|    std                  | 5.42        |
|    value_loss           | 1.28        |
-----------------------------------------
Iteration: 1911 | Episodes: 155000 | Median Reward: 36.24 | Max Reward: 48.17
Iteration: 1912 | Episodes: 155100 | Median Reward: 39.12 | Max Reward: 48.17
Iteration: 1913 | Episodes: 155200 | Median Reward: 42.42 | Max Reward: 48.17
Iteration: 1914 | Episodes: 155300 | Median Reward: 41.47 | Max Reward: 48.17
Iteration: 1915 | Episodes: 155400 | Median Reward: 39.58 | Max Reward: 48.17
Iteration: 1917 | Episodes: 155500 | Median Reward: 38.90 | Max Reward: 48.17
Iteration: 1918 | Episodes: 155600 | Median Reward: 29.98 | Max Reward: 48.17
Iteration: 1919 | Episodes: 155700 | Median Reward: 40.29 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.5        |
| time/                   |              |
|    fps                  | 169          |
|    iterations           | 1920         |
|    time_elapsed         | 92617        |
|    total_timesteps      | 15728640     |
| train/                  |              |
|    approx_kl            | 0.0005326284 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -124         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.46        |
|    n_updates            | 19190        |
|    policy_gradient_loss | -0.002       |
|    std                  | 5.45         |
|    value_loss           | 1.85         |
------------------------------------------
Iteration: 1920 | Episodes: 155800 | Median Reward: 40.44 | Max Reward: 48.17
Iteration: 1922 | Episodes: 155900 | Median Reward: 41.62 | Max Reward: 48.17
Iteration: 1923 | Episodes: 156000 | Median Reward: 36.75 | Max Reward: 48.17
Iteration: 1924 | Episodes: 156100 | Median Reward: 29.99 | Max Reward: 48.17
Iteration: 1925 | Episodes: 156200 | Median Reward: 38.99 | Max Reward: 48.17
Iteration: 1927 | Episodes: 156300 | Median Reward: 37.85 | Max Reward: 48.17
Iteration: 1928 | Episodes: 156400 | Median Reward: 38.81 | Max Reward: 48.17
Iteration: 1929 | Episodes: 156500 | Median Reward: 37.99 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.2        |
| time/                   |              |
|    fps                  | 170          |
|    iterations           | 1930         |
|    time_elapsed         | 92706        |
|    total_timesteps      | 15810560     |
| train/                  |              |
|    approx_kl            | 0.0013132876 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -124         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.46        |
|    n_updates            | 19290        |
|    policy_gradient_loss | -0.00288     |
|    std                  | 5.44         |
|    value_loss           | 0.921        |
------------------------------------------
Iteration: 1930 | Episodes: 156600 | Median Reward: 37.51 | Max Reward: 48.17
Iteration: 1931 | Episodes: 156700 | Median Reward: 34.96 | Max Reward: 48.17
Iteration: 1933 | Episodes: 156800 | Median Reward: 39.10 | Max Reward: 48.17
Iteration: 1934 | Episodes: 156900 | Median Reward: 41.26 | Max Reward: 48.17
Iteration: 1935 | Episodes: 157000 | Median Reward: 40.75 | Max Reward: 48.17
Iteration: 1936 | Episodes: 157100 | Median Reward: 40.02 | Max Reward: 48.17
Iteration: 1938 | Episodes: 157200 | Median Reward: 39.50 | Max Reward: 48.17
Iteration: 1939 | Episodes: 157300 | Median Reward: 38.65 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.5         |
| time/                   |               |
|    fps                  | 171           |
|    iterations           | 1940          |
|    time_elapsed         | 92794         |
|    total_timesteps      | 15892480      |
| train/                  |               |
|    approx_kl            | 0.00066308567 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -124          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.91         |
|    n_updates            | 19390         |
|    policy_gradient_loss | -0.002        |
|    std                  | 5.47          |
|    value_loss           | 0.757         |
-------------------------------------------
Iteration: 1940 | Episodes: 157400 | Median Reward: 33.78 | Max Reward: 48.17
Iteration: 1941 | Episodes: 157500 | Median Reward: 35.14 | Max Reward: 48.17
Iteration: 1943 | Episodes: 157600 | Median Reward: 37.75 | Max Reward: 48.17
Iteration: 1944 | Episodes: 157700 | Median Reward: 39.41 | Max Reward: 48.17
Iteration: 1945 | Episodes: 157800 | Median Reward: 41.08 | Max Reward: 48.17
Iteration: 1946 | Episodes: 157900 | Median Reward: 34.73 | Max Reward: 48.17
Iteration: 1947 | Episodes: 158000 | Median Reward: 39.73 | Max Reward: 48.17
Iteration: 1949 | Episodes: 158100 | Median Reward: 39.17 | Max Reward: 48.17
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -61.5      |
| time/                   |            |
|    fps                  | 171        |
|    iterations           | 1950       |
|    time_elapsed         | 92883      |
|    total_timesteps      | 15974400   |
| train/                  |            |
|    approx_kl            | 0.00801993 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -124       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000707   |
|    loss                 | -5.97      |
|    n_updates            | 19490      |
|    policy_gradient_loss | -0.0211    |
|    std                  | 5.5        |
|    value_loss           | 0.525      |
----------------------------------------
Iteration: 1950 | Episodes: 158200 | Median Reward: 39.17 | Max Reward: 48.17
Iteration: 1951 | Episodes: 158300 | Median Reward: 42.42 | Max Reward: 48.17
Iteration: 1952 | Episodes: 158400 | Median Reward: 38.45 | Max Reward: 48.17
Iteration: 1954 | Episodes: 158500 | Median Reward: 40.44 | Max Reward: 48.17
Iteration: 1955 | Episodes: 158600 | Median Reward: 39.27 | Max Reward: 48.17
Iteration: 1956 | Episodes: 158700 | Median Reward: 40.14 | Max Reward: 48.17
Iteration: 1957 | Episodes: 158800 | Median Reward: 40.29 | Max Reward: 48.17
Iteration: 1959 | Episodes: 158900 | Median Reward: 40.88 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.3        |
| time/                   |              |
|    fps                  | 172          |
|    iterations           | 1960         |
|    time_elapsed         | 92972        |
|    total_timesteps      | 16056320     |
| train/                  |              |
|    approx_kl            | 0.0041764555 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -124         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.04        |
|    n_updates            | 19590        |
|    policy_gradient_loss | -0.00591     |
|    std                  | 5.54         |
|    value_loss           | 0.65         |
------------------------------------------
Iteration: 1960 | Episodes: 159000 | Median Reward: 40.90 | Max Reward: 48.17
Iteration: 1961 | Episodes: 159100 | Median Reward: 41.41 | Max Reward: 48.17
Iteration: 1962 | Episodes: 159200 | Median Reward: 39.93 | Max Reward: 48.17
Iteration: 1964 | Episodes: 159300 | Median Reward: 39.70 | Max Reward: 48.17
Iteration: 1965 | Episodes: 159400 | Median Reward: 37.57 | Max Reward: 48.17
Iteration: 1966 | Episodes: 159500 | Median Reward: 38.15 | Max Reward: 48.17
Iteration: 1967 | Episodes: 159600 | Median Reward: 39.55 | Max Reward: 48.17
Iteration: 1968 | Episodes: 159700 | Median Reward: 41.95 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.6        |
| time/                   |              |
|    fps                  | 173          |
|    iterations           | 1970         |
|    time_elapsed         | 93060        |
|    total_timesteps      | 16138240     |
| train/                  |              |
|    approx_kl            | 0.0041411864 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -124         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.13        |
|    n_updates            | 19690        |
|    policy_gradient_loss | -0.00709     |
|    std                  | 5.61         |
|    value_loss           | 0.998        |
------------------------------------------
Iteration: 1970 | Episodes: 159800 | Median Reward: 42.27 | Max Reward: 48.17
Iteration: 1971 | Episodes: 159900 | Median Reward: 44.09 | Max Reward: 48.17
Iteration: 1972 | Episodes: 160000 | Median Reward: 38.09 | Max Reward: 48.17
Iteration: 1973 | Episodes: 160100 | Median Reward: 38.09 | Max Reward: 48.17
Iteration: 1975 | Episodes: 160200 | Median Reward: 42.48 | Max Reward: 48.17
Iteration: 1976 | Episodes: 160300 | Median Reward: 42.40 | Max Reward: 48.17
Iteration: 1977 | Episodes: 160400 | Median Reward: 37.55 | Max Reward: 48.17
Iteration: 1978 | Episodes: 160500 | Median Reward: 29.04 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.7         |
| time/                   |               |
|    fps                  | 174           |
|    iterations           | 1980          |
|    time_elapsed         | 93149         |
|    total_timesteps      | 16220160      |
| train/                  |               |
|    approx_kl            | 0.00018748001 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -125          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.92         |
|    n_updates            | 19790         |
|    policy_gradient_loss | 0.00196       |
|    std                  | 5.63          |
|    value_loss           | 1.66          |
-------------------------------------------
Iteration: 1980 | Episodes: 160600 | Median Reward: 41.53 | Max Reward: 48.17
Iteration: 1981 | Episodes: 160700 | Median Reward: 37.05 | Max Reward: 48.17
Iteration: 1982 | Episodes: 160800 | Median Reward: 39.44 | Max Reward: 48.17
Iteration: 1983 | Episodes: 160900 | Median Reward: 41.02 | Max Reward: 48.17
Iteration: 1984 | Episodes: 161000 | Median Reward: 35.26 | Max Reward: 48.17
Iteration: 1986 | Episodes: 161100 | Median Reward: 41.22 | Max Reward: 48.17
Iteration: 1987 | Episodes: 161200 | Median Reward: 39.51 | Max Reward: 48.17
Iteration: 1988 | Episodes: 161300 | Median Reward: 35.29 | Max Reward: 48.17
Iteration: 1989 | Episodes: 161400 | Median Reward: 30.98 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69.1        |
| time/                   |              |
|    fps                  | 174          |
|    iterations           | 1990         |
|    time_elapsed         | 93238        |
|    total_timesteps      | 16302080     |
| train/                  |              |
|    approx_kl            | 0.0011333061 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -125         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -6.17        |
|    n_updates            | 19890        |
|    policy_gradient_loss | -0.00322     |
|    std                  | 5.66         |
|    value_loss           | 0.297        |
------------------------------------------
Iteration: 1991 | Episodes: 161500 | Median Reward: 38.58 | Max Reward: 48.17
Iteration: 1992 | Episodes: 161600 | Median Reward: 33.39 | Max Reward: 48.17
Iteration: 1993 | Episodes: 161700 | Median Reward: 32.81 | Max Reward: 48.17
Iteration: 1994 | Episodes: 161800 | Median Reward: 37.30 | Max Reward: 48.17
Iteration: 1996 | Episodes: 161900 | Median Reward: 29.62 | Max Reward: 48.17
Iteration: 1997 | Episodes: 162000 | Median Reward: 35.64 | Max Reward: 48.17
Iteration: 1998 | Episodes: 162100 | Median Reward: 37.92 | Max Reward: 48.17
Iteration: 1999 | Episodes: 162200 | Median Reward: 40.30 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.1         |
| time/                   |               |
|    fps                  | 175           |
|    iterations           | 2000          |
|    time_elapsed         | 93327         |
|    total_timesteps      | 16384000      |
| train/                  |               |
|    approx_kl            | 5.6687757e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -125          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.3          |
|    n_updates            | 19990         |
|    policy_gradient_loss | -0.000291     |
|    std                  | 5.69          |
|    value_loss           | 2.26          |
-------------------------------------------
Iteration: 2001 | Episodes: 162300 | Median Reward: 41.93 | Max Reward: 48.17
Iteration: 2002 | Episodes: 162400 | Median Reward: 39.61 | Max Reward: 48.17
Iteration: 2003 | Episodes: 162500 | Median Reward: 41.43 | Max Reward: 48.17
Iteration: 2004 | Episodes: 162600 | Median Reward: 41.63 | Max Reward: 48.17
Iteration: 2005 | Episodes: 162700 | Median Reward: 40.25 | Max Reward: 48.17
Iteration: 2007 | Episodes: 162800 | Median Reward: 40.07 | Max Reward: 48.17
Iteration: 2008 | Episodes: 162900 | Median Reward: 39.53 | Max Reward: 48.17
Iteration: 2009 | Episodes: 163000 | Median Reward: 42.63 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.6        |
| time/                   |              |
|    fps                  | 176          |
|    iterations           | 2010         |
|    time_elapsed         | 93416        |
|    total_timesteps      | 16465920     |
| train/                  |              |
|    approx_kl            | 0.0022608752 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -125         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -5.42        |
|    n_updates            | 20090        |
|    policy_gradient_loss | -0.00136     |
|    std                  | 5.72         |
|    value_loss           | 1.29         |
------------------------------------------
Iteration: 2010 | Episodes: 163100 | Median Reward: 38.90 | Max Reward: 48.17
Iteration: 2012 | Episodes: 163200 | Median Reward: 38.66 | Max Reward: 48.17
Iteration: 2013 | Episodes: 163300 | Median Reward: 33.72 | Max Reward: 48.17
Iteration: 2014 | Episodes: 163400 | Median Reward: 34.14 | Max Reward: 48.17
Iteration: 2015 | Episodes: 163500 | Median Reward: 37.14 | Max Reward: 48.17
Iteration: 2017 | Episodes: 163600 | Median Reward: 37.99 | Max Reward: 48.17
Iteration: 2018 | Episodes: 163700 | Median Reward: 33.66 | Max Reward: 48.17
Iteration: 2019 | Episodes: 163800 | Median Reward: 38.12 | Max Reward: 48.17
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -61.9      |
| time/                   |            |
|    fps                  | 176        |
|    iterations           | 2020       |
|    time_elapsed         | 93504      |
|    total_timesteps      | 16547840   |
| train/                  |            |
|    approx_kl            | 0.01248478 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -125       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.000707   |
|    loss                 | -5.87      |
|    n_updates            | 20190      |
|    policy_gradient_loss | -0.0157    |
|    std                  | 5.76       |
|    value_loss           | 0.68       |
----------------------------------------
Iteration: 2020 | Episodes: 163900 | Median Reward: 37.61 | Max Reward: 48.17
Iteration: 2021 | Episodes: 164000 | Median Reward: 37.01 | Max Reward: 48.17
Iteration: 2023 | Episodes: 164100 | Median Reward: 33.74 | Max Reward: 48.17
Iteration: 2024 | Episodes: 164200 | Median Reward: 40.90 | Max Reward: 48.17
Iteration: 2025 | Episodes: 164300 | Median Reward: 40.22 | Max Reward: 48.17
Iteration: 2026 | Episodes: 164400 | Median Reward: 37.60 | Max Reward: 48.17
Iteration: 2028 | Episodes: 164500 | Median Reward: 38.98 | Max Reward: 48.17
Iteration: 2029 | Episodes: 164600 | Median Reward: 39.35 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.9        |
| time/                   |              |
|    fps                  | 177          |
|    iterations           | 2030         |
|    time_elapsed         | 93593        |
|    total_timesteps      | 16629760     |
| train/                  |              |
|    approx_kl            | 0.0038310653 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -125         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.21        |
|    n_updates            | 20290        |
|    policy_gradient_loss | -0.00547     |
|    std                  | 5.79         |
|    value_loss           | 0.231        |
------------------------------------------
Iteration: 2030 | Episodes: 164700 | Median Reward: 39.76 | Max Reward: 48.17
Iteration: 2031 | Episodes: 164800 | Median Reward: 35.40 | Max Reward: 48.17
Iteration: 2033 | Episodes: 164900 | Median Reward: 42.95 | Max Reward: 48.17
Iteration: 2034 | Episodes: 165000 | Median Reward: 38.70 | Max Reward: 48.17
Iteration: 2035 | Episodes: 165100 | Median Reward: 28.05 | Max Reward: 48.17
Iteration: 2036 | Episodes: 165200 | Median Reward: 33.60 | Max Reward: 48.17
Iteration: 2038 | Episodes: 165300 | Median Reward: 36.07 | Max Reward: 48.17
Iteration: 2039 | Episodes: 165400 | Median Reward: 42.65 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.9         |
| time/                   |               |
|    fps                  | 178           |
|    iterations           | 2040          |
|    time_elapsed         | 93682         |
|    total_timesteps      | 16711680      |
| train/                  |               |
|    approx_kl            | 2.8804854e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -125          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.55         |
|    n_updates            | 20390         |
|    policy_gradient_loss | -0.000185     |
|    std                  | 5.84          |
|    value_loss           | 1.68          |
-------------------------------------------
Iteration: 2040 | Episodes: 165500 | Median Reward: 38.81 | Max Reward: 48.17
Iteration: 2041 | Episodes: 165600 | Median Reward: 45.93 | Max Reward: 48.17
Iteration: 2042 | Episodes: 165700 | Median Reward: 42.95 | Max Reward: 48.17
Iteration: 2044 | Episodes: 165800 | Median Reward: 39.11 | Max Reward: 48.17
Iteration: 2045 | Episodes: 165900 | Median Reward: 38.98 | Max Reward: 48.17
Iteration: 2046 | Episodes: 166000 | Median Reward: 39.03 | Max Reward: 48.17
Iteration: 2047 | Episodes: 166100 | Median Reward: 40.09 | Max Reward: 48.17
Iteration: 2049 | Episodes: 166200 | Median Reward: 43.52 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57.7       |
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 2050        |
|    time_elapsed         | 93772       |
|    total_timesteps      | 16793600    |
| train/                  |             |
|    approx_kl            | 0.003502993 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -125        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.82       |
|    n_updates            | 20490       |
|    policy_gradient_loss | 0.0014      |
|    std                  | 5.84        |
|    value_loss           | 1           |
-----------------------------------------
Iteration: 2050 | Episodes: 166300 | Median Reward: 42.42 | Max Reward: 48.17
Iteration: 2051 | Episodes: 166400 | Median Reward: 42.77 | Max Reward: 48.17
Iteration: 2052 | Episodes: 166500 | Median Reward: 36.17 | Max Reward: 48.17
Iteration: 2054 | Episodes: 166600 | Median Reward: 40.05 | Max Reward: 48.17
Iteration: 2055 | Episodes: 166700 | Median Reward: 35.86 | Max Reward: 48.17
Iteration: 2056 | Episodes: 166800 | Median Reward: 41.87 | Max Reward: 48.17
Iteration: 2057 | Episodes: 166900 | Median Reward: 40.27 | Max Reward: 48.17
Iteration: 2058 | Episodes: 167000 | Median Reward: 40.81 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.4        |
| time/                   |              |
|    fps                  | 179          |
|    iterations           | 2060         |
|    time_elapsed         | 93861        |
|    total_timesteps      | 16875520     |
| train/                  |              |
|    approx_kl            | 0.0019667773 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -126         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.13        |
|    n_updates            | 20590        |
|    policy_gradient_loss | -0.00508     |
|    std                  | 5.88         |
|    value_loss           | 0.916        |
------------------------------------------
Iteration: 2060 | Episodes: 167100 | Median Reward: 36.91 | Max Reward: 48.17
Iteration: 2061 | Episodes: 167200 | Median Reward: 30.64 | Max Reward: 48.17
Iteration: 2062 | Episodes: 167300 | Median Reward: 39.20 | Max Reward: 48.17
Iteration: 2063 | Episodes: 167400 | Median Reward: 41.40 | Max Reward: 48.17
Iteration: 2065 | Episodes: 167500 | Median Reward: 43.57 | Max Reward: 48.17
Iteration: 2066 | Episodes: 167600 | Median Reward: 34.94 | Max Reward: 48.17
Iteration: 2067 | Episodes: 167700 | Median Reward: 37.28 | Max Reward: 48.17
Iteration: 2068 | Episodes: 167800 | Median Reward: 35.33 | Max Reward: 48.17
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 101            |
|    ep_rew_mean          | -60.6          |
| time/                   |                |
|    fps                  | 180            |
|    iterations           | 2070           |
|    time_elapsed         | 93950          |
|    total_timesteps      | 16957440       |
| train/                  |                |
|    approx_kl            | 0.000118018535 |
|    clip_fraction        | 0              |
|    clip_range           | 0.4            |
|    entropy_loss         | -126           |
|    explained_variance   | 0.997          |
|    learning_rate        | 0.000707       |
|    loss                 | -5.73          |
|    n_updates            | 20690          |
|    policy_gradient_loss | -0.000319      |
|    std                  | 5.92           |
|    value_loss           | 1.83           |
--------------------------------------------
Iteration: 2070 | Episodes: 167900 | Median Reward: 40.11 | Max Reward: 48.17
Iteration: 2071 | Episodes: 168000 | Median Reward: 45.11 | Max Reward: 48.17
Iteration: 2072 | Episodes: 168100 | Median Reward: 40.06 | Max Reward: 48.17
Iteration: 2073 | Episodes: 168200 | Median Reward: 38.84 | Max Reward: 48.17
Iteration: 2074 | Episodes: 168300 | Median Reward: 44.52 | Max Reward: 48.17
Iteration: 2076 | Episodes: 168400 | Median Reward: 39.45 | Max Reward: 48.17
Iteration: 2077 | Episodes: 168500 | Median Reward: 43.53 | Max Reward: 48.17
Iteration: 2078 | Episodes: 168600 | Median Reward: 41.50 | Max Reward: 48.17
Iteration: 2079 | Episodes: 168700 | Median Reward: 37.12 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.9         |
| time/                   |               |
|    fps                  | 181           |
|    iterations           | 2080          |
|    time_elapsed         | 94040         |
|    total_timesteps      | 17039360      |
| train/                  |               |
|    approx_kl            | 0.00038730545 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -126          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.31         |
|    n_updates            | 20790         |
|    policy_gradient_loss | -0.00395      |
|    std                  | 5.96          |
|    value_loss           | 3.8           |
-------------------------------------------
Iteration: 2081 | Episodes: 168800 | Median Reward: 38.93 | Max Reward: 48.17
Iteration: 2082 | Episodes: 168900 | Median Reward: 37.11 | Max Reward: 48.17
Iteration: 2083 | Episodes: 169000 | Median Reward: 37.06 | Max Reward: 48.17
Iteration: 2084 | Episodes: 169100 | Median Reward: 37.41 | Max Reward: 48.17
Iteration: 2086 | Episodes: 169200 | Median Reward: 41.04 | Max Reward: 48.17
Iteration: 2087 | Episodes: 169300 | Median Reward: 39.41 | Max Reward: 48.17
Iteration: 2088 | Episodes: 169400 | Median Reward: 38.31 | Max Reward: 48.17
Iteration: 2089 | Episodes: 169500 | Median Reward: 35.32 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.4        |
| time/                   |              |
|    fps                  | 181          |
|    iterations           | 2090         |
|    time_elapsed         | 94128        |
|    total_timesteps      | 17121280     |
| train/                  |              |
|    approx_kl            | 0.0012898676 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -126         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.89        |
|    n_updates            | 20890        |
|    policy_gradient_loss | -0.00306     |
|    std                  | 5.99         |
|    value_loss           | 1.17         |
------------------------------------------
Iteration: 2091 | Episodes: 169600 | Median Reward: 32.33 | Max Reward: 48.17
Iteration: 2092 | Episodes: 169700 | Median Reward: 40.08 | Max Reward: 48.17
Iteration: 2093 | Episodes: 169800 | Median Reward: 39.96 | Max Reward: 48.17
Iteration: 2094 | Episodes: 169900 | Median Reward: 36.73 | Max Reward: 48.17
Iteration: 2095 | Episodes: 170000 | Median Reward: 37.02 | Max Reward: 48.17
Iteration: 2097 | Episodes: 170100 | Median Reward: 39.43 | Max Reward: 48.17
Iteration: 2098 | Episodes: 170200 | Median Reward: 44.53 | Max Reward: 48.17
Iteration: 2099 | Episodes: 170300 | Median Reward: 39.77 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.4        |
| time/                   |              |
|    fps                  | 182          |
|    iterations           | 2100         |
|    time_elapsed         | 94217        |
|    total_timesteps      | 17203200     |
| train/                  |              |
|    approx_kl            | 0.0039223516 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -126         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -3.82        |
|    n_updates            | 20990        |
|    policy_gradient_loss | -0.0104      |
|    std                  | 6.03         |
|    value_loss           | 1.76         |
------------------------------------------
Iteration: 2100 | Episodes: 170400 | Median Reward: 35.14 | Max Reward: 48.17
Iteration: 2102 | Episodes: 170500 | Median Reward: 34.78 | Max Reward: 48.17
Iteration: 2103 | Episodes: 170600 | Median Reward: 33.46 | Max Reward: 48.17
Iteration: 2104 | Episodes: 170700 | Median Reward: 33.46 | Max Reward: 48.17
Iteration: 2105 | Episodes: 170800 | Median Reward: 39.84 | Max Reward: 48.17
Iteration: 2107 | Episodes: 170900 | Median Reward: 43.52 | Max Reward: 48.17
Iteration: 2108 | Episodes: 171000 | Median Reward: 35.54 | Max Reward: 48.17
Iteration: 2109 | Episodes: 171100 | Median Reward: 39.20 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.3         |
| time/                   |               |
|    fps                  | 183           |
|    iterations           | 2110          |
|    time_elapsed         | 94305         |
|    total_timesteps      | 17285120      |
| train/                  |               |
|    approx_kl            | 0.00012308234 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -126          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.95         |
|    n_updates            | 21090         |
|    policy_gradient_loss | -0.000432     |
|    std                  | 6.07          |
|    value_loss           | 0.879         |
-------------------------------------------
Iteration: 2110 | Episodes: 171200 | Median Reward: 43.00 | Max Reward: 48.17
Iteration: 2111 | Episodes: 171300 | Median Reward: 43.00 | Max Reward: 48.17
Iteration: 2113 | Episodes: 171400 | Median Reward: 43.61 | Max Reward: 48.17
Iteration: 2114 | Episodes: 171500 | Median Reward: 34.92 | Max Reward: 48.17
Iteration: 2115 | Episodes: 171600 | Median Reward: 46.17 | Max Reward: 48.17
Iteration: 2116 | Episodes: 171700 | Median Reward: 39.66 | Max Reward: 48.17
Iteration: 2118 | Episodes: 171800 | Median Reward: 37.44 | Max Reward: 48.17
Iteration: 2119 | Episodes: 171900 | Median Reward: 33.88 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.3         |
| time/                   |               |
|    fps                  | 183           |
|    iterations           | 2120          |
|    time_elapsed         | 94394         |
|    total_timesteps      | 17367040      |
| train/                  |               |
|    approx_kl            | 2.5762143e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -126          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.08         |
|    n_updates            | 21190         |
|    policy_gradient_loss | -2.59e-07     |
|    std                  | 6.12          |
|    value_loss           | 1.03          |
-------------------------------------------
Iteration: 2120 | Episodes: 172000 | Median Reward: 32.17 | Max Reward: 48.17
Iteration: 2121 | Episodes: 172100 | Median Reward: 33.17 | Max Reward: 48.17
Iteration: 2123 | Episodes: 172200 | Median Reward: 34.40 | Max Reward: 48.17
Iteration: 2124 | Episodes: 172300 | Median Reward: 41.53 | Max Reward: 48.17
Iteration: 2125 | Episodes: 172400 | Median Reward: 38.83 | Max Reward: 48.17
Iteration: 2126 | Episodes: 172500 | Median Reward: 38.29 | Max Reward: 48.17
Iteration: 2128 | Episodes: 172600 | Median Reward: 43.69 | Max Reward: 48.17
Iteration: 2129 | Episodes: 172700 | Median Reward: 43.39 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.4         |
| time/                   |               |
|    fps                  | 184           |
|    iterations           | 2130          |
|    time_elapsed         | 94482         |
|    total_timesteps      | 17448960      |
| train/                  |               |
|    approx_kl            | 0.00023071544 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -127          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.93         |
|    n_updates            | 21290         |
|    policy_gradient_loss | -0.0008       |
|    std                  | 6.19          |
|    value_loss           | 0.705         |
-------------------------------------------
Iteration: 2130 | Episodes: 172800 | Median Reward: 35.89 | Max Reward: 48.17
Iteration: 2131 | Episodes: 172900 | Median Reward: 38.98 | Max Reward: 48.17
Iteration: 2132 | Episodes: 173000 | Median Reward: 39.05 | Max Reward: 48.17
Iteration: 2134 | Episodes: 173100 | Median Reward: 37.38 | Max Reward: 48.17
Iteration: 2135 | Episodes: 173200 | Median Reward: 36.93 | Max Reward: 48.17
Iteration: 2136 | Episodes: 173300 | Median Reward: 38.36 | Max Reward: 48.17
Iteration: 2137 | Episodes: 173400 | Median Reward: 39.27 | Max Reward: 48.17
Iteration: 2139 | Episodes: 173500 | Median Reward: 40.46 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.7       |
| time/                   |             |
|    fps                  | 185         |
|    iterations           | 2140        |
|    time_elapsed         | 94570       |
|    total_timesteps      | 17530880    |
| train/                  |             |
|    approx_kl            | 0.002566466 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -127        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -5.77       |
|    n_updates            | 21390       |
|    policy_gradient_loss | -0.00992    |
|    std                  | 6.25        |
|    value_loss           | 0.659       |
-----------------------------------------
Iteration: 2140 | Episodes: 173600 | Median Reward: 38.57 | Max Reward: 48.17
Iteration: 2141 | Episodes: 173700 | Median Reward: 38.93 | Max Reward: 48.17
Iteration: 2142 | Episodes: 173800 | Median Reward: 42.87 | Max Reward: 48.17
Iteration: 2144 | Episodes: 173900 | Median Reward: 38.58 | Max Reward: 48.17
Iteration: 2145 | Episodes: 174000 | Median Reward: 38.62 | Max Reward: 48.17
Iteration: 2146 | Episodes: 174100 | Median Reward: 40.81 | Max Reward: 48.17
Iteration: 2147 | Episodes: 174200 | Median Reward: 32.86 | Max Reward: 48.17
Iteration: 2148 | Episodes: 174300 | Median Reward: 39.11 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.3         |
| time/                   |               |
|    fps                  | 186           |
|    iterations           | 2150          |
|    time_elapsed         | 94658         |
|    total_timesteps      | 17612800      |
| train/                  |               |
|    approx_kl            | 0.00028023822 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -127          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.01         |
|    n_updates            | 21490         |
|    policy_gradient_loss | -0.000615     |
|    std                  | 6.28          |
|    value_loss           | 1.28          |
-------------------------------------------
Iteration: 2150 | Episodes: 174400 | Median Reward: 38.66 | Max Reward: 48.17
Iteration: 2151 | Episodes: 174500 | Median Reward: 38.29 | Max Reward: 48.17
Iteration: 2152 | Episodes: 174600 | Median Reward: 39.66 | Max Reward: 48.17
Iteration: 2153 | Episodes: 174700 | Median Reward: 44.55 | Max Reward: 48.17
Iteration: 2155 | Episodes: 174800 | Median Reward: 42.09 | Max Reward: 48.17
Iteration: 2156 | Episodes: 174900 | Median Reward: 38.67 | Max Reward: 48.17
Iteration: 2157 | Episodes: 175000 | Median Reward: 43.24 | Max Reward: 48.17
Iteration: 2158 | Episodes: 175100 | Median Reward: 36.50 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.2         |
| time/                   |               |
|    fps                  | 186           |
|    iterations           | 2160          |
|    time_elapsed         | 94747         |
|    total_timesteps      | 17694720      |
| train/                  |               |
|    approx_kl            | 0.00076635205 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -127          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.68         |
|    n_updates            | 21590         |
|    policy_gradient_loss | -0.00302      |
|    std                  | 6.31          |
|    value_loss           | 0.842         |
-------------------------------------------
Iteration: 2160 | Episodes: 175200 | Median Reward: 44.06 | Max Reward: 48.17
Iteration: 2161 | Episodes: 175300 | Median Reward: 43.11 | Max Reward: 48.17
Iteration: 2162 | Episodes: 175400 | Median Reward: 40.78 | Max Reward: 48.17
Iteration: 2163 | Episodes: 175500 | Median Reward: 38.76 | Max Reward: 48.17
Iteration: 2164 | Episodes: 175600 | Median Reward: 37.82 | Max Reward: 48.17
Iteration: 2166 | Episodes: 175700 | Median Reward: 41.64 | Max Reward: 48.17
Iteration: 2167 | Episodes: 175800 | Median Reward: 43.48 | Max Reward: 48.17
Iteration: 2168 | Episodes: 175900 | Median Reward: 42.02 | Max Reward: 48.17
Iteration: 2169 | Episodes: 176000 | Median Reward: 42.45 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 187           |
|    iterations           | 2170          |
|    time_elapsed         | 94835         |
|    total_timesteps      | 17776640      |
| train/                  |               |
|    approx_kl            | 1.1839184e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -127          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -6.11         |
|    n_updates            | 21690         |
|    policy_gradient_loss | -0.000167     |
|    std                  | 6.37          |
|    value_loss           | 0.725         |
-------------------------------------------
Iteration: 2171 | Episodes: 176100 | Median Reward: 37.55 | Max Reward: 48.17
Iteration: 2172 | Episodes: 176200 | Median Reward: 30.87 | Max Reward: 48.17
Iteration: 2173 | Episodes: 176300 | Median Reward: 35.41 | Max Reward: 48.17
Iteration: 2174 | Episodes: 176400 | Median Reward: 35.28 | Max Reward: 48.17
Iteration: 2176 | Episodes: 176500 | Median Reward: 38.57 | Max Reward: 48.17
Iteration: 2177 | Episodes: 176600 | Median Reward: 38.00 | Max Reward: 48.17
Iteration: 2178 | Episodes: 176700 | Median Reward: 37.28 | Max Reward: 48.17
Iteration: 2179 | Episodes: 176800 | Median Reward: 33.48 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.6        |
| time/                   |              |
|    fps                  | 188          |
|    iterations           | 2180         |
|    time_elapsed         | 94924        |
|    total_timesteps      | 17858560     |
| train/                  |              |
|    approx_kl            | 0.0011155385 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.000707     |
|    loss                 | -4.97        |
|    n_updates            | 21790        |
|    policy_gradient_loss | -0.0031      |
|    std                  | 6.43         |
|    value_loss           | 1.64         |
------------------------------------------
Iteration: 2181 | Episodes: 176900 | Median Reward: 36.24 | Max Reward: 48.17
Iteration: 2182 | Episodes: 177000 | Median Reward: 43.33 | Max Reward: 48.17
Iteration: 2183 | Episodes: 177100 | Median Reward: 42.16 | Max Reward: 48.17
Iteration: 2184 | Episodes: 177200 | Median Reward: 35.99 | Max Reward: 48.17
Iteration: 2185 | Episodes: 177300 | Median Reward: 42.45 | Max Reward: 48.17
Iteration: 2187 | Episodes: 177400 | Median Reward: 42.17 | Max Reward: 48.17
Iteration: 2188 | Episodes: 177500 | Median Reward: 44.09 | Max Reward: 48.17
Iteration: 2189 | Episodes: 177600 | Median Reward: 42.46 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.2        |
| time/                   |              |
|    fps                  | 188          |
|    iterations           | 2190         |
|    time_elapsed         | 95012        |
|    total_timesteps      | 17940480     |
| train/                  |              |
|    approx_kl            | 0.0004159049 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.25        |
|    n_updates            | 21890        |
|    policy_gradient_loss | -0.00125     |
|    std                  | 6.44         |
|    value_loss           | 0.644        |
------------------------------------------
Iteration: 2190 | Episodes: 177700 | Median Reward: 40.03 | Max Reward: 48.17
Iteration: 2192 | Episodes: 177800 | Median Reward: 42.87 | Max Reward: 48.17
Iteration: 2193 | Episodes: 177900 | Median Reward: 43.06 | Max Reward: 48.17
Iteration: 2194 | Episodes: 178000 | Median Reward: 36.78 | Max Reward: 48.17
Iteration: 2195 | Episodes: 178100 | Median Reward: 38.72 | Max Reward: 48.17
Iteration: 2197 | Episodes: 178200 | Median Reward: 41.21 | Max Reward: 48.17
Iteration: 2198 | Episodes: 178300 | Median Reward: 41.37 | Max Reward: 48.17
Iteration: 2199 | Episodes: 178400 | Median Reward: 39.48 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61          |
| time/                   |              |
|    fps                  | 189          |
|    iterations           | 2200         |
|    time_elapsed         | 95100        |
|    total_timesteps      | 18022400     |
| train/                  |              |
|    approx_kl            | 0.0006441246 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.22        |
|    n_updates            | 21990        |
|    policy_gradient_loss | -0.000113    |
|    std                  | 6.47         |
|    value_loss           | 0.535        |
------------------------------------------
Iteration: 2200 | Episodes: 178500 | Median Reward: 37.34 | Max Reward: 48.17
Iteration: 2201 | Episodes: 178600 | Median Reward: 40.96 | Max Reward: 48.17
Iteration: 2203 | Episodes: 178700 | Median Reward: 36.98 | Max Reward: 48.17
Iteration: 2204 | Episodes: 178800 | Median Reward: 39.72 | Max Reward: 48.17
Iteration: 2205 | Episodes: 178900 | Median Reward: 40.86 | Max Reward: 48.17
Iteration: 2206 | Episodes: 179000 | Median Reward: 38.43 | Max Reward: 48.17
Iteration: 2208 | Episodes: 179100 | Median Reward: 42.32 | Max Reward: 48.17
Iteration: 2209 | Episodes: 179200 | Median Reward: 33.44 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.5        |
| time/                   |              |
|    fps                  | 190          |
|    iterations           | 2210         |
|    time_elapsed         | 95188        |
|    total_timesteps      | 18104320     |
| train/                  |              |
|    approx_kl            | 6.150534e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.28        |
|    n_updates            | 22090        |
|    policy_gradient_loss | -9.26e-05    |
|    std                  | 6.51         |
|    value_loss           | 0.525        |
------------------------------------------
Iteration: 2210 | Episodes: 179300 | Median Reward: 42.16 | Max Reward: 48.17
Iteration: 2211 | Episodes: 179400 | Median Reward: 41.29 | Max Reward: 48.17
Iteration: 2213 | Episodes: 179500 | Median Reward: 39.66 | Max Reward: 48.17
Iteration: 2214 | Episodes: 179600 | Median Reward: 39.62 | Max Reward: 48.17
Iteration: 2215 | Episodes: 179700 | Median Reward: 39.90 | Max Reward: 48.17
Iteration: 2216 | Episodes: 179800 | Median Reward: 38.16 | Max Reward: 48.17
Iteration: 2218 | Episodes: 179900 | Median Reward: 37.19 | Max Reward: 48.17
Iteration: 2219 | Episodes: 180000 | Median Reward: 44.62 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 190          |
|    iterations           | 2220         |
|    time_elapsed         | 95277        |
|    total_timesteps      | 18186240     |
| train/                  |              |
|    approx_kl            | 0.0074074212 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.15        |
|    n_updates            | 22190        |
|    policy_gradient_loss | -0.0248      |
|    std                  | 6.59         |
|    value_loss           | 0.703        |
------------------------------------------
Iteration: 2220 | Episodes: 180100 | Median Reward: 42.84 | Max Reward: 48.17
Iteration: 2221 | Episodes: 180200 | Median Reward: 41.04 | Max Reward: 48.17
Iteration: 2222 | Episodes: 180300 | Median Reward: 42.11 | Max Reward: 48.17
Iteration: 2224 | Episodes: 180400 | Median Reward: 41.95 | Max Reward: 48.17
Iteration: 2225 | Episodes: 180500 | Median Reward: 37.24 | Max Reward: 48.17
Iteration: 2226 | Episodes: 180600 | Median Reward: 34.03 | Max Reward: 48.17
Iteration: 2227 | Episodes: 180700 | Median Reward: 40.12 | Max Reward: 48.17
Iteration: 2229 | Episodes: 180800 | Median Reward: 43.83 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.7         |
| time/                   |               |
|    fps                  | 191           |
|    iterations           | 2230          |
|    time_elapsed         | 95365         |
|    total_timesteps      | 18268160      |
| train/                  |               |
|    approx_kl            | 3.7981226e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -128          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.3          |
|    n_updates            | 22290         |
|    policy_gradient_loss | -9.88e-06     |
|    std                  | 6.62          |
|    value_loss           | 1.36          |
-------------------------------------------
Iteration: 2230 | Episodes: 180900 | Median Reward: 43.71 | Max Reward: 48.17
Iteration: 2231 | Episodes: 181000 | Median Reward: 38.27 | Max Reward: 48.17
Iteration: 2232 | Episodes: 181100 | Median Reward: 40.16 | Max Reward: 48.17
Iteration: 2234 | Episodes: 181200 | Median Reward: 37.18 | Max Reward: 48.17
Iteration: 2235 | Episodes: 181300 | Median Reward: 37.11 | Max Reward: 48.17
Iteration: 2236 | Episodes: 181400 | Median Reward: 38.19 | Max Reward: 48.17
Iteration: 2237 | Episodes: 181500 | Median Reward: 44.54 | Max Reward: 48.17
Iteration: 2238 | Episodes: 181600 | Median Reward: 39.17 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.6        |
| time/                   |              |
|    fps                  | 192          |
|    iterations           | 2240         |
|    time_elapsed         | 95453        |
|    total_timesteps      | 18350080     |
| train/                  |              |
|    approx_kl            | 0.0015186593 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.19        |
|    n_updates            | 22390        |
|    policy_gradient_loss | -0.0056      |
|    std                  | 6.66         |
|    value_loss           | 0.575        |
------------------------------------------
Iteration: 2240 | Episodes: 181700 | Median Reward: 39.04 | Max Reward: 48.17
Iteration: 2241 | Episodes: 181800 | Median Reward: 43.02 | Max Reward: 48.17
Iteration: 2242 | Episodes: 181900 | Median Reward: 37.28 | Max Reward: 48.17
Iteration: 2243 | Episodes: 182000 | Median Reward: 41.61 | Max Reward: 48.17
Iteration: 2245 | Episodes: 182100 | Median Reward: 39.88 | Max Reward: 48.17
Iteration: 2246 | Episodes: 182200 | Median Reward: 33.81 | Max Reward: 48.17
Iteration: 2247 | Episodes: 182300 | Median Reward: 33.30 | Max Reward: 48.17
Iteration: 2248 | Episodes: 182400 | Median Reward: 34.99 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 192          |
|    iterations           | 2250         |
|    time_elapsed         | 95542        |
|    total_timesteps      | 18432000     |
| train/                  |              |
|    approx_kl            | 0.0029175729 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.3         |
|    n_updates            | 22490        |
|    policy_gradient_loss | -0.0136      |
|    std                  | 6.7          |
|    value_loss           | 0.612        |
------------------------------------------
Iteration: 2250 | Episodes: 182500 | Median Reward: 39.94 | Max Reward: 48.17
Iteration: 2251 | Episodes: 182600 | Median Reward: 43.35 | Max Reward: 48.17
Iteration: 2252 | Episodes: 182700 | Median Reward: 43.62 | Max Reward: 48.17
Iteration: 2253 | Episodes: 182800 | Median Reward: 38.01 | Max Reward: 48.17
Iteration: 2254 | Episodes: 182900 | Median Reward: 42.48 | Max Reward: 48.17
Iteration: 2256 | Episodes: 183000 | Median Reward: 40.12 | Max Reward: 48.17
Iteration: 2257 | Episodes: 183100 | Median Reward: 36.01 | Max Reward: 48.17
Iteration: 2258 | Episodes: 183200 | Median Reward: 36.20 | Max Reward: 48.17
Iteration: 2259 | Episodes: 183300 | Median Reward: 43.14 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.5        |
| time/                   |              |
|    fps                  | 193          |
|    iterations           | 2260         |
|    time_elapsed         | 95630        |
|    total_timesteps      | 18513920     |
| train/                  |              |
|    approx_kl            | 0.0019374748 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -128         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.32        |
|    n_updates            | 22590        |
|    policy_gradient_loss | -0.00335     |
|    std                  | 6.7          |
|    value_loss           | 0.339        |
------------------------------------------
Iteration: 2261 | Episodes: 183400 | Median Reward: 44.90 | Max Reward: 48.17
Iteration: 2262 | Episodes: 183500 | Median Reward: 37.11 | Max Reward: 48.17
Iteration: 2263 | Episodes: 183600 | Median Reward: 38.64 | Max Reward: 48.17
Iteration: 2264 | Episodes: 183700 | Median Reward: 41.61 | Max Reward: 48.17
Iteration: 2266 | Episodes: 183800 | Median Reward: 44.25 | Max Reward: 48.17
Iteration: 2267 | Episodes: 183900 | Median Reward: 43.07 | Max Reward: 48.17
Iteration: 2268 | Episodes: 184000 | Median Reward: 40.06 | Max Reward: 48.17
Iteration: 2269 | Episodes: 184100 | Median Reward: 32.31 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.8         |
| time/                   |               |
|    fps                  | 194           |
|    iterations           | 2270          |
|    time_elapsed         | 95719         |
|    total_timesteps      | 18595840      |
| train/                  |               |
|    approx_kl            | 0.00022755266 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -129          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -6.35         |
|    n_updates            | 22690         |
|    policy_gradient_loss | -0.000237     |
|    std                  | 6.75          |
|    value_loss           | 0.382         |
-------------------------------------------
Iteration: 2271 | Episodes: 184200 | Median Reward: 32.31 | Max Reward: 48.17
Iteration: 2272 | Episodes: 184300 | Median Reward: 42.10 | Max Reward: 48.17
Iteration: 2273 | Episodes: 184400 | Median Reward: 42.75 | Max Reward: 48.17
Iteration: 2274 | Episodes: 184500 | Median Reward: 42.67 | Max Reward: 48.17
Iteration: 2275 | Episodes: 184600 | Median Reward: 43.23 | Max Reward: 48.17
Iteration: 2277 | Episodes: 184700 | Median Reward: 40.51 | Max Reward: 48.17
Iteration: 2278 | Episodes: 184800 | Median Reward: 40.51 | Max Reward: 48.17
Iteration: 2279 | Episodes: 184900 | Median Reward: 42.54 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.6        |
| time/                   |              |
|    fps                  | 194          |
|    iterations           | 2280         |
|    time_elapsed         | 95806        |
|    total_timesteps      | 18677760     |
| train/                  |              |
|    approx_kl            | 5.124253e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -129         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.97        |
|    n_updates            | 22790        |
|    policy_gradient_loss | -2.92e-05    |
|    std                  | 6.75         |
|    value_loss           | 1.09         |
------------------------------------------
Iteration: 2280 | Episodes: 185000 | Median Reward: 42.94 | Max Reward: 48.17
Iteration: 2282 | Episodes: 185100 | Median Reward: 36.25 | Max Reward: 48.17
Iteration: 2283 | Episodes: 185200 | Median Reward: 39.14 | Max Reward: 48.17
Iteration: 2284 | Episodes: 185300 | Median Reward: 38.07 | Max Reward: 48.17
Iteration: 2285 | Episodes: 185400 | Median Reward: 37.36 | Max Reward: 48.17
Iteration: 2287 | Episodes: 185500 | Median Reward: 37.36 | Max Reward: 48.17
Iteration: 2288 | Episodes: 185600 | Median Reward: 47.21 | Max Reward: 48.17
Iteration: 2289 | Episodes: 185700 | Median Reward: 46.33 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60           |
| time/                   |               |
|    fps                  | 195           |
|    iterations           | 2290          |
|    time_elapsed         | 95895         |
|    total_timesteps      | 18759680      |
| train/                  |               |
|    approx_kl            | 0.00011918501 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -129          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -5.97         |
|    n_updates            | 22890         |
|    policy_gradient_loss | -0.000601     |
|    std                  | 6.8           |
|    value_loss           | 0.837         |
-------------------------------------------
Iteration: 2290 | Episodes: 185800 | Median Reward: 43.44 | Max Reward: 48.17
Iteration: 2291 | Episodes: 185900 | Median Reward: 39.78 | Max Reward: 48.17
Iteration: 2293 | Episodes: 186000 | Median Reward: 39.38 | Max Reward: 48.17
Iteration: 2294 | Episodes: 186100 | Median Reward: 38.18 | Max Reward: 48.17
Iteration: 2295 | Episodes: 186200 | Median Reward: 38.93 | Max Reward: 48.17
Iteration: 2296 | Episodes: 186300 | Median Reward: 38.67 | Max Reward: 48.17
Iteration: 2298 | Episodes: 186400 | Median Reward: 38.94 | Max Reward: 48.17
Iteration: 2299 | Episodes: 186500 | Median Reward: 40.78 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.4         |
| time/                   |               |
|    fps                  | 196           |
|    iterations           | 2300          |
|    time_elapsed         | 95984         |
|    total_timesteps      | 18841600      |
| train/                  |               |
|    approx_kl            | 0.00016888577 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -129          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.31         |
|    n_updates            | 22990         |
|    policy_gradient_loss | -0.00127      |
|    std                  | 6.83          |
|    value_loss           | 0.623         |
-------------------------------------------
Iteration: 2300 | Episodes: 186600 | Median Reward: 43.03 | Max Reward: 48.17
Iteration: 2301 | Episodes: 186700 | Median Reward: 38.39 | Max Reward: 48.17
Iteration: 2303 | Episodes: 186800 | Median Reward: 37.75 | Max Reward: 48.17
Iteration: 2304 | Episodes: 186900 | Median Reward: 37.15 | Max Reward: 48.17
Iteration: 2305 | Episodes: 187000 | Median Reward: 38.69 | Max Reward: 48.17
Iteration: 2306 | Episodes: 187100 | Median Reward: 39.80 | Max Reward: 48.17
Iteration: 2308 | Episodes: 187200 | Median Reward: 41.74 | Max Reward: 48.17
Iteration: 2309 | Episodes: 187300 | Median Reward: 43.52 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -57         |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 2310        |
|    time_elapsed         | 96072       |
|    total_timesteps      | 18923520    |
| train/                  |             |
|    approx_kl            | 0.016683485 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -129        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.38       |
|    n_updates            | 23090       |
|    policy_gradient_loss | -0.0268     |
|    std                  | 6.88        |
|    value_loss           | 0.369       |
-----------------------------------------
Iteration: 2310 | Episodes: 187400 | Median Reward: 43.28 | Max Reward: 48.17
Iteration: 2311 | Episodes: 187500 | Median Reward: 42.80 | Max Reward: 48.17
Iteration: 2312 | Episodes: 187600 | Median Reward: 40.70 | Max Reward: 48.17
Iteration: 2314 | Episodes: 187700 | Median Reward: 43.30 | Max Reward: 48.17
Iteration: 2315 | Episodes: 187800 | Median Reward: 40.38 | Max Reward: 48.17
Iteration: 2316 | Episodes: 187900 | Median Reward: 39.01 | Max Reward: 48.17
Iteration: 2317 | Episodes: 188000 | Median Reward: 43.56 | Max Reward: 48.17
Iteration: 2319 | Episodes: 188100 | Median Reward: 38.20 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.5        |
| time/                   |              |
|    fps                  | 197          |
|    iterations           | 2320         |
|    time_elapsed         | 96161        |
|    total_timesteps      | 19005440     |
| train/                  |              |
|    approx_kl            | 4.633238e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -129         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.12        |
|    n_updates            | 23190        |
|    policy_gradient_loss | -0.000644    |
|    std                  | 6.92         |
|    value_loss           | 1.92         |
------------------------------------------
Iteration: 2320 | Episodes: 188200 | Median Reward: 36.52 | Max Reward: 48.17
Iteration: 2321 | Episodes: 188300 | Median Reward: 38.84 | Max Reward: 48.17
Iteration: 2322 | Episodes: 188400 | Median Reward: 38.59 | Max Reward: 48.17
Iteration: 2324 | Episodes: 188500 | Median Reward: 38.52 | Max Reward: 48.17
Iteration: 2325 | Episodes: 188600 | Median Reward: 38.24 | Max Reward: 48.17
Iteration: 2326 | Episodes: 188700 | Median Reward: 41.55 | Max Reward: 48.17
Iteration: 2327 | Episodes: 188800 | Median Reward: 43.62 | Max Reward: 48.17
Iteration: 2328 | Episodes: 188900 | Median Reward: 43.56 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.2         |
| time/                   |               |
|    fps                  | 198           |
|    iterations           | 2330          |
|    time_elapsed         | 96249         |
|    total_timesteps      | 19087360      |
| train/                  |               |
|    approx_kl            | 0.00051827956 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -129          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -6.34         |
|    n_updates            | 23290         |
|    policy_gradient_loss | -0.00207      |
|    std                  | 6.92          |
|    value_loss           | 0.379         |
-------------------------------------------
Iteration: 2330 | Episodes: 189000 | Median Reward: 40.86 | Max Reward: 48.17
Iteration: 2331 | Episodes: 189100 | Median Reward: 38.31 | Max Reward: 48.17
Iteration: 2332 | Episodes: 189200 | Median Reward: 41.05 | Max Reward: 48.17
Iteration: 2333 | Episodes: 189300 | Median Reward: 44.17 | Max Reward: 48.17
Iteration: 2335 | Episodes: 189400 | Median Reward: 45.47 | Max Reward: 48.17
Iteration: 2336 | Episodes: 189500 | Median Reward: 43.65 | Max Reward: 48.17
Iteration: 2337 | Episodes: 189600 | Median Reward: 43.38 | Max Reward: 48.17
Iteration: 2338 | Episodes: 189700 | Median Reward: 42.34 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.3         |
| time/                   |               |
|    fps                  | 198           |
|    iterations           | 2340          |
|    time_elapsed         | 96337         |
|    total_timesteps      | 19169280      |
| train/                  |               |
|    approx_kl            | 6.8601454e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -129          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.97         |
|    n_updates            | 23390         |
|    policy_gradient_loss | 1.98e-05      |
|    std                  | 6.95          |
|    value_loss           | 0.876         |
-------------------------------------------
Iteration: 2340 | Episodes: 189800 | Median Reward: 42.10 | Max Reward: 48.17
Iteration: 2341 | Episodes: 189900 | Median Reward: 40.12 | Max Reward: 48.17
Iteration: 2342 | Episodes: 190000 | Median Reward: 37.17 | Max Reward: 48.17
Iteration: 2343 | Episodes: 190100 | Median Reward: 42.07 | Max Reward: 48.17
Iteration: 2344 | Episodes: 190200 | Median Reward: 41.82 | Max Reward: 48.17
Iteration: 2346 | Episodes: 190300 | Median Reward: 42.19 | Max Reward: 48.17
Iteration: 2347 | Episodes: 190400 | Median Reward: 43.20 | Max Reward: 48.17
Iteration: 2348 | Episodes: 190500 | Median Reward: 35.44 | Max Reward: 48.17
Iteration: 2349 | Episodes: 190600 | Median Reward: 39.26 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.2        |
| time/                   |              |
|    fps                  | 199          |
|    iterations           | 2350         |
|    time_elapsed         | 96426        |
|    total_timesteps      | 19251200     |
| train/                  |              |
|    approx_kl            | 0.0038730772 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -129         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.21        |
|    n_updates            | 23490        |
|    policy_gradient_loss | -0.00425     |
|    std                  | 7.01         |
|    value_loss           | 1.3          |
------------------------------------------
Iteration: 2351 | Episodes: 190700 | Median Reward: 38.82 | Max Reward: 48.17
Iteration: 2352 | Episodes: 190800 | Median Reward: 37.93 | Max Reward: 48.17
Iteration: 2353 | Episodes: 190900 | Median Reward: 39.17 | Max Reward: 48.17
Iteration: 2354 | Episodes: 191000 | Median Reward: 38.07 | Max Reward: 48.17
Iteration: 2356 | Episodes: 191100 | Median Reward: 40.25 | Max Reward: 48.17
Iteration: 2357 | Episodes: 191200 | Median Reward: 40.37 | Max Reward: 48.17
Iteration: 2358 | Episodes: 191300 | Median Reward: 35.97 | Max Reward: 48.17
Iteration: 2359 | Episodes: 191400 | Median Reward: 46.67 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.1         |
| time/                   |               |
|    fps                  | 200           |
|    iterations           | 2360          |
|    time_elapsed         | 96514         |
|    total_timesteps      | 19333120      |
| train/                  |               |
|    approx_kl            | 0.00043649127 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -130          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.12         |
|    n_updates            | 23590         |
|    policy_gradient_loss | -0.00203      |
|    std                  | 7.05          |
|    value_loss           | 1.16          |
-------------------------------------------
Iteration: 2361 | Episodes: 191500 | Median Reward: 38.75 | Max Reward: 48.17
Iteration: 2362 | Episodes: 191600 | Median Reward: 39.04 | Max Reward: 48.17
Iteration: 2363 | Episodes: 191700 | Median Reward: 42.47 | Max Reward: 48.17
Iteration: 2364 | Episodes: 191800 | Median Reward: 40.21 | Max Reward: 48.17
Iteration: 2365 | Episodes: 191900 | Median Reward: 40.25 | Max Reward: 48.17
Iteration: 2367 | Episodes: 192000 | Median Reward: 40.55 | Max Reward: 48.17
Iteration: 2368 | Episodes: 192100 | Median Reward: 36.70 | Max Reward: 48.17
Iteration: 2369 | Episodes: 192200 | Median Reward: 35.60 | Max Reward: 48.17
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.9        |
| time/                   |              |
|    fps                  | 200          |
|    iterations           | 2370         |
|    time_elapsed         | 96602        |
|    total_timesteps      | 19415040     |
| train/                  |              |
|    approx_kl            | 7.206082e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -130         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.37        |
|    n_updates            | 23690        |
|    policy_gradient_loss | 3.92e-05     |
|    std                  | 7.08         |
|    value_loss           | 0.403        |
------------------------------------------
Iteration: 2370 | Episodes: 192300 | Median Reward: 37.17 | Max Reward: 48.17
Iteration: 2372 | Episodes: 192400 | Median Reward: 38.94 | Max Reward: 48.17
Iteration: 2373 | Episodes: 192500 | Median Reward: 42.78 | Max Reward: 48.17
Iteration: 2374 | Episodes: 192600 | Median Reward: 44.00 | Max Reward: 48.17
Iteration: 2375 | Episodes: 192700 | Median Reward: 43.96 | Max Reward: 48.17
Iteration: 2377 | Episodes: 192800 | Median Reward: 38.83 | Max Reward: 48.17
Iteration: 2378 | Episodes: 192900 | Median Reward: 41.48 | Max Reward: 48.17
Iteration: 2379 | Episodes: 193000 | Median Reward: 39.91 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 2380        |
|    time_elapsed         | 96691       |
|    total_timesteps      | 19496960    |
| train/                  |             |
|    approx_kl            | 0.020362202 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -130        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.4        |
|    n_updates            | 23790       |
|    policy_gradient_loss | -0.0381     |
|    std                  | 7.11        |
|    value_loss           | 0.361       |
-----------------------------------------
Iteration: 2380 | Episodes: 193100 | Median Reward: 42.13 | Max Reward: 48.17
Iteration: 2381 | Episodes: 193200 | Median Reward: 41.34 | Max Reward: 48.17
Iteration: 2383 | Episodes: 193300 | Median Reward: 37.07 | Max Reward: 48.17
Iteration: 2384 | Episodes: 193400 | Median Reward: 38.37 | Max Reward: 48.17
Iteration: 2385 | Episodes: 193500 | Median Reward: 31.80 | Max Reward: 48.17
Iteration: 2386 | Episodes: 193600 | Median Reward: 38.82 | Max Reward: 48.17
Iteration: 2388 | Episodes: 193700 | Median Reward: 38.82 | Max Reward: 48.17
Iteration: 2389 | Episodes: 193800 | Median Reward: 41.47 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -54.8         |
| time/                   |               |
|    fps                  | 202           |
|    iterations           | 2390          |
|    time_elapsed         | 96779         |
|    total_timesteps      | 19578880      |
| train/                  |               |
|    approx_kl            | 0.00028154138 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -130          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.3          |
|    n_updates            | 23890         |
|    policy_gradient_loss | -0.00191      |
|    std                  | 7.15          |
|    value_loss           | 0.647         |
-------------------------------------------
Iteration: 2390 | Episodes: 193900 | Median Reward: 43.92 | Max Reward: 48.17
Iteration: 2391 | Episodes: 194000 | Median Reward: 39.26 | Max Reward: 48.17
Iteration: 2393 | Episodes: 194100 | Median Reward: 42.18 | Max Reward: 48.17
Iteration: 2394 | Episodes: 194200 | Median Reward: 43.85 | Max Reward: 48.17
Iteration: 2395 | Episodes: 194300 | Median Reward: 38.79 | Max Reward: 48.17
Iteration: 2396 | Episodes: 194400 | Median Reward: 37.33 | Max Reward: 48.17
Iteration: 2398 | Episodes: 194500 | Median Reward: 39.42 | Max Reward: 48.17
Iteration: 2399 | Episodes: 194600 | Median Reward: 38.83 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.7       |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 2400        |
|    time_elapsed         | 96867       |
|    total_timesteps      | 19660800    |
| train/                  |             |
|    approx_kl            | 0.005283867 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -130        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.09       |
|    n_updates            | 23990       |
|    policy_gradient_loss | -0.0154     |
|    std                  | 7.19        |
|    value_loss           | 0.702       |
-----------------------------------------
Iteration: 2400 | Episodes: 194700 | Median Reward: 42.28 | Max Reward: 48.17
Iteration: 2401 | Episodes: 194800 | Median Reward: 36.42 | Max Reward: 48.17
Iteration: 2402 | Episodes: 194900 | Median Reward: 43.28 | Max Reward: 48.17
Iteration: 2404 | Episodes: 195000 | Median Reward: 43.51 | Max Reward: 48.17
Iteration: 2405 | Episodes: 195100 | Median Reward: 44.20 | Max Reward: 48.17
Iteration: 2406 | Episodes: 195200 | Median Reward: 42.92 | Max Reward: 48.17
Iteration: 2407 | Episodes: 195300 | Median Reward: 42.92 | Max Reward: 48.17
Iteration: 2409 | Episodes: 195400 | Median Reward: 40.10 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.8         |
| time/                   |               |
|    fps                  | 203           |
|    iterations           | 2410          |
|    time_elapsed         | 96955         |
|    total_timesteps      | 19742720      |
| train/                  |               |
|    approx_kl            | 0.00054115255 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -130          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.42         |
|    n_updates            | 24090         |
|    policy_gradient_loss | -0.00413      |
|    std                  | 7.21          |
|    value_loss           | 0.307         |
-------------------------------------------
Iteration: 2410 | Episodes: 195500 | Median Reward: 39.77 | Max Reward: 48.17
Iteration: 2411 | Episodes: 195600 | Median Reward: 38.56 | Max Reward: 48.17
Iteration: 2412 | Episodes: 195700 | Median Reward: 39.60 | Max Reward: 48.17
Iteration: 2414 | Episodes: 195800 | Median Reward: 43.74 | Max Reward: 48.17
Iteration: 2415 | Episodes: 195900 | Median Reward: 43.37 | Max Reward: 48.17
Iteration: 2416 | Episodes: 196000 | Median Reward: 41.60 | Max Reward: 48.17
Iteration: 2417 | Episodes: 196100 | Median Reward: 41.76 | Max Reward: 48.17
Iteration: 2418 | Episodes: 196200 | Median Reward: 42.45 | Max Reward: 48.17
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.8         |
| time/                   |               |
|    fps                  | 204           |
|    iterations           | 2420          |
|    time_elapsed         | 97044         |
|    total_timesteps      | 19824640      |
| train/                  |               |
|    approx_kl            | 0.00024866685 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -130          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.35         |
|    n_updates            | 24190         |
|    policy_gradient_loss | -0.000302     |
|    std                  | 7.27          |
|    value_loss           | 0.479         |
-------------------------------------------
Iteration: 2420 | Episodes: 196300 | Median Reward: 40.69 | Max Reward: 48.17
Iteration: 2421 | Episodes: 196400 | Median Reward: 41.44 | Max Reward: 48.17
Iteration: 2422 | Episodes: 196500 | Median Reward: 35.39 | Max Reward: 48.17
Iteration: 2423 | Episodes: 196600 | Median Reward: 36.54 | Max Reward: 48.17
Iteration: 2425 | Episodes: 196700 | Median Reward: 40.40 | Max Reward: 48.17
Iteration: 2426 | Episodes: 196800 | Median Reward: 38.78 | Max Reward: 48.17
Iteration: 2427 | Episodes: 196900 | Median Reward: 39.80 | Max Reward: 48.17
Iteration: 2428 | Episodes: 197000 | Median Reward: 44.90 | Max Reward: 48.17
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.4       |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 2430        |
|    time_elapsed         | 97133       |
|    total_timesteps      | 19906560    |
| train/                  |             |
|    approx_kl            | 0.000700484 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -130        |
|    explained_variance   | 1           |
|    learning_rate        | 0.000707    |
|    loss                 | -6.32       |
|    n_updates            | 24290       |
|    policy_gradient_loss | -0.00279    |
|    std                  | 7.31        |
|    value_loss           | 2.09        |
-----------------------------------------
Iteration: 2430 | Episodes: 197100 | Median Reward: 42.53 | Max Reward: 48.17
Iteration: 2431 | Episodes: 197200 | Median Reward: 40.47 | Max Reward: 48.17
Iteration: 2432 | Episodes: 197300 | Median Reward: 36.46 | Max Reward: 48.17
Iteration: 2433 | Episodes: 197400 | Median Reward: 33.18 | Max Reward: 48.17
Iteration: 2434 | Episodes: 197500 | Median Reward: 43.14 | Max Reward: 48.17
Iteration: 2436 | Episodes: 197600 | Median Reward: 39.50 | Max Reward: 48.17
Iteration: 2437 | Episodes: 197700 | Median Reward: 39.50 | Max Reward: 48.49
Iteration: 2438 | Episodes: 197800 | Median Reward: 41.42 | Max Reward: 48.49
Iteration: 2439 | Episodes: 197900 | Median Reward: 42.98 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.1        |
| time/                   |              |
|    fps                  | 205          |
|    iterations           | 2440         |
|    time_elapsed         | 97221        |
|    total_timesteps      | 19988480     |
| train/                  |              |
|    approx_kl            | 0.0024569703 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.61        |
|    n_updates            | 24390        |
|    policy_gradient_loss | -0.00402     |
|    std                  | 7.35         |
|    value_loss           | 0.774        |
------------------------------------------
Iteration: 2441 | Episodes: 198000 | Median Reward: 34.72 | Max Reward: 48.49
Iteration: 2442 | Episodes: 198100 | Median Reward: 34.72 | Max Reward: 48.49
Iteration: 2443 | Episodes: 198200 | Median Reward: 40.65 | Max Reward: 48.49
Iteration: 2444 | Episodes: 198300 | Median Reward: 41.12 | Max Reward: 48.49
Iteration: 2446 | Episodes: 198400 | Median Reward: 37.53 | Max Reward: 48.49
Iteration: 2447 | Episodes: 198500 | Median Reward: 37.14 | Max Reward: 48.49
Iteration: 2448 | Episodes: 198600 | Median Reward: 33.91 | Max Reward: 48.49
Iteration: 2449 | Episodes: 198700 | Median Reward: 41.01 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.2         |
| time/                   |               |
|    fps                  | 206           |
|    iterations           | 2450          |
|    time_elapsed         | 97309         |
|    total_timesteps      | 20070400      |
| train/                  |               |
|    approx_kl            | 0.00037757133 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.34         |
|    n_updates            | 24490         |
|    policy_gradient_loss | -0.000267     |
|    std                  | 7.39          |
|    value_loss           | 0.527         |
-------------------------------------------
Iteration: 2451 | Episodes: 198800 | Median Reward: 40.31 | Max Reward: 48.49
Iteration: 2452 | Episodes: 198900 | Median Reward: 38.79 | Max Reward: 48.49
Iteration: 2453 | Episodes: 199000 | Median Reward: 41.40 | Max Reward: 48.49
Iteration: 2454 | Episodes: 199100 | Median Reward: 41.34 | Max Reward: 48.49
Iteration: 2455 | Episodes: 199200 | Median Reward: 38.37 | Max Reward: 48.49
Iteration: 2457 | Episodes: 199300 | Median Reward: 37.84 | Max Reward: 48.49
Iteration: 2458 | Episodes: 199400 | Median Reward: 38.95 | Max Reward: 48.49
Iteration: 2459 | Episodes: 199500 | Median Reward: 41.61 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.2        |
| time/                   |              |
|    fps                  | 206          |
|    iterations           | 2460         |
|    time_elapsed         | 97398        |
|    total_timesteps      | 20152320     |
| train/                  |              |
|    approx_kl            | 0.0023893833 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.27        |
|    n_updates            | 24590        |
|    policy_gradient_loss | -0.00856     |
|    std                  | 7.41         |
|    value_loss           | 1.04         |
------------------------------------------
Iteration: 2460 | Episodes: 199600 | Median Reward: 43.80 | Max Reward: 48.49
Iteration: 2462 | Episodes: 199700 | Median Reward: 34.08 | Max Reward: 48.49
Iteration: 2463 | Episodes: 199800 | Median Reward: 34.23 | Max Reward: 48.49
Iteration: 2464 | Episodes: 199900 | Median Reward: 39.27 | Max Reward: 48.49
Iteration: 2465 | Episodes: 200000 | Median Reward: 35.90 | Max Reward: 48.49
Iteration: 2467 | Episodes: 200100 | Median Reward: 39.32 | Max Reward: 48.49
Iteration: 2468 | Episodes: 200200 | Median Reward: 38.41 | Max Reward: 48.49
Iteration: 2469 | Episodes: 200300 | Median Reward: 38.81 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.6         |
| time/                   |               |
|    fps                  | 207           |
|    iterations           | 2470          |
|    time_elapsed         | 97485         |
|    total_timesteps      | 20234240      |
| train/                  |               |
|    approx_kl            | 0.00018378212 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.33         |
|    n_updates            | 24690         |
|    policy_gradient_loss | -0.00057      |
|    std                  | 7.45          |
|    value_loss           | 1.56          |
-------------------------------------------
Iteration: 2470 | Episodes: 200400 | Median Reward: 38.19 | Max Reward: 48.49
Iteration: 2471 | Episodes: 200500 | Median Reward: 38.61 | Max Reward: 48.49
Iteration: 2473 | Episodes: 200600 | Median Reward: 39.49 | Max Reward: 48.49
Iteration: 2474 | Episodes: 200700 | Median Reward: 43.69 | Max Reward: 48.49
Iteration: 2475 | Episodes: 200800 | Median Reward: 40.04 | Max Reward: 48.49
Iteration: 2476 | Episodes: 200900 | Median Reward: 36.72 | Max Reward: 48.49
Iteration: 2478 | Episodes: 201000 | Median Reward: 39.05 | Max Reward: 48.49
Iteration: 2479 | Episodes: 201100 | Median Reward: 39.91 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.8         |
| time/                   |               |
|    fps                  | 208           |
|    iterations           | 2480          |
|    time_elapsed         | 97574         |
|    total_timesteps      | 20316160      |
| train/                  |               |
|    approx_kl            | 0.00039999333 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -6.26         |
|    n_updates            | 24790         |
|    policy_gradient_loss | -0.000876     |
|    std                  | 7.48          |
|    value_loss           | 0.3           |
-------------------------------------------
Iteration: 2480 | Episodes: 201200 | Median Reward: 40.89 | Max Reward: 48.49
Iteration: 2481 | Episodes: 201300 | Median Reward: 39.94 | Max Reward: 48.49
Iteration: 2483 | Episodes: 201400 | Median Reward: 38.11 | Max Reward: 48.49
Iteration: 2484 | Episodes: 201500 | Median Reward: 37.86 | Max Reward: 48.49
Iteration: 2485 | Episodes: 201600 | Median Reward: 34.54 | Max Reward: 48.49
Iteration: 2486 | Episodes: 201700 | Median Reward: 36.54 | Max Reward: 48.49
Iteration: 2488 | Episodes: 201800 | Median Reward: 40.47 | Max Reward: 48.49
Iteration: 2489 | Episodes: 201900 | Median Reward: 37.48 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 2490         |
|    time_elapsed         | 97662        |
|    total_timesteps      | 20398080     |
| train/                  |              |
|    approx_kl            | 0.0006252526 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.08        |
|    n_updates            | 24890        |
|    policy_gradient_loss | -0.00275     |
|    std                  | 7.55         |
|    value_loss           | 0.947        |
------------------------------------------
Iteration: 2490 | Episodes: 202000 | Median Reward: 39.70 | Max Reward: 48.49
Iteration: 2491 | Episodes: 202100 | Median Reward: 39.56 | Max Reward: 48.49
Iteration: 2492 | Episodes: 202200 | Median Reward: 43.11 | Max Reward: 48.49
Iteration: 2494 | Episodes: 202300 | Median Reward: 42.91 | Max Reward: 48.49
Iteration: 2495 | Episodes: 202400 | Median Reward: 41.25 | Max Reward: 48.49
Iteration: 2496 | Episodes: 202500 | Median Reward: 39.30 | Max Reward: 48.49
Iteration: 2497 | Episodes: 202600 | Median Reward: 36.39 | Max Reward: 48.49
Iteration: 2499 | Episodes: 202700 | Median Reward: 38.28 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.3        |
| time/                   |              |
|    fps                  | 209          |
|    iterations           | 2500         |
|    time_elapsed         | 97750        |
|    total_timesteps      | 20480000     |
| train/                  |              |
|    approx_kl            | 0.0072937496 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -131         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.23        |
|    n_updates            | 24990        |
|    policy_gradient_loss | -0.0209      |
|    std                  | 7.58         |
|    value_loss           | 0.623        |
------------------------------------------
Iteration: 2500 | Episodes: 202800 | Median Reward: 38.29 | Max Reward: 48.49
Iteration: 2501 | Episodes: 202900 | Median Reward: 39.79 | Max Reward: 48.49
Iteration: 2502 | Episodes: 203000 | Median Reward: 34.85 | Max Reward: 48.49
Iteration: 2504 | Episodes: 203100 | Median Reward: 39.30 | Max Reward: 48.49
Iteration: 2505 | Episodes: 203200 | Median Reward: 39.93 | Max Reward: 48.49
Iteration: 2506 | Episodes: 203300 | Median Reward: 37.71 | Max Reward: 48.49
Iteration: 2507 | Episodes: 203400 | Median Reward: 36.91 | Max Reward: 48.49
Iteration: 2508 | Episodes: 203500 | Median Reward: 42.18 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.2         |
| time/                   |               |
|    fps                  | 210           |
|    iterations           | 2510          |
|    time_elapsed         | 97839         |
|    total_timesteps      | 20561920      |
| train/                  |               |
|    approx_kl            | 0.00015429861 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -131          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.32         |
|    n_updates            | 25090         |
|    policy_gradient_loss | -0.000306     |
|    std                  | 7.62          |
|    value_loss           | 1.04          |
-------------------------------------------
Iteration: 2510 | Episodes: 203600 | Median Reward: 38.94 | Max Reward: 48.49
Iteration: 2511 | Episodes: 203700 | Median Reward: 38.94 | Max Reward: 48.49
Iteration: 2512 | Episodes: 203800 | Median Reward: 41.36 | Max Reward: 48.49
Iteration: 2513 | Episodes: 203900 | Median Reward: 39.10 | Max Reward: 48.49
Iteration: 2515 | Episodes: 204000 | Median Reward: 41.17 | Max Reward: 48.49
Iteration: 2516 | Episodes: 204100 | Median Reward: 41.31 | Max Reward: 48.49
Iteration: 2517 | Episodes: 204200 | Median Reward: 42.62 | Max Reward: 48.49
Iteration: 2518 | Episodes: 204300 | Median Reward: 44.42 | Max Reward: 48.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -62.8       |
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 2520        |
|    time_elapsed         | 97928       |
|    total_timesteps      | 20643840    |
| train/                  |             |
|    approx_kl            | 0.037638538 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -131        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.27       |
|    n_updates            | 25190       |
|    policy_gradient_loss | -0.047      |
|    std                  | 7.66        |
|    value_loss           | 0.631       |
-----------------------------------------
Iteration: 2520 | Episodes: 204400 | Median Reward: 37.72 | Max Reward: 48.49
Iteration: 2521 | Episodes: 204500 | Median Reward: 38.45 | Max Reward: 48.49
Iteration: 2522 | Episodes: 204600 | Median Reward: 40.47 | Max Reward: 48.49
Iteration: 2523 | Episodes: 204700 | Median Reward: 38.22 | Max Reward: 48.49
Iteration: 2524 | Episodes: 204800 | Median Reward: 36.78 | Max Reward: 48.49
Iteration: 2526 | Episodes: 204900 | Median Reward: 38.79 | Max Reward: 48.49
Iteration: 2527 | Episodes: 205000 | Median Reward: 39.49 | Max Reward: 48.49
Iteration: 2528 | Episodes: 205100 | Median Reward: 40.94 | Max Reward: 48.49
Iteration: 2529 | Episodes: 205200 | Median Reward: 46.13 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -54.6        |
| time/                   |              |
|    fps                  | 211          |
|    iterations           | 2530         |
|    time_elapsed         | 98016        |
|    total_timesteps      | 20725760     |
| train/                  |              |
|    approx_kl            | 0.0019692976 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -132         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -6.47        |
|    n_updates            | 25290        |
|    policy_gradient_loss | -0.00432     |
|    std                  | 7.76         |
|    value_loss           | 0.295        |
------------------------------------------
Iteration: 2531 | Episodes: 205300 | Median Reward: 39.05 | Max Reward: 48.49
Iteration: 2532 | Episodes: 205400 | Median Reward: 38.17 | Max Reward: 48.49
Iteration: 2533 | Episodes: 205500 | Median Reward: 39.66 | Max Reward: 48.49
Iteration: 2534 | Episodes: 205600 | Median Reward: 43.71 | Max Reward: 48.49
Iteration: 2536 | Episodes: 205700 | Median Reward: 35.64 | Max Reward: 48.49
Iteration: 2537 | Episodes: 205800 | Median Reward: 43.58 | Max Reward: 48.49
Iteration: 2538 | Episodes: 205900 | Median Reward: 43.03 | Max Reward: 48.49
Iteration: 2539 | Episodes: 206000 | Median Reward: 34.73 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.7         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 2540          |
|    time_elapsed         | 98104         |
|    total_timesteps      | 20807680      |
| train/                  |               |
|    approx_kl            | 0.00069081644 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -132          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.21         |
|    n_updates            | 25390         |
|    policy_gradient_loss | -0.0017       |
|    std                  | 7.81          |
|    value_loss           | 1.11          |
-------------------------------------------
Iteration: 2541 | Episodes: 206100 | Median Reward: 37.22 | Max Reward: 48.49
Iteration: 2542 | Episodes: 206200 | Median Reward: 42.40 | Max Reward: 48.49
Iteration: 2543 | Episodes: 206300 | Median Reward: 40.62 | Max Reward: 48.49
Iteration: 2544 | Episodes: 206400 | Median Reward: 37.96 | Max Reward: 48.49
Iteration: 2545 | Episodes: 206500 | Median Reward: 38.21 | Max Reward: 48.49
Iteration: 2547 | Episodes: 206600 | Median Reward: 38.70 | Max Reward: 48.49
Iteration: 2548 | Episodes: 206700 | Median Reward: 40.95 | Max Reward: 48.49
Iteration: 2549 | Episodes: 206800 | Median Reward: 42.50 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.6         |
| time/                   |               |
|    fps                  | 212           |
|    iterations           | 2550          |
|    time_elapsed         | 98193         |
|    total_timesteps      | 20889600      |
| train/                  |               |
|    approx_kl            | 6.5609056e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -132          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -4.9          |
|    n_updates            | 25490         |
|    policy_gradient_loss | -0.000389     |
|    std                  | 7.87          |
|    value_loss           | 1.52          |
-------------------------------------------
Iteration: 2550 | Episodes: 206900 | Median Reward: 42.33 | Max Reward: 48.49
Iteration: 2552 | Episodes: 207000 | Median Reward: 41.09 | Max Reward: 48.49
Iteration: 2553 | Episodes: 207100 | Median Reward: 42.25 | Max Reward: 48.49
Iteration: 2554 | Episodes: 207200 | Median Reward: 33.61 | Max Reward: 48.49
Iteration: 2555 | Episodes: 207300 | Median Reward: 47.07 | Max Reward: 48.49
Iteration: 2557 | Episodes: 207400 | Median Reward: 41.63 | Max Reward: 48.49
Iteration: 2558 | Episodes: 207500 | Median Reward: 41.99 | Max Reward: 48.49
Iteration: 2559 | Episodes: 207600 | Median Reward: 39.11 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.7        |
| time/                   |              |
|    fps                  | 213          |
|    iterations           | 2560         |
|    time_elapsed         | 98281        |
|    total_timesteps      | 20971520     |
| train/                  |              |
|    approx_kl            | 6.414386e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -132         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.34        |
|    n_updates            | 25590        |
|    policy_gradient_loss | -0.00136     |
|    std                  | 7.94         |
|    value_loss           | 3.43         |
------------------------------------------
Iteration: 2560 | Episodes: 207700 | Median Reward: 37.09 | Max Reward: 48.49
Iteration: 2561 | Episodes: 207800 | Median Reward: 37.73 | Max Reward: 48.49
Iteration: 2563 | Episodes: 207900 | Median Reward: 44.35 | Max Reward: 48.49
Iteration: 2564 | Episodes: 208000 | Median Reward: 44.02 | Max Reward: 48.49
Iteration: 2565 | Episodes: 208100 | Median Reward: 40.76 | Max Reward: 48.49
Iteration: 2566 | Episodes: 208200 | Median Reward: 40.57 | Max Reward: 48.49
Iteration: 2568 | Episodes: 208300 | Median Reward: 37.15 | Max Reward: 48.49
Iteration: 2569 | Episodes: 208400 | Median Reward: 41.97 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.5        |
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 2570         |
|    time_elapsed         | 98369        |
|    total_timesteps      | 21053440     |
| train/                  |              |
|    approx_kl            | 4.643797e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -132         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.35        |
|    n_updates            | 25690        |
|    policy_gradient_loss | -0.000316    |
|    std                  | 7.99         |
|    value_loss           | 1.34         |
------------------------------------------
Iteration: 2570 | Episodes: 208500 | Median Reward: 39.45 | Max Reward: 48.49
Iteration: 2571 | Episodes: 208600 | Median Reward: 40.90 | Max Reward: 48.49
Iteration: 2573 | Episodes: 208700 | Median Reward: 39.36 | Max Reward: 48.49
Iteration: 2574 | Episodes: 208800 | Median Reward: 39.58 | Max Reward: 48.49
Iteration: 2575 | Episodes: 208900 | Median Reward: 38.14 | Max Reward: 48.49
Iteration: 2576 | Episodes: 209000 | Median Reward: 36.92 | Max Reward: 48.49
Iteration: 2578 | Episodes: 209100 | Median Reward: 44.36 | Max Reward: 48.49
Iteration: 2579 | Episodes: 209200 | Median Reward: 43.48 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.9         |
| time/                   |               |
|    fps                  | 214           |
|    iterations           | 2580          |
|    time_elapsed         | 98458         |
|    total_timesteps      | 21135360      |
| train/                  |               |
|    approx_kl            | 1.0788441e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.5          |
|    n_updates            | 25790         |
|    policy_gradient_loss | 7.98e-07      |
|    std                  | 8.05          |
|    value_loss           | 0.537         |
-------------------------------------------
Iteration: 2580 | Episodes: 209300 | Median Reward: 40.29 | Max Reward: 48.49
Iteration: 2581 | Episodes: 209400 | Median Reward: 40.20 | Max Reward: 48.49
Iteration: 2582 | Episodes: 209500 | Median Reward: 37.64 | Max Reward: 48.49
Iteration: 2584 | Episodes: 209600 | Median Reward: 40.87 | Max Reward: 48.49
Iteration: 2585 | Episodes: 209700 | Median Reward: 41.46 | Max Reward: 48.49
Iteration: 2586 | Episodes: 209800 | Median Reward: 33.22 | Max Reward: 48.49
Iteration: 2587 | Episodes: 209900 | Median Reward: 33.39 | Max Reward: 48.49
Iteration: 2589 | Episodes: 210000 | Median Reward: 37.68 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.9         |
| time/                   |               |
|    fps                  | 215           |
|    iterations           | 2590          |
|    time_elapsed         | 98546         |
|    total_timesteps      | 21217280      |
| train/                  |               |
|    approx_kl            | 2.6104448e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.2          |
|    n_updates            | 25890         |
|    policy_gradient_loss | -2.01e-05     |
|    std                  | 8.08          |
|    value_loss           | 1.93          |
-------------------------------------------
Iteration: 2590 | Episodes: 210100 | Median Reward: 43.44 | Max Reward: 48.49
Iteration: 2591 | Episodes: 210200 | Median Reward: 43.20 | Max Reward: 48.49
Iteration: 2592 | Episodes: 210300 | Median Reward: 32.26 | Max Reward: 48.49
Iteration: 2594 | Episodes: 210400 | Median Reward: 39.90 | Max Reward: 48.49
Iteration: 2595 | Episodes: 210500 | Median Reward: 40.06 | Max Reward: 48.49
Iteration: 2596 | Episodes: 210600 | Median Reward: 40.07 | Max Reward: 48.49
Iteration: 2597 | Episodes: 210700 | Median Reward: 41.13 | Max Reward: 48.49
Iteration: 2598 | Episodes: 210800 | Median Reward: 41.58 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.2         |
| time/                   |               |
|    fps                  | 215           |
|    iterations           | 2600          |
|    time_elapsed         | 98634         |
|    total_timesteps      | 21299200      |
| train/                  |               |
|    approx_kl            | 0.00018353127 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.07         |
|    n_updates            | 25990         |
|    policy_gradient_loss | -0.000499     |
|    std                  | 8.14          |
|    value_loss           | 0.583         |
-------------------------------------------
Iteration: 2600 | Episodes: 210900 | Median Reward: 35.84 | Max Reward: 48.49
Iteration: 2601 | Episodes: 211000 | Median Reward: 41.57 | Max Reward: 48.49
Iteration: 2602 | Episodes: 211100 | Median Reward: 43.17 | Max Reward: 48.49
Iteration: 2603 | Episodes: 211200 | Median Reward: 38.68 | Max Reward: 48.49
Iteration: 2605 | Episodes: 211300 | Median Reward: 38.54 | Max Reward: 48.49
Iteration: 2606 | Episodes: 211400 | Median Reward: 41.82 | Max Reward: 48.49
Iteration: 2607 | Episodes: 211500 | Median Reward: 45.21 | Max Reward: 48.49
Iteration: 2608 | Episodes: 211600 | Median Reward: 43.57 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.1        |
| time/                   |              |
|    fps                  | 216          |
|    iterations           | 2610         |
|    time_elapsed         | 98723        |
|    total_timesteps      | 21381120     |
| train/                  |              |
|    approx_kl            | 0.0029382175 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -133         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.41        |
|    n_updates            | 26090        |
|    policy_gradient_loss | -0.0091      |
|    std                  | 8.2          |
|    value_loss           | 1.02         |
------------------------------------------
Iteration: 2610 | Episodes: 211700 | Median Reward: 43.12 | Max Reward: 48.49
Iteration: 2611 | Episodes: 211800 | Median Reward: 43.36 | Max Reward: 48.49
Iteration: 2612 | Episodes: 211900 | Median Reward: 42.60 | Max Reward: 48.49
Iteration: 2613 | Episodes: 212000 | Median Reward: 42.23 | Max Reward: 48.49
Iteration: 2615 | Episodes: 212100 | Median Reward: 42.04 | Max Reward: 48.49
Iteration: 2616 | Episodes: 212200 | Median Reward: 43.56 | Max Reward: 48.49
Iteration: 2617 | Episodes: 212300 | Median Reward: 42.18 | Max Reward: 48.49
Iteration: 2618 | Episodes: 212400 | Median Reward: 44.61 | Max Reward: 48.49
Iteration: 2619 | Episodes: 212500 | Median Reward: 41.09 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59          |
| time/                   |              |
|    fps                  | 217          |
|    iterations           | 2620         |
|    time_elapsed         | 98811        |
|    total_timesteps      | 21463040     |
| train/                  |              |
|    approx_kl            | 0.0012686627 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -133         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -6.54        |
|    n_updates            | 26190        |
|    policy_gradient_loss | -0.00197     |
|    std                  | 8.24         |
|    value_loss           | 0.312        |
------------------------------------------
Iteration: 2621 | Episodes: 212600 | Median Reward: 38.00 | Max Reward: 48.49
Iteration: 2622 | Episodes: 212700 | Median Reward: 36.29 | Max Reward: 48.49
Iteration: 2623 | Episodes: 212800 | Median Reward: 39.74 | Max Reward: 48.49
Iteration: 2624 | Episodes: 212900 | Median Reward: 39.34 | Max Reward: 48.49
Iteration: 2626 | Episodes: 213000 | Median Reward: 39.34 | Max Reward: 48.49
Iteration: 2627 | Episodes: 213100 | Median Reward: 36.68 | Max Reward: 48.49
Iteration: 2628 | Episodes: 213200 | Median Reward: 41.70 | Max Reward: 48.49
Iteration: 2629 | Episodes: 213300 | Median Reward: 43.74 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55           |
| time/                   |               |
|    fps                  | 217           |
|    iterations           | 2630          |
|    time_elapsed         | 98899         |
|    total_timesteps      | 21544960      |
| train/                  |               |
|    approx_kl            | 9.4842166e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.2          |
|    n_updates            | 26290         |
|    policy_gradient_loss | -0.000845     |
|    std                  | 8.3           |
|    value_loss           | 1.09          |
-------------------------------------------
Iteration: 2631 | Episodes: 213400 | Median Reward: 43.60 | Max Reward: 48.49
Iteration: 2632 | Episodes: 213500 | Median Reward: 37.40 | Max Reward: 48.49
Iteration: 2633 | Episodes: 213600 | Median Reward: 39.34 | Max Reward: 48.49
Iteration: 2634 | Episodes: 213700 | Median Reward: 42.28 | Max Reward: 48.49
Iteration: 2635 | Episodes: 213800 | Median Reward: 44.61 | Max Reward: 48.49
Iteration: 2637 | Episodes: 213900 | Median Reward: 36.76 | Max Reward: 48.49
Iteration: 2638 | Episodes: 214000 | Median Reward: 39.00 | Max Reward: 48.49
Iteration: 2639 | Episodes: 214100 | Median Reward: 40.59 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61           |
| time/                   |               |
|    fps                  | 218           |
|    iterations           | 2640          |
|    time_elapsed         | 98988         |
|    total_timesteps      | 21626880      |
| train/                  |               |
|    approx_kl            | 0.00058736035 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -133          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.28         |
|    n_updates            | 26390         |
|    policy_gradient_loss | -0.000985     |
|    std                  | 8.35          |
|    value_loss           | 0.886         |
-------------------------------------------
Iteration: 2640 | Episodes: 214200 | Median Reward: 37.37 | Max Reward: 48.49
Iteration: 2642 | Episodes: 214300 | Median Reward: 38.44 | Max Reward: 48.49
Iteration: 2643 | Episodes: 214400 | Median Reward: 37.00 | Max Reward: 48.49
Iteration: 2644 | Episodes: 214500 | Median Reward: 39.12 | Max Reward: 48.49
Iteration: 2645 | Episodes: 214600 | Median Reward: 40.70 | Max Reward: 48.49
Iteration: 2647 | Episodes: 214700 | Median Reward: 42.18 | Max Reward: 48.49
Iteration: 2648 | Episodes: 214800 | Median Reward: 41.77 | Max Reward: 48.49
Iteration: 2649 | Episodes: 214900 | Median Reward: 40.38 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.7        |
| time/                   |              |
|    fps                  | 219          |
|    iterations           | 2650         |
|    time_elapsed         | 99076        |
|    total_timesteps      | 21708800     |
| train/                  |              |
|    approx_kl            | 5.181253e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -134         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.09        |
|    n_updates            | 26490        |
|    policy_gradient_loss | -0.000125    |
|    std                  | 8.41         |
|    value_loss           | 1.23         |
------------------------------------------
Iteration: 2650 | Episodes: 215000 | Median Reward: 37.36 | Max Reward: 48.49
Iteration: 2651 | Episodes: 215100 | Median Reward: 34.13 | Max Reward: 48.49
Iteration: 2653 | Episodes: 215200 | Median Reward: 43.56 | Max Reward: 48.49
Iteration: 2654 | Episodes: 215300 | Median Reward: 44.57 | Max Reward: 48.49
Iteration: 2655 | Episodes: 215400 | Median Reward: 34.20 | Max Reward: 48.49
Iteration: 2656 | Episodes: 215500 | Median Reward: 41.13 | Max Reward: 48.49
Iteration: 2658 | Episodes: 215600 | Median Reward: 38.28 | Max Reward: 48.49
Iteration: 2659 | Episodes: 215700 | Median Reward: 37.82 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.3         |
| time/                   |               |
|    fps                  | 219           |
|    iterations           | 2660          |
|    time_elapsed         | 99165         |
|    total_timesteps      | 21790720      |
| train/                  |               |
|    approx_kl            | 0.00010686725 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.34         |
|    n_updates            | 26590         |
|    policy_gradient_loss | -0.000654     |
|    std                  | 8.49          |
|    value_loss           | 0.567         |
-------------------------------------------
Iteration: 2660 | Episodes: 215800 | Median Reward: 36.91 | Max Reward: 48.49
Iteration: 2661 | Episodes: 215900 | Median Reward: 43.34 | Max Reward: 48.49
Iteration: 2663 | Episodes: 216000 | Median Reward: 46.32 | Max Reward: 48.49
Iteration: 2664 | Episodes: 216100 | Median Reward: 41.01 | Max Reward: 48.49
Iteration: 2665 | Episodes: 216200 | Median Reward: 42.56 | Max Reward: 48.49
Iteration: 2666 | Episodes: 216300 | Median Reward: 42.39 | Max Reward: 48.49
Iteration: 2668 | Episodes: 216400 | Median Reward: 46.77 | Max Reward: 48.49
Iteration: 2669 | Episodes: 216500 | Median Reward: 35.11 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65          |
| time/                   |              |
|    fps                  | 220          |
|    iterations           | 2670         |
|    time_elapsed         | 99253        |
|    total_timesteps      | 21872640     |
| train/                  |              |
|    approx_kl            | 0.0027638748 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -134         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.48        |
|    n_updates            | 26690        |
|    policy_gradient_loss | -0.012       |
|    std                  | 8.53         |
|    value_loss           | 0.814        |
------------------------------------------
Iteration: 2670 | Episodes: 216600 | Median Reward: 35.22 | Max Reward: 48.49
Iteration: 2671 | Episodes: 216700 | Median Reward: 39.09 | Max Reward: 48.49
Iteration: 2672 | Episodes: 216800 | Median Reward: 35.15 | Max Reward: 48.49
Iteration: 2674 | Episodes: 216900 | Median Reward: 38.91 | Max Reward: 48.49
Iteration: 2675 | Episodes: 217000 | Median Reward: 42.77 | Max Reward: 48.49
Iteration: 2676 | Episodes: 217100 | Median Reward: 39.32 | Max Reward: 48.49
Iteration: 2677 | Episodes: 217200 | Median Reward: 38.63 | Max Reward: 48.49
Iteration: 2679 | Episodes: 217300 | Median Reward: 38.08 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.3         |
| time/                   |               |
|    fps                  | 220           |
|    iterations           | 2680          |
|    time_elapsed         | 99342         |
|    total_timesteps      | 21954560      |
| train/                  |               |
|    approx_kl            | 1.3083816e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -3.78         |
|    n_updates            | 26790         |
|    policy_gradient_loss | -0.000163     |
|    std                  | 8.57          |
|    value_loss           | 3.22          |
-------------------------------------------
Iteration: 2680 | Episodes: 217400 | Median Reward: 43.42 | Max Reward: 48.49
Iteration: 2681 | Episodes: 217500 | Median Reward: 40.27 | Max Reward: 48.49
Iteration: 2682 | Episodes: 217600 | Median Reward: 37.28 | Max Reward: 48.49
Iteration: 2684 | Episodes: 217700 | Median Reward: 42.10 | Max Reward: 48.49
Iteration: 2685 | Episodes: 217800 | Median Reward: 40.17 | Max Reward: 48.49
Iteration: 2686 | Episodes: 217900 | Median Reward: 35.93 | Max Reward: 48.49
Iteration: 2687 | Episodes: 218000 | Median Reward: 40.90 | Max Reward: 48.49
Iteration: 2688 | Episodes: 218100 | Median Reward: 41.60 | Max Reward: 48.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.5       |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 2690        |
|    time_elapsed         | 99431       |
|    total_timesteps      | 22036480    |
| train/                  |             |
|    approx_kl            | 7.47185e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -134        |
|    explained_variance   | 1           |
|    learning_rate        | 0.000707    |
|    loss                 | -6.43       |
|    n_updates            | 26890       |
|    policy_gradient_loss | -0.000324   |
|    std                  | 8.6         |
|    value_loss           | 0.756       |
-----------------------------------------
Iteration: 2690 | Episodes: 218200 | Median Reward: 34.61 | Max Reward: 48.49
Iteration: 2691 | Episodes: 218300 | Median Reward: 34.67 | Max Reward: 48.49
Iteration: 2692 | Episodes: 218400 | Median Reward: 42.94 | Max Reward: 48.49
Iteration: 2693 | Episodes: 218500 | Median Reward: 43.19 | Max Reward: 48.49
Iteration: 2695 | Episodes: 218600 | Median Reward: 40.00 | Max Reward: 48.49
Iteration: 2696 | Episodes: 218700 | Median Reward: 35.39 | Max Reward: 48.49
Iteration: 2697 | Episodes: 218800 | Median Reward: 42.92 | Max Reward: 48.49
Iteration: 2698 | Episodes: 218900 | Median Reward: 38.51 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.9         |
| time/                   |               |
|    fps                  | 222           |
|    iterations           | 2700          |
|    time_elapsed         | 99519         |
|    total_timesteps      | 22118400      |
| train/                  |               |
|    approx_kl            | 0.00034522364 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -134          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.22         |
|    n_updates            | 26990         |
|    policy_gradient_loss | -0.00157      |
|    std                  | 8.62          |
|    value_loss           | 0.843         |
-------------------------------------------
Iteration: 2700 | Episodes: 219000 | Median Reward: 38.59 | Max Reward: 48.49
Iteration: 2701 | Episodes: 219100 | Median Reward: 42.41 | Max Reward: 48.49
Iteration: 2702 | Episodes: 219200 | Median Reward: 30.34 | Max Reward: 48.49
Iteration: 2703 | Episodes: 219300 | Median Reward: 39.33 | Max Reward: 48.49
Iteration: 2705 | Episodes: 219400 | Median Reward: 41.65 | Max Reward: 48.49
Iteration: 2706 | Episodes: 219500 | Median Reward: 39.99 | Max Reward: 48.49
Iteration: 2707 | Episodes: 219600 | Median Reward: 41.95 | Max Reward: 48.49
Iteration: 2708 | Episodes: 219700 | Median Reward: 35.08 | Max Reward: 48.49
Iteration: 2709 | Episodes: 219800 | Median Reward: 34.96 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.4        |
| time/                   |              |
|    fps                  | 222          |
|    iterations           | 2710         |
|    time_elapsed         | 99608        |
|    total_timesteps      | 22200320     |
| train/                  |              |
|    approx_kl            | 0.0003310807 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -134         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -5.53        |
|    n_updates            | 27090        |
|    policy_gradient_loss | -0.00151     |
|    std                  | 8.67         |
|    value_loss           | 2.59         |
------------------------------------------
Iteration: 2711 | Episodes: 219900 | Median Reward: 37.99 | Max Reward: 48.49
Iteration: 2712 | Episodes: 220000 | Median Reward: 41.51 | Max Reward: 48.49
Iteration: 2713 | Episodes: 220100 | Median Reward: 38.71 | Max Reward: 48.49
Iteration: 2714 | Episodes: 220200 | Median Reward: 37.94 | Max Reward: 48.49
Iteration: 2716 | Episodes: 220300 | Median Reward: 38.69 | Max Reward: 48.49
Iteration: 2717 | Episodes: 220400 | Median Reward: 39.73 | Max Reward: 48.49
Iteration: 2718 | Episodes: 220500 | Median Reward: 40.19 | Max Reward: 48.49
Iteration: 2719 | Episodes: 220600 | Median Reward: 43.52 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56          |
| time/                   |              |
|    fps                  | 223          |
|    iterations           | 2720         |
|    time_elapsed         | 99696        |
|    total_timesteps      | 22282240     |
| train/                  |              |
|    approx_kl            | 0.0032921112 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -135         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.53        |
|    n_updates            | 27190        |
|    policy_gradient_loss | -0.00359     |
|    std                  | 8.78         |
|    value_loss           | 0.371        |
------------------------------------------
Iteration: 2721 | Episodes: 220700 | Median Reward: 43.52 | Max Reward: 48.49
Iteration: 2722 | Episodes: 220800 | Median Reward: 37.55 | Max Reward: 48.49
Iteration: 2723 | Episodes: 220900 | Median Reward: 36.29 | Max Reward: 48.49
Iteration: 2724 | Episodes: 221000 | Median Reward: 41.82 | Max Reward: 48.49
Iteration: 2725 | Episodes: 221100 | Median Reward: 43.26 | Max Reward: 48.49
Iteration: 2727 | Episodes: 221200 | Median Reward: 38.63 | Max Reward: 48.49
Iteration: 2728 | Episodes: 221300 | Median Reward: 40.29 | Max Reward: 48.49
Iteration: 2729 | Episodes: 221400 | Median Reward: 40.79 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.1        |
| time/                   |              |
|    fps                  | 224          |
|    iterations           | 2730         |
|    time_elapsed         | 99784        |
|    total_timesteps      | 22364160     |
| train/                  |              |
|    approx_kl            | 0.0009398302 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -135         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.39        |
|    n_updates            | 27290        |
|    policy_gradient_loss | -0.0062      |
|    std                  | 8.85         |
|    value_loss           | 0.703        |
------------------------------------------
Iteration: 2730 | Episodes: 221500 | Median Reward: 40.86 | Max Reward: 48.49
Iteration: 2732 | Episodes: 221600 | Median Reward: 41.98 | Max Reward: 48.49
Iteration: 2733 | Episodes: 221700 | Median Reward: 42.14 | Max Reward: 48.49
Iteration: 2734 | Episodes: 221800 | Median Reward: 40.81 | Max Reward: 48.49
Iteration: 2735 | Episodes: 221900 | Median Reward: 44.63 | Max Reward: 48.49
Iteration: 2737 | Episodes: 222000 | Median Reward: 40.89 | Max Reward: 48.49
Iteration: 2738 | Episodes: 222100 | Median Reward: 40.73 | Max Reward: 48.49
Iteration: 2739 | Episodes: 222200 | Median Reward: 36.04 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.7        |
| time/                   |              |
|    fps                  | 224          |
|    iterations           | 2740         |
|    time_elapsed         | 99873        |
|    total_timesteps      | 22446080     |
| train/                  |              |
|    approx_kl            | 0.0006130903 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -135         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.42        |
|    n_updates            | 27390        |
|    policy_gradient_loss | -0.003       |
|    std                  | 8.88         |
|    value_loss           | 0.738        |
------------------------------------------
Iteration: 2740 | Episodes: 222300 | Median Reward: 36.01 | Max Reward: 48.49
Iteration: 2741 | Episodes: 222400 | Median Reward: 39.21 | Max Reward: 48.49
Iteration: 2743 | Episodes: 222500 | Median Reward: 37.41 | Max Reward: 48.49
Iteration: 2744 | Episodes: 222600 | Median Reward: 35.06 | Max Reward: 48.49
Iteration: 2745 | Episodes: 222700 | Median Reward: 40.25 | Max Reward: 48.49
Iteration: 2746 | Episodes: 222800 | Median Reward: 40.80 | Max Reward: 48.49
Iteration: 2748 | Episodes: 222900 | Median Reward: 42.61 | Max Reward: 48.49
Iteration: 2749 | Episodes: 223000 | Median Reward: 41.87 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.6         |
| time/                   |               |
|    fps                  | 225           |
|    iterations           | 2750          |
|    time_elapsed         | 99962         |
|    total_timesteps      | 22528000      |
| train/                  |               |
|    approx_kl            | 2.2715096e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -135          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.51         |
|    n_updates            | 27490         |
|    policy_gradient_loss | 6.08e-05      |
|    std                  | 8.97          |
|    value_loss           | 1.22          |
-------------------------------------------
Iteration: 2750 | Episodes: 223100 | Median Reward: 38.02 | Max Reward: 48.49
Iteration: 2751 | Episodes: 223200 | Median Reward: 37.15 | Max Reward: 48.49
Iteration: 2753 | Episodes: 223300 | Median Reward: 35.99 | Max Reward: 48.49
Iteration: 2754 | Episodes: 223400 | Median Reward: 36.15 | Max Reward: 48.49
Iteration: 2755 | Episodes: 223500 | Median Reward: 43.44 | Max Reward: 48.49
Iteration: 2756 | Episodes: 223600 | Median Reward: 46.91 | Max Reward: 48.49
Iteration: 2758 | Episodes: 223700 | Median Reward: 38.76 | Max Reward: 48.49
Iteration: 2759 | Episodes: 223800 | Median Reward: 39.47 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 2760         |
|    time_elapsed         | 100050       |
|    total_timesteps      | 22609920     |
| train/                  |              |
|    approx_kl            | 0.0056899088 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -135         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.35        |
|    n_updates            | 27590        |
|    policy_gradient_loss | -0.00469     |
|    std                  | 9.08         |
|    value_loss           | 0.722        |
------------------------------------------
Iteration: 2760 | Episodes: 223900 | Median Reward: 42.60 | Max Reward: 48.49
Iteration: 2761 | Episodes: 224000 | Median Reward: 43.21 | Max Reward: 48.49
Iteration: 2762 | Episodes: 224100 | Median Reward: 41.53 | Max Reward: 48.49
Iteration: 2764 | Episodes: 224200 | Median Reward: 42.04 | Max Reward: 48.49
Iteration: 2765 | Episodes: 224300 | Median Reward: 39.35 | Max Reward: 48.49
Iteration: 2766 | Episodes: 224400 | Median Reward: 39.44 | Max Reward: 48.49
Iteration: 2767 | Episodes: 224500 | Median Reward: 37.98 | Max Reward: 48.49
Iteration: 2769 | Episodes: 224600 | Median Reward: 38.37 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.9         |
| time/                   |               |
|    fps                  | 226           |
|    iterations           | 2770          |
|    time_elapsed         | 100139        |
|    total_timesteps      | 22691840      |
| train/                  |               |
|    approx_kl            | 0.00038829792 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -135          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.46         |
|    n_updates            | 27690         |
|    policy_gradient_loss | -0.000456     |
|    std                  | 9.15          |
|    value_loss           | 0.394         |
-------------------------------------------
Iteration: 2770 | Episodes: 224700 | Median Reward: 37.67 | Max Reward: 48.49
Iteration: 2771 | Episodes: 224800 | Median Reward: 38.99 | Max Reward: 48.49
Iteration: 2772 | Episodes: 224900 | Median Reward: 39.44 | Max Reward: 48.49
Iteration: 2774 | Episodes: 225000 | Median Reward: 42.67 | Max Reward: 48.49
Iteration: 2775 | Episodes: 225100 | Median Reward: 42.05 | Max Reward: 48.49
Iteration: 2776 | Episodes: 225200 | Median Reward: 41.90 | Max Reward: 48.49
Iteration: 2777 | Episodes: 225300 | Median Reward: 40.19 | Max Reward: 48.49
Iteration: 2778 | Episodes: 225400 | Median Reward: 38.02 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61           |
| time/                   |               |
|    fps                  | 227           |
|    iterations           | 2780          |
|    time_elapsed         | 100227        |
|    total_timesteps      | 22773760      |
| train/                  |               |
|    approx_kl            | 0.00019601475 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -135          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.27         |
|    n_updates            | 27790         |
|    policy_gradient_loss | -0.000759     |
|    std                  | 9.19          |
|    value_loss           | 0.711         |
-------------------------------------------
Iteration: 2780 | Episodes: 225500 | Median Reward: 38.98 | Max Reward: 48.49
Iteration: 2781 | Episodes: 225600 | Median Reward: 40.33 | Max Reward: 48.49
Iteration: 2782 | Episodes: 225700 | Median Reward: 41.24 | Max Reward: 48.49
Iteration: 2783 | Episodes: 225800 | Median Reward: 40.97 | Max Reward: 48.49
Iteration: 2785 | Episodes: 225900 | Median Reward: 40.26 | Max Reward: 48.49
Iteration: 2786 | Episodes: 226000 | Median Reward: 32.68 | Max Reward: 48.49
Iteration: 2787 | Episodes: 226100 | Median Reward: 33.97 | Max Reward: 48.49
Iteration: 2788 | Episodes: 226200 | Median Reward: 39.07 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.7         |
| time/                   |               |
|    fps                  | 227           |
|    iterations           | 2790          |
|    time_elapsed         | 100316        |
|    total_timesteps      | 22855680      |
| train/                  |               |
|    approx_kl            | 1.2445664e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.95         |
|    n_updates            | 27890         |
|    policy_gradient_loss | -6.77e-05     |
|    std                  | 9.28          |
|    value_loss           | 1.07          |
-------------------------------------------
Iteration: 2790 | Episodes: 226300 | Median Reward: 42.77 | Max Reward: 48.49
Iteration: 2791 | Episodes: 226400 | Median Reward: 36.53 | Max Reward: 48.49
Iteration: 2792 | Episodes: 226500 | Median Reward: 34.73 | Max Reward: 48.49
Iteration: 2793 | Episodes: 226600 | Median Reward: 38.88 | Max Reward: 48.49
Iteration: 2795 | Episodes: 226700 | Median Reward: 35.58 | Max Reward: 48.49
Iteration: 2796 | Episodes: 226800 | Median Reward: 38.74 | Max Reward: 48.49
Iteration: 2797 | Episodes: 226900 | Median Reward: 40.26 | Max Reward: 48.49
Iteration: 2798 | Episodes: 227000 | Median Reward: 39.14 | Max Reward: 48.49
Iteration: 2799 | Episodes: 227100 | Median Reward: 43.31 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.5        |
| time/                   |              |
|    fps                  | 228          |
|    iterations           | 2800         |
|    time_elapsed         | 100404       |
|    total_timesteps      | 22937600     |
| train/                  |              |
|    approx_kl            | 0.0071366434 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -136         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.41        |
|    n_updates            | 27990        |
|    policy_gradient_loss | -0.023       |
|    std                  | 9.37         |
|    value_loss           | 0.662        |
------------------------------------------
Iteration: 2801 | Episodes: 227200 | Median Reward: 38.99 | Max Reward: 48.49
Iteration: 2802 | Episodes: 227300 | Median Reward: 38.80 | Max Reward: 48.49
Iteration: 2803 | Episodes: 227400 | Median Reward: 40.13 | Max Reward: 48.49
Iteration: 2804 | Episodes: 227500 | Median Reward: 40.13 | Max Reward: 48.49
Iteration: 2806 | Episodes: 227600 | Median Reward: 40.28 | Max Reward: 48.49
Iteration: 2807 | Episodes: 227700 | Median Reward: 41.08 | Max Reward: 48.49
Iteration: 2808 | Episodes: 227800 | Median Reward: 39.45 | Max Reward: 48.49
Iteration: 2809 | Episodes: 227900 | Median Reward: 43.37 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.1         |
| time/                   |               |
|    fps                  | 229           |
|    iterations           | 2810          |
|    time_elapsed         | 100493        |
|    total_timesteps      | 23019520      |
| train/                  |               |
|    approx_kl            | 0.00012247375 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.68         |
|    n_updates            | 28090         |
|    policy_gradient_loss | -4.99e-05     |
|    std                  | 9.45          |
|    value_loss           | 0.625         |
-------------------------------------------
Iteration: 2811 | Episodes: 228000 | Median Reward: 39.20 | Max Reward: 48.49
Iteration: 2812 | Episodes: 228100 | Median Reward: 44.01 | Max Reward: 48.49
Iteration: 2813 | Episodes: 228200 | Median Reward: 42.23 | Max Reward: 48.49
Iteration: 2814 | Episodes: 228300 | Median Reward: 41.86 | Max Reward: 48.49
Iteration: 2815 | Episodes: 228400 | Median Reward: 41.92 | Max Reward: 48.49
Iteration: 2817 | Episodes: 228500 | Median Reward: 39.72 | Max Reward: 48.49
Iteration: 2818 | Episodes: 228600 | Median Reward: 35.56 | Max Reward: 48.49
Iteration: 2819 | Episodes: 228700 | Median Reward: 37.62 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.4         |
| time/                   |               |
|    fps                  | 229           |
|    iterations           | 2820          |
|    time_elapsed         | 100581        |
|    total_timesteps      | 23101440      |
| train/                  |               |
|    approx_kl            | 2.6874593e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -136          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.06         |
|    n_updates            | 28190         |
|    policy_gradient_loss | -0.000511     |
|    std                  | 9.53          |
|    value_loss           | 2.07          |
-------------------------------------------
Iteration: 2820 | Episodes: 228800 | Median Reward: 37.94 | Max Reward: 48.49
Iteration: 2822 | Episodes: 228900 | Median Reward: 40.76 | Max Reward: 48.49
Iteration: 2823 | Episodes: 229000 | Median Reward: 39.94 | Max Reward: 48.49
Iteration: 2824 | Episodes: 229100 | Median Reward: 41.26 | Max Reward: 48.49
Iteration: 2825 | Episodes: 229200 | Median Reward: 44.66 | Max Reward: 48.49
Iteration: 2827 | Episodes: 229300 | Median Reward: 41.90 | Max Reward: 48.49
Iteration: 2828 | Episodes: 229400 | Median Reward: 39.50 | Max Reward: 48.49
Iteration: 2829 | Episodes: 229500 | Median Reward: 38.23 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.8        |
| time/                   |              |
|    fps                  | 230          |
|    iterations           | 2830         |
|    time_elapsed         | 100669       |
|    total_timesteps      | 23183360     |
| train/                  |              |
|    approx_kl            | 0.0016333554 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -136         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.64        |
|    n_updates            | 28290        |
|    policy_gradient_loss | -0.00728     |
|    std                  | 9.56         |
|    value_loss           | 0.588        |
------------------------------------------
Iteration: 2830 | Episodes: 229600 | Median Reward: 37.09 | Max Reward: 48.49
Iteration: 2831 | Episodes: 229700 | Median Reward: 39.56 | Max Reward: 48.49
Iteration: 2833 | Episodes: 229800 | Median Reward: 31.01 | Max Reward: 48.49
Iteration: 2834 | Episodes: 229900 | Median Reward: 36.72 | Max Reward: 48.49
Iteration: 2835 | Episodes: 230000 | Median Reward: 40.85 | Max Reward: 48.49
Iteration: 2836 | Episodes: 230100 | Median Reward: 35.71 | Max Reward: 48.49
Iteration: 2838 | Episodes: 230200 | Median Reward: 39.06 | Max Reward: 48.49
Iteration: 2839 | Episodes: 230300 | Median Reward: 39.78 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.4        |
| time/                   |              |
|    fps                  | 230          |
|    iterations           | 2840         |
|    time_elapsed         | 100757       |
|    total_timesteps      | 23265280     |
| train/                  |              |
|    approx_kl            | 0.0006219421 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -137         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -6.6         |
|    n_updates            | 28390        |
|    policy_gradient_loss | 0.00528      |
|    std                  | 9.68         |
|    value_loss           | 0.696        |
------------------------------------------
Iteration: 2840 | Episodes: 230400 | Median Reward: 38.47 | Max Reward: 48.49
Iteration: 2841 | Episodes: 230500 | Median Reward: 37.48 | Max Reward: 48.49
Iteration: 2843 | Episodes: 230600 | Median Reward: 39.96 | Max Reward: 48.49
Iteration: 2844 | Episodes: 230700 | Median Reward: 33.10 | Max Reward: 48.49
Iteration: 2845 | Episodes: 230800 | Median Reward: 37.12 | Max Reward: 48.49
Iteration: 2846 | Episodes: 230900 | Median Reward: 42.76 | Max Reward: 48.49
Iteration: 2848 | Episodes: 231000 | Median Reward: 39.41 | Max Reward: 48.49
Iteration: 2849 | Episodes: 231100 | Median Reward: 45.77 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.5        |
| time/                   |              |
|    fps                  | 231          |
|    iterations           | 2850         |
|    time_elapsed         | 100845       |
|    total_timesteps      | 23347200     |
| train/                  |              |
|    approx_kl            | 0.0058995276 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -137         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -6.52        |
|    n_updates            | 28490        |
|    policy_gradient_loss | -0.0131      |
|    std                  | 9.74         |
|    value_loss           | 0.361        |
------------------------------------------
Iteration: 2850 | Episodes: 231200 | Median Reward: 45.93 | Max Reward: 48.49
Iteration: 2851 | Episodes: 231300 | Median Reward: 45.91 | Max Reward: 48.49
Iteration: 2852 | Episodes: 231400 | Median Reward: 43.10 | Max Reward: 48.49
Iteration: 2854 | Episodes: 231500 | Median Reward: 42.05 | Max Reward: 48.49
Iteration: 2855 | Episodes: 231600 | Median Reward: 41.73 | Max Reward: 48.49
Iteration: 2856 | Episodes: 231700 | Median Reward: 40.18 | Max Reward: 48.49
Iteration: 2857 | Episodes: 231800 | Median Reward: 40.22 | Max Reward: 48.49
Iteration: 2859 | Episodes: 231900 | Median Reward: 42.62 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.5        |
| time/                   |              |
|    fps                  | 232          |
|    iterations           | 2860         |
|    time_elapsed         | 100933       |
|    total_timesteps      | 23429120     |
| train/                  |              |
|    approx_kl            | 9.262181e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -137         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.54        |
|    n_updates            | 28590        |
|    policy_gradient_loss | -0.000388    |
|    std                  | 9.81         |
|    value_loss           | 1.75         |
------------------------------------------
Iteration: 2860 | Episodes: 232000 | Median Reward: 36.87 | Max Reward: 48.49
Iteration: 2861 | Episodes: 232100 | Median Reward: 39.98 | Max Reward: 48.49
Iteration: 2862 | Episodes: 232200 | Median Reward: 37.04 | Max Reward: 48.49
Iteration: 2864 | Episodes: 232300 | Median Reward: 36.48 | Max Reward: 48.49
Iteration: 2865 | Episodes: 232400 | Median Reward: 39.43 | Max Reward: 48.49
Iteration: 2866 | Episodes: 232500 | Median Reward: 39.42 | Max Reward: 48.49
Iteration: 2867 | Episodes: 232600 | Median Reward: 38.80 | Max Reward: 48.49
Iteration: 2868 | Episodes: 232700 | Median Reward: 38.13 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.3         |
| time/                   |               |
|    fps                  | 232           |
|    iterations           | 2870          |
|    time_elapsed         | 101022        |
|    total_timesteps      | 23511040      |
| train/                  |               |
|    approx_kl            | 0.00031879422 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.63         |
|    n_updates            | 28690         |
|    policy_gradient_loss | -0.000944     |
|    std                  | 9.86          |
|    value_loss           | 0.449         |
-------------------------------------------
Iteration: 2870 | Episodes: 232800 | Median Reward: 44.68 | Max Reward: 48.49
Iteration: 2871 | Episodes: 232900 | Median Reward: 42.19 | Max Reward: 48.49
Iteration: 2872 | Episodes: 233000 | Median Reward: 34.31 | Max Reward: 48.49
Iteration: 2873 | Episodes: 233100 | Median Reward: 35.25 | Max Reward: 48.49
Iteration: 2875 | Episodes: 233200 | Median Reward: 45.70 | Max Reward: 48.49
Iteration: 2876 | Episodes: 233300 | Median Reward: 42.19 | Max Reward: 48.49
Iteration: 2877 | Episodes: 233400 | Median Reward: 41.33 | Max Reward: 48.49
Iteration: 2878 | Episodes: 233500 | Median Reward: 44.94 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -56.3        |
| time/                   |              |
|    fps                  | 233          |
|    iterations           | 2880         |
|    time_elapsed         | 101110       |
|    total_timesteps      | 23592960     |
| train/                  |              |
|    approx_kl            | 0.0006181989 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -137         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -6.68        |
|    n_updates            | 28790        |
|    policy_gradient_loss | -0.00443     |
|    std                  | 9.9          |
|    value_loss           | 0.401        |
------------------------------------------
Iteration: 2880 | Episodes: 233600 | Median Reward: 43.42 | Max Reward: 48.49
Iteration: 2881 | Episodes: 233700 | Median Reward: 40.04 | Max Reward: 48.49
Iteration: 2882 | Episodes: 233800 | Median Reward: 45.82 | Max Reward: 48.49
Iteration: 2883 | Episodes: 233900 | Median Reward: 40.03 | Max Reward: 48.49
Iteration: 2885 | Episodes: 234000 | Median Reward: 39.89 | Max Reward: 48.49
Iteration: 2886 | Episodes: 234100 | Median Reward: 39.11 | Max Reward: 48.49
Iteration: 2887 | Episodes: 234200 | Median Reward: 42.19 | Max Reward: 48.49
Iteration: 2888 | Episodes: 234300 | Median Reward: 40.55 | Max Reward: 48.49
Iteration: 2889 | Episodes: 234400 | Median Reward: 34.72 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.6         |
| time/                   |               |
|    fps                  | 233           |
|    iterations           | 2890          |
|    time_elapsed         | 101198        |
|    total_timesteps      | 23674880      |
| train/                  |               |
|    approx_kl            | 1.5465048e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -6.49         |
|    n_updates            | 28890         |
|    policy_gradient_loss | -0.000216     |
|    std                  | 9.99          |
|    value_loss           | 1.45          |
-------------------------------------------
Iteration: 2891 | Episodes: 234500 | Median Reward: 38.78 | Max Reward: 48.49
Iteration: 2892 | Episodes: 234600 | Median Reward: 37.41 | Max Reward: 48.49
Iteration: 2893 | Episodes: 234700 | Median Reward: 41.51 | Max Reward: 48.49
Iteration: 2894 | Episodes: 234800 | Median Reward: 42.43 | Max Reward: 48.49
Iteration: 2896 | Episodes: 234900 | Median Reward: 39.79 | Max Reward: 48.49
Iteration: 2897 | Episodes: 235000 | Median Reward: 40.53 | Max Reward: 48.49
Iteration: 2898 | Episodes: 235100 | Median Reward: 37.98 | Max Reward: 48.49
Iteration: 2899 | Episodes: 235200 | Median Reward: 40.37 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.6         |
| time/                   |               |
|    fps                  | 234           |
|    iterations           | 2900          |
|    time_elapsed         | 101287        |
|    total_timesteps      | 23756800      |
| train/                  |               |
|    approx_kl            | 7.3466435e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.28         |
|    n_updates            | 28990         |
|    policy_gradient_loss | -0.000673     |
|    std                  | 10            |
|    value_loss           | 1.48          |
-------------------------------------------
Iteration: 2901 | Episodes: 235300 | Median Reward: 43.65 | Max Reward: 48.49
Iteration: 2902 | Episodes: 235400 | Median Reward: 35.05 | Max Reward: 48.49
Iteration: 2903 | Episodes: 235500 | Median Reward: 36.38 | Max Reward: 48.49
Iteration: 2904 | Episodes: 235600 | Median Reward: 41.42 | Max Reward: 48.49
Iteration: 2905 | Episodes: 235700 | Median Reward: 40.29 | Max Reward: 48.49
Iteration: 2907 | Episodes: 235800 | Median Reward: 41.10 | Max Reward: 48.49
Iteration: 2908 | Episodes: 235900 | Median Reward: 42.93 | Max Reward: 48.49
Iteration: 2909 | Episodes: 236000 | Median Reward: 42.86 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.9         |
| time/                   |               |
|    fps                  | 235           |
|    iterations           | 2910          |
|    time_elapsed         | 101377        |
|    total_timesteps      | 23838720      |
| train/                  |               |
|    approx_kl            | 4.1047424e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -137          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.48         |
|    n_updates            | 29090         |
|    policy_gradient_loss | -0.000149     |
|    std                  | 10.1          |
|    value_loss           | 0.67          |
-------------------------------------------
Iteration: 2910 | Episodes: 236100 | Median Reward: 41.44 | Max Reward: 48.49
Iteration: 2912 | Episodes: 236200 | Median Reward: 40.34 | Max Reward: 48.49
Iteration: 2913 | Episodes: 236300 | Median Reward: 40.18 | Max Reward: 48.49
Iteration: 2914 | Episodes: 236400 | Median Reward: 36.56 | Max Reward: 48.49
Iteration: 2915 | Episodes: 236500 | Median Reward: 37.46 | Max Reward: 48.49
Iteration: 2917 | Episodes: 236600 | Median Reward: 43.31 | Max Reward: 48.49
Iteration: 2918 | Episodes: 236700 | Median Reward: 44.21 | Max Reward: 48.49
Iteration: 2919 | Episodes: 236800 | Median Reward: 44.40 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.1         |
| time/                   |               |
|    fps                  | 235           |
|    iterations           | 2920          |
|    time_elapsed         | 101466        |
|    total_timesteps      | 23920640      |
| train/                  |               |
|    approx_kl            | 0.00048489473 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -138          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -6.74         |
|    n_updates            | 29190         |
|    policy_gradient_loss | -0.00176      |
|    std                  | 10.2          |
|    value_loss           | 0.627         |
-------------------------------------------
Iteration: 2920 | Episodes: 236900 | Median Reward: 42.22 | Max Reward: 48.49
Iteration: 2921 | Episodes: 237000 | Median Reward: 41.98 | Max Reward: 48.49
Iteration: 2923 | Episodes: 237100 | Median Reward: 41.71 | Max Reward: 48.49
Iteration: 2924 | Episodes: 237200 | Median Reward: 40.64 | Max Reward: 48.49
Iteration: 2925 | Episodes: 237300 | Median Reward: 45.52 | Max Reward: 48.49
Iteration: 2926 | Episodes: 237400 | Median Reward: 46.57 | Max Reward: 48.49
Iteration: 2928 | Episodes: 237500 | Median Reward: 40.14 | Max Reward: 48.49
Iteration: 2929 | Episodes: 237600 | Median Reward: 37.50 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.6         |
| time/                   |               |
|    fps                  | 236           |
|    iterations           | 2930          |
|    time_elapsed         | 101555        |
|    total_timesteps      | 24002560      |
| train/                  |               |
|    approx_kl            | 0.00022164226 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -138          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.31         |
|    n_updates            | 29290         |
|    policy_gradient_loss | -0.00125      |
|    std                  | 10.2          |
|    value_loss           | 1.53          |
-------------------------------------------
Iteration: 2930 | Episodes: 237700 | Median Reward: 41.27 | Max Reward: 48.49
Iteration: 2931 | Episodes: 237800 | Median Reward: 38.00 | Max Reward: 48.49
Iteration: 2933 | Episodes: 237900 | Median Reward: 39.49 | Max Reward: 48.49
Iteration: 2934 | Episodes: 238000 | Median Reward: 43.59 | Max Reward: 48.49
Iteration: 2935 | Episodes: 238100 | Median Reward: 41.58 | Max Reward: 48.49
Iteration: 2936 | Episodes: 238200 | Median Reward: 39.77 | Max Reward: 48.49
Iteration: 2938 | Episodes: 238300 | Median Reward: 37.26 | Max Reward: 48.49
Iteration: 2939 | Episodes: 238400 | Median Reward: 40.52 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60          |
| time/                   |              |
|    fps                  | 236          |
|    iterations           | 2940         |
|    time_elapsed         | 101644       |
|    total_timesteps      | 24084480     |
| train/                  |              |
|    approx_kl            | 0.0001963237 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -138         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -6.43        |
|    n_updates            | 29390        |
|    policy_gradient_loss | -0.00186     |
|    std                  | 10.3         |
|    value_loss           | 1.12         |
------------------------------------------
Iteration: 2940 | Episodes: 238500 | Median Reward: 40.17 | Max Reward: 48.49
Iteration: 2941 | Episodes: 238600 | Median Reward: 40.65 | Max Reward: 48.49
Iteration: 2942 | Episodes: 238700 | Median Reward: 40.92 | Max Reward: 48.49
Iteration: 2944 | Episodes: 238800 | Median Reward: 42.29 | Max Reward: 48.49
Iteration: 2945 | Episodes: 238900 | Median Reward: 40.14 | Max Reward: 48.49
Iteration: 2946 | Episodes: 239000 | Median Reward: 38.93 | Max Reward: 48.49
Iteration: 2947 | Episodes: 239100 | Median Reward: 35.50 | Max Reward: 48.49
Iteration: 2949 | Episodes: 239200 | Median Reward: 34.74 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.7         |
| time/                   |               |
|    fps                  | 237           |
|    iterations           | 2950          |
|    time_elapsed         | 101733        |
|    total_timesteps      | 24166400      |
| train/                  |               |
|    approx_kl            | 1.7102153e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -138          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.6          |
|    n_updates            | 29490         |
|    policy_gradient_loss | -0.000132     |
|    std                  | 10.4          |
|    value_loss           | 1.46          |
-------------------------------------------
Iteration: 2950 | Episodes: 239300 | Median Reward: 38.13 | Max Reward: 48.49
Iteration: 2951 | Episodes: 239400 | Median Reward: 39.41 | Max Reward: 48.49
Iteration: 2952 | Episodes: 239500 | Median Reward: 34.76 | Max Reward: 48.49
Iteration: 2954 | Episodes: 239600 | Median Reward: 35.81 | Max Reward: 48.49
Iteration: 2955 | Episodes: 239700 | Median Reward: 39.93 | Max Reward: 48.49
Iteration: 2956 | Episodes: 239800 | Median Reward: 33.34 | Max Reward: 48.49
Iteration: 2957 | Episodes: 239900 | Median Reward: 40.51 | Max Reward: 48.49
Iteration: 2958 | Episodes: 240000 | Median Reward: 45.61 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.7        |
| time/                   |              |
|    fps                  | 238          |
|    iterations           | 2960         |
|    time_elapsed         | 101822       |
|    total_timesteps      | 24248320     |
| train/                  |              |
|    approx_kl            | 0.0008093534 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -138         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.59        |
|    n_updates            | 29590        |
|    policy_gradient_loss | -0.000399    |
|    std                  | 10.5         |
|    value_loss           | 1.04         |
------------------------------------------
Iteration: 2960 | Episodes: 240100 | Median Reward: 34.49 | Max Reward: 48.49
Iteration: 2961 | Episodes: 240200 | Median Reward: 40.91 | Max Reward: 48.49
Iteration: 2962 | Episodes: 240300 | Median Reward: 40.94 | Max Reward: 48.49
Iteration: 2963 | Episodes: 240400 | Median Reward: 38.89 | Max Reward: 48.49
Iteration: 2965 | Episodes: 240500 | Median Reward: 38.89 | Max Reward: 48.49
Iteration: 2966 | Episodes: 240600 | Median Reward: 42.66 | Max Reward: 48.49
Iteration: 2967 | Episodes: 240700 | Median Reward: 40.86 | Max Reward: 48.49
Iteration: 2968 | Episodes: 240800 | Median Reward: 38.80 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 238          |
|    iterations           | 2970         |
|    time_elapsed         | 101911       |
|    total_timesteps      | 24330240     |
| train/                  |              |
|    approx_kl            | 1.974529e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -139         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.59        |
|    n_updates            | 29690        |
|    policy_gradient_loss | -9.91e-06    |
|    std                  | 10.7         |
|    value_loss           | 0.65         |
------------------------------------------
Iteration: 2970 | Episodes: 240900 | Median Reward: 42.07 | Max Reward: 48.49
Iteration: 2971 | Episodes: 241000 | Median Reward: 43.13 | Max Reward: 48.49
Iteration: 2972 | Episodes: 241100 | Median Reward: 39.98 | Max Reward: 48.49
Iteration: 2973 | Episodes: 241200 | Median Reward: 42.77 | Max Reward: 48.49
Iteration: 2975 | Episodes: 241300 | Median Reward: 40.07 | Max Reward: 48.49
Iteration: 2976 | Episodes: 241400 | Median Reward: 34.53 | Max Reward: 48.49
Iteration: 2977 | Episodes: 241500 | Median Reward: 36.68 | Max Reward: 48.49
Iteration: 2978 | Episodes: 241600 | Median Reward: 38.29 | Max Reward: 48.49
Iteration: 2979 | Episodes: 241700 | Median Reward: 39.55 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.3        |
| time/                   |              |
|    fps                  | 239          |
|    iterations           | 2980         |
|    time_elapsed         | 102000       |
|    total_timesteps      | 24412160     |
| train/                  |              |
|    approx_kl            | 0.0006741033 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -139         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.55        |
|    n_updates            | 29790        |
|    policy_gradient_loss | -0.00194     |
|    std                  | 10.7         |
|    value_loss           | 1.05         |
------------------------------------------
Iteration: 2981 | Episodes: 241800 | Median Reward: 33.32 | Max Reward: 48.49
Iteration: 2982 | Episodes: 241900 | Median Reward: 39.33 | Max Reward: 48.49
Iteration: 2983 | Episodes: 242000 | Median Reward: 39.93 | Max Reward: 48.49
Iteration: 2984 | Episodes: 242100 | Median Reward: 40.33 | Max Reward: 48.49
Iteration: 2986 | Episodes: 242200 | Median Reward: 37.55 | Max Reward: 48.49
Iteration: 2987 | Episodes: 242300 | Median Reward: 37.29 | Max Reward: 48.49
Iteration: 2988 | Episodes: 242400 | Median Reward: 43.59 | Max Reward: 48.49
Iteration: 2989 | Episodes: 242500 | Median Reward: 43.88 | Max Reward: 48.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -58.5       |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 2990        |
|    time_elapsed         | 102088      |
|    total_timesteps      | 24494080    |
| train/                  |             |
|    approx_kl            | 0.004214308 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -139        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.54       |
|    n_updates            | 29890       |
|    policy_gradient_loss | -0.00346    |
|    std                  | 10.8        |
|    value_loss           | 0.494       |
-----------------------------------------
Iteration: 2991 | Episodes: 242600 | Median Reward: 38.05 | Max Reward: 48.49
Iteration: 2992 | Episodes: 242700 | Median Reward: 38.25 | Max Reward: 48.49
Iteration: 2993 | Episodes: 242800 | Median Reward: 36.90 | Max Reward: 48.49
Iteration: 2994 | Episodes: 242900 | Median Reward: 40.91 | Max Reward: 48.49
Iteration: 2995 | Episodes: 243000 | Median Reward: 34.36 | Max Reward: 48.49
Iteration: 2997 | Episodes: 243100 | Median Reward: 43.40 | Max Reward: 48.49
Iteration: 2998 | Episodes: 243200 | Median Reward: 44.27 | Max Reward: 48.49
Iteration: 2999 | Episodes: 243300 | Median Reward: 38.08 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.5        |
| time/                   |              |
|    fps                  | 240          |
|    iterations           | 3000         |
|    time_elapsed         | 102176       |
|    total_timesteps      | 24576000     |
| train/                  |              |
|    approx_kl            | 0.0004008313 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -139         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.9         |
|    n_updates            | 29990        |
|    policy_gradient_loss | -0.00214     |
|    std                  | 10.9         |
|    value_loss           | 1.56         |
------------------------------------------
Iteration: 3000 | Episodes: 243400 | Median Reward: 40.59 | Max Reward: 48.49
Iteration: 3002 | Episodes: 243500 | Median Reward: 40.27 | Max Reward: 48.49
Iteration: 3003 | Episodes: 243600 | Median Reward: 38.67 | Max Reward: 48.49
Iteration: 3004 | Episodes: 243700 | Median Reward: 36.81 | Max Reward: 48.49
Iteration: 3005 | Episodes: 243800 | Median Reward: 44.50 | Max Reward: 48.49
Iteration: 3007 | Episodes: 243900 | Median Reward: 46.96 | Max Reward: 48.49
Iteration: 3008 | Episodes: 244000 | Median Reward: 43.13 | Max Reward: 48.49
Iteration: 3009 | Episodes: 244100 | Median Reward: 40.17 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.5        |
| time/                   |              |
|    fps                  | 241          |
|    iterations           | 3010         |
|    time_elapsed         | 102266       |
|    total_timesteps      | 24657920     |
| train/                  |              |
|    approx_kl            | 0.0008273935 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -139         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.65        |
|    n_updates            | 30090        |
|    policy_gradient_loss | -0.00228     |
|    std                  | 11           |
|    value_loss           | 0.814        |
------------------------------------------
Iteration: 3010 | Episodes: 244200 | Median Reward: 39.78 | Max Reward: 48.49
Iteration: 3011 | Episodes: 244300 | Median Reward: 39.05 | Max Reward: 48.49
Iteration: 3013 | Episodes: 244400 | Median Reward: 41.08 | Max Reward: 48.49
Iteration: 3014 | Episodes: 244500 | Median Reward: 40.82 | Max Reward: 48.49
Iteration: 3015 | Episodes: 244600 | Median Reward: 43.37 | Max Reward: 48.49
Iteration: 3016 | Episodes: 244700 | Median Reward: 44.83 | Max Reward: 48.49
Iteration: 3018 | Episodes: 244800 | Median Reward: 36.54 | Max Reward: 48.49
Iteration: 3019 | Episodes: 244900 | Median Reward: 39.22 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.8         |
| time/                   |               |
|    fps                  | 241           |
|    iterations           | 3020          |
|    time_elapsed         | 102354        |
|    total_timesteps      | 24739840      |
| train/                  |               |
|    approx_kl            | 0.00026547155 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -139          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.86         |
|    n_updates            | 30190         |
|    policy_gradient_loss | -0.00133      |
|    std                  | 11.1          |
|    value_loss           | 0.728         |
-------------------------------------------
Iteration: 3020 | Episodes: 245000 | Median Reward: 36.17 | Max Reward: 48.49
Iteration: 3021 | Episodes: 245100 | Median Reward: 41.63 | Max Reward: 48.49
Iteration: 3023 | Episodes: 245200 | Median Reward: 41.72 | Max Reward: 48.49
Iteration: 3024 | Episodes: 245300 | Median Reward: 42.74 | Max Reward: 48.49
Iteration: 3025 | Episodes: 245400 | Median Reward: 42.18 | Max Reward: 48.49
Iteration: 3026 | Episodes: 245500 | Median Reward: 38.55 | Max Reward: 48.49
Iteration: 3028 | Episodes: 245600 | Median Reward: 41.73 | Max Reward: 48.49
Iteration: 3029 | Episodes: 245700 | Median Reward: 43.13 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.1         |
| time/                   |               |
|    fps                  | 242           |
|    iterations           | 3030          |
|    time_elapsed         | 102443        |
|    total_timesteps      | 24821760      |
| train/                  |               |
|    approx_kl            | 0.00023962339 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -140          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.52         |
|    n_updates            | 30290         |
|    policy_gradient_loss | -0.000911     |
|    std                  | 11.2          |
|    value_loss           | 0.818         |
-------------------------------------------
Iteration: 3030 | Episodes: 245800 | Median Reward: 37.20 | Max Reward: 48.49
Iteration: 3031 | Episodes: 245900 | Median Reward: 39.05 | Max Reward: 48.49
Iteration: 3032 | Episodes: 246000 | Median Reward: 40.73 | Max Reward: 48.49
Iteration: 3034 | Episodes: 246100 | Median Reward: 38.53 | Max Reward: 48.49
Iteration: 3035 | Episodes: 246200 | Median Reward: 36.64 | Max Reward: 48.49
Iteration: 3036 | Episodes: 246300 | Median Reward: 36.33 | Max Reward: 48.49
Iteration: 3037 | Episodes: 246400 | Median Reward: 36.98 | Max Reward: 48.49
Iteration: 3039 | Episodes: 246500 | Median Reward: 40.85 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.8         |
| time/                   |               |
|    fps                  | 242           |
|    iterations           | 3040          |
|    time_elapsed         | 102532        |
|    total_timesteps      | 24903680      |
| train/                  |               |
|    approx_kl            | 5.3905926e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -140          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -6.46         |
|    n_updates            | 30390         |
|    policy_gradient_loss | 0.000216      |
|    std                  | 11.2          |
|    value_loss           | 1.23          |
-------------------------------------------
Iteration: 3040 | Episodes: 246600 | Median Reward: 40.81 | Max Reward: 48.49
Iteration: 3041 | Episodes: 246700 | Median Reward: 40.81 | Max Reward: 48.49
Iteration: 3042 | Episodes: 246800 | Median Reward: 35.74 | Max Reward: 48.49
Iteration: 3044 | Episodes: 246900 | Median Reward: 42.71 | Max Reward: 48.49
Iteration: 3045 | Episodes: 247000 | Median Reward: 38.43 | Max Reward: 48.49
Iteration: 3046 | Episodes: 247100 | Median Reward: 35.54 | Max Reward: 48.49
Iteration: 3047 | Episodes: 247200 | Median Reward: 44.62 | Max Reward: 48.49
Iteration: 3048 | Episodes: 247300 | Median Reward: 39.30 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66.2        |
| time/                   |              |
|    fps                  | 243          |
|    iterations           | 3050         |
|    time_elapsed         | 102621       |
|    total_timesteps      | 24985600     |
| train/                  |              |
|    approx_kl            | 0.0046430286 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -140         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -6.79        |
|    n_updates            | 30490        |
|    policy_gradient_loss | -0.0138      |
|    std                  | 11.3         |
|    value_loss           | 0.34         |
------------------------------------------
Iteration: 3050 | Episodes: 247400 | Median Reward: 39.45 | Max Reward: 48.49
Iteration: 3051 | Episodes: 247500 | Median Reward: 40.66 | Max Reward: 48.49
Iteration: 3052 | Episodes: 247600 | Median Reward: 35.95 | Max Reward: 48.49
Iteration: 3053 | Episodes: 247700 | Median Reward: 35.40 | Max Reward: 48.49
Iteration: 3055 | Episodes: 247800 | Median Reward: 37.09 | Max Reward: 48.49
Iteration: 3056 | Episodes: 247900 | Median Reward: 42.48 | Max Reward: 48.49
Iteration: 3057 | Episodes: 248000 | Median Reward: 42.91 | Max Reward: 48.49
Iteration: 3058 | Episodes: 248100 | Median Reward: 35.64 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.1        |
| time/                   |              |
|    fps                  | 244          |
|    iterations           | 3060         |
|    time_elapsed         | 102710       |
|    total_timesteps      | 25067520     |
| train/                  |              |
|    approx_kl            | 0.0021103434 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -140         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.52        |
|    n_updates            | 30590        |
|    policy_gradient_loss | -0.00257     |
|    std                  | 11.3         |
|    value_loss           | 1.11         |
------------------------------------------
Iteration: 3060 | Episodes: 248200 | Median Reward: 35.64 | Max Reward: 48.49
Iteration: 3061 | Episodes: 248300 | Median Reward: 33.02 | Max Reward: 48.49
Iteration: 3062 | Episodes: 248400 | Median Reward: 41.91 | Max Reward: 48.49
Iteration: 3063 | Episodes: 248500 | Median Reward: 35.87 | Max Reward: 48.49
Iteration: 3065 | Episodes: 248600 | Median Reward: 38.50 | Max Reward: 48.49
Iteration: 3066 | Episodes: 248700 | Median Reward: 44.02 | Max Reward: 48.49
Iteration: 3067 | Episodes: 248800 | Median Reward: 47.24 | Max Reward: 48.49
Iteration: 3068 | Episodes: 248900 | Median Reward: 33.61 | Max Reward: 48.49
Iteration: 3069 | Episodes: 249000 | Median Reward: 34.60 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.4        |
| time/                   |              |
|    fps                  | 244          |
|    iterations           | 3070         |
|    time_elapsed         | 102799       |
|    total_timesteps      | 25149440     |
| train/                  |              |
|    approx_kl            | 0.0009490301 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -140         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.83        |
|    n_updates            | 30690        |
|    policy_gradient_loss | -0.00574     |
|    std                  | 11.4         |
|    value_loss           | 1.04         |
------------------------------------------
Iteration: 3071 | Episodes: 249100 | Median Reward: 35.11 | Max Reward: 48.49
Iteration: 3072 | Episodes: 249200 | Median Reward: 35.82 | Max Reward: 48.49
Iteration: 3073 | Episodes: 249300 | Median Reward: 42.48 | Max Reward: 48.49
Iteration: 3074 | Episodes: 249400 | Median Reward: 42.05 | Max Reward: 48.49
Iteration: 3076 | Episodes: 249500 | Median Reward: 41.76 | Max Reward: 48.49
Iteration: 3077 | Episodes: 249600 | Median Reward: 36.93 | Max Reward: 48.49
Iteration: 3078 | Episodes: 249700 | Median Reward: 39.84 | Max Reward: 48.49
Iteration: 3079 | Episodes: 249800 | Median Reward: 41.63 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.6         |
| time/                   |               |
|    fps                  | 245           |
|    iterations           | 3080          |
|    time_elapsed         | 102888        |
|    total_timesteps      | 25231360      |
| train/                  |               |
|    approx_kl            | 1.1025964e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -140          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.82         |
|    n_updates            | 30790         |
|    policy_gradient_loss | 9.21e-05      |
|    std                  | 11.4          |
|    value_loss           | 1.01          |
-------------------------------------------
Iteration: 3081 | Episodes: 249900 | Median Reward: 32.16 | Max Reward: 48.49
Iteration: 3082 | Episodes: 250000 | Median Reward: 42.28 | Max Reward: 48.49
Iteration: 3083 | Episodes: 250100 | Median Reward: 38.97 | Max Reward: 48.49
Iteration: 3084 | Episodes: 250200 | Median Reward: 40.98 | Max Reward: 48.49
Iteration: 3085 | Episodes: 250300 | Median Reward: 41.93 | Max Reward: 48.49
Iteration: 3087 | Episodes: 250400 | Median Reward: 40.58 | Max Reward: 48.49
Iteration: 3088 | Episodes: 250500 | Median Reward: 42.09 | Max Reward: 48.49
Iteration: 3089 | Episodes: 250600 | Median Reward: 25.53 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -73.9        |
| time/                   |              |
|    fps                  | 245          |
|    iterations           | 3090         |
|    time_elapsed         | 102977       |
|    total_timesteps      | 25313280     |
| train/                  |              |
|    approx_kl            | 0.0019874284 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -140         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.84        |
|    n_updates            | 30890        |
|    policy_gradient_loss | -0.00213     |
|    std                  | 11.5         |
|    value_loss           | 0.614        |
------------------------------------------
Iteration: 3090 | Episodes: 250700 | Median Reward: 33.35 | Max Reward: 48.49
Iteration: 3092 | Episodes: 250800 | Median Reward: 39.20 | Max Reward: 48.49
Iteration: 3093 | Episodes: 250900 | Median Reward: 45.50 | Max Reward: 48.49
Iteration: 3094 | Episodes: 251000 | Median Reward: 41.04 | Max Reward: 48.49
Iteration: 3095 | Episodes: 251100 | Median Reward: 41.19 | Max Reward: 48.49
Iteration: 3097 | Episodes: 251200 | Median Reward: 41.22 | Max Reward: 48.49
Iteration: 3098 | Episodes: 251300 | Median Reward: 42.74 | Max Reward: 48.49
Iteration: 3099 | Episodes: 251400 | Median Reward: 43.22 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.7         |
| time/                   |               |
|    fps                  | 246           |
|    iterations           | 3100          |
|    time_elapsed         | 103065        |
|    total_timesteps      | 25395200      |
| train/                  |               |
|    approx_kl            | 0.00095185835 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -140          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.61         |
|    n_updates            | 30990         |
|    policy_gradient_loss | -0.00107      |
|    std                  | 11.5          |
|    value_loss           | 0.651         |
-------------------------------------------
Iteration: 3100 | Episodes: 251500 | Median Reward: 29.95 | Max Reward: 48.49
Iteration: 3102 | Episodes: 251600 | Median Reward: 38.50 | Max Reward: 48.49
Iteration: 3103 | Episodes: 251700 | Median Reward: 40.37 | Max Reward: 48.49
Iteration: 3104 | Episodes: 251800 | Median Reward: 38.06 | Max Reward: 48.49
Iteration: 3105 | Episodes: 251900 | Median Reward: 38.47 | Max Reward: 48.49
Iteration: 3106 | Episodes: 252000 | Median Reward: 39.47 | Max Reward: 48.49
Iteration: 3108 | Episodes: 252100 | Median Reward: 42.15 | Max Reward: 48.49
Iteration: 3109 | Episodes: 252200 | Median Reward: 37.17 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62           |
| time/                   |               |
|    fps                  | 246           |
|    iterations           | 3110          |
|    time_elapsed         | 103154        |
|    total_timesteps      | 25477120      |
| train/                  |               |
|    approx_kl            | 0.00086977484 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -140          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -6.92         |
|    n_updates            | 31090         |
|    policy_gradient_loss | -0.00562      |
|    std                  | 11.5          |
|    value_loss           | 0.33          |
-------------------------------------------
Iteration: 3110 | Episodes: 252300 | Median Reward: 37.72 | Max Reward: 48.49
Iteration: 3111 | Episodes: 252400 | Median Reward: 37.07 | Max Reward: 48.49
Iteration: 3113 | Episodes: 252500 | Median Reward: 33.41 | Max Reward: 48.49
Iteration: 3114 | Episodes: 252600 | Median Reward: 39.42 | Max Reward: 48.49
Iteration: 3115 | Episodes: 252700 | Median Reward: 41.75 | Max Reward: 48.49
Iteration: 3116 | Episodes: 252800 | Median Reward: 42.83 | Max Reward: 48.49
Iteration: 3118 | Episodes: 252900 | Median Reward: 42.83 | Max Reward: 48.49
Iteration: 3119 | Episodes: 253000 | Median Reward: 38.75 | Max Reward: 48.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.4       |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 3120        |
|    time_elapsed         | 103243      |
|    total_timesteps      | 25559040    |
| train/                  |             |
|    approx_kl            | 0.000840747 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -140        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.31       |
|    n_updates            | 31190       |
|    policy_gradient_loss | -0.00617    |
|    std                  | 11.6        |
|    value_loss           | 1.26        |
-----------------------------------------
Iteration: 3120 | Episodes: 253100 | Median Reward: 35.21 | Max Reward: 48.49
Iteration: 3121 | Episodes: 253200 | Median Reward: 36.91 | Max Reward: 48.49
Iteration: 3122 | Episodes: 253300 | Median Reward: 42.52 | Max Reward: 48.49
Iteration: 3124 | Episodes: 253400 | Median Reward: 38.88 | Max Reward: 48.49
Iteration: 3125 | Episodes: 253500 | Median Reward: 38.70 | Max Reward: 48.49
Iteration: 3126 | Episodes: 253600 | Median Reward: 37.43 | Max Reward: 48.49
Iteration: 3127 | Episodes: 253700 | Median Reward: 38.89 | Max Reward: 48.49
Iteration: 3129 | Episodes: 253800 | Median Reward: 42.17 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -58.4        |
| time/                   |              |
|    fps                  | 248          |
|    iterations           | 3130         |
|    time_elapsed         | 103332       |
|    total_timesteps      | 25640960     |
| train/                  |              |
|    approx_kl            | 0.0005849997 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -141         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.84        |
|    n_updates            | 31290        |
|    policy_gradient_loss | -0.00105     |
|    std                  | 11.7         |
|    value_loss           | 0.311        |
------------------------------------------
Iteration: 3130 | Episodes: 253900 | Median Reward: 41.48 | Max Reward: 48.49
Iteration: 3131 | Episodes: 254000 | Median Reward: 40.18 | Max Reward: 48.49
Iteration: 3132 | Episodes: 254100 | Median Reward: 35.06 | Max Reward: 48.49
Iteration: 3134 | Episodes: 254200 | Median Reward: 37.76 | Max Reward: 48.49
Iteration: 3135 | Episodes: 254300 | Median Reward: 42.95 | Max Reward: 48.49
Iteration: 3136 | Episodes: 254400 | Median Reward: 42.78 | Max Reward: 48.49
Iteration: 3137 | Episodes: 254500 | Median Reward: 35.87 | Max Reward: 48.49
Iteration: 3138 | Episodes: 254600 | Median Reward: 41.09 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.8         |
| time/                   |               |
|    fps                  | 248           |
|    iterations           | 3140          |
|    time_elapsed         | 103424        |
|    total_timesteps      | 25722880      |
| train/                  |               |
|    approx_kl            | 0.00035806265 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 0.996         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.7          |
|    n_updates            | 31390         |
|    policy_gradient_loss | -0.0012       |
|    std                  | 11.8          |
|    value_loss           | 1.63          |
-------------------------------------------
Iteration: 3140 | Episodes: 254700 | Median Reward: 37.49 | Max Reward: 48.49
Iteration: 3141 | Episodes: 254800 | Median Reward: 39.79 | Max Reward: 48.49
Iteration: 3142 | Episodes: 254900 | Median Reward: 41.71 | Max Reward: 48.49
Iteration: 3143 | Episodes: 255000 | Median Reward: 37.77 | Max Reward: 48.49
Iteration: 3145 | Episodes: 255100 | Median Reward: 37.79 | Max Reward: 48.49
Iteration: 3146 | Episodes: 255200 | Median Reward: 38.79 | Max Reward: 48.49
Iteration: 3147 | Episodes: 255300 | Median Reward: 31.83 | Max Reward: 48.49
Iteration: 3148 | Episodes: 255400 | Median Reward: 31.48 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.4        |
| time/                   |              |
|    fps                  | 249          |
|    iterations           | 3150         |
|    time_elapsed         | 103517       |
|    total_timesteps      | 25804800     |
| train/                  |              |
|    approx_kl            | 0.0007304732 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -141         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.5         |
|    n_updates            | 31490        |
|    policy_gradient_loss | -0.00256     |
|    std                  | 11.9         |
|    value_loss           | 0.882        |
------------------------------------------
Iteration: 3150 | Episodes: 255500 | Median Reward: 37.15 | Max Reward: 48.49
Iteration: 3151 | Episodes: 255600 | Median Reward: 34.06 | Max Reward: 48.49
Iteration: 3152 | Episodes: 255700 | Median Reward: 30.82 | Max Reward: 48.49
Iteration: 3153 | Episodes: 255800 | Median Reward: 41.62 | Max Reward: 48.49
Iteration: 3155 | Episodes: 255900 | Median Reward: 43.26 | Max Reward: 48.49
Iteration: 3156 | Episodes: 256000 | Median Reward: 42.77 | Max Reward: 48.49
Iteration: 3157 | Episodes: 256100 | Median Reward: 38.94 | Max Reward: 48.49
Iteration: 3158 | Episodes: 256200 | Median Reward: 39.89 | Max Reward: 48.49
Iteration: 3159 | Episodes: 256300 | Median Reward: 43.75 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -55.7        |
| time/                   |              |
|    fps                  | 249          |
|    iterations           | 3160         |
|    time_elapsed         | 103610       |
|    total_timesteps      | 25886720     |
| train/                  |              |
|    approx_kl            | 0.0016641603 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -141         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.89        |
|    n_updates            | 31590        |
|    policy_gradient_loss | -0.00714     |
|    std                  | 12           |
|    value_loss           | 0.677        |
------------------------------------------
Iteration: 3161 | Episodes: 256400 | Median Reward: 37.31 | Max Reward: 48.49
Iteration: 3162 | Episodes: 256500 | Median Reward: 36.25 | Max Reward: 48.49
Iteration: 3163 | Episodes: 256600 | Median Reward: 36.32 | Max Reward: 48.49
Iteration: 3164 | Episodes: 256700 | Median Reward: 34.45 | Max Reward: 48.49
Iteration: 3166 | Episodes: 256800 | Median Reward: 34.85 | Max Reward: 48.49
Iteration: 3167 | Episodes: 256900 | Median Reward: 38.13 | Max Reward: 48.49
Iteration: 3168 | Episodes: 257000 | Median Reward: 38.10 | Max Reward: 48.49
Iteration: 3169 | Episodes: 257100 | Median Reward: 41.22 | Max Reward: 48.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.2       |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 3170        |
|    time_elapsed         | 103704      |
|    total_timesteps      | 25968640    |
| train/                  |             |
|    approx_kl            | 0.000453002 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -141        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.88       |
|    n_updates            | 31690       |
|    policy_gradient_loss | -0.00207    |
|    std                  | 12          |
|    value_loss           | 1.02        |
-----------------------------------------
Iteration: 3171 | Episodes: 257200 | Median Reward: 40.03 | Max Reward: 48.49
Iteration: 3172 | Episodes: 257300 | Median Reward: 40.71 | Max Reward: 48.49
Iteration: 3173 | Episodes: 257400 | Median Reward: 41.15 | Max Reward: 48.49
Iteration: 3174 | Episodes: 257500 | Median Reward: 39.32 | Max Reward: 48.49
Iteration: 3175 | Episodes: 257600 | Median Reward: 39.36 | Max Reward: 48.49
Iteration: 3177 | Episodes: 257700 | Median Reward: 32.88 | Max Reward: 48.49
Iteration: 3178 | Episodes: 257800 | Median Reward: 35.16 | Max Reward: 48.49
Iteration: 3179 | Episodes: 257900 | Median Reward: 35.10 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.7         |
| time/                   |               |
|    fps                  | 250           |
|    iterations           | 3180          |
|    time_elapsed         | 103797        |
|    total_timesteps      | 26050560      |
| train/                  |               |
|    approx_kl            | 0.00041923445 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.67         |
|    n_updates            | 31790         |
|    policy_gradient_loss | -0.00157      |
|    std                  | 12.1          |
|    value_loss           | 0.789         |
-------------------------------------------
Iteration: 3180 | Episodes: 258000 | Median Reward: 35.77 | Max Reward: 48.49
Iteration: 3182 | Episodes: 258100 | Median Reward: 43.91 | Max Reward: 48.49
Iteration: 3183 | Episodes: 258200 | Median Reward: 44.49 | Max Reward: 48.49
Iteration: 3184 | Episodes: 258300 | Median Reward: 36.87 | Max Reward: 48.49
Iteration: 3185 | Episodes: 258400 | Median Reward: 42.94 | Max Reward: 48.49
Iteration: 3187 | Episodes: 258500 | Median Reward: 39.63 | Max Reward: 48.49
Iteration: 3188 | Episodes: 258600 | Median Reward: 38.96 | Max Reward: 48.49
Iteration: 3189 | Episodes: 258700 | Median Reward: 39.86 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.1         |
| time/                   |               |
|    fps                  | 251           |
|    iterations           | 3190          |
|    time_elapsed         | 103889        |
|    total_timesteps      | 26132480      |
| train/                  |               |
|    approx_kl            | 0.00010168075 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -141          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.68         |
|    n_updates            | 31890         |
|    policy_gradient_loss | -0.000471     |
|    std                  | 12.2          |
|    value_loss           | 0.773         |
-------------------------------------------
Iteration: 3190 | Episodes: 258800 | Median Reward: 28.92 | Max Reward: 48.49
Iteration: 3192 | Episodes: 258900 | Median Reward: 41.62 | Max Reward: 48.49
Iteration: 3193 | Episodes: 259000 | Median Reward: 40.33 | Max Reward: 48.49
Iteration: 3194 | Episodes: 259100 | Median Reward: 39.32 | Max Reward: 48.49
Iteration: 3195 | Episodes: 259200 | Median Reward: 34.54 | Max Reward: 48.49
Iteration: 3196 | Episodes: 259300 | Median Reward: 30.77 | Max Reward: 48.49
Iteration: 3198 | Episodes: 259400 | Median Reward: 32.44 | Max Reward: 48.49
Iteration: 3199 | Episodes: 259500 | Median Reward: 38.83 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.2        |
| time/                   |              |
|    fps                  | 252          |
|    iterations           | 3200         |
|    time_elapsed         | 103978       |
|    total_timesteps      | 26214400     |
| train/                  |              |
|    approx_kl            | 0.0013609319 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -142         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.81        |
|    n_updates            | 31990        |
|    policy_gradient_loss | -0.00795     |
|    std                  | 12.2         |
|    value_loss           | 0.876        |
------------------------------------------
Iteration: 3200 | Episodes: 259600 | Median Reward: 39.99 | Max Reward: 48.49
Iteration: 3201 | Episodes: 259700 | Median Reward: 41.01 | Max Reward: 48.49
Iteration: 3203 | Episodes: 259800 | Median Reward: 45.02 | Max Reward: 48.49
Iteration: 3204 | Episodes: 259900 | Median Reward: 43.39 | Max Reward: 48.49
Iteration: 3205 | Episodes: 260000 | Median Reward: 40.87 | Max Reward: 48.49
Iteration: 3206 | Episodes: 260100 | Median Reward: 37.30 | Max Reward: 48.49
Iteration: 3208 | Episodes: 260200 | Median Reward: 37.30 | Max Reward: 48.49
Iteration: 3209 | Episodes: 260300 | Median Reward: 42.99 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56.7         |
| time/                   |               |
|    fps                  | 252           |
|    iterations           | 3210          |
|    time_elapsed         | 104066        |
|    total_timesteps      | 26296320      |
| train/                  |               |
|    approx_kl            | 8.7236345e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -142          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.49         |
|    n_updates            | 32090         |
|    policy_gradient_loss | -2.42e-05     |
|    std                  | 12.4          |
|    value_loss           | 0.921         |
-------------------------------------------
Iteration: 3210 | Episodes: 260400 | Median Reward: 40.29 | Max Reward: 48.49
Iteration: 3211 | Episodes: 260500 | Median Reward: 39.43 | Max Reward: 48.49
Iteration: 3212 | Episodes: 260600 | Median Reward: 39.49 | Max Reward: 48.49
Iteration: 3214 | Episodes: 260700 | Median Reward: 38.22 | Max Reward: 48.49
Iteration: 3215 | Episodes: 260800 | Median Reward: 39.80 | Max Reward: 48.49
Iteration: 3216 | Episodes: 260900 | Median Reward: 39.21 | Max Reward: 48.49
Iteration: 3217 | Episodes: 261000 | Median Reward: 39.52 | Max Reward: 48.49
Iteration: 3219 | Episodes: 261100 | Median Reward: 43.21 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -55.3         |
| time/                   |               |
|    fps                  | 253           |
|    iterations           | 3220          |
|    time_elapsed         | 104155        |
|    total_timesteps      | 26378240      |
| train/                  |               |
|    approx_kl            | 0.00011048629 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -142          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.8          |
|    n_updates            | 32190         |
|    policy_gradient_loss | -0.000647     |
|    std                  | 12.5          |
|    value_loss           | 1             |
-------------------------------------------
Iteration: 3220 | Episodes: 261200 | Median Reward: 43.28 | Max Reward: 48.49
Iteration: 3221 | Episodes: 261300 | Median Reward: 40.89 | Max Reward: 48.49
Iteration: 3222 | Episodes: 261400 | Median Reward: 43.53 | Max Reward: 48.49
Iteration: 3224 | Episodes: 261500 | Median Reward: 42.95 | Max Reward: 48.49
Iteration: 3225 | Episodes: 261600 | Median Reward: 35.40 | Max Reward: 48.49
Iteration: 3226 | Episodes: 261700 | Median Reward: 39.26 | Max Reward: 48.49
Iteration: 3227 | Episodes: 261800 | Median Reward: 40.09 | Max Reward: 48.49
Iteration: 3228 | Episodes: 261900 | Median Reward: 35.30 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.3        |
| time/                   |              |
|    fps                  | 253          |
|    iterations           | 3230         |
|    time_elapsed         | 104243       |
|    total_timesteps      | 26460160     |
| train/                  |              |
|    approx_kl            | 0.0032742093 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -142         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -6.53        |
|    n_updates            | 32290        |
|    policy_gradient_loss | -0.00752     |
|    std                  | 12.6         |
|    value_loss           | 0.421        |
------------------------------------------
Iteration: 3230 | Episodes: 262000 | Median Reward: 41.13 | Max Reward: 48.49
Iteration: 3231 | Episodes: 262100 | Median Reward: 38.46 | Max Reward: 48.49
Iteration: 3232 | Episodes: 262200 | Median Reward: 35.14 | Max Reward: 48.49
Iteration: 3233 | Episodes: 262300 | Median Reward: 34.99 | Max Reward: 48.49
Iteration: 3235 | Episodes: 262400 | Median Reward: 42.57 | Max Reward: 48.49
Iteration: 3236 | Episodes: 262500 | Median Reward: 38.84 | Max Reward: 48.49
Iteration: 3237 | Episodes: 262600 | Median Reward: 37.89 | Max Reward: 48.49
Iteration: 3238 | Episodes: 262700 | Median Reward: 36.12 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.9         |
| time/                   |               |
|    fps                  | 254           |
|    iterations           | 3240          |
|    time_elapsed         | 104331        |
|    total_timesteps      | 26542080      |
| train/                  |               |
|    approx_kl            | 5.3257318e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -142          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.77         |
|    n_updates            | 32390         |
|    policy_gradient_loss | -0.00038      |
|    std                  | 12.6          |
|    value_loss           | 1.96          |
-------------------------------------------
Iteration: 3240 | Episodes: 262800 | Median Reward: 38.38 | Max Reward: 48.49
Iteration: 3241 | Episodes: 262900 | Median Reward: 41.23 | Max Reward: 48.49
Iteration: 3242 | Episodes: 263000 | Median Reward: 33.89 | Max Reward: 48.49
Iteration: 3243 | Episodes: 263100 | Median Reward: 41.08 | Max Reward: 48.49
Iteration: 3245 | Episodes: 263200 | Median Reward: 42.80 | Max Reward: 48.49
Iteration: 3246 | Episodes: 263300 | Median Reward: 37.88 | Max Reward: 48.49
Iteration: 3247 | Episodes: 263400 | Median Reward: 39.54 | Max Reward: 48.49
Iteration: 3248 | Episodes: 263500 | Median Reward: 34.17 | Max Reward: 48.49
Iteration: 3249 | Episodes: 263600 | Median Reward: 40.73 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.7         |
| time/                   |               |
|    fps                  | 254           |
|    iterations           | 3250          |
|    time_elapsed         | 104419        |
|    total_timesteps      | 26624000      |
| train/                  |               |
|    approx_kl            | 0.00020772625 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -142          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.88         |
|    n_updates            | 32490         |
|    policy_gradient_loss | -3.51e-05     |
|    std                  | 12.8          |
|    value_loss           | 0.937         |
-------------------------------------------
Iteration: 3251 | Episodes: 263700 | Median Reward: 43.90 | Max Reward: 48.49
Iteration: 3252 | Episodes: 263800 | Median Reward: 40.85 | Max Reward: 48.49
Iteration: 3253 | Episodes: 263900 | Median Reward: 32.40 | Max Reward: 48.49
Iteration: 3254 | Episodes: 264000 | Median Reward: 28.84 | Max Reward: 48.49
Iteration: 3256 | Episodes: 264100 | Median Reward: 37.33 | Max Reward: 48.49
Iteration: 3257 | Episodes: 264200 | Median Reward: 38.68 | Max Reward: 48.49
Iteration: 3258 | Episodes: 264300 | Median Reward: 41.04 | Max Reward: 48.49
Iteration: 3259 | Episodes: 264400 | Median Reward: 38.57 | Max Reward: 48.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.6       |
| time/                   |             |
|    fps                  | 255         |
|    iterations           | 3260        |
|    time_elapsed         | 104508      |
|    total_timesteps      | 26705920    |
| train/                  |             |
|    approx_kl            | 2.77746e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -143        |
|    explained_variance   | 1           |
|    learning_rate        | 0.000707    |
|    loss                 | -7.06       |
|    n_updates            | 32590       |
|    policy_gradient_loss | -9.72e-05   |
|    std                  | 12.9        |
|    value_loss           | 0.251       |
-----------------------------------------
Iteration: 3261 | Episodes: 264500 | Median Reward: 39.76 | Max Reward: 48.49
Iteration: 3262 | Episodes: 264600 | Median Reward: 42.51 | Max Reward: 48.49
Iteration: 3263 | Episodes: 264700 | Median Reward: 36.84 | Max Reward: 48.49
Iteration: 3264 | Episodes: 264800 | Median Reward: 33.06 | Max Reward: 48.49
Iteration: 3265 | Episodes: 264900 | Median Reward: 32.44 | Max Reward: 48.49
Iteration: 3267 | Episodes: 265000 | Median Reward: 39.65 | Max Reward: 48.49
Iteration: 3268 | Episodes: 265100 | Median Reward: 40.24 | Max Reward: 48.49
Iteration: 3269 | Episodes: 265200 | Median Reward: 37.03 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.6         |
| time/                   |               |
|    fps                  | 256           |
|    iterations           | 3270          |
|    time_elapsed         | 104596        |
|    total_timesteps      | 26787840      |
| train/                  |               |
|    approx_kl            | 0.00030270306 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -143          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -6.78         |
|    n_updates            | 32690         |
|    policy_gradient_loss | -0.0042       |
|    std                  | 13            |
|    value_loss           | 1.02          |
-------------------------------------------
Iteration: 3270 | Episodes: 265300 | Median Reward: 37.47 | Max Reward: 48.49
Iteration: 3272 | Episodes: 265400 | Median Reward: 38.84 | Max Reward: 48.49
Iteration: 3273 | Episodes: 265500 | Median Reward: 34.71 | Max Reward: 48.49
Iteration: 3274 | Episodes: 265600 | Median Reward: 41.16 | Max Reward: 48.49
Iteration: 3275 | Episodes: 265700 | Median Reward: 31.22 | Max Reward: 48.49
Iteration: 3277 | Episodes: 265800 | Median Reward: 35.50 | Max Reward: 48.49
Iteration: 3278 | Episodes: 265900 | Median Reward: 40.83 | Max Reward: 48.49
Iteration: 3279 | Episodes: 266000 | Median Reward: 38.99 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65          |
| time/                   |              |
|    fps                  | 256          |
|    iterations           | 3280         |
|    time_elapsed         | 104684       |
|    total_timesteps      | 26869760     |
| train/                  |              |
|    approx_kl            | 0.0001492655 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -143         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.5         |
|    n_updates            | 32790        |
|    policy_gradient_loss | -0.00192     |
|    std                  | 13.1         |
|    value_loss           | 1.26         |
------------------------------------------
Iteration: 3280 | Episodes: 266100 | Median Reward: 41.82 | Max Reward: 48.49
Iteration: 3282 | Episodes: 266200 | Median Reward: 38.04 | Max Reward: 48.49
Iteration: 3283 | Episodes: 266300 | Median Reward: 42.80 | Max Reward: 48.49
Iteration: 3284 | Episodes: 266400 | Median Reward: 36.89 | Max Reward: 48.49
Iteration: 3285 | Episodes: 266500 | Median Reward: 31.24 | Max Reward: 48.49
Iteration: 3286 | Episodes: 266600 | Median Reward: 35.42 | Max Reward: 48.49
Iteration: 3288 | Episodes: 266700 | Median Reward: 36.54 | Max Reward: 48.49
Iteration: 3289 | Episodes: 266800 | Median Reward: 38.17 | Max Reward: 48.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -63.7       |
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 3290        |
|    time_elapsed         | 104772      |
|    total_timesteps      | 26951680    |
| train/                  |             |
|    approx_kl            | 0.001280853 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -143        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -7.08       |
|    n_updates            | 32890       |
|    policy_gradient_loss | -0.00848    |
|    std                  | 13.1        |
|    value_loss           | 0.49        |
-----------------------------------------
Iteration: 3290 | Episodes: 266900 | Median Reward: 39.68 | Max Reward: 48.49
Iteration: 3291 | Episodes: 267000 | Median Reward: 38.60 | Max Reward: 48.49
Iteration: 3293 | Episodes: 267100 | Median Reward: 37.45 | Max Reward: 48.49
Iteration: 3294 | Episodes: 267200 | Median Reward: 42.37 | Max Reward: 48.49
Iteration: 3295 | Episodes: 267300 | Median Reward: 43.47 | Max Reward: 48.49
Iteration: 3296 | Episodes: 267400 | Median Reward: 39.49 | Max Reward: 48.49
Iteration: 3298 | Episodes: 267500 | Median Reward: 28.51 | Max Reward: 48.49
Iteration: 3299 | Episodes: 267600 | Median Reward: 41.86 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.7         |
| time/                   |               |
|    fps                  | 257           |
|    iterations           | 3300          |
|    time_elapsed         | 104860        |
|    total_timesteps      | 27033600      |
| train/                  |               |
|    approx_kl            | 0.00029799255 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -143          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.9          |
|    n_updates            | 32990         |
|    policy_gradient_loss | -0.000703     |
|    std                  | 13.3          |
|    value_loss           | 0.808         |
-------------------------------------------
Iteration: 3300 | Episodes: 267700 | Median Reward: 35.54 | Max Reward: 48.49
Iteration: 3301 | Episodes: 267800 | Median Reward: 28.67 | Max Reward: 48.49
Iteration: 3302 | Episodes: 267900 | Median Reward: 43.04 | Max Reward: 48.49
Iteration: 3304 | Episodes: 268000 | Median Reward: 41.22 | Max Reward: 48.49
Iteration: 3305 | Episodes: 268100 | Median Reward: 44.25 | Max Reward: 48.49
Iteration: 3306 | Episodes: 268200 | Median Reward: 42.55 | Max Reward: 48.49
Iteration: 3307 | Episodes: 268300 | Median Reward: 38.79 | Max Reward: 48.49
Iteration: 3309 | Episodes: 268400 | Median Reward: 41.29 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -61.3         |
| time/                   |               |
|    fps                  | 258           |
|    iterations           | 3310          |
|    time_elapsed         | 104948        |
|    total_timesteps      | 27115520      |
| train/                  |               |
|    approx_kl            | 2.4495195e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -144          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.28         |
|    n_updates            | 33090         |
|    policy_gradient_loss | -0.000167     |
|    std                  | 13.4          |
|    value_loss           | 3.22          |
-------------------------------------------
Iteration: 3310 | Episodes: 268500 | Median Reward: 38.77 | Max Reward: 48.49
Iteration: 3311 | Episodes: 268600 | Median Reward: 38.69 | Max Reward: 48.49
Iteration: 3312 | Episodes: 268700 | Median Reward: 29.30 | Max Reward: 48.49
Iteration: 3314 | Episodes: 268800 | Median Reward: 30.11 | Max Reward: 48.49
Iteration: 3315 | Episodes: 268900 | Median Reward: 42.15 | Max Reward: 48.49
Iteration: 3316 | Episodes: 269000 | Median Reward: 41.26 | Max Reward: 48.49
Iteration: 3317 | Episodes: 269100 | Median Reward: 43.18 | Max Reward: 48.49
Iteration: 3318 | Episodes: 269200 | Median Reward: 39.17 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.1        |
| time/                   |              |
|    fps                  | 258          |
|    iterations           | 3320         |
|    time_elapsed         | 105036       |
|    total_timesteps      | 27197440     |
| train/                  |              |
|    approx_kl            | 0.0006571136 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.82        |
|    n_updates            | 33190        |
|    policy_gradient_loss | -0.00514     |
|    std                  | 13.4         |
|    value_loss           | 1.34         |
------------------------------------------
Iteration: 3320 | Episodes: 269300 | Median Reward: 39.15 | Max Reward: 48.49
Iteration: 3321 | Episodes: 269400 | Median Reward: 40.90 | Max Reward: 48.49
Iteration: 3322 | Episodes: 269500 | Median Reward: 37.98 | Max Reward: 48.49
Iteration: 3323 | Episodes: 269600 | Median Reward: 43.14 | Max Reward: 48.49
Iteration: 3325 | Episodes: 269700 | Median Reward: 41.90 | Max Reward: 48.49
Iteration: 3326 | Episodes: 269800 | Median Reward: 27.29 | Max Reward: 48.49
Iteration: 3327 | Episodes: 269900 | Median Reward: 34.66 | Max Reward: 48.49
Iteration: 3328 | Episodes: 270000 | Median Reward: 43.06 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.2         |
| time/                   |               |
|    fps                  | 259           |
|    iterations           | 3330          |
|    time_elapsed         | 105125        |
|    total_timesteps      | 27279360      |
| train/                  |               |
|    approx_kl            | 0.00034306495 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -144          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -6.88         |
|    n_updates            | 33290         |
|    policy_gradient_loss | -0.00078      |
|    std                  | 13.6          |
|    value_loss           | 0.399         |
-------------------------------------------
Iteration: 3330 | Episodes: 270100 | Median Reward: 42.49 | Max Reward: 48.49
Iteration: 3331 | Episodes: 270200 | Median Reward: 38.43 | Max Reward: 48.49
Iteration: 3332 | Episodes: 270300 | Median Reward: 34.63 | Max Reward: 48.49
Iteration: 3333 | Episodes: 270400 | Median Reward: 34.71 | Max Reward: 48.49
Iteration: 3335 | Episodes: 270500 | Median Reward: 39.34 | Max Reward: 48.49
Iteration: 3336 | Episodes: 270600 | Median Reward: 40.70 | Max Reward: 48.49
Iteration: 3337 | Episodes: 270700 | Median Reward: 39.11 | Max Reward: 48.49
Iteration: 3338 | Episodes: 270800 | Median Reward: 39.00 | Max Reward: 48.49
Iteration: 3339 | Episodes: 270900 | Median Reward: 37.04 | Max Reward: 48.49
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -66.4       |
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 3340        |
|    time_elapsed         | 105213      |
|    total_timesteps      | 27361280    |
| train/                  |             |
|    approx_kl            | 0.005960712 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -144        |
|    explained_variance   | 1           |
|    learning_rate        | 0.000707    |
|    loss                 | -7.02       |
|    n_updates            | 33390       |
|    policy_gradient_loss | -0.0104     |
|    std                  | 13.7        |
|    value_loss           | 0.283       |
-----------------------------------------
Iteration: 3341 | Episodes: 271000 | Median Reward: 36.57 | Max Reward: 48.49
Iteration: 3342 | Episodes: 271100 | Median Reward: 40.56 | Max Reward: 48.49
Iteration: 3343 | Episodes: 271200 | Median Reward: 39.68 | Max Reward: 48.49
Iteration: 3344 | Episodes: 271300 | Median Reward: 40.69 | Max Reward: 48.49
Iteration: 3346 | Episodes: 271400 | Median Reward: 41.57 | Max Reward: 48.49
Iteration: 3347 | Episodes: 271500 | Median Reward: 39.59 | Max Reward: 48.49
Iteration: 3348 | Episodes: 271600 | Median Reward: 36.31 | Max Reward: 48.49
Iteration: 3349 | Episodes: 271700 | Median Reward: 34.11 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.7         |
| time/                   |               |
|    fps                  | 260           |
|    iterations           | 3350          |
|    time_elapsed         | 105302        |
|    total_timesteps      | 27443200      |
| train/                  |               |
|    approx_kl            | 1.1537064e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -144          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.94         |
|    n_updates            | 33490         |
|    policy_gradient_loss | -3.03e-05     |
|    std                  | 13.8          |
|    value_loss           | 0.616         |
-------------------------------------------
Iteration: 3351 | Episodes: 271800 | Median Reward: 36.23 | Max Reward: 48.49
Iteration: 3352 | Episodes: 271900 | Median Reward: 42.85 | Max Reward: 48.49
Iteration: 3353 | Episodes: 272000 | Median Reward: 42.82 | Max Reward: 48.49
Iteration: 3354 | Episodes: 272100 | Median Reward: 35.83 | Max Reward: 48.49
Iteration: 3355 | Episodes: 272200 | Median Reward: 38.71 | Max Reward: 48.49
Iteration: 3357 | Episodes: 272300 | Median Reward: 35.04 | Max Reward: 48.49
Iteration: 3358 | Episodes: 272400 | Median Reward: 36.67 | Max Reward: 48.49
Iteration: 3359 | Episodes: 272500 | Median Reward: 40.39 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60          |
| time/                   |              |
|    fps                  | 261          |
|    iterations           | 3360         |
|    time_elapsed         | 105390       |
|    total_timesteps      | 27525120     |
| train/                  |              |
|    approx_kl            | 0.0046348763 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -144         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.82        |
|    n_updates            | 33590        |
|    policy_gradient_loss | -0.0154      |
|    std                  | 13.8         |
|    value_loss           | 0.567        |
------------------------------------------
Iteration: 3360 | Episodes: 272600 | Median Reward: 39.31 | Max Reward: 48.49
Iteration: 3362 | Episodes: 272700 | Median Reward: 30.81 | Max Reward: 48.49
Iteration: 3363 | Episodes: 272800 | Median Reward: 30.52 | Max Reward: 48.49
Iteration: 3364 | Episodes: 272900 | Median Reward: 40.14 | Max Reward: 48.49
Iteration: 3365 | Episodes: 273000 | Median Reward: 37.55 | Max Reward: 48.49
Iteration: 3367 | Episodes: 273100 | Median Reward: 37.33 | Max Reward: 48.49
Iteration: 3368 | Episodes: 273200 | Median Reward: 34.88 | Max Reward: 48.49
Iteration: 3369 | Episodes: 273300 | Median Reward: 39.56 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.3         |
| time/                   |               |
|    fps                  | 261           |
|    iterations           | 3370          |
|    time_elapsed         | 105479        |
|    total_timesteps      | 27607040      |
| train/                  |               |
|    approx_kl            | 2.2734363e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -145          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.75         |
|    n_updates            | 33690         |
|    policy_gradient_loss | -0.000127     |
|    std                  | 14            |
|    value_loss           | 0.568         |
-------------------------------------------
Iteration: 3370 | Episodes: 273400 | Median Reward: 39.20 | Max Reward: 48.49
Iteration: 3372 | Episodes: 273500 | Median Reward: 37.98 | Max Reward: 48.49
Iteration: 3373 | Episodes: 273600 | Median Reward: 37.51 | Max Reward: 48.49
Iteration: 3374 | Episodes: 273700 | Median Reward: 38.45 | Max Reward: 48.49
Iteration: 3375 | Episodes: 273800 | Median Reward: 30.02 | Max Reward: 48.49
Iteration: 3376 | Episodes: 273900 | Median Reward: 35.00 | Max Reward: 48.49
Iteration: 3378 | Episodes: 274000 | Median Reward: 40.21 | Max Reward: 48.49
Iteration: 3379 | Episodes: 274100 | Median Reward: 38.16 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.4         |
| time/                   |               |
|    fps                  | 262           |
|    iterations           | 3380          |
|    time_elapsed         | 105567        |
|    total_timesteps      | 27688960      |
| train/                  |               |
|    approx_kl            | 2.2114029e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -145          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -7            |
|    n_updates            | 33790         |
|    policy_gradient_loss | -0.000158     |
|    std                  | 14.2          |
|    value_loss           | 0.502         |
-------------------------------------------
Iteration: 3380 | Episodes: 274200 | Median Reward: 36.46 | Max Reward: 48.49
Iteration: 3381 | Episodes: 274300 | Median Reward: 35.47 | Max Reward: 48.49
Iteration: 3383 | Episodes: 274400 | Median Reward: 40.29 | Max Reward: 48.49
Iteration: 3384 | Episodes: 274500 | Median Reward: 38.91 | Max Reward: 48.49
Iteration: 3385 | Episodes: 274600 | Median Reward: 41.64 | Max Reward: 48.49
Iteration: 3386 | Episodes: 274700 | Median Reward: 32.83 | Max Reward: 48.49
Iteration: 3388 | Episodes: 274800 | Median Reward: 32.83 | Max Reward: 48.49
Iteration: 3389 | Episodes: 274900 | Median Reward: 40.50 | Max Reward: 48.49
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.4         |
| time/                   |               |
|    fps                  | 262           |
|    iterations           | 3390          |
|    time_elapsed         | 105657        |
|    total_timesteps      | 27770880      |
| train/                  |               |
|    approx_kl            | 0.00021205505 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -145          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.09         |
|    n_updates            | 33890         |
|    policy_gradient_loss | -0.0022       |
|    std                  | 14.3          |
|    value_loss           | 0.484         |
-------------------------------------------
Iteration: 3390 | Episodes: 275000 | Median Reward: 40.75 | Max Reward: 48.49
Iteration: 3391 | Episodes: 275100 | Median Reward: 37.76 | Max Reward: 48.49
Iteration: 3392 | Episodes: 275200 | Median Reward: 41.02 | Max Reward: 48.49
Iteration: 3394 | Episodes: 275300 | Median Reward: 38.37 | Max Reward: 48.49
Iteration: 3395 | Episodes: 275400 | Median Reward: 38.17 | Max Reward: 48.49
Iteration: 3396 | Episodes: 275500 | Median Reward: 42.67 | Max Reward: 48.49
Iteration: 3397 | Episodes: 275600 | Median Reward: 38.09 | Max Reward: 48.49
Iteration: 3399 | Episodes: 275700 | Median Reward: 34.13 | Max Reward: 48.49
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.2        |
| time/                   |              |
|    fps                  | 263          |
|    iterations           | 3400         |
|    time_elapsed         | 105745       |
|    total_timesteps      | 27852800     |
| train/                  |              |
|    approx_kl            | 0.0034263257 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -145         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.03        |
|    n_updates            | 33990        |
|    policy_gradient_loss | -0.0063      |
|    std                  | 14.4         |
|    value_loss           | 0.379        |
------------------------------------------
Iteration: 3400 | Episodes: 275800 | Median Reward: 31.35 | Max Reward: 48.49
Iteration: 3401 | Episodes: 275900 | Median Reward: 36.37 | Max Reward: 48.49
Iteration: 3402 | Episodes: 276000 | Median Reward: 40.71 | Max Reward: 48.49
Iteration: 3404 | Episodes: 276100 | Median Reward: 34.79 | Max Reward: 48.49
Iteration: 3405 | Episodes: 276200 | Median Reward: 38.79 | Max Reward: 48.86
Iteration: 3406 | Episodes: 276300 | Median Reward: 42.48 | Max Reward: 48.86
Iteration: 3407 | Episodes: 276400 | Median Reward: 30.33 | Max Reward: 48.86
Iteration: 3408 | Episodes: 276500 | Median Reward: 38.71 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -61.4        |
| time/                   |              |
|    fps                  | 263          |
|    iterations           | 3410         |
|    time_elapsed         | 105833       |
|    total_timesteps      | 27934720     |
| train/                  |              |
|    approx_kl            | 3.105372e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -145         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.29        |
|    n_updates            | 34090        |
|    policy_gradient_loss | -0.000299    |
|    std                  | 14.5         |
|    value_loss           | 1.21         |
------------------------------------------
Iteration: 3410 | Episodes: 276600 | Median Reward: 37.61 | Max Reward: 48.86
Iteration: 3411 | Episodes: 276700 | Median Reward: 37.61 | Max Reward: 48.86
Iteration: 3412 | Episodes: 276800 | Median Reward: 38.30 | Max Reward: 48.86
Iteration: 3413 | Episodes: 276900 | Median Reward: 37.36 | Max Reward: 48.86
Iteration: 3415 | Episodes: 277000 | Median Reward: 37.26 | Max Reward: 48.86
Iteration: 3416 | Episodes: 277100 | Median Reward: 42.40 | Max Reward: 48.86
Iteration: 3417 | Episodes: 277200 | Median Reward: 38.54 | Max Reward: 48.86
Iteration: 3418 | Episodes: 277300 | Median Reward: 39.85 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.8        |
| time/                   |              |
|    fps                  | 264          |
|    iterations           | 3420         |
|    time_elapsed         | 105921       |
|    total_timesteps      | 28016640     |
| train/                  |              |
|    approx_kl            | 0.0010821901 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -145         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -7.05        |
|    n_updates            | 34190        |
|    policy_gradient_loss | -0.00832     |
|    std                  | 14.6         |
|    value_loss           | 0.555        |
------------------------------------------
Iteration: 3420 | Episodes: 277400 | Median Reward: 38.70 | Max Reward: 48.86
Iteration: 3421 | Episodes: 277500 | Median Reward: 34.47 | Max Reward: 48.86
Iteration: 3422 | Episodes: 277600 | Median Reward: 34.47 | Max Reward: 48.86
Iteration: 3423 | Episodes: 277700 | Median Reward: 31.39 | Max Reward: 48.86
Iteration: 3425 | Episodes: 277800 | Median Reward: 36.97 | Max Reward: 48.86
Iteration: 3426 | Episodes: 277900 | Median Reward: 42.87 | Max Reward: 48.86
Iteration: 3427 | Episodes: 278000 | Median Reward: 32.48 | Max Reward: 48.86
Iteration: 3428 | Episodes: 278100 | Median Reward: 38.05 | Max Reward: 48.86
Iteration: 3429 | Episodes: 278200 | Median Reward: 38.23 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.5         |
| time/                   |               |
|    fps                  | 265           |
|    iterations           | 3430          |
|    time_elapsed         | 106008        |
|    total_timesteps      | 28098560      |
| train/                  |               |
|    approx_kl            | 0.00020846236 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -146          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.43         |
|    n_updates            | 34290         |
|    policy_gradient_loss | -0.000549     |
|    std                  | 14.8          |
|    value_loss           | 0.942         |
-------------------------------------------
Iteration: 3431 | Episodes: 278300 | Median Reward: 30.35 | Max Reward: 48.86
Iteration: 3432 | Episodes: 278400 | Median Reward: 39.30 | Max Reward: 48.86
Iteration: 3433 | Episodes: 278500 | Median Reward: 39.69 | Max Reward: 48.86
Iteration: 3434 | Episodes: 278600 | Median Reward: 39.81 | Max Reward: 48.86
Iteration: 3436 | Episodes: 278700 | Median Reward: 42.84 | Max Reward: 48.86
Iteration: 3437 | Episodes: 278800 | Median Reward: 39.84 | Max Reward: 48.86
Iteration: 3438 | Episodes: 278900 | Median Reward: 37.85 | Max Reward: 48.86
Iteration: 3439 | Episodes: 279000 | Median Reward: 38.23 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.3         |
| time/                   |               |
|    fps                  | 265           |
|    iterations           | 3440          |
|    time_elapsed         | 106097        |
|    total_timesteps      | 28180480      |
| train/                  |               |
|    approx_kl            | 3.1978183e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -146          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.75         |
|    n_updates            | 34390         |
|    policy_gradient_loss | 2.21e-05      |
|    std                  | 14.9          |
|    value_loss           | 0.999         |
-------------------------------------------
Iteration: 3441 | Episodes: 279100 | Median Reward: 39.34 | Max Reward: 48.86
Iteration: 3442 | Episodes: 279200 | Median Reward: 35.62 | Max Reward: 48.86
Iteration: 3443 | Episodes: 279300 | Median Reward: 34.00 | Max Reward: 48.86
Iteration: 3444 | Episodes: 279400 | Median Reward: 37.47 | Max Reward: 48.86
Iteration: 3445 | Episodes: 279500 | Median Reward: 30.18 | Max Reward: 48.86
Iteration: 3447 | Episodes: 279600 | Median Reward: 31.84 | Max Reward: 48.86
Iteration: 3448 | Episodes: 279700 | Median Reward: 29.19 | Max Reward: 48.86
Iteration: 3449 | Episodes: 279800 | Median Reward: 37.81 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.2        |
| time/                   |              |
|    fps                  | 266          |
|    iterations           | 3450         |
|    time_elapsed         | 106185       |
|    total_timesteps      | 28262400     |
| train/                  |              |
|    approx_kl            | 0.0014967833 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -146         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.81        |
|    n_updates            | 34490        |
|    policy_gradient_loss | -0.00411     |
|    std                  | 15.1         |
|    value_loss           | 1.16         |
------------------------------------------
Iteration: 3450 | Episodes: 279900 | Median Reward: 31.47 | Max Reward: 48.86
Iteration: 3452 | Episodes: 280000 | Median Reward: 34.53 | Max Reward: 48.86
Iteration: 3453 | Episodes: 280100 | Median Reward: 37.56 | Max Reward: 48.86
Iteration: 3454 | Episodes: 280200 | Median Reward: 40.51 | Max Reward: 48.86
Iteration: 3455 | Episodes: 280300 | Median Reward: 42.25 | Max Reward: 48.86
Iteration: 3457 | Episodes: 280400 | Median Reward: 44.60 | Max Reward: 48.86
Iteration: 3458 | Episodes: 280500 | Median Reward: 36.12 | Max Reward: 48.86
Iteration: 3459 | Episodes: 280600 | Median Reward: 32.76 | Max Reward: 48.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -74.6       |
| time/                   |             |
|    fps                  | 266         |
|    iterations           | 3460        |
|    time_elapsed         | 106273      |
|    total_timesteps      | 28344320    |
| train/                  |             |
|    approx_kl            | 0.003964926 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -146        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.64       |
|    n_updates            | 34590       |
|    policy_gradient_loss | -0.0119     |
|    std                  | 15.2        |
|    value_loss           | 0.784       |
-----------------------------------------
Iteration: 3460 | Episodes: 280700 | Median Reward: 31.39 | Max Reward: 48.86
Iteration: 3462 | Episodes: 280800 | Median Reward: 35.41 | Max Reward: 48.86
Iteration: 3463 | Episodes: 280900 | Median Reward: 39.36 | Max Reward: 48.86
Iteration: 3464 | Episodes: 281000 | Median Reward: 39.44 | Max Reward: 48.86
Iteration: 3465 | Episodes: 281100 | Median Reward: 42.90 | Max Reward: 48.86
Iteration: 3466 | Episodes: 281200 | Median Reward: 36.29 | Max Reward: 48.86
Iteration: 3468 | Episodes: 281300 | Median Reward: 34.24 | Max Reward: 48.86
Iteration: 3469 | Episodes: 281400 | Median Reward: 35.32 | Max Reward: 48.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -64.1       |
| time/                   |             |
|    fps                  | 267         |
|    iterations           | 3470        |
|    time_elapsed         | 106360      |
|    total_timesteps      | 28426240    |
| train/                  |             |
|    approx_kl            | 0.008270405 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -146        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.89       |
|    n_updates            | 34690       |
|    policy_gradient_loss | -0.00977    |
|    std                  | 15.2        |
|    value_loss           | 1.13        |
-----------------------------------------
Iteration: 3470 | Episodes: 281500 | Median Reward: 39.19 | Max Reward: 48.86
Iteration: 3471 | Episodes: 281600 | Median Reward: 37.13 | Max Reward: 48.86
Iteration: 3473 | Episodes: 281700 | Median Reward: 31.67 | Max Reward: 48.86
Iteration: 3474 | Episodes: 281800 | Median Reward: 37.82 | Max Reward: 48.86
Iteration: 3475 | Episodes: 281900 | Median Reward: 38.02 | Max Reward: 48.86
Iteration: 3476 | Episodes: 282000 | Median Reward: 36.20 | Max Reward: 48.86
Iteration: 3478 | Episodes: 282100 | Median Reward: 32.27 | Max Reward: 48.86
Iteration: 3479 | Episodes: 282200 | Median Reward: 39.57 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.5        |
| time/                   |              |
|    fps                  | 267          |
|    iterations           | 3480         |
|    time_elapsed         | 106449       |
|    total_timesteps      | 28508160     |
| train/                  |              |
|    approx_kl            | 3.726661e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -146         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -7.11        |
|    n_updates            | 34790        |
|    policy_gradient_loss | 4.73e-05     |
|    std                  | 15.3         |
|    value_loss           | 0.409        |
------------------------------------------
Iteration: 3480 | Episodes: 282300 | Median Reward: 39.52 | Max Reward: 48.86
Iteration: 3481 | Episodes: 282400 | Median Reward: 40.55 | Max Reward: 48.86
Iteration: 3482 | Episodes: 282500 | Median Reward: 33.38 | Max Reward: 48.86
Iteration: 3484 | Episodes: 282600 | Median Reward: 28.73 | Max Reward: 48.86
Iteration: 3485 | Episodes: 282700 | Median Reward: 38.41 | Max Reward: 48.86
Iteration: 3486 | Episodes: 282800 | Median Reward: 39.54 | Max Reward: 48.86
Iteration: 3487 | Episodes: 282900 | Median Reward: 34.71 | Max Reward: 48.86
Iteration: 3489 | Episodes: 283000 | Median Reward: 37.17 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60.3         |
| time/                   |               |
|    fps                  | 268           |
|    iterations           | 3490          |
|    time_elapsed         | 106537        |
|    total_timesteps      | 28590080      |
| train/                  |               |
|    approx_kl            | 0.00016693978 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -147          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.85         |
|    n_updates            | 34890         |
|    policy_gradient_loss | -0.000725     |
|    std                  | 15.5          |
|    value_loss           | 1.38          |
-------------------------------------------
Iteration: 3490 | Episodes: 283100 | Median Reward: 38.59 | Max Reward: 48.86
Iteration: 3491 | Episodes: 283200 | Median Reward: 38.13 | Max Reward: 48.86
Iteration: 3492 | Episodes: 283300 | Median Reward: 40.75 | Max Reward: 48.86
Iteration: 3494 | Episodes: 283400 | Median Reward: 38.16 | Max Reward: 48.86
Iteration: 3495 | Episodes: 283500 | Median Reward: 40.25 | Max Reward: 48.86
Iteration: 3496 | Episodes: 283600 | Median Reward: 41.11 | Max Reward: 48.86
Iteration: 3497 | Episodes: 283700 | Median Reward: 43.07 | Max Reward: 48.86
Iteration: 3498 | Episodes: 283800 | Median Reward: 38.55 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -71           |
| time/                   |               |
|    fps                  | 268           |
|    iterations           | 3500          |
|    time_elapsed         | 106625        |
|    total_timesteps      | 28672000      |
| train/                  |               |
|    approx_kl            | 0.00022096989 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -147          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.98         |
|    n_updates            | 34990         |
|    policy_gradient_loss | 0.000123      |
|    std                  | 15.7          |
|    value_loss           | 0.836         |
-------------------------------------------
Iteration: 3500 | Episodes: 283900 | Median Reward: 30.19 | Max Reward: 48.86
Iteration: 3501 | Episodes: 284000 | Median Reward: 36.97 | Max Reward: 48.86
Iteration: 3502 | Episodes: 284100 | Median Reward: 35.70 | Max Reward: 48.86
Iteration: 3503 | Episodes: 284200 | Median Reward: 33.45 | Max Reward: 48.86
Iteration: 3505 | Episodes: 284300 | Median Reward: 34.93 | Max Reward: 48.86
Iteration: 3506 | Episodes: 284400 | Median Reward: 35.74 | Max Reward: 48.86
Iteration: 3507 | Episodes: 284500 | Median Reward: 44.30 | Max Reward: 48.86
Iteration: 3508 | Episodes: 284600 | Median Reward: 39.83 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.7         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 3510          |
|    time_elapsed         | 106713        |
|    total_timesteps      | 28753920      |
| train/                  |               |
|    approx_kl            | 0.00085126294 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -147          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.97         |
|    n_updates            | 35090         |
|    policy_gradient_loss | -0.00292      |
|    std                  | 15.9          |
|    value_loss           | 0.724         |
-------------------------------------------
Iteration: 3510 | Episodes: 284700 | Median Reward: 37.70 | Max Reward: 48.86
Iteration: 3511 | Episodes: 284800 | Median Reward: 37.28 | Max Reward: 48.86
Iteration: 3512 | Episodes: 284900 | Median Reward: 34.56 | Max Reward: 48.86
Iteration: 3513 | Episodes: 285000 | Median Reward: 43.69 | Max Reward: 48.86
Iteration: 3515 | Episodes: 285100 | Median Reward: 40.01 | Max Reward: 48.86
Iteration: 3516 | Episodes: 285200 | Median Reward: 38.91 | Max Reward: 48.86
Iteration: 3517 | Episodes: 285300 | Median Reward: 41.42 | Max Reward: 48.86
Iteration: 3518 | Episodes: 285400 | Median Reward: 37.01 | Max Reward: 48.86
Iteration: 3519 | Episodes: 285500 | Median Reward: 37.35 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -62.4         |
| time/                   |               |
|    fps                  | 269           |
|    iterations           | 3520          |
|    time_elapsed         | 106800        |
|    total_timesteps      | 28835840      |
| train/                  |               |
|    approx_kl            | 1.5884936e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -147          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.02         |
|    n_updates            | 35190         |
|    policy_gradient_loss | -0.000372     |
|    std                  | 16            |
|    value_loss           | 1.26          |
-------------------------------------------
Iteration: 3521 | Episodes: 285600 | Median Reward: 40.50 | Max Reward: 48.86
Iteration: 3522 | Episodes: 285700 | Median Reward: 38.84 | Max Reward: 48.86
Iteration: 3523 | Episodes: 285800 | Median Reward: 34.73 | Max Reward: 48.86
Iteration: 3524 | Episodes: 285900 | Median Reward: 36.84 | Max Reward: 48.86
Iteration: 3526 | Episodes: 286000 | Median Reward: 35.34 | Max Reward: 48.86
Iteration: 3527 | Episodes: 286100 | Median Reward: 39.22 | Max Reward: 48.86
Iteration: 3528 | Episodes: 286200 | Median Reward: 42.54 | Max Reward: 48.86
Iteration: 3529 | Episodes: 286300 | Median Reward: 41.20 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -58.1         |
| time/                   |               |
|    fps                  | 270           |
|    iterations           | 3530          |
|    time_elapsed         | 106888        |
|    total_timesteps      | 28917760      |
| train/                  |               |
|    approx_kl            | 4.8533955e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -148          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -5.59         |
|    n_updates            | 35290         |
|    policy_gradient_loss | -0.000293     |
|    std                  | 16.1          |
|    value_loss           | 1.21          |
-------------------------------------------
Iteration: 3531 | Episodes: 286400 | Median Reward: 34.82 | Max Reward: 48.86
Iteration: 3532 | Episodes: 286500 | Median Reward: 27.29 | Max Reward: 48.86
Iteration: 3533 | Episodes: 286600 | Median Reward: 30.04 | Max Reward: 48.86
Iteration: 3534 | Episodes: 286700 | Median Reward: 39.15 | Max Reward: 48.86
Iteration: 3535 | Episodes: 286800 | Median Reward: 37.47 | Max Reward: 48.86
Iteration: 3537 | Episodes: 286900 | Median Reward: 36.34 | Max Reward: 48.86
Iteration: 3538 | Episodes: 287000 | Median Reward: 38.79 | Max Reward: 48.86
Iteration: 3539 | Episodes: 287100 | Median Reward: 37.28 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64          |
| time/                   |              |
|    fps                  | 271          |
|    iterations           | 3540         |
|    time_elapsed         | 106976       |
|    total_timesteps      | 28999680     |
| train/                  |              |
|    approx_kl            | 7.888602e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -148         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.14        |
|    n_updates            | 35390        |
|    policy_gradient_loss | -0.00054     |
|    std                  | 16.2         |
|    value_loss           | 1.04         |
------------------------------------------
Iteration: 3540 | Episodes: 287200 | Median Reward: 37.89 | Max Reward: 48.86
Iteration: 3542 | Episodes: 287300 | Median Reward: 38.67 | Max Reward: 48.86
Iteration: 3543 | Episodes: 287400 | Median Reward: 40.80 | Max Reward: 48.86
Iteration: 3544 | Episodes: 287500 | Median Reward: 39.04 | Max Reward: 48.86
Iteration: 3545 | Episodes: 287600 | Median Reward: 33.91 | Max Reward: 48.86
Iteration: 3547 | Episodes: 287700 | Median Reward: 33.77 | Max Reward: 48.86
Iteration: 3548 | Episodes: 287800 | Median Reward: 34.80 | Max Reward: 48.86
Iteration: 3549 | Episodes: 287900 | Median Reward: 34.59 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63           |
| time/                   |               |
|    fps                  | 271           |
|    iterations           | 3550          |
|    time_elapsed         | 107063        |
|    total_timesteps      | 29081600      |
| train/                  |               |
|    approx_kl            | 5.7149795e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -148          |
|    explained_variance   | 0.997         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.7          |
|    n_updates            | 35490         |
|    policy_gradient_loss | 4.41e-05      |
|    std                  | 16.3          |
|    value_loss           | 1.13          |
-------------------------------------------
Iteration: 3550 | Episodes: 288000 | Median Reward: 38.29 | Max Reward: 48.86
Iteration: 3552 | Episodes: 288100 | Median Reward: 39.35 | Max Reward: 48.86
Iteration: 3553 | Episodes: 288200 | Median Reward: 36.00 | Max Reward: 48.86
Iteration: 3554 | Episodes: 288300 | Median Reward: 29.95 | Max Reward: 48.86
Iteration: 3555 | Episodes: 288400 | Median Reward: 38.20 | Max Reward: 48.86
Iteration: 3556 | Episodes: 288500 | Median Reward: 33.33 | Max Reward: 48.86
Iteration: 3558 | Episodes: 288600 | Median Reward: 42.42 | Max Reward: 48.86
Iteration: 3559 | Episodes: 288700 | Median Reward: 40.69 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.5         |
| time/                   |               |
|    fps                  | 272           |
|    iterations           | 3560          |
|    time_elapsed         | 107151        |
|    total_timesteps      | 29163520      |
| train/                  |               |
|    approx_kl            | 1.5556812e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -148          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.12         |
|    n_updates            | 35590         |
|    policy_gradient_loss | -2.94e-06     |
|    std                  | 16.3          |
|    value_loss           | 1.14          |
-------------------------------------------
Iteration: 3560 | Episodes: 288800 | Median Reward: 38.60 | Max Reward: 48.86
Iteration: 3561 | Episodes: 288900 | Median Reward: 38.91 | Max Reward: 48.86
Iteration: 3563 | Episodes: 289000 | Median Reward: 41.39 | Max Reward: 48.86
Iteration: 3564 | Episodes: 289100 | Median Reward: 35.86 | Max Reward: 48.86
Iteration: 3565 | Episodes: 289200 | Median Reward: 36.21 | Max Reward: 48.86
Iteration: 3566 | Episodes: 289300 | Median Reward: 39.22 | Max Reward: 48.86
Iteration: 3568 | Episodes: 289400 | Median Reward: 34.65 | Max Reward: 48.86
Iteration: 3569 | Episodes: 289500 | Median Reward: 38.74 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.2        |
| time/                   |              |
|    fps                  | 272          |
|    iterations           | 3570         |
|    time_elapsed         | 107239       |
|    total_timesteps      | 29245440     |
| train/                  |              |
|    approx_kl            | 0.0011894297 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -148         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -7.26        |
|    n_updates            | 35690        |
|    policy_gradient_loss | -0.00186     |
|    std                  | 16.4         |
|    value_loss           | 0.347        |
------------------------------------------
Iteration: 3570 | Episodes: 289600 | Median Reward: 38.92 | Max Reward: 48.86
Iteration: 3571 | Episodes: 289700 | Median Reward: 39.31 | Max Reward: 48.86
Iteration: 3572 | Episodes: 289800 | Median Reward: 35.16 | Max Reward: 48.86
Iteration: 3574 | Episodes: 289900 | Median Reward: 38.10 | Max Reward: 48.86
Iteration: 3575 | Episodes: 290000 | Median Reward: 36.96 | Max Reward: 48.86
Iteration: 3576 | Episodes: 290100 | Median Reward: 36.51 | Max Reward: 48.86
Iteration: 3577 | Episodes: 290200 | Median Reward: 35.95 | Max Reward: 48.86
Iteration: 3579 | Episodes: 290300 | Median Reward: 35.37 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64.5        |
| time/                   |              |
|    fps                  | 273          |
|    iterations           | 3580         |
|    time_elapsed         | 107327       |
|    total_timesteps      | 29327360     |
| train/                  |              |
|    approx_kl            | 3.970512e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -148         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.69        |
|    n_updates            | 35790        |
|    policy_gradient_loss | 6.81e-05     |
|    std                  | 16.5         |
|    value_loss           | 0.823        |
------------------------------------------
Iteration: 3580 | Episodes: 290400 | Median Reward: 39.01 | Max Reward: 48.86
Iteration: 3581 | Episodes: 290500 | Median Reward: 40.64 | Max Reward: 48.86
Iteration: 3582 | Episodes: 290600 | Median Reward: 38.52 | Max Reward: 48.86
Iteration: 3584 | Episodes: 290700 | Median Reward: 38.94 | Max Reward: 48.86
Iteration: 3585 | Episodes: 290800 | Median Reward: 39.55 | Max Reward: 48.86
Iteration: 3586 | Episodes: 290900 | Median Reward: 32.64 | Max Reward: 48.86
Iteration: 3587 | Episodes: 291000 | Median Reward: 32.02 | Max Reward: 48.86
Iteration: 3589 | Episodes: 291100 | Median Reward: 37.95 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.3         |
| time/                   |               |
|    fps                  | 273           |
|    iterations           | 3590          |
|    time_elapsed         | 107415        |
|    total_timesteps      | 29409280      |
| train/                  |               |
|    approx_kl            | 0.00019408914 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -148          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -6.45         |
|    n_updates            | 35890         |
|    policy_gradient_loss | -0.000366     |
|    std                  | 16.6          |
|    value_loss           | 1.97          |
-------------------------------------------
Iteration: 3590 | Episodes: 291200 | Median Reward: 36.88 | Max Reward: 48.86
Iteration: 3591 | Episodes: 291300 | Median Reward: 37.14 | Max Reward: 48.86
Iteration: 3592 | Episodes: 291400 | Median Reward: 36.83 | Max Reward: 48.86
Iteration: 3593 | Episodes: 291500 | Median Reward: 36.08 | Max Reward: 48.86
Iteration: 3595 | Episodes: 291600 | Median Reward: 39.86 | Max Reward: 48.86
Iteration: 3596 | Episodes: 291700 | Median Reward: 34.17 | Max Reward: 48.86
Iteration: 3597 | Episodes: 291800 | Median Reward: 35.80 | Max Reward: 48.86
Iteration: 3598 | Episodes: 291900 | Median Reward: 42.29 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.1        |
| time/                   |              |
|    fps                  | 274          |
|    iterations           | 3600         |
|    time_elapsed         | 107503       |
|    total_timesteps      | 29491200     |
| train/                  |              |
|    approx_kl            | 0.0023681023 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -148         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.21        |
|    n_updates            | 35990        |
|    policy_gradient_loss | -0.00264     |
|    std                  | 16.8         |
|    value_loss           | 0.617        |
------------------------------------------
Iteration: 3600 | Episodes: 292000 | Median Reward: 41.68 | Max Reward: 48.86
Iteration: 3601 | Episodes: 292100 | Median Reward: 41.07 | Max Reward: 48.86
Iteration: 3602 | Episodes: 292200 | Median Reward: 26.09 | Max Reward: 48.86
Iteration: 3603 | Episodes: 292300 | Median Reward: 34.67 | Max Reward: 48.86
Iteration: 3605 | Episodes: 292400 | Median Reward: 34.46 | Max Reward: 48.86
Iteration: 3606 | Episodes: 292500 | Median Reward: 35.97 | Max Reward: 48.86
Iteration: 3607 | Episodes: 292600 | Median Reward: 34.82 | Max Reward: 48.86
Iteration: 3608 | Episodes: 292700 | Median Reward: 31.75 | Max Reward: 48.86
Iteration: 3609 | Episodes: 292800 | Median Reward: 31.73 | Max Reward: 48.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.3       |
| time/                   |             |
|    fps                  | 274         |
|    iterations           | 3610        |
|    time_elapsed         | 107590      |
|    total_timesteps      | 29573120    |
| train/                  |             |
|    approx_kl            | 0.002429997 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -149        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.64       |
|    n_updates            | 36090       |
|    policy_gradient_loss | -0.00221    |
|    std                  | 17          |
|    value_loss           | 0.863       |
-----------------------------------------
Iteration: 3611 | Episodes: 292900 | Median Reward: 38.96 | Max Reward: 48.86
Iteration: 3612 | Episodes: 293000 | Median Reward: 37.27 | Max Reward: 48.86
Iteration: 3613 | Episodes: 293100 | Median Reward: 36.65 | Max Reward: 48.86
Iteration: 3614 | Episodes: 293200 | Median Reward: 38.63 | Max Reward: 48.86
Iteration: 3616 | Episodes: 293300 | Median Reward: 38.33 | Max Reward: 48.86
Iteration: 3617 | Episodes: 293400 | Median Reward: 37.87 | Max Reward: 48.86
Iteration: 3618 | Episodes: 293500 | Median Reward: 37.42 | Max Reward: 48.86
Iteration: 3619 | Episodes: 293600 | Median Reward: 33.60 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.7        |
| time/                   |              |
|    fps                  | 275          |
|    iterations           | 3620         |
|    time_elapsed         | 107678       |
|    total_timesteps      | 29655040     |
| train/                  |              |
|    approx_kl            | 0.0012720805 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -149         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.36        |
|    n_updates            | 36190        |
|    policy_gradient_loss | -0.00486     |
|    std                  | 17.1         |
|    value_loss           | 0.416        |
------------------------------------------
Iteration: 3621 | Episodes: 293700 | Median Reward: 36.52 | Max Reward: 48.86
Iteration: 3622 | Episodes: 293800 | Median Reward: 42.71 | Max Reward: 48.86
Iteration: 3623 | Episodes: 293900 | Median Reward: 30.40 | Max Reward: 48.86
Iteration: 3624 | Episodes: 294000 | Median Reward: 31.09 | Max Reward: 48.86
Iteration: 3625 | Episodes: 294100 | Median Reward: 34.65 | Max Reward: 48.86
Iteration: 3627 | Episodes: 294200 | Median Reward: 40.93 | Max Reward: 48.86
Iteration: 3628 | Episodes: 294300 | Median Reward: 38.24 | Max Reward: 48.86
Iteration: 3629 | Episodes: 294400 | Median Reward: 41.28 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.9        |
| time/                   |              |
|    fps                  | 275          |
|    iterations           | 3630         |
|    time_elapsed         | 107766       |
|    total_timesteps      | 29736960     |
| train/                  |              |
|    approx_kl            | 0.0028992556 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -149         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.38        |
|    n_updates            | 36290        |
|    policy_gradient_loss | -0.00616     |
|    std                  | 17.1         |
|    value_loss           | 0.434        |
------------------------------------------
Iteration: 3630 | Episodes: 294500 | Median Reward: 43.78 | Max Reward: 48.86
Iteration: 3632 | Episodes: 294600 | Median Reward: 39.36 | Max Reward: 48.86
Iteration: 3633 | Episodes: 294700 | Median Reward: 39.48 | Max Reward: 48.86
Iteration: 3634 | Episodes: 294800 | Median Reward: 40.60 | Max Reward: 48.86
Iteration: 3635 | Episodes: 294900 | Median Reward: 33.61 | Max Reward: 48.86
Iteration: 3637 | Episodes: 295000 | Median Reward: 34.57 | Max Reward: 48.86
Iteration: 3638 | Episodes: 295100 | Median Reward: 26.96 | Max Reward: 48.86
Iteration: 3639 | Episodes: 295200 | Median Reward: 29.20 | Max Reward: 48.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -65.3       |
| time/                   |             |
|    fps                  | 276         |
|    iterations           | 3640        |
|    time_elapsed         | 107853      |
|    total_timesteps      | 29818880    |
| train/                  |             |
|    approx_kl            | 0.008675149 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -149        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -6.99       |
|    n_updates            | 36390       |
|    policy_gradient_loss | -0.0296     |
|    std                  | 17.3        |
|    value_loss           | 0.727       |
-----------------------------------------
Iteration: 3640 | Episodes: 295300 | Median Reward: 34.03 | Max Reward: 48.86
Iteration: 3642 | Episodes: 295400 | Median Reward: 37.64 | Max Reward: 48.86
Iteration: 3643 | Episodes: 295500 | Median Reward: 38.66 | Max Reward: 48.86
Iteration: 3644 | Episodes: 295600 | Median Reward: 40.62 | Max Reward: 48.86
Iteration: 3645 | Episodes: 295700 | Median Reward: 31.09 | Max Reward: 48.86
Iteration: 3646 | Episodes: 295800 | Median Reward: 30.98 | Max Reward: 48.86
Iteration: 3648 | Episodes: 295900 | Median Reward: 36.21 | Max Reward: 48.86
Iteration: 3649 | Episodes: 296000 | Median Reward: 38.82 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.7         |
| time/                   |               |
|    fps                  | 277           |
|    iterations           | 3650          |
|    time_elapsed         | 107941        |
|    total_timesteps      | 29900800      |
| train/                  |               |
|    approx_kl            | 7.5638236e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -149          |
|    explained_variance   | 1             |
|    learning_rate        | 0.000707      |
|    loss                 | -5.16         |
|    n_updates            | 36490         |
|    policy_gradient_loss | 4.47e-05      |
|    std                  | 17.5          |
|    value_loss           | 2.84          |
-------------------------------------------
Iteration: 3650 | Episodes: 296100 | Median Reward: 32.50 | Max Reward: 48.86
Iteration: 3651 | Episodes: 296200 | Median Reward: 35.63 | Max Reward: 48.86
Iteration: 3653 | Episodes: 296300 | Median Reward: 37.71 | Max Reward: 48.86
Iteration: 3654 | Episodes: 296400 | Median Reward: 34.03 | Max Reward: 48.86
Iteration: 3655 | Episodes: 296500 | Median Reward: 37.92 | Max Reward: 48.86
Iteration: 3656 | Episodes: 296600 | Median Reward: 38.29 | Max Reward: 48.86
Iteration: 3658 | Episodes: 296700 | Median Reward: 37.78 | Max Reward: 48.86
Iteration: 3659 | Episodes: 296800 | Median Reward: 37.09 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -59.9        |
| time/                   |              |
|    fps                  | 277          |
|    iterations           | 3660         |
|    time_elapsed         | 108029       |
|    total_timesteps      | 29982720     |
| train/                  |              |
|    approx_kl            | 0.0020177206 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -149         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.93        |
|    n_updates            | 36590        |
|    policy_gradient_loss | -0.0133      |
|    std                  | 17.6         |
|    value_loss           | 1.08         |
------------------------------------------
Iteration: 3660 | Episodes: 296900 | Median Reward: 41.48 | Max Reward: 48.86
Iteration: 3661 | Episodes: 297000 | Median Reward: 35.28 | Max Reward: 48.86
Iteration: 3662 | Episodes: 297100 | Median Reward: 36.55 | Max Reward: 48.86
Iteration: 3664 | Episodes: 297200 | Median Reward: 40.15 | Max Reward: 48.86
Iteration: 3665 | Episodes: 297300 | Median Reward: 42.23 | Max Reward: 48.86
Iteration: 3666 | Episodes: 297400 | Median Reward: 34.77 | Max Reward: 48.86
Iteration: 3667 | Episodes: 297500 | Median Reward: 33.42 | Max Reward: 48.86
Iteration: 3669 | Episodes: 297600 | Median Reward: 40.74 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -56           |
| time/                   |               |
|    fps                  | 278           |
|    iterations           | 3670          |
|    time_elapsed         | 108117        |
|    total_timesteps      | 30064640      |
| train/                  |               |
|    approx_kl            | 0.00032901228 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -150          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.99         |
|    n_updates            | 36690         |
|    policy_gradient_loss | -0.00365      |
|    std                  | 17.7          |
|    value_loss           | 0.691         |
-------------------------------------------
Iteration: 3670 | Episodes: 297700 | Median Reward: 44.22 | Max Reward: 48.86
Iteration: 3671 | Episodes: 297800 | Median Reward: 40.42 | Max Reward: 48.86
Iteration: 3672 | Episodes: 297900 | Median Reward: 43.34 | Max Reward: 48.86
Iteration: 3674 | Episodes: 298000 | Median Reward: 43.34 | Max Reward: 48.86
Iteration: 3675 | Episodes: 298100 | Median Reward: 34.26 | Max Reward: 48.86
Iteration: 3676 | Episodes: 298200 | Median Reward: 36.27 | Max Reward: 48.86
Iteration: 3677 | Episodes: 298300 | Median Reward: 36.40 | Max Reward: 48.86
Iteration: 3679 | Episodes: 298400 | Median Reward: 35.44 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -60           |
| time/                   |               |
|    fps                  | 278           |
|    iterations           | 3680          |
|    time_elapsed         | 108205        |
|    total_timesteps      | 30146560      |
| train/                  |               |
|    approx_kl            | 1.3084107e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -150          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.36         |
|    n_updates            | 36790         |
|    policy_gradient_loss | -4.25e-05     |
|    std                  | 18            |
|    value_loss           | 0.291         |
-------------------------------------------
Iteration: 3680 | Episodes: 298500 | Median Reward: 37.27 | Max Reward: 48.86
Iteration: 3681 | Episodes: 298600 | Median Reward: 37.50 | Max Reward: 48.86
Iteration: 3682 | Episodes: 298700 | Median Reward: 38.14 | Max Reward: 48.86
Iteration: 3683 | Episodes: 298800 | Median Reward: 40.71 | Max Reward: 48.86
Iteration: 3685 | Episodes: 298900 | Median Reward: 41.15 | Max Reward: 48.86
Iteration: 3686 | Episodes: 299000 | Median Reward: 41.07 | Max Reward: 48.86
Iteration: 3687 | Episodes: 299100 | Median Reward: 38.20 | Max Reward: 48.86
Iteration: 3688 | Episodes: 299200 | Median Reward: 34.78 | Max Reward: 48.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -59.3       |
| time/                   |             |
|    fps                  | 279         |
|    iterations           | 3690        |
|    time_elapsed         | 108293      |
|    total_timesteps      | 30228480    |
| train/                  |             |
|    approx_kl            | 0.012112164 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -150        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000707    |
|    loss                 | -7.19       |
|    n_updates            | 36890       |
|    policy_gradient_loss | -0.0133     |
|    std                  | 18.2        |
|    value_loss           | 0.93        |
-----------------------------------------
Iteration: 3690 | Episodes: 299300 | Median Reward: 42.51 | Max Reward: 48.86
Iteration: 3691 | Episodes: 299400 | Median Reward: 35.39 | Max Reward: 48.86
Iteration: 3692 | Episodes: 299500 | Median Reward: 35.22 | Max Reward: 48.86
Iteration: 3693 | Episodes: 299600 | Median Reward: 36.27 | Max Reward: 48.86
Iteration: 3695 | Episodes: 299700 | Median Reward: 37.03 | Max Reward: 48.86
Iteration: 3696 | Episodes: 299800 | Median Reward: 38.59 | Max Reward: 48.86
Iteration: 3697 | Episodes: 299900 | Median Reward: 26.52 | Max Reward: 48.86
Iteration: 3698 | Episodes: 300000 | Median Reward: 31.86 | Max Reward: 48.86
Iteration: 3699 | Episodes: 300100 | Median Reward: 33.68 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -66.6         |
| time/                   |               |
|    fps                  | 279           |
|    iterations           | 3700          |
|    time_elapsed         | 108381        |
|    total_timesteps      | 30310400      |
| train/                  |               |
|    approx_kl            | 7.2626804e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -150          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.16         |
|    n_updates            | 36990         |
|    policy_gradient_loss | -0.00073      |
|    std                  | 18.3          |
|    value_loss           | 0.734         |
-------------------------------------------
Iteration: 3701 | Episodes: 300200 | Median Reward: 35.35 | Max Reward: 48.86
Iteration: 3702 | Episodes: 300300 | Median Reward: 34.49 | Max Reward: 48.86
Iteration: 3703 | Episodes: 300400 | Median Reward: 42.51 | Max Reward: 48.86
Iteration: 3704 | Episodes: 300500 | Median Reward: 39.34 | Max Reward: 48.86
Iteration: 3706 | Episodes: 300600 | Median Reward: 33.55 | Max Reward: 48.86
Iteration: 3707 | Episodes: 300700 | Median Reward: 31.51 | Max Reward: 48.86
Iteration: 3708 | Episodes: 300800 | Median Reward: 29.17 | Max Reward: 48.86
Iteration: 3709 | Episodes: 300900 | Median Reward: 30.35 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -65.6         |
| time/                   |               |
|    fps                  | 280           |
|    iterations           | 3710          |
|    time_elapsed         | 108469        |
|    total_timesteps      | 30392320      |
| train/                  |               |
|    approx_kl            | 0.00013655679 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -150          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.28         |
|    n_updates            | 37090         |
|    policy_gradient_loss | -0.000497     |
|    std                  | 18.4          |
|    value_loss           | 0.442         |
-------------------------------------------
Iteration: 3711 | Episodes: 301000 | Median Reward: 40.53 | Max Reward: 48.86
Iteration: 3712 | Episodes: 301100 | Median Reward: 38.71 | Max Reward: 48.86
Iteration: 3713 | Episodes: 301200 | Median Reward: 32.07 | Max Reward: 48.86
Iteration: 3714 | Episodes: 301300 | Median Reward: 42.57 | Max Reward: 48.86
Iteration: 3715 | Episodes: 301400 | Median Reward: 42.57 | Max Reward: 48.86
Iteration: 3717 | Episodes: 301500 | Median Reward: 37.85 | Max Reward: 48.86
Iteration: 3718 | Episodes: 301600 | Median Reward: 33.15 | Max Reward: 48.86
Iteration: 3719 | Episodes: 301700 | Median Reward: 32.36 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.9        |
| time/                   |              |
|    fps                  | 280          |
|    iterations           | 3720         |
|    time_elapsed         | 108557       |
|    total_timesteps      | 30474240     |
| train/                  |              |
|    approx_kl            | 0.0007716819 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -150         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.35        |
|    n_updates            | 37190        |
|    policy_gradient_loss | -0.00141     |
|    std                  | 18.6         |
|    value_loss           | 0.489        |
------------------------------------------
Iteration: 3720 | Episodes: 301800 | Median Reward: 33.03 | Max Reward: 48.86
Iteration: 3722 | Episodes: 301900 | Median Reward: 41.09 | Max Reward: 48.86
Iteration: 3723 | Episodes: 302000 | Median Reward: 35.06 | Max Reward: 48.86
Iteration: 3724 | Episodes: 302100 | Median Reward: 39.70 | Max Reward: 48.86
Iteration: 3725 | Episodes: 302200 | Median Reward: 30.32 | Max Reward: 48.86
Iteration: 3727 | Episodes: 302300 | Median Reward: 39.02 | Max Reward: 48.86
Iteration: 3728 | Episodes: 302400 | Median Reward: 39.56 | Max Reward: 48.86
Iteration: 3729 | Episodes: 302500 | Median Reward: 37.73 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -62.8        |
| time/                   |              |
|    fps                  | 281          |
|    iterations           | 3730         |
|    time_elapsed         | 108645       |
|    total_timesteps      | 30556160     |
| train/                  |              |
|    approx_kl            | 6.651542e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -151         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.27        |
|    n_updates            | 37290        |
|    policy_gradient_loss | -4.08e-05    |
|    std                  | 18.7         |
|    value_loss           | 1.14         |
------------------------------------------
Iteration: 3730 | Episodes: 302600 | Median Reward: 39.00 | Max Reward: 48.86
Iteration: 3732 | Episodes: 302700 | Median Reward: 38.79 | Max Reward: 48.86
Iteration: 3733 | Episodes: 302800 | Median Reward: 35.50 | Max Reward: 48.86
Iteration: 3734 | Episodes: 302900 | Median Reward: 36.75 | Max Reward: 48.86
Iteration: 3735 | Episodes: 303000 | Median Reward: 39.52 | Max Reward: 48.86
Iteration: 3736 | Episodes: 303100 | Median Reward: 29.03 | Max Reward: 48.86
Iteration: 3738 | Episodes: 303200 | Median Reward: 42.69 | Max Reward: 48.86
Iteration: 3739 | Episodes: 303300 | Median Reward: 34.75 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.4        |
| time/                   |              |
|    fps                  | 281          |
|    iterations           | 3740         |
|    time_elapsed         | 108732       |
|    total_timesteps      | 30638080     |
| train/                  |              |
|    approx_kl            | 0.0010071677 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -151         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -6.85        |
|    n_updates            | 37390        |
|    policy_gradient_loss | -0.00513     |
|    std                  | 18.8         |
|    value_loss           | 1.3          |
------------------------------------------
Iteration: 3740 | Episodes: 303400 | Median Reward: 38.30 | Max Reward: 48.86
Iteration: 3741 | Episodes: 303500 | Median Reward: 37.83 | Max Reward: 48.86
Iteration: 3743 | Episodes: 303600 | Median Reward: 38.54 | Max Reward: 48.86
Iteration: 3744 | Episodes: 303700 | Median Reward: 34.51 | Max Reward: 48.86
Iteration: 3745 | Episodes: 303800 | Median Reward: 35.12 | Max Reward: 48.86
Iteration: 3746 | Episodes: 303900 | Median Reward: 31.57 | Max Reward: 48.86
Iteration: 3748 | Episodes: 304000 | Median Reward: 38.57 | Max Reward: 48.86
Iteration: 3749 | Episodes: 304100 | Median Reward: 38.63 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -57.5        |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 3750         |
|    time_elapsed         | 108820       |
|    total_timesteps      | 30720000     |
| train/                  |              |
|    approx_kl            | 0.0020069862 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -151         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.28        |
|    n_updates            | 37490        |
|    policy_gradient_loss | -0.00373     |
|    std                  | 19           |
|    value_loss           | 0.932        |
------------------------------------------
Iteration: 3750 | Episodes: 304200 | Median Reward: 44.95 | Max Reward: 48.86
Iteration: 3751 | Episodes: 304300 | Median Reward: 34.75 | Max Reward: 48.86
Iteration: 3752 | Episodes: 304400 | Median Reward: 38.07 | Max Reward: 48.86
Iteration: 3754 | Episodes: 304500 | Median Reward: 42.14 | Max Reward: 48.86
Iteration: 3755 | Episodes: 304600 | Median Reward: 39.15 | Max Reward: 48.86
Iteration: 3756 | Episodes: 304700 | Median Reward: 30.16 | Max Reward: 48.86
Iteration: 3757 | Episodes: 304800 | Median Reward: 35.22 | Max Reward: 48.86
Iteration: 3759 | Episodes: 304900 | Median Reward: 36.56 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.6        |
| time/                   |              |
|    fps                  | 282          |
|    iterations           | 3760         |
|    time_elapsed         | 108908       |
|    total_timesteps      | 30801920     |
| train/                  |              |
|    approx_kl            | 0.0015370077 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -151         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -7           |
|    n_updates            | 37590        |
|    policy_gradient_loss | -0.00989     |
|    std                  | 19.1         |
|    value_loss           | 1.1          |
------------------------------------------
Iteration: 3760 | Episodes: 305000 | Median Reward: 35.55 | Max Reward: 48.86
Iteration: 3761 | Episodes: 305100 | Median Reward: 34.61 | Max Reward: 48.86
Iteration: 3762 | Episodes: 305200 | Median Reward: 33.14 | Max Reward: 48.86
Iteration: 3764 | Episodes: 305300 | Median Reward: 39.98 | Max Reward: 48.86
Iteration: 3765 | Episodes: 305400 | Median Reward: 37.72 | Max Reward: 48.86
Iteration: 3766 | Episodes: 305500 | Median Reward: 35.00 | Max Reward: 48.86
Iteration: 3767 | Episodes: 305600 | Median Reward: 39.42 | Max Reward: 48.86
Iteration: 3769 | Episodes: 305700 | Median Reward: 33.61 | Max Reward: 48.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -71.1       |
| time/                   |             |
|    fps                  | 283         |
|    iterations           | 3770        |
|    time_elapsed         | 108995      |
|    total_timesteps      | 30883840    |
| train/                  |             |
|    approx_kl            | 0.002273604 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -151        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.000707    |
|    loss                 | -7.23       |
|    n_updates            | 37690       |
|    policy_gradient_loss | -0.00484    |
|    std                  | 19.2        |
|    value_loss           | 1.09        |
-----------------------------------------
Iteration: 3770 | Episodes: 305800 | Median Reward: 28.17 | Max Reward: 48.86
Iteration: 3771 | Episodes: 305900 | Median Reward: 31.37 | Max Reward: 48.86
Iteration: 3772 | Episodes: 306000 | Median Reward: 37.09 | Max Reward: 48.86
Iteration: 3773 | Episodes: 306100 | Median Reward: 38.05 | Max Reward: 48.86
Iteration: 3775 | Episodes: 306200 | Median Reward: 33.87 | Max Reward: 48.86
Iteration: 3776 | Episodes: 306300 | Median Reward: 36.72 | Max Reward: 48.86
Iteration: 3777 | Episodes: 306400 | Median Reward: 36.71 | Max Reward: 48.86
Iteration: 3778 | Episodes: 306500 | Median Reward: 36.02 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -60.4        |
| time/                   |              |
|    fps                  | 283          |
|    iterations           | 3780         |
|    time_elapsed         | 109083       |
|    total_timesteps      | 30965760     |
| train/                  |              |
|    approx_kl            | 0.0012568713 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -151         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.44        |
|    n_updates            | 37790        |
|    policy_gradient_loss | -0.00586     |
|    std                  | 19.3         |
|    value_loss           | 0.918        |
------------------------------------------
Iteration: 3780 | Episodes: 306600 | Median Reward: 39.14 | Max Reward: 48.86
Iteration: 3781 | Episodes: 306700 | Median Reward: 38.80 | Max Reward: 48.86
Iteration: 3782 | Episodes: 306800 | Median Reward: 37.58 | Max Reward: 48.86
Iteration: 3783 | Episodes: 306900 | Median Reward: 40.31 | Max Reward: 48.86
Iteration: 3785 | Episodes: 307000 | Median Reward: 40.57 | Max Reward: 48.86
Iteration: 3786 | Episodes: 307100 | Median Reward: 34.26 | Max Reward: 48.86
Iteration: 3787 | Episodes: 307200 | Median Reward: 34.29 | Max Reward: 48.86
Iteration: 3788 | Episodes: 307300 | Median Reward: 36.94 | Max Reward: 48.86
Iteration: 3789 | Episodes: 307400 | Median Reward: 29.30 | Max Reward: 48.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -70.4       |
| time/                   |             |
|    fps                  | 284         |
|    iterations           | 3790        |
|    time_elapsed         | 109171      |
|    total_timesteps      | 31047680    |
| train/                  |             |
|    approx_kl            | 0.013728621 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -152        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -7.06       |
|    n_updates            | 37890       |
|    policy_gradient_loss | -0.024      |
|    std                  | 19.4        |
|    value_loss           | 0.731       |
-----------------------------------------
Iteration: 3791 | Episodes: 307500 | Median Reward: 35.30 | Max Reward: 48.86
Iteration: 3792 | Episodes: 307600 | Median Reward: 32.00 | Max Reward: 48.86
Iteration: 3793 | Episodes: 307700 | Median Reward: 24.10 | Max Reward: 48.86
Iteration: 3794 | Episodes: 307800 | Median Reward: 39.24 | Max Reward: 48.86
Iteration: 3796 | Episodes: 307900 | Median Reward: 39.67 | Max Reward: 48.86
Iteration: 3797 | Episodes: 308000 | Median Reward: 38.83 | Max Reward: 48.86
Iteration: 3798 | Episodes: 308100 | Median Reward: 35.53 | Max Reward: 48.86
Iteration: 3799 | Episodes: 308200 | Median Reward: 39.35 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -57.6         |
| time/                   |               |
|    fps                  | 284           |
|    iterations           | 3800          |
|    time_elapsed         | 109258        |
|    total_timesteps      | 31129600      |
| train/                  |               |
|    approx_kl            | 3.9221217e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.34         |
|    n_updates            | 37990         |
|    policy_gradient_loss | -9.38e-05     |
|    std                  | 19.6          |
|    value_loss           | 0.634         |
-------------------------------------------
Iteration: 3801 | Episodes: 308300 | Median Reward: 34.96 | Max Reward: 48.86
Iteration: 3802 | Episodes: 308400 | Median Reward: 37.53 | Max Reward: 48.86
Iteration: 3803 | Episodes: 308500 | Median Reward: 38.78 | Max Reward: 48.86
Iteration: 3804 | Episodes: 308600 | Median Reward: 32.74 | Max Reward: 48.86
Iteration: 3805 | Episodes: 308700 | Median Reward: 38.64 | Max Reward: 48.86
Iteration: 3807 | Episodes: 308800 | Median Reward: 35.08 | Max Reward: 48.86
Iteration: 3808 | Episodes: 308900 | Median Reward: 40.41 | Max Reward: 48.86
Iteration: 3809 | Episodes: 309000 | Median Reward: 41.52 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.2         |
| time/                   |               |
|    fps                  | 285           |
|    iterations           | 3810          |
|    time_elapsed         | 109346        |
|    total_timesteps      | 31211520      |
| train/                  |               |
|    approx_kl            | 5.4938646e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -6.82         |
|    n_updates            | 38090         |
|    policy_gradient_loss | -7.14e-05     |
|    std                  | 19.7          |
|    value_loss           | 1.82          |
-------------------------------------------
Iteration: 3810 | Episodes: 309100 | Median Reward: 40.52 | Max Reward: 48.86
Iteration: 3812 | Episodes: 309200 | Median Reward: 41.78 | Max Reward: 48.86
Iteration: 3813 | Episodes: 309300 | Median Reward: 34.36 | Max Reward: 48.86
Iteration: 3814 | Episodes: 309400 | Median Reward: 38.84 | Max Reward: 48.86
Iteration: 3815 | Episodes: 309500 | Median Reward: 39.34 | Max Reward: 48.86
Iteration: 3817 | Episodes: 309600 | Median Reward: 39.34 | Max Reward: 48.86
Iteration: 3818 | Episodes: 309700 | Median Reward: 38.06 | Max Reward: 48.86
Iteration: 3819 | Episodes: 309800 | Median Reward: 35.84 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -66          |
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 3820         |
|    time_elapsed         | 109434       |
|    total_timesteps      | 31293440     |
| train/                  |              |
|    approx_kl            | 9.065218e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -152         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -7.56        |
|    n_updates            | 38190        |
|    policy_gradient_loss | -0.00035     |
|    std                  | 19.8         |
|    value_loss           | 0.294        |
------------------------------------------
Iteration: 3820 | Episodes: 309900 | Median Reward: 40.69 | Max Reward: 48.86
Iteration: 3822 | Episodes: 310000 | Median Reward: 35.82 | Max Reward: 48.86
Iteration: 3823 | Episodes: 310100 | Median Reward: 30.42 | Max Reward: 48.86
Iteration: 3824 | Episodes: 310200 | Median Reward: 35.37 | Max Reward: 48.86
Iteration: 3825 | Episodes: 310300 | Median Reward: 34.00 | Max Reward: 48.86
Iteration: 3826 | Episodes: 310400 | Median Reward: 35.10 | Max Reward: 48.86
Iteration: 3828 | Episodes: 310500 | Median Reward: 34.83 | Max Reward: 48.86
Iteration: 3829 | Episodes: 310600 | Median Reward: 30.88 | Max Reward: 48.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -69.4       |
| time/                   |             |
|    fps                  | 286         |
|    iterations           | 3830        |
|    time_elapsed         | 109522      |
|    total_timesteps      | 31375360    |
| train/                  |             |
|    approx_kl            | 0.014732272 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -152        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -7.51       |
|    n_updates            | 38290       |
|    policy_gradient_loss | -0.0379     |
|    std                  | 19.9        |
|    value_loss           | 0.826       |
-----------------------------------------
Iteration: 3830 | Episodes: 310700 | Median Reward: 37.75 | Max Reward: 48.86
Iteration: 3831 | Episodes: 310800 | Median Reward: 41.80 | Max Reward: 48.86
Iteration: 3833 | Episodes: 310900 | Median Reward: 35.75 | Max Reward: 48.86
Iteration: 3834 | Episodes: 311000 | Median Reward: 36.80 | Max Reward: 48.86
Iteration: 3835 | Episodes: 311100 | Median Reward: 38.32 | Max Reward: 48.86
Iteration: 3836 | Episodes: 311200 | Median Reward: 42.81 | Max Reward: 48.86
Iteration: 3838 | Episodes: 311300 | Median Reward: 38.97 | Max Reward: 48.86
Iteration: 3839 | Episodes: 311400 | Median Reward: 33.16 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -68.5         |
| time/                   |               |
|    fps                  | 286           |
|    iterations           | 3840          |
|    time_elapsed         | 109610        |
|    total_timesteps      | 31457280      |
| train/                  |               |
|    approx_kl            | 0.00018775673 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -152          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.42         |
|    n_updates            | 38390         |
|    policy_gradient_loss | -0.000586     |
|    std                  | 20            |
|    value_loss           | 0.542         |
-------------------------------------------
Iteration: 3840 | Episodes: 311500 | Median Reward: 30.92 | Max Reward: 48.86
Iteration: 3841 | Episodes: 311600 | Median Reward: 43.76 | Max Reward: 48.86
Iteration: 3842 | Episodes: 311700 | Median Reward: 38.30 | Max Reward: 48.86
Iteration: 3844 | Episodes: 311800 | Median Reward: 37.69 | Max Reward: 48.86
Iteration: 3845 | Episodes: 311900 | Median Reward: 37.62 | Max Reward: 48.86
Iteration: 3846 | Episodes: 312000 | Median Reward: 36.94 | Max Reward: 48.86
Iteration: 3847 | Episodes: 312100 | Median Reward: 36.28 | Max Reward: 48.86
Iteration: 3849 | Episodes: 312200 | Median Reward: 38.32 | Max Reward: 48.86
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | -62        |
| time/                   |            |
|    fps                  | 287        |
|    iterations           | 3850       |
|    time_elapsed         | 109698     |
|    total_timesteps      | 31539200   |
| train/                  |            |
|    approx_kl            | 4.9625e-05 |
|    clip_fraction        | 0          |
|    clip_range           | 0.4        |
|    entropy_loss         | -153       |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.000707   |
|    loss                 | -6.64      |
|    n_updates            | 38490      |
|    policy_gradient_loss | -3.11e-06  |
|    std                  | 20.2       |
|    value_loss           | 2.03       |
----------------------------------------
Iteration: 3850 | Episodes: 312300 | Median Reward: 39.29 | Max Reward: 48.86
Iteration: 3851 | Episodes: 312400 | Median Reward: 40.70 | Max Reward: 48.86
Iteration: 3852 | Episodes: 312500 | Median Reward: 38.50 | Max Reward: 48.86
Iteration: 3854 | Episodes: 312600 | Median Reward: 34.14 | Max Reward: 48.86
Iteration: 3855 | Episodes: 312700 | Median Reward: 36.24 | Max Reward: 48.86
Iteration: 3856 | Episodes: 312800 | Median Reward: 40.38 | Max Reward: 48.86
Iteration: 3857 | Episodes: 312900 | Median Reward: 36.53 | Max Reward: 48.86
Iteration: 3859 | Episodes: 313000 | Median Reward: 29.72 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -68.5        |
| time/                   |              |
|    fps                  | 288          |
|    iterations           | 3860         |
|    time_elapsed         | 109786       |
|    total_timesteps      | 31621120     |
| train/                  |              |
|    approx_kl            | 0.0018578882 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -153         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.4         |
|    n_updates            | 38590        |
|    policy_gradient_loss | -0.0104      |
|    std                  | 20.3         |
|    value_loss           | 1.02         |
------------------------------------------
Iteration: 3860 | Episodes: 313100 | Median Reward: 33.18 | Max Reward: 48.86
Iteration: 3861 | Episodes: 313200 | Median Reward: 37.68 | Max Reward: 48.86
Iteration: 3862 | Episodes: 313300 | Median Reward: 39.51 | Max Reward: 48.86
Iteration: 3863 | Episodes: 313400 | Median Reward: 28.87 | Max Reward: 48.86
Iteration: 3865 | Episodes: 313500 | Median Reward: 39.03 | Max Reward: 48.86
Iteration: 3866 | Episodes: 313600 | Median Reward: 40.49 | Max Reward: 48.86
Iteration: 3867 | Episodes: 313700 | Median Reward: 32.93 | Max Reward: 48.86
Iteration: 3868 | Episodes: 313800 | Median Reward: 36.27 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.2         |
| time/                   |               |
|    fps                  | 288           |
|    iterations           | 3870          |
|    time_elapsed         | 109874        |
|    total_timesteps      | 31703040      |
| train/                  |               |
|    approx_kl            | 0.00025369285 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -153          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.41         |
|    n_updates            | 38690         |
|    policy_gradient_loss | -0.00129      |
|    std                  | 20.5          |
|    value_loss           | 0.683         |
-------------------------------------------
Iteration: 3870 | Episodes: 313900 | Median Reward: 37.23 | Max Reward: 48.86
Iteration: 3871 | Episodes: 314000 | Median Reward: 40.60 | Max Reward: 48.86
Iteration: 3872 | Episodes: 314100 | Median Reward: 40.12 | Max Reward: 48.86
Iteration: 3873 | Episodes: 314200 | Median Reward: 32.65 | Max Reward: 48.86
Iteration: 3875 | Episodes: 314300 | Median Reward: 33.68 | Max Reward: 48.86
Iteration: 3876 | Episodes: 314400 | Median Reward: 39.03 | Max Reward: 48.86
Iteration: 3877 | Episodes: 314500 | Median Reward: 39.24 | Max Reward: 48.86
Iteration: 3878 | Episodes: 314600 | Median Reward: 42.35 | Max Reward: 48.86
Iteration: 3879 | Episodes: 314700 | Median Reward: 37.11 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -64          |
| time/                   |              |
|    fps                  | 289          |
|    iterations           | 3880         |
|    time_elapsed         | 109961       |
|    total_timesteps      | 31784960     |
| train/                  |              |
|    approx_kl            | 0.0033255431 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -153         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.51        |
|    n_updates            | 38790        |
|    policy_gradient_loss | -0.0105      |
|    std                  | 20.6         |
|    value_loss           | 0.67         |
------------------------------------------
Iteration: 3881 | Episodes: 314800 | Median Reward: 34.92 | Max Reward: 48.86
Iteration: 3882 | Episodes: 314900 | Median Reward: 41.69 | Max Reward: 48.86
Iteration: 3883 | Episodes: 315000 | Median Reward: 43.71 | Max Reward: 48.86
Iteration: 3884 | Episodes: 315100 | Median Reward: 39.42 | Max Reward: 48.86
Iteration: 3886 | Episodes: 315200 | Median Reward: 36.63 | Max Reward: 48.86
Iteration: 3887 | Episodes: 315300 | Median Reward: 34.34 | Max Reward: 48.86
Iteration: 3888 | Episodes: 315400 | Median Reward: 34.51 | Max Reward: 48.86
Iteration: 3889 | Episodes: 315500 | Median Reward: 38.22 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.2         |
| time/                   |               |
|    fps                  | 289           |
|    iterations           | 3890          |
|    time_elapsed         | 110049        |
|    total_timesteps      | 31866880      |
| train/                  |               |
|    approx_kl            | 0.00015257463 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -153          |
|    explained_variance   | 0.993         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.1          |
|    n_updates            | 38890         |
|    policy_gradient_loss | -0.000708     |
|    std                  | 20.7          |
|    value_loss           | 2.99          |
-------------------------------------------
Iteration: 3891 | Episodes: 315600 | Median Reward: 36.09 | Max Reward: 48.86
Iteration: 3892 | Episodes: 315700 | Median Reward: 32.45 | Max Reward: 48.86
Iteration: 3893 | Episodes: 315800 | Median Reward: 28.73 | Max Reward: 48.86
Iteration: 3894 | Episodes: 315900 | Median Reward: 35.12 | Max Reward: 48.86
Iteration: 3895 | Episodes: 316000 | Median Reward: 33.32 | Max Reward: 48.86
Iteration: 3897 | Episodes: 316100 | Median Reward: 40.37 | Max Reward: 48.86
Iteration: 3898 | Episodes: 316200 | Median Reward: 40.31 | Max Reward: 48.86
Iteration: 3899 | Episodes: 316300 | Median Reward: 32.80 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -69          |
| time/                   |              |
|    fps                  | 290          |
|    iterations           | 3900         |
|    time_elapsed         | 110137       |
|    total_timesteps      | 31948800     |
| train/                  |              |
|    approx_kl            | 2.917675e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -153         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.55        |
|    n_updates            | 38990        |
|    policy_gradient_loss | -0.000269    |
|    std                  | 20.8         |
|    value_loss           | 0.553        |
------------------------------------------
Iteration: 3900 | Episodes: 316400 | Median Reward: 30.39 | Max Reward: 48.86
Iteration: 3902 | Episodes: 316500 | Median Reward: 33.60 | Max Reward: 48.86
Iteration: 3903 | Episodes: 316600 | Median Reward: 34.67 | Max Reward: 48.86
Iteration: 3904 | Episodes: 316700 | Median Reward: 34.85 | Max Reward: 48.86
Iteration: 3905 | Episodes: 316800 | Median Reward: 33.07 | Max Reward: 48.86
Iteration: 3907 | Episodes: 316900 | Median Reward: 33.44 | Max Reward: 48.86
Iteration: 3908 | Episodes: 317000 | Median Reward: 39.34 | Max Reward: 48.86
Iteration: 3909 | Episodes: 317100 | Median Reward: 34.60 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -67.1         |
| time/                   |               |
|    fps                  | 290           |
|    iterations           | 3910          |
|    time_elapsed         | 110225        |
|    total_timesteps      | 32030720      |
| train/                  |               |
|    approx_kl            | 0.00040841103 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -153          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.36         |
|    n_updates            | 39090         |
|    policy_gradient_loss | -0.00194      |
|    std                  | 20.9          |
|    value_loss           | 1.12          |
-------------------------------------------
Iteration: 3910 | Episodes: 317200 | Median Reward: 34.91 | Max Reward: 48.86
Iteration: 3912 | Episodes: 317300 | Median Reward: 38.89 | Max Reward: 48.86
Iteration: 3913 | Episodes: 317400 | Median Reward: 36.55 | Max Reward: 48.86
Iteration: 3914 | Episodes: 317500 | Median Reward: 35.54 | Max Reward: 48.86
Iteration: 3915 | Episodes: 317600 | Median Reward: 35.46 | Max Reward: 48.86
Iteration: 3916 | Episodes: 317700 | Median Reward: 35.04 | Max Reward: 48.86
Iteration: 3918 | Episodes: 317800 | Median Reward: 38.73 | Max Reward: 48.86
Iteration: 3919 | Episodes: 317900 | Median Reward: 38.12 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -72.5        |
| time/                   |              |
|    fps                  | 291          |
|    iterations           | 3920         |
|    time_elapsed         | 110313       |
|    total_timesteps      | 32112640     |
| train/                  |              |
|    approx_kl            | 0.0032053147 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -154         |
|    explained_variance   | 1            |
|    learning_rate        | 0.000707     |
|    loss                 | -7.55        |
|    n_updates            | 39190        |
|    policy_gradient_loss | -0.00387     |
|    std                  | 21.2         |
|    value_loss           | 0.25         |
------------------------------------------
Iteration: 3920 | Episodes: 318000 | Median Reward: 33.75 | Max Reward: 48.86
Iteration: 3921 | Episodes: 318100 | Median Reward: 31.92 | Max Reward: 48.86
Iteration: 3923 | Episodes: 318200 | Median Reward: 34.74 | Max Reward: 48.86
Iteration: 3924 | Episodes: 318300 | Median Reward: 38.88 | Max Reward: 48.86
Iteration: 3925 | Episodes: 318400 | Median Reward: 34.47 | Max Reward: 48.86
Iteration: 3926 | Episodes: 318500 | Median Reward: 36.93 | Max Reward: 48.86
Iteration: 3928 | Episodes: 318600 | Median Reward: 37.56 | Max Reward: 48.86
Iteration: 3929 | Episodes: 318700 | Median Reward: 38.34 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -64.5         |
| time/                   |               |
|    fps                  | 291           |
|    iterations           | 3930          |
|    time_elapsed         | 110400        |
|    total_timesteps      | 32194560      |
| train/                  |               |
|    approx_kl            | 1.3064244e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -154          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.46         |
|    n_updates            | 39290         |
|    policy_gradient_loss | 0.000133      |
|    std                  | 21.4          |
|    value_loss           | 0.919         |
-------------------------------------------
Iteration: 3930 | Episodes: 318800 | Median Reward: 32.09 | Max Reward: 48.86
Iteration: 3931 | Episodes: 318900 | Median Reward: 33.26 | Max Reward: 48.86
Iteration: 3932 | Episodes: 319000 | Median Reward: 32.11 | Max Reward: 48.86
Iteration: 3934 | Episodes: 319100 | Median Reward: 36.65 | Max Reward: 48.86
Iteration: 3935 | Episodes: 319200 | Median Reward: 34.94 | Max Reward: 48.86
Iteration: 3936 | Episodes: 319300 | Median Reward: 37.45 | Max Reward: 48.86
Iteration: 3937 | Episodes: 319400 | Median Reward: 37.45 | Max Reward: 48.86
Iteration: 3939 | Episodes: 319500 | Median Reward: 34.60 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -63.8        |
| time/                   |              |
|    fps                  | 292          |
|    iterations           | 3940         |
|    time_elapsed         | 110488       |
|    total_timesteps      | 32276480     |
| train/                  |              |
|    approx_kl            | 5.893383e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -154         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.45        |
|    n_updates            | 39390        |
|    policy_gradient_loss | -0.000526    |
|    std                  | 21.5         |
|    value_loss           | 1.34         |
------------------------------------------
Iteration: 3940 | Episodes: 319600 | Median Reward: 39.36 | Max Reward: 48.86
Iteration: 3941 | Episodes: 319700 | Median Reward: 33.79 | Max Reward: 48.86
Iteration: 3942 | Episodes: 319800 | Median Reward: 38.78 | Max Reward: 48.86
Iteration: 3944 | Episodes: 319900 | Median Reward: 38.78 | Max Reward: 48.86
Iteration: 3945 | Episodes: 320000 | Median Reward: 31.47 | Max Reward: 48.86
Iteration: 3946 | Episodes: 320100 | Median Reward: 31.70 | Max Reward: 48.86
Iteration: 3947 | Episodes: 320200 | Median Reward: 36.74 | Max Reward: 48.86
Iteration: 3949 | Episodes: 320300 | Median Reward: 36.07 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.4        |
| time/                   |              |
|    fps                  | 292          |
|    iterations           | 3950         |
|    time_elapsed         | 110577       |
|    total_timesteps      | 32358400     |
| train/                  |              |
|    approx_kl            | 8.397845e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -154         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.07        |
|    n_updates            | 39490        |
|    policy_gradient_loss | -0.000391    |
|    std                  | 21.6         |
|    value_loss           | 0.633        |
------------------------------------------
Iteration: 3950 | Episodes: 320400 | Median Reward: 33.57 | Max Reward: 48.86
Iteration: 3951 | Episodes: 320500 | Median Reward: 36.10 | Max Reward: 48.86
Iteration: 3952 | Episodes: 320600 | Median Reward: 33.99 | Max Reward: 48.86
Iteration: 3953 | Episodes: 320700 | Median Reward: 33.75 | Max Reward: 48.86
Iteration: 3955 | Episodes: 320800 | Median Reward: 43.08 | Max Reward: 48.86
Iteration: 3956 | Episodes: 320900 | Median Reward: 38.76 | Max Reward: 48.86
Iteration: 3957 | Episodes: 321000 | Median Reward: 37.59 | Max Reward: 48.86
Iteration: 3958 | Episodes: 321100 | Median Reward: 38.54 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -65.9        |
| time/                   |              |
|    fps                  | 293          |
|    iterations           | 3960         |
|    time_elapsed         | 110664       |
|    total_timesteps      | 32440320     |
| train/                  |              |
|    approx_kl            | 9.152778e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -154         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.51        |
|    n_updates            | 39590        |
|    policy_gradient_loss | -0.000578    |
|    std                  | 21.9         |
|    value_loss           | 0.766        |
------------------------------------------
Iteration: 3960 | Episodes: 321200 | Median Reward: 34.53 | Max Reward: 48.86
Iteration: 3961 | Episodes: 321300 | Median Reward: 42.73 | Max Reward: 48.86
Iteration: 3962 | Episodes: 321400 | Median Reward: 31.48 | Max Reward: 48.86
Iteration: 3963 | Episodes: 321500 | Median Reward: 35.70 | Max Reward: 48.86
Iteration: 3965 | Episodes: 321600 | Median Reward: 32.19 | Max Reward: 48.86
Iteration: 3966 | Episodes: 321700 | Median Reward: 34.86 | Max Reward: 48.86
Iteration: 3967 | Episodes: 321800 | Median Reward: 28.08 | Max Reward: 48.86
Iteration: 3968 | Episodes: 321900 | Median Reward: 30.05 | Max Reward: 48.86
Iteration: 3969 | Episodes: 322000 | Median Reward: 33.61 | Max Reward: 48.86
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | -67.7        |
| time/                   |              |
|    fps                  | 293          |
|    iterations           | 3970         |
|    time_elapsed         | 110752       |
|    total_timesteps      | 32522240     |
| train/                  |              |
|    approx_kl            | 0.0026290673 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -154         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.000707     |
|    loss                 | -7.16        |
|    n_updates            | 39690        |
|    policy_gradient_loss | -0.00698     |
|    std                  | 22           |
|    value_loss           | 1.15         |
------------------------------------------
Iteration: 3971 | Episodes: 322100 | Median Reward: 32.35 | Max Reward: 48.86
Iteration: 3972 | Episodes: 322200 | Median Reward: 30.46 | Max Reward: 48.86
Iteration: 3973 | Episodes: 322300 | Median Reward: 30.38 | Max Reward: 48.86
Iteration: 3974 | Episodes: 322400 | Median Reward: 28.91 | Max Reward: 48.86
Iteration: 3976 | Episodes: 322500 | Median Reward: 34.53 | Max Reward: 48.86
Iteration: 3977 | Episodes: 322600 | Median Reward: 37.90 | Max Reward: 48.86
Iteration: 3978 | Episodes: 322700 | Median Reward: 37.19 | Max Reward: 48.86
Iteration: 3979 | Episodes: 322800 | Median Reward: 40.71 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -59.5         |
| time/                   |               |
|    fps                  | 294           |
|    iterations           | 3980          |
|    time_elapsed         | 110840        |
|    total_timesteps      | 32604160      |
| train/                  |               |
|    approx_kl            | 3.4560217e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 0.999         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.5          |
|    n_updates            | 39790         |
|    policy_gradient_loss | -7.01e-05     |
|    std                  | 22.2          |
|    value_loss           | 0.391         |
-------------------------------------------
Iteration: 3981 | Episodes: 322900 | Median Reward: 40.71 | Max Reward: 48.86
Iteration: 3982 | Episodes: 323000 | Median Reward: 34.08 | Max Reward: 48.86
Iteration: 3983 | Episodes: 323100 | Median Reward: 36.05 | Max Reward: 48.86
Iteration: 3984 | Episodes: 323200 | Median Reward: 40.10 | Max Reward: 48.86
Iteration: 3985 | Episodes: 323300 | Median Reward: 31.18 | Max Reward: 48.86
Iteration: 3987 | Episodes: 323400 | Median Reward: 33.16 | Max Reward: 48.86
Iteration: 3988 | Episodes: 323500 | Median Reward: 34.48 | Max Reward: 48.86
Iteration: 3989 | Episodes: 323600 | Median Reward: 37.98 | Max Reward: 48.86
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 101           |
|    ep_rew_mean          | -63.8         |
| time/                   |               |
|    fps                  | 294           |
|    iterations           | 3990          |
|    time_elapsed         | 110928        |
|    total_timesteps      | 32686080      |
| train/                  |               |
|    approx_kl            | 5.8722653e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -155          |
|    explained_variance   | 0.998         |
|    learning_rate        | 0.000707      |
|    loss                 | -7.4          |
|    n_updates            | 39890         |
|    policy_gradient_loss | -0.000226     |
|    std                  | 22.4          |
|    value_loss           | 0.85          |
-------------------------------------------
Iteration: 3990 | Episodes: 323700 | Median Reward: 36.01 | Max Reward: 48.86
Iteration: 3992 | Episodes: 323800 | Median Reward: 36.32 | Max Reward: 48.86
Iteration: 3993 | Episodes: 323900 | Median Reward: 39.39 | Max Reward: 48.86
Iteration: 3994 | Episodes: 324000 | Median Reward: 36.17 | Max Reward: 48.86
Iteration: 3995 | Episodes: 324100 | Median Reward: 34.09 | Max Reward: 48.86
Iteration: 3997 | Episodes: 324200 | Median Reward: 29.50 | Max Reward: 48.86
Iteration: 3998 | Episodes: 324300 | Median Reward: 30.75 | Max Reward: 48.86
Iteration: 3999 | Episodes: 324400 | Median Reward: 31.37 | Max Reward: 48.86
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | -61.3       |
| time/                   |             |
|    fps                  | 295         |
|    iterations           | 4000        |
|    time_elapsed         | 111015      |
|    total_timesteps      | 32768000    |
| train/                  |             |
|    approx_kl            | 0.007002076 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -155        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.000707    |
|    loss                 | -7.71       |
|    n_updates            | 39990       |
|    policy_gradient_loss | -0.0258     |
|    std                  | 22.5        |
|    value_loss           | 0.682       |
-----------------------------------------
Training End | Episodes: 324434 | Median Reward: 40.71 | Max Reward: 48.86
Plot saved as fig_code1_4.png
(mujoco_test) [4mdeepbull5[24m:[1m~/Bot_hand/codes[0m> asc[K[K[Kext[Kit()
exit

Script done on 2024-10-24 05:52:31-04:00 [COMMAND_EXIT_CODE="0"]
